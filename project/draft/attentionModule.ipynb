{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: 0.24.2\n",
      "pandas: 1.1.5\n",
      "tensorflow: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", palette=[sns.color_palette('muted')[i] for i in [0,2]], \n",
    "        color_codes=True, context=\"talk\")\n",
    "from IPython import display\n",
    "from scipy.interpolate import make_interp_spline\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight  # Estimate class weights for unbalanced datasets\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "create_gif = False\n",
    "\n",
    "print(f\"sklearn: {sk.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"tensorflow: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mut_info(input_data, sensitive_feature, feature_class):\n",
    "    data = input_data[(input_data[sensitive_feature]==feature_class)]\n",
    "    sf_list = ['race', 'sex']\n",
    "    sensitive_data = data.loc[:, sf_list]\n",
    "    sf_list.remove(sensitive_feature)\n",
    "    sensitive_data = sensitive_data.drop(columns=sf_list)\n",
    "    data = data.drop(columns=['target', 'race', 'sex'])\n",
    "    mut_data = mutual_info_classif(data, sensitive_data[sensitive_feature])\n",
    "    return mut_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentionModule(input_data, threshold):\n",
    "    input_data = input_data.dropna()\n",
    "    # encoding the features that has class values\n",
    "    lb_make = LabelEncoder()\n",
    "    input_data['workclass'] = lb_make.fit_transform(input_data['workclass'])\n",
    "    input_data['education'] = lb_make.fit_transform(input_data['workclass'])\n",
    "    input_data['marital_status'] = lb_make.fit_transform(input_data['marital_status'])\n",
    "    input_data['occupation'] = lb_make.fit_transform(input_data['occupation'])\n",
    "    input_data['relationship'] = lb_make.fit_transform(input_data['relationship'])\n",
    "    input_data['country'] = lb_make.fit_transform(input_data['country'])\n",
    "    input_data['target'] = lb_make.fit_transform(input_data['target'])\n",
    "\n",
    "    # mutual information calculate\n",
    "    mut_race_white = calculate_mut_info(input_data, 'race', 'White')\n",
    "    mut_race_black = calculate_mut_info(input_data, 'race', 'Black')\n",
    "    mut_sex_male = calculate_mut_info(input_data, 'sex', 'Male')\n",
    "    mut_sex_female = calculate_mut_info(input_data, 'sex', 'Female')\n",
    "\n",
    "    # Feature selection based on threshold\n",
    "    mut_race = mut_race_white * mut_race_black\n",
    "    mut_sex = mut_sex_male * mut_sex_female\n",
    "    mut = mut_race + mut_sex\n",
    "    mut_percent = (mut/np.sum(mut))*100\n",
    "    remove_features = []\n",
    "    features_name = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', \n",
    "                'marital_status', 'occupation', 'relationship','capital_gain',\n",
    "                'capital_loss', 'hours_per_week', 'country']\n",
    "\n",
    "    for i in range(len(features_name)):\n",
    "        if mut_percent[i] < threshold:\n",
    "            remove_features.append(features_name[i])\n",
    "\n",
    "    return remove_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention maually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28750, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>39</td>\n",
       "      <td>50</td>\n",
       "      <td>38</td>\n",
       "      <td>53</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "      <td>31</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workclass</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fnlwgt</th>\n",
       "      <td>77516</td>\n",
       "      <td>83311</td>\n",
       "      <td>215646</td>\n",
       "      <td>234721</td>\n",
       "      <td>338409</td>\n",
       "      <td>284582</td>\n",
       "      <td>160187</td>\n",
       "      <td>209642</td>\n",
       "      <td>45781</td>\n",
       "      <td>159449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_num</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital_status</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occupation</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relationship</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black</td>\n",
       "      <td>White</td>\n",
       "      <td>Black</td>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_gain</th>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14084</td>\n",
       "      <td>5178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_loss</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours_per_week</th>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>45</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0      1       2       3       4       5       6       7  \\\n",
       "age                39     50      38      53      28      37      49      52   \n",
       "workclass           5      4       2       2       2       2       2       4   \n",
       "fnlwgt          77516  83311  215646  234721  338409  284582  160187  209642   \n",
       "education           5      4       2       2       2       2       2       4   \n",
       "education_num      13     13       9       7      13      14       5       9   \n",
       "marital_status      4      2       0       2       2       2       3       2   \n",
       "occupation          0      3       5       5       9       3       7       3   \n",
       "relationship        1      0       1       0       5       5       1       0   \n",
       "race            White  White   White   Black   Black   White   Black   White   \n",
       "sex              Male   Male    Male    Male  Female  Female  Female    Male   \n",
       "capital_gain     2174      0       0       0       0       0       0       0   \n",
       "capital_loss        0      0       0       0       0       0       0       0   \n",
       "hours_per_week     40     13      40      40      40      40      16      45   \n",
       "country            36     36      36      36       3      36      21      36   \n",
       "target              0      0       0       0       0       0       0       1   \n",
       "\n",
       "                     8       9  \n",
       "age                 31      42  \n",
       "workclass            2       2  \n",
       "fnlwgt           45781  159449  \n",
       "education            2       2  \n",
       "education_num       14      13  \n",
       "marital_status       4       2  \n",
       "occupation           9       3  \n",
       "relationship         1       0  \n",
       "race             White   White  \n",
       "sex             Female    Male  \n",
       "capital_gain     14084    5178  \n",
       "capital_loss         0       0  \n",
       "hours_per_week      50      40  \n",
       "country             36      36  \n",
       "target               1       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', \n",
    "                'marital_status', 'occupation', 'relationship', 'race', 'sex', \n",
    "                'capital_gain', 'capital_loss', 'hours_per_week', 'country', 'target']\n",
    "\n",
    "input_data = (pd.read_csv('E:/canada syntex/Github/fair_classifier_ml/data/adult.data', names=column_names, \n",
    "                              na_values=\"?\", sep=r'\\s*,\\s*', engine='python') # here seperator -- 0 or more whitespace then , then 0 or more whitespace --\n",
    "                              .loc[lambda df: df['race'].isin(['White', 'Black'])])\n",
    "\n",
    "input_data = input_data.dropna()\n",
    "\n",
    "# labeling data\n",
    "lb_make = LabelEncoder()\n",
    "input_data['workclass'] = lb_make.fit_transform(input_data['workclass'])\n",
    "input_data['education'] = lb_make.fit_transform(input_data['workclass'])\n",
    "input_data['marital_status'] = lb_make.fit_transform(input_data['marital_status'])\n",
    "input_data['occupation'] = lb_make.fit_transform(input_data['occupation'])\n",
    "input_data['relationship'] = lb_make.fit_transform(input_data['relationship'])\n",
    "input_data['country'] = lb_make.fit_transform(input_data['country'])\n",
    "input_data['target'] = lb_make.fit_transform(input_data['target'])\n",
    "\n",
    "print(input_data.shape)\n",
    "input_data.head(10).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating dataset for white race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25933, 12)\n",
      "(25933, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>39</td>\n",
       "      <td>50</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workclass</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fnlwgt</th>\n",
       "      <td>77516</td>\n",
       "      <td>83311</td>\n",
       "      <td>215646</td>\n",
       "      <td>284582</td>\n",
       "      <td>209642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_num</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital_status</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occupation</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relationship</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_gain</th>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_loss</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours_per_week</th>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0      1       2       5       7\n",
       "age                39     50      38      37      52\n",
       "workclass           5      4       2       2       4\n",
       "fnlwgt          77516  83311  215646  284582  209642\n",
       "education           5      4       2       2       4\n",
       "education_num      13     13       9      14       9\n",
       "marital_status      4      2       0       2       2\n",
       "occupation          0      3       5       3       3\n",
       "relationship        1      0       1       5       0\n",
       "capital_gain     2174      0       0       0       0\n",
       "capital_loss        0      0       0       0       0\n",
       "hours_per_week     40     13      40      40      45\n",
       "country            36     36      36      36      36"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white_data = input_data[(input_data[\"race\"]==\"White\")]\n",
    "sensitive_data = white_data.loc[:, ['race', 'sex']].drop(columns=['sex'])\n",
    "# sensitive_data[sensitive_data[\"race\"]==\"White\"] = 1\n",
    "white_data = white_data.drop(columns=['target', 'race', 'sex'])\n",
    "\n",
    "print(white_data.shape)\n",
    "print(sensitive_data.shape)\n",
    "white_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[1.54243628e-04 3.04631165e-03 0.00000000e+00 3.23911618e-03\n",
      " 9.83303127e-04 2.35221532e-03 7.13376779e-04 5.01291790e-04\n",
      " 1.92804535e-05 5.78413604e-05 1.38819265e-03 6.72887826e-03]\n"
     ]
    }
   ],
   "source": [
    "mut_race_white = mutual_info_classif(white_data, sensitive_data['race'])\n",
    "print(type(mut_race_white))\n",
    "print(mut_race_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.15682721e-04 3.75968843e-03 0.00000000e+00 3.45120117e-03\n",
      " 1.09898585e-03 2.00516716e-03 7.51937686e-04 9.25461767e-04\n",
      " 3.85609070e-05 0.00000000e+00 1.48459492e-03 6.78671962e-03]\n"
     ]
    }
   ],
   "source": [
    "def mut_data(input_data, sensitive_feature, feature_class):\n",
    "    data = input_data[(input_data[sensitive_feature]==feature_class)]\n",
    "    sf_list = ['race', 'sex']\n",
    "    sensitive_data = data.loc[:, sf_list]\n",
    "    sf_list.remove(sensitive_feature)\n",
    "    sensitive_data = sensitive_data.drop(columns=sf_list)\n",
    "    data = data.drop(columns=['target', 'race', 'sex'])\n",
    "    mut = mutual_info_classif(data, sensitive_data[sensitive_feature])\n",
    "    return mut\n",
    "\n",
    "mut_race_white = mut_data(input_data, 'race', 'White')\n",
    "print(mut_race_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "sf_list = ['race', 'sex']\n",
    "newlist = sf_list.remove('race')\n",
    "print(newlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating dataset for black race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2817, 12)\n",
      "(2817, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>53</td>\n",
       "      <td>28</td>\n",
       "      <td>49</td>\n",
       "      <td>37</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workclass</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fnlwgt</th>\n",
       "      <td>234721</td>\n",
       "      <td>338409</td>\n",
       "      <td>160187</td>\n",
       "      <td>280464</td>\n",
       "      <td>205019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_num</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital_status</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occupation</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relationship</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_gain</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_loss</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours_per_week</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>80</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    3       4       6       10      13\n",
       "age                 53      28      49      37      32\n",
       "workclass            2       2       2       2       2\n",
       "fnlwgt          234721  338409  160187  280464  205019\n",
       "education            2       2       2       2       2\n",
       "education_num        7      13       5      10      12\n",
       "marital_status       2       2       3       2       4\n",
       "occupation           5       9       7       3      11\n",
       "relationship         0       5       1       0       1\n",
       "capital_gain         0       0       0       0       0\n",
       "capital_loss         0       0       0       0       0\n",
       "hours_per_week      40      40      16      80      50\n",
       "country             36       3      21      36      36"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_data = input_data[(input_data[\"race\"]==\"Black\")]\n",
    "sensitive_data = black_data.loc[:, ['race', 'sex']].drop(columns=['sex'])\n",
    "black_data = black_data.drop(columns=['target', 'race', 'sex'])\n",
    "\n",
    "print(black_data.shape)\n",
    "print(sensitive_data.shape)\n",
    "black_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.33066907e-16 3.54987575e-04 3.33066907e-16 5.32481363e-04\n",
      " 1.77493788e-04 1.77493788e-04 3.54987575e-04 3.33066907e-16\n",
      " 3.33066907e-16 3.33066907e-16 1.77493788e-04 7.09975151e-04]\n"
     ]
    }
   ],
   "source": [
    "mut_race_black = mutual_info_classif(black_data, sensitive_data['race'])\n",
    "print(mut_race_black)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19456, 12)\n",
      "(19456, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>39</td>\n",
       "      <td>50</td>\n",
       "      <td>38</td>\n",
       "      <td>53</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workclass</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fnlwgt</th>\n",
       "      <td>77516</td>\n",
       "      <td>83311</td>\n",
       "      <td>215646</td>\n",
       "      <td>234721</td>\n",
       "      <td>209642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_num</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital_status</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occupation</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relationship</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_gain</th>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_loss</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours_per_week</th>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0      1       2       3       7\n",
       "age                39     50      38      53      52\n",
       "workclass           5      4       2       2       4\n",
       "fnlwgt          77516  83311  215646  234721  209642\n",
       "education           5      4       2       2       4\n",
       "education_num      13     13       9       7       9\n",
       "marital_status      4      2       0       2       2\n",
       "occupation          0      3       5       5       3\n",
       "relationship        1      0       1       0       0\n",
       "capital_gain     2174      0       0       0       0\n",
       "capital_loss        0      0       0       0       0\n",
       "hours_per_week     40     13      40      40      45\n",
       "country            36     36      36      36      36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_data = input_data[(input_data[\"sex\"]==\"Male\")]\n",
    "sensitive_data = male_data.loc[:, ['race', 'sex']].drop(columns=['race'])\n",
    "male_data = male_data.drop(columns=['target', 'race', 'sex'])\n",
    "\n",
    "print(male_data.shape)\n",
    "print(sensitive_data.shape)\n",
    "male_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.13980263e-05 1.97882401e-03 1.22124533e-15 2.28721217e-03\n",
      " 9.76562500e-04 2.67269737e-03 3.85485197e-04 3.85485197e-04\n",
      " 7.70970395e-05 1.22124533e-15 1.43914474e-03 4.98560855e-03]\n"
     ]
    }
   ],
   "source": [
    "mut_sex_male = mutual_info_classif(male_data, sensitive_data['sex'])\n",
    "print(mut_sex_male)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9294, 12)\n",
      "(9294, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>8</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>49</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workclass</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fnlwgt</th>\n",
       "      <td>338409</td>\n",
       "      <td>284582</td>\n",
       "      <td>160187</td>\n",
       "      <td>45781</td>\n",
       "      <td>122272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_num</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital_status</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occupation</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relationship</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_gain</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14084</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_loss</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours_per_week</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    4       5       6      8       12\n",
       "age                 28      37      49     31      23\n",
       "workclass            2       2       2      2       2\n",
       "fnlwgt          338409  284582  160187  45781  122272\n",
       "education            2       2       2      2       2\n",
       "education_num       13      14       5     14      13\n",
       "marital_status       2       2       3      4       4\n",
       "occupation           9       3       7      9       0\n",
       "relationship         5       5       1      1       3\n",
       "capital_gain         0       0       0  14084       0\n",
       "capital_loss         0       0       0      0       0\n",
       "hours_per_week      40      40      16     50      30\n",
       "country              3      36      21     36      36"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female_data = input_data[(input_data[\"sex\"]==\"Female\")]\n",
    "sensitive_data = female_data.loc[:, ['race', 'sex']].drop(columns=['race'])\n",
    "female_data = female_data.drop(columns=['target', 'race', 'sex'])\n",
    "\n",
    "print(female_data.shape)\n",
    "print(sensitive_data.shape)\n",
    "female_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.00096837 0.         0.00118356 0.00080697 0.00043039\n",
      " 0.00037659 0.00026899 0.         0.         0.00043039 0.00252851]\n"
     ]
    }
   ],
   "source": [
    "mut_sex_female = mutual_info_classif(female_data, sensitive_data['sex'])\n",
    "print(mut_sex_female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.13734481e-20 2.99763004e-06 0.00000000e+00 4.43182020e-06\n",
      " 9.62589025e-07 1.56779298e-06 3.98408625e-07 1.03691951e-07\n",
      " 6.42168101e-21 1.92650430e-20 8.65782159e-07 1.73835125e-05]\n"
     ]
    }
   ],
   "source": [
    "mut_race = mut_race_white * mut_race_black\n",
    "mut_sex = mut_sex_male * mut_sex_female\n",
    "mut = mut_race + mut_sex\n",
    "print(mut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.78931563e-13 1.04406196e+01 0.00000000e+00 1.54358437e+01\n",
      " 3.35265716e+00 5.46055715e+00 1.38764051e+00 3.61154713e-01\n",
      " 2.23664454e-14 6.70993361e-14 3.01548291e+00 6.05460442e+01]\n"
     ]
    }
   ],
   "source": [
    "mut_percent = (mut/np.sum(mut))*100\n",
    "print(mut_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7893156301064985e-13\n",
      "10.440619594048515\n",
      "0.0\n",
      "15.435843718120537\n",
      "3.352657162812929\n",
      "5.460557148328273\n",
      "1.3876405136102177\n",
      "0.3611547125342888\n",
      "2.2366445375944854e-14\n",
      "6.709933612873611e-14\n",
      "3.015482914711145\n",
      "60.54604423583383\n"
     ]
    }
   ],
   "source": [
    "for i in mut_percent:\n",
    "    print(i)\n",
    "# [0.1, 0.5, 1.5, 3.1, 4.5, 5.5, 10.5]\n",
    "# [  4,   1,   1,   1,   1,   1, 7]\n",
    "\n",
    "# 0.0\n",
    "# 2.2366445375944854e-14\n",
    "# 6.709933612873611e-14\n",
    "# 1.7893156301064985e-13\n",
    "\n",
    "# 0.3611547125342888\n",
    "\n",
    "# 1.3876405136102177\n",
    "\n",
    "# 3.015482914711145\n",
    "# 3.352657162812929\n",
    "\n",
    "# 5.460557148328273\n",
    "\n",
    "# 10.440619594048515\n",
    "# 15.435843718120537\n",
    "# 60.54604423583383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features = []\n",
    "features_name = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', \n",
    "            'marital_status', 'occupation', 'relationship','capital_gain',\n",
    "            'capital_loss', 'hours_per_week', 'country']\n",
    "\n",
    "for i in range(len(features_name)):\n",
    "    if mut_percent[i] < 0.1:\n",
    "        remove_features.append(features_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'fnlwgt', 'capital_gain', 'capital_loss']\n"
     ]
    }
   ],
   "source": [
    "print(remove_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adverserial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ICU_data(path, threshold=0):\n",
    "    column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', \n",
    "                    'marital_status', 'occupation', 'relationship', 'race', 'sex', \n",
    "                    'capital_gain', 'capital_loss', 'hours_per_week', 'country', 'target']\n",
    "    input_data = (pd.read_csv(path, names=column_names, \n",
    "                              na_values=\"?\", sep=r'\\s*,\\s*', engine='python') # here seperator -- 0 or more whitespace then , then 0 or more whitespace --\n",
    "                              .loc[lambda df: df['race'].isin(['White', 'Black'])])\n",
    "\n",
    "    remove_features = attentionModule(input_data, threshold)\n",
    "    # sensitive attributes; we identify 'race' and 'sex' as sensitive attributes\n",
    "    sensitive_attribs = ['race', 'sex']\n",
    "    Z = (input_data.loc[:, sensitive_attribs]\n",
    "         .assign(race=lambda df: (df['race'] == 'White').astype(int),\n",
    "                 sex=lambda df: (df['sex'] == 'Male').astype(int)))\n",
    "\n",
    "    # targets; 1 when someone makes over 50k , otherwise 0\n",
    "    y = (input_data['target'] == '>50K').astype(int) # Cast a pandas object to a specified dtype dtype.\n",
    "\n",
    "    # features; note that the 'target' and sentive attribute columns are dropped\n",
    "    remove_features.extend(['target', 'race', 'sex'])\n",
    "    X = (input_data\n",
    "         .drop(columns=remove_features)\n",
    "         .fillna('Unknown')   # The fillna() function is used to fill NA/NaN values using the specified method\n",
    "         .pipe(pd.get_dummies, drop_first=True)) # Use .pipe when chaining together functions that expect Series, DataFrames or GroupBy objects.\n",
    "                                                 # pd.get_dummies=Convert categorical variable into dummy/indicator variables\n",
    "                                                 # drop_first: bool function(default False) Whether to get k-1 dummies out of k categorical levels by removing the first level.\n",
    "    print(f\"features X: {X.shape[0]} samples, {X.shape[1]} attributes\")\n",
    "    print(f\"targets y: {y.shape[0]} samples\")\n",
    "    print(f\"sensitives Z: {Z.shape[0]} samples, {Z.shape[1]} attributes\")\n",
    "    return X, y, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rkana\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\rkana\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\rkana\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\rkana\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\rkana\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\rkana\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\rkana\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features X: 30940 samples, 90 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "X, y, Z = load_ICU_data('E:/canada syntex/Github/fair_classifier_ml/data/adult.data', threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test set\n",
    "X_train, X_test, y_train, y_test, Z_train, Z_test = train_test_split(X, y, Z, test_size=0.2,\n",
    "                                                                     stratify=y, random_state=7)\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "scale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), columns=df.columns, index=df.index)\n",
    "X_train = X_train.pipe(scale_df, scaler) \n",
    "X_test = X_test.pipe(scale_df, scaler) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions(y, Z, iteration=None, val_metrics=None, p_rules=None, fm= None, fname=None):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "    legend={'race': ['black','white'],\n",
    "            'sex': ['female','male']}\n",
    "    for idx, attr in enumerate(Z.columns):\n",
    "        for attr_val in [0, 1]:\n",
    "            ax = sns.kdeplot(data=y[Z[attr] == attr_val],\n",
    "                             label='{}'.format(legend[attr][attr_val]), \n",
    "                             ax=axes[idx], fill=True)\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.set_ylim(0,7)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"sensitive attibute: {}\".format(attr))\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('prediction distribution')\n",
    "        ax.set_xlabel(r'$P({{income>50K}}|z_{{{}}})$'.format(attr))\n",
    "    if iteration:\n",
    "        fig.text(1.0, 0.9, f\"Training iteration #{iteration}\", fontsize='16')\n",
    "    if val_metrics is not None:\n",
    "        fig.text(1.0, 0.65, '\\n'.join([\"Prediction performance:\",\n",
    "                                       f\"- ROC AUC: {val_metrics['ROC AUC']:.2f}\",\n",
    "                                       f\"- Accuracy: {val_metrics['Accuracy']:.1f}\"\n",
    "                                       ]),\n",
    "                                       fontsize='16')\n",
    "    if p_rules is not None:\n",
    "        fig.text(1.0, 0.4, '\\n'.join([\"Satisfied p%-rules:\"] +\n",
    "                                     [f\"- {attr}: {p_rules[attr]:.0f}%-rule\" \n",
    "                                      for attr in p_rules.keys()]), \n",
    "                 fontsize='16')\n",
    "\n",
    "    if fm is not None:\n",
    "        fig.text(1.0, 0.2, '\\n'.join([\"FM:\"] +\n",
    "                                     [f\"- {attr}: {fm[attr]}\" \n",
    "                                      for attr in fm.keys()]), \n",
    "                 fontsize='16')\n",
    "    fig.tight_layout()\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname, bbox_inches='tight',  dpi = 800)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_rule(y_pred, z_values, threshold=0.5):\n",
    "    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
    "    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
    "    odds = y_z_1.mean() / y_z_0.mean()\n",
    "    return np.min([odds, 1/odds]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fair_metrics(y_val, y_pred):\n",
    "    \n",
    "    cm=confusion_matrix(y_val, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    N = TP+FP+FN+TN #Total population\n",
    "    # ACC = (TP+TN)/N #Accuracy\n",
    "    TPR = TP/(TP+FN) # True positive rate\n",
    "    FPR = FP/(FP+TN) # False positive rate\n",
    "    FNR = FN/(TP+FN) # False negative rate\n",
    "    PPP = (TP + FP)/N # % predicted as positive\n",
    "    return np.array([TPR, FPR, FNR, PPP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_metrics(y_val, y_pred, z_values):\n",
    "    \"\"\"Calculate fairness for subgroup of population\"\"\"\n",
    "    # print(type(y_val))\n",
    "    # print(type(y_pred))\n",
    "    # print(type(z_values))\n",
    "    y_val_0 = []\n",
    "    y_val_1 = []\n",
    "    y_pred_0 = []\n",
    "    y_pred_1 = []\n",
    "    for i  in range(len(z_values)):\n",
    "        if z_values[i] == 1:\n",
    "            y_val_1.append(y_val[i])\n",
    "            y_pred_1.append(y_pred[i])\n",
    "        else:\n",
    "            y_val_0.append(y_val[i])\n",
    "            y_pred_0.append(y_pred[i])\n",
    "\n",
    "    # For class 0\n",
    "    fm_0 = calculate_fair_metrics(y_val_0, y_pred_0)\n",
    "    # For class 1\n",
    "    fm_1 = calculate_fair_metrics(y_val_1, y_pred_1)\n",
    "\n",
    "    fm_ratio = fm_0 / fm_1\n",
    "    #print(fm_ratio)\n",
    "    # res = \"TPR:{:.2f} FPR:{:.2f} FNR:{:.2f} PPP:{:.2f}\".format(fm_ratio[0], fm_ratio[1], fm_ratio[2], fm_ratio[3])\n",
    "\n",
    "    # res ={'TPR': fm_ratio[0],\n",
    "    #       'FPR': fm_ratio[1],\n",
    "    #       'FNR': fm_ratio[2],\n",
    "    #       'PPP': fm_ratio[3],\n",
    "    #     }\n",
    "    return  fm_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FairClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairClassifier(object):\n",
    "    \n",
    "    def __init__(self, n_features, n_sensitive, lambdas):\n",
    "        self.lambdas = lambdas\n",
    "        self.n_features = n_features\n",
    "\n",
    "        clf_inputs = Input(shape=(n_features,))\n",
    "        adv_inputs = Input(shape=(1,))\n",
    "        \n",
    "        clf_net = self._create_clf_net(clf_inputs)\n",
    "        adv_net = self._create_adv_net(adv_inputs, n_sensitive)\n",
    "        #print(adv_net.summary())\n",
    "        self._trainable_clf_net = self._make_trainable(clf_net)\n",
    "        self._trainable_adv_net = self._make_trainable(adv_net)\n",
    "\n",
    "        # compile model. Three model compiletion: clf, clf_w_adv and adv\n",
    "        self._clf = self._compile_clf(clf_net)\n",
    "        self._clf_w_adv = self._compile_clf_w_adv(clf_inputs, clf_net, adv_net)\n",
    "        self._adv = self._compile_adv(clf_inputs, clf_net, adv_net, n_sensitive)\n",
    "        # print(self._adv.summary())\n",
    "\n",
    "        self._val_metrics = None\n",
    "        self._fairness_metrics = None\n",
    "        self.fm_metrics = None\n",
    "        # self.metrics_res = pd.DataFrame()\n",
    "\n",
    "        self.predict = self._clf.predict\n",
    "        \n",
    "    # def attention(input, target, sen_attr):\n",
    "        \n",
    "    #     return X, y, Z\n",
    "\n",
    "    # making all layer trainable\n",
    "    def _make_trainable(self, net):\n",
    "        def make_trainable(flag):\n",
    "            net.trainable = flag\n",
    "            for layer in net.layers:\n",
    "                layer.trainable = flag\n",
    "        return make_trainable\n",
    "\n",
    "    # construct model ----------------------------------------------------------\n",
    "\n",
    "    # clf net    input layer + 3 hidden layer + output layer\n",
    "    def _create_clf_net(self, inputs):\n",
    "        dense1 = Dense(32, activation='relu')(inputs)\n",
    "        dropout1 = Dropout(0.2)(dense1)\n",
    "        dense2 = Dense(32, activation='relu')(dropout1)\n",
    "        dropout2 = Dropout(0.2)(dense2)\n",
    "        dense3 = Dense(32, activation='relu')(dropout2)\n",
    "        dropout3 = Dropout(0.2)(dense3)\n",
    "        outputs = Dense(1, activation='sigmoid', name='y')(dropout3)\n",
    "        return Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    # adv net    input layer + 3 hidden layer + output layer * n_sensitive\n",
    "    def _create_adv_net(self, inputs, n_sensitive):\n",
    "        dense1 = Dense(32, activation='relu')(inputs)\n",
    "        dense2 = Dense(32, activation='relu')(dense1)\n",
    "        dense3 = Dense(32, activation='relu')(dense2)\n",
    "        outputs = [Dense(1, activation='sigmoid')(dense3) for _ in range(n_sensitive)]\n",
    "        return Model(inputs=[inputs], outputs=outputs)\n",
    "\n",
    "    # compile  -----------------------------------------------------------------\n",
    "\n",
    "    # compile clf\n",
    "    def _compile_clf(self, clf_net):\n",
    "        clf = clf_net\n",
    "        self._trainable_clf_net(True)\n",
    "        clf.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        return clf\n",
    "        \n",
    "    # compile clf_w_adv\n",
    "    def _compile_clf_w_adv(self, inputs, clf_net, adv_net):\n",
    "        clf_w_adv = Model(inputs=[inputs], outputs=[clf_net(inputs)]+adv_net(clf_net(inputs))) # outputs=[clf_net(inputs)]+adv_net(clf_net(inputs))\n",
    "        self._trainable_clf_net(True)\n",
    "        self._trainable_adv_net(False)\n",
    "        loss_weights = [1.]+[-lambda_param for lambda_param in self.lambdas] # loss [1.0, -5.0, -5.0]\n",
    "        clf_w_adv.compile(loss=['binary_crossentropy']*(len(loss_weights)), \n",
    "                          loss_weights=loss_weights,  # adding loss weight\n",
    "                          optimizer='adam')\n",
    "        return clf_w_adv\n",
    "\n",
    "    # compile adv\n",
    "    def _compile_adv(self, inputs, clf_net, adv_net, n_sensitive):\n",
    "        adv = Model(inputs=[inputs], outputs=adv_net(clf_net(inputs))) # outputs=adv_net(clf_net(inputs))\n",
    "        self._trainable_clf_net(False)\n",
    "        self._trainable_adv_net(True)\n",
    "        adv.compile(loss=['binary_crossentropy']*n_sensitive, loss_weights=self.lambdas,  # added loss weights and loss=['binary_crossentropy']*n_sensitive\n",
    "                    optimizer='adam')\n",
    "        return adv\n",
    "\n",
    "    # compute weights based on features\n",
    "    def _compute_class_weights(self, data_set, classes=[0, 1]):\n",
    "        class_weights = []\n",
    "        if len(data_set.shape) == 1: \n",
    "            # for single feature\n",
    "            balanced_weights = compute_class_weight('balanced', classes=classes, y=data_set)\n",
    "            class_weights.append(dict(zip(classes, balanced_weights)))\n",
    "        else:\n",
    "            # for multiple feature\n",
    "            n_attr =  data_set.shape[1]\n",
    "            for attr_idx in range(n_attr):\n",
    "                balanced_weights = compute_class_weight('balanced', classes=classes,\n",
    "                                                        y=np.array(data_set)[:,attr_idx])\n",
    "                class_weights.append(dict(zip(classes, balanced_weights)))\n",
    "        return class_weights          \n",
    "    \n",
    "    # compute weights based on targets\n",
    "    def _compute_target_class_weights(self, y, classes=[0, 1]):\n",
    "        balanced_weights =  compute_class_weight('balanced', classes=classes, y=y)\n",
    "        class_weights = {'y': dict(zip(classes, balanced_weights))}\n",
    "        return class_weights\n",
    "        \n",
    "    \n",
    "    def pretrain(self, x, y, z, epochs=10, verbose=0):\n",
    "        self._trainable_clf_net(True)\n",
    "        self._clf.fit(x.values, y.values, epochs=epochs, verbose=verbose) # training clf\n",
    "        self._trainable_clf_net(False)\n",
    "\n",
    "        self._trainable_adv_net(True)\n",
    "        class_weight_adv = self._compute_class_weights(z)\n",
    "        # passing z value instead of y and split the z into two\n",
    "        self._adv.fit(x.values, np.hsplit(z.values, z.shape[1]), class_weight=class_weight_adv, # Split an array into multiple sub-arrays horizontally (column-wise)\n",
    "                      epochs=epochs, verbose=verbose) # training adv\n",
    "        \n",
    "    \n",
    "    def fit(self, x, y, z, validation_data=None, T_iter=250, batch_size=128,\n",
    "            save_figs=False, verbose=0):\n",
    "        n_sensitive = z.shape[1]\n",
    "        if validation_data is not None:\n",
    "            x_val, y_val, z_val = validation_data\n",
    "        \n",
    "        class_weight_clf = [{0:1., 1:1.}]\n",
    "        class_weight_adv = self._compute_class_weights(z)\n",
    "        class_weight_clf_w_adv = class_weight_clf+class_weight_adv\n",
    "        self._val_metrics = pd.DataFrame()\n",
    "        self._fairness_metrics = pd.DataFrame()\n",
    "        self.fm_metrics = pd.DataFrame(columns=z_val.columns)  \n",
    "        \n",
    "        for idx in range(T_iter):\n",
    "            if validation_data is not None:\n",
    "                y_pred = pd.Series(self._clf.predict(x_val.values).ravel(), index=y_val.index)\n",
    "                self._val_metrics.loc[idx, 'ROC AUC'] = roc_auc_score(y_val, y_pred)\n",
    "                self._val_metrics.loc[idx, 'Accuracy'] = (accuracy_score(y_val, (y_pred>0.5))*100)\n",
    "                # self._val_metrics.loc[idx, 'EO'] = fairness_metrics(y_val.values, (y_pred>0.5).values, z_val[sensitive_attr].values)\n",
    "                \n",
    "                for sensitive_attr in z_val.columns:\n",
    "                    self._fairness_metrics.loc[idx, sensitive_attr] = p_rule(y_pred, z_val[sensitive_attr])\n",
    "\n",
    "                    self.fm_metrics[sensitive_attr] = self.fm_metrics[sensitive_attr].astype(object)    # by default pandas do not except object \n",
    "                    self.fm_metrics.loc[idx, sensitive_attr] = fairness_metrics(y_val.values, (y_pred>0.5).values, z_val[sensitive_attr].values)\n",
    "\n",
    "\n",
    "                # pathlib.Path(f'E:/canada syntex/Github/fair_classifier_ml/output/output_{self.n_features}').mkdir(parents=True, exist_ok=True)\n",
    "                # display.clear_output(wait=True)\n",
    "                # plot_distributions(y_pred, z_val, idx+1, self._val_metrics.loc[idx],\n",
    "                #                    self._fairness_metrics.loc[idx], \n",
    "                #                    self.fm_metrics.loc[idx],\n",
    "                #                    fname=f'E:/canada syntex/Github/fair_classifier_ml/output/output_{self.n_features}/{idx+1:08d}.jpg' if save_figs else None)\n",
    "                #                    # /home/mdsamiul/github_project/fair_classifier_ml/output\n",
    "                # plt.show(plt.gcf())\n",
    "            \n",
    "            \n",
    "            # train adverserial\n",
    "            self._trainable_clf_net(False)\n",
    "            self._trainable_adv_net(True)\n",
    "            self._adv.fit(x.values, np.hsplit(z.values, z.shape[1]), batch_size=batch_size, \n",
    "                          class_weight=class_weight_adv, epochs=1, verbose=verbose)\n",
    "            \n",
    "            # train clf_w_adv.fit \n",
    "            # !Changed this into several epochs on whole dataset instead of single random minibatch!\n",
    "            self._trainable_clf_net(True)\n",
    "            self._trainable_adv_net(False)\n",
    "            indices = np.random.permutation(len(x))[:batch_size]\n",
    "            self._clf_w_adv.fit(x.values, [y.values]+np.hsplit(z.values, z.shape[1]), batch_size=len(x), \n",
    "                                class_weight=class_weight_clf_w_adv, epochs=5, verbose=verbose)\n",
    "\n",
    "        # self.metrics_res.append(p_rule)\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24752, 90)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 0.4219\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 0.3800\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 0.3716\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 0.3655\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 0.3646\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 2s 66us/sample - loss: 6.4715 - model_5_loss: 0.6633 - model_5_1_loss: 0.6312\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.3880 - model_5_loss: 0.6562 - model_5_1_loss: 0.6216\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.3883 - model_5_loss: 0.6567 - model_5_1_loss: 0.6213\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.3833 - model_5_loss: 0.6554 - model_5_1_loss: 0.6214\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: 6.3954 - model_5_loss: 0.6575 - model_5_1_loss: 0.6216\n"
     ]
    }
   ],
   "source": [
    "# initialise FairClassifier\n",
    "clf = FairClassifier(n_features=X_train.shape[1], n_sensitive=Z_train.shape[1],\n",
    "                     lambdas=[5., 5.])\n",
    "\n",
    "# pre-train both adverserial and classifier networks\n",
    "clf.pretrain(X_train, y_train, Z_train, verbose=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9290 - model_5_loss: 0.6932 - model_5_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5064 - model_4_loss: 0.4221 - model_5_loss: 0.6930 - model_5_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5062 - model_4_loss: 0.4212 - model_5_loss: 0.6929 - model_5_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5057 - model_4_loss: 0.4211 - model_5_loss: 0.6929 - model_5_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5072 - model_4_loss: 0.4187 - model_5_loss: 0.6928 - model_5_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5087 - model_4_loss: 0.4175 - model_5_loss: 0.6928 - model_5_1_loss: 0.6924\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train, Z_train, \n",
    "        validation_data=(X_test, y_test, Z_test),\n",
    "        T_iter=50, save_figs=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Attention Module: 0.1\n",
      "features X: 30940 samples, 90 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.3893 - model_1_loss: 0.6560 - model_1_1_loss: 0.6222\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0278 - model_loss: 0.3550 - model_1_loss: 0.6553 - model_1_1_loss: 0.6212\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0513 - model_loss: 0.3571 - model_1_loss: 0.6570 - model_1_1_loss: 0.6247\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0577 - model_loss: 0.3570 - model_1_loss: 0.6573 - model_1_1_loss: 0.6256\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0816 - model_loss: 0.3575 - model_1_loss: 0.6577 - model_1_1_loss: 0.6301\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0905 - model_loss: 0.3582 - model_1_loss: 0.6582 - model_1_1_loss: 0.6315\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.4629 - model_1_loss: 0.6585 - model_1_1_loss: 0.6333\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.0965 - model_loss: 0.3582 - model_1_loss: 0.6583 - model_1_1_loss: 0.6326\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1098 - model_loss: 0.3604 - model_1_loss: 0.6588 - model_1_1_loss: 0.6353\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1303 - model_loss: 0.3589 - model_1_loss: 0.6599 - model_1_1_loss: 0.6379\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1453 - model_loss: 0.3614 - model_1_loss: 0.6612 - model_1_1_loss: 0.6401\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1560 - model_loss: 0.3626 - model_1_loss: 0.6608 - model_1_1_loss: 0.6429\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.5424 - model_1_loss: 0.6628 - model_1_1_loss: 0.6451\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1714 - model_loss: 0.3648 - model_1_loss: 0.6621 - model_1_1_loss: 0.6451\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1859 - model_loss: 0.3655 - model_1_loss: 0.6634 - model_1_1_loss: 0.6469\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2046 - model_loss: 0.3642 - model_1_loss: 0.6636 - model_1_1_loss: 0.6502\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2141 - model_loss: 0.3648 - model_1_loss: 0.6650 - model_1_1_loss: 0.6508\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2244 - model_loss: 0.3675 - model_1_loss: 0.6659 - model_1_1_loss: 0.6525\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.5931 - model_1_loss: 0.6646 - model_1_1_loss: 0.6542\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2332 - model_loss: 0.3696 - model_1_loss: 0.6661 - model_1_1_loss: 0.6545\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2401 - model_loss: 0.3700 - model_1_loss: 0.6667 - model_1_1_loss: 0.6554\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2495 - model_loss: 0.3722 - model_1_loss: 0.6667 - model_1_1_loss: 0.6576\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2756 - model_loss: 0.3729 - model_1_loss: 0.6691 - model_1_1_loss: 0.6606\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2907 - model_loss: 0.3732 - model_1_loss: 0.6691 - model_1_1_loss: 0.6637\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.6693 - model_1_loss: 0.6700 - model_1_1_loss: 0.6635\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2845 - model_loss: 0.3760 - model_1_loss: 0.6697 - model_1_1_loss: 0.6624\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2963 - model_loss: 0.3789 - model_1_loss: 0.6701 - model_1_1_loss: 0.6649\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3116 - model_loss: 0.3778 - model_1_loss: 0.6708 - model_1_1_loss: 0.6671\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3235 - model_loss: 0.3809 - model_1_loss: 0.6718 - model_1_1_loss: 0.6691\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3311 - model_loss: 0.3816 - model_1_loss: 0.6719 - model_1_1_loss: 0.6707\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.7182 - model_1_loss: 0.6733 - model_1_1_loss: 0.6701\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3410 - model_loss: 0.3829 - model_1_loss: 0.6734 - model_1_1_loss: 0.6714\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3520 - model_loss: 0.3858 - model_1_loss: 0.6745 - model_1_1_loss: 0.6730\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3547 - model_loss: 0.3883 - model_1_loss: 0.6749 - model_1_1_loss: 0.6737\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3672 - model_loss: 0.3921 - model_1_loss: 0.6759 - model_1_1_loss: 0.6760\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3803 - model_loss: 0.3913 - model_1_loss: 0.6767 - model_1_1_loss: 0.6776\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.7802 - model_1_loss: 0.6780 - model_1_1_loss: 0.6785\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3759 - model_loss: 0.3951 - model_1_loss: 0.6772 - model_1_1_loss: 0.6770\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3938 - model_loss: 0.3952 - model_1_loss: 0.6789 - model_1_1_loss: 0.6789\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3942 - model_loss: 0.3986 - model_1_loss: 0.6781 - model_1_1_loss: 0.6804\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4036 - model_loss: 0.4011 - model_1_loss: 0.6796 - model_1_1_loss: 0.6813\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4188 - model_loss: 0.4025 - model_1_loss: 0.6814 - model_1_1_loss: 0.6829\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.8249 - model_1_loss: 0.6809 - model_1_1_loss: 0.6840\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4202 - model_loss: 0.4056 - model_1_loss: 0.6824 - model_1_1_loss: 0.6828\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4271 - model_loss: 0.4082 - model_1_loss: 0.6827 - model_1_1_loss: 0.6844\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4329 - model_loss: 0.4102 - model_1_loss: 0.6832 - model_1_1_loss: 0.6854\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4409 - model_loss: 0.4130 - model_1_loss: 0.6849 - model_1_1_loss: 0.6859\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4433 - model_loss: 0.4160 - model_1_loss: 0.6849 - model_1_1_loss: 0.6870\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.8687 - model_1_loss: 0.6862 - model_1_1_loss: 0.6875\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4423 - model_loss: 0.4186 - model_1_loss: 0.6849 - model_1_1_loss: 0.6873\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4432 - model_loss: 0.4204 - model_1_loss: 0.6853 - model_1_1_loss: 0.6874\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4521 - model_loss: 0.4224 - model_1_loss: 0.6864 - model_1_1_loss: 0.6885\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4585 - model_loss: 0.4246 - model_1_loss: 0.6872 - model_1_1_loss: 0.6894\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4566 - model_loss: 0.4278 - model_1_loss: 0.6874 - model_1_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.8908 - model_1_loss: 0.6882 - model_1_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4609 - model_loss: 0.4290 - model_1_loss: 0.6883 - model_1_1_loss: 0.6897\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4664 - model_loss: 0.4299 - model_1_loss: 0.6890 - model_1_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4600 - model_loss: 0.4339 - model_1_loss: 0.6887 - model_1_1_loss: 0.6901\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4641 - model_loss: 0.4360 - model_1_loss: 0.6893 - model_1_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4673 - model_loss: 0.4369 - model_1_loss: 0.6898 - model_1_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9062 - model_1_loss: 0.6898 - model_1_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4656 - model_loss: 0.4370 - model_1_loss: 0.6899 - model_1_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4684 - model_loss: 0.4386 - model_1_loss: 0.6907 - model_1_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4673 - model_loss: 0.4411 - model_1_loss: 0.6905 - model_1_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4667 - model_loss: 0.4437 - model_1_loss: 0.6910 - model_1_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4703 - model_loss: 0.4432 - model_1_loss: 0.6914 - model_1_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9163 - model_1_loss: 0.6911 - model_1_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4693 - model_loss: 0.4448 - model_1_loss: 0.6916 - model_1_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4714 - model_loss: 0.4440 - model_1_loss: 0.6915 - model_1_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4722 - model_loss: 0.4441 - model_1_loss: 0.6916 - model_1_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4756 - model_loss: 0.4444 - model_1_loss: 0.6919 - model_1_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4776 - model_loss: 0.4430 - model_1_loss: 0.6920 - model_1_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9239 - model_1_loss: 0.6921 - model_1_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4727 - model_loss: 0.4467 - model_1_loss: 0.6917 - model_1_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4773 - model_loss: 0.4434 - model_1_loss: 0.6919 - model_1_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4784 - model_loss: 0.4442 - model_1_loss: 0.6919 - model_1_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4815 - model_loss: 0.4431 - model_1_loss: 0.6925 - model_1_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4825 - model_loss: 0.4418 - model_1_loss: 0.6922 - model_1_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9245 - model_1_loss: 0.6914 - model_1_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4820 - model_loss: 0.4409 - model_1_loss: 0.6920 - model_1_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4819 - model_loss: 0.4416 - model_1_loss: 0.6921 - model_1_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4846 - model_loss: 0.4407 - model_1_loss: 0.6921 - model_1_1_loss: 0.6929\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4840 - model_loss: 0.4400 - model_1_loss: 0.6919 - model_1_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4851 - model_loss: 0.4395 - model_1_loss: 0.6921 - model_1_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9233 - model_1_loss: 0.6920 - model_1_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4832 - model_loss: 0.4389 - model_1_loss: 0.6918 - model_1_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4829 - model_loss: 0.4385 - model_1_loss: 0.6917 - model_1_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4815 - model_loss: 0.4395 - model_1_loss: 0.6915 - model_1_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4839 - model_loss: 0.4369 - model_1_loss: 0.6914 - model_1_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4866 - model_loss: 0.4367 - model_1_loss: 0.6918 - model_1_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9199 - model_1_loss: 0.6923 - model_1_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4820 - model_loss: 0.4359 - model_1_loss: 0.6914 - model_1_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4834 - model_loss: 0.4343 - model_1_loss: 0.6914 - model_1_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4834 - model_loss: 0.4325 - model_1_loss: 0.6913 - model_1_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4830 - model_loss: 0.4320 - model_1_loss: 0.6913 - model_1_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4818 - model_loss: 0.4347 - model_1_loss: 0.6913 - model_1_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9161 - model_1_loss: 0.6914 - model_1_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4764 - model_loss: 0.4349 - model_1_loss: 0.6911 - model_1_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4782 - model_loss: 0.4334 - model_1_loss: 0.6911 - model_1_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4759 - model_loss: 0.4351 - model_1_loss: 0.6913 - model_1_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4792 - model_loss: 0.4329 - model_1_loss: 0.6911 - model_1_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4812 - model_loss: 0.4316 - model_1_loss: 0.6914 - model_1_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9129 - model_1_loss: 0.6915 - model_1_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4764 - model_loss: 0.4308 - model_1_loss: 0.6909 - model_1_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4783 - model_loss: 0.4312 - model_1_loss: 0.6913 - model_1_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4788 - model_loss: 0.4303 - model_1_loss: 0.6911 - model_1_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4804 - model_loss: 0.4311 - model_1_loss: 0.6912 - model_1_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4799 - model_loss: 0.4311 - model_1_loss: 0.6913 - model_1_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9109 - model_1_loss: 0.6909 - model_1_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4783 - model_loss: 0.4297 - model_1_loss: 0.6908 - model_1_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4786 - model_loss: 0.4292 - model_1_loss: 0.6908 - model_1_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4827 - model_loss: 0.4283 - model_1_loss: 0.6908 - model_1_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4790 - model_loss: 0.4313 - model_1_loss: 0.6909 - model_1_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4842 - model_loss: 0.4305 - model_1_loss: 0.6914 - model_1_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9129 - model_1_loss: 0.6913 - model_1_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4840 - model_loss: 0.4299 - model_1_loss: 0.6913 - model_1_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4832 - model_loss: 0.4306 - model_1_loss: 0.6912 - model_1_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4848 - model_loss: 0.4314 - model_1_loss: 0.6915 - model_1_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4877 - model_loss: 0.4297 - model_1_loss: 0.6915 - model_1_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4923 - model_loss: 0.4301 - model_1_loss: 0.6917 - model_1_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9205 - model_1_loss: 0.6918 - model_1_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4855 - model_loss: 0.4301 - model_1_loss: 0.6911 - model_1_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4863 - model_loss: 0.4312 - model_1_loss: 0.6912 - model_1_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4882 - model_loss: 0.4308 - model_1_loss: 0.6916 - model_1_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4900 - model_loss: 0.4311 - model_1_loss: 0.6918 - model_1_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4914 - model_loss: 0.4296 - model_1_loss: 0.6916 - model_1_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9202 - model_1_loss: 0.6918 - model_1_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4844 - model_loss: 0.4318 - model_1_loss: 0.6913 - model_1_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4884 - model_loss: 0.4292 - model_1_loss: 0.6914 - model_1_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4868 - model_loss: 0.4300 - model_1_loss: 0.6913 - model_1_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4882 - model_loss: 0.4294 - model_1_loss: 0.6916 - model_1_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4859 - model_loss: 0.4313 - model_1_loss: 0.6914 - model_1_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9168 - model_1_loss: 0.6908 - model_1_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4806 - model_loss: 0.4301 - model_1_loss: 0.6906 - model_1_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4853 - model_loss: 0.4277 - model_1_loss: 0.6910 - model_1_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4848 - model_loss: 0.4271 - model_1_loss: 0.6909 - model_1_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4836 - model_loss: 0.4276 - model_1_loss: 0.6907 - model_1_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4833 - model_loss: 0.4296 - model_1_loss: 0.6910 - model_1_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9128 - model_1_loss: 0.6907 - model_1_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4834 - model_loss: 0.4261 - model_1_loss: 0.6904 - model_1_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4868 - model_loss: 0.4247 - model_1_loss: 0.6905 - model_1_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4840 - model_loss: 0.4271 - model_1_loss: 0.6903 - model_1_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4876 - model_loss: 0.4261 - model_1_loss: 0.6909 - model_1_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4864 - model_loss: 0.4265 - model_1_loss: 0.6907 - model_1_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9124 - model_1_loss: 0.6908 - model_1_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4894 - model_loss: 0.4265 - model_1_loss: 0.6912 - model_1_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4911 - model_loss: 0.4259 - model_1_loss: 0.6913 - model_1_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4925 - model_loss: 0.4241 - model_1_loss: 0.6911 - model_1_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4936 - model_loss: 0.4232 - model_1_loss: 0.6910 - model_1_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4966 - model_loss: 0.4227 - model_1_loss: 0.6914 - model_1_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9182 - model_1_loss: 0.6915 - model_1_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4952 - model_loss: 0.4225 - model_1_loss: 0.6914 - model_1_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4959 - model_loss: 0.4222 - model_1_loss: 0.6912 - model_1_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4974 - model_loss: 0.4207 - model_1_loss: 0.6913 - model_1_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4971 - model_loss: 0.4221 - model_1_loss: 0.6913 - model_1_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4994 - model_loss: 0.4222 - model_1_loss: 0.6916 - model_1_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9238 - model_1_loss: 0.6918 - model_1_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4962 - model_loss: 0.4232 - model_1_loss: 0.6913 - model_1_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4985 - model_loss: 0.4224 - model_1_loss: 0.6918 - model_1_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4994 - model_loss: 0.4223 - model_1_loss: 0.6918 - model_1_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5002 - model_loss: 0.4220 - model_1_loss: 0.6918 - model_1_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5004 - model_loss: 0.4231 - model_1_loss: 0.6921 - model_1_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9230 - model_1_loss: 0.6921 - model_1_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4965 - model_loss: 0.4207 - model_1_loss: 0.6919 - model_1_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4931 - model_loss: 0.4219 - model_1_loss: 0.6915 - model_1_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4972 - model_loss: 0.4202 - model_1_loss: 0.6919 - model_1_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4964 - model_loss: 0.4220 - model_1_loss: 0.6920 - model_1_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4982 - model_loss: 0.4219 - model_1_loss: 0.6924 - model_1_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9190 - model_1_loss: 0.6924 - model_1_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4900 - model_loss: 0.4233 - model_1_loss: 0.6920 - model_1_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4922 - model_loss: 0.4230 - model_1_loss: 0.6922 - model_1_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4931 - model_loss: 0.4230 - model_1_loss: 0.6923 - model_1_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4927 - model_loss: 0.4225 - model_1_loss: 0.6919 - model_1_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4910 - model_loss: 0.4250 - model_1_loss: 0.6922 - model_1_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9152 - model_1_loss: 0.6918 - model_1_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4986 - model_loss: 0.4219 - model_1_loss: 0.6933 - model_1_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4945 - model_loss: 0.4249 - model_1_loss: 0.6933 - model_1_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4975 - model_loss: 0.4235 - model_1_loss: 0.6933 - model_1_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4969 - model_loss: 0.4248 - model_1_loss: 0.6931 - model_1_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5017 - model_loss: 0.4239 - model_1_loss: 0.6936 - model_1_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9226 - model_1_loss: 0.6937 - model_1_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4963 - model_loss: 0.4237 - model_1_loss: 0.6924 - model_1_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4994 - model_loss: 0.4221 - model_1_loss: 0.6925 - model_1_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4982 - model_loss: 0.4235 - model_1_loss: 0.6925 - model_1_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4975 - model_loss: 0.4247 - model_1_loss: 0.6926 - model_1_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5024 - model_loss: 0.4216 - model_1_loss: 0.6925 - model_1_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9240 - model_1_loss: 0.6926 - model_1_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4980 - model_loss: 0.4248 - model_1_loss: 0.6921 - model_1_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4991 - model_loss: 0.4239 - model_1_loss: 0.6922 - model_1_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4991 - model_loss: 0.4228 - model_1_loss: 0.6917 - model_1_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5000 - model_loss: 0.4219 - model_1_loss: 0.6919 - model_1_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5016 - model_loss: 0.4216 - model_1_loss: 0.6922 - model_1_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9214 - model_1_loss: 0.6912 - model_1_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4959 - model_loss: 0.4239 - model_1_loss: 0.6918 - model_1_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4979 - model_loss: 0.4236 - model_1_loss: 0.6921 - model_1_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5029 - model_loss: 0.4207 - model_1_loss: 0.6920 - model_1_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5010 - model_loss: 0.4230 - model_1_loss: 0.6922 - model_1_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4999 - model_loss: 0.4232 - model_1_loss: 0.6920 - model_1_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9172 - model_1_loss: 0.6918 - model_1_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4966 - model_loss: 0.4211 - model_1_loss: 0.6915 - model_1_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4952 - model_loss: 0.4247 - model_1_loss: 0.6918 - model_1_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4964 - model_loss: 0.4233 - model_1_loss: 0.6918 - model_1_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4978 - model_loss: 0.4230 - model_1_loss: 0.6917 - model_1_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4993 - model_loss: 0.4233 - model_1_loss: 0.6920 - model_1_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9264 - model_1_loss: 0.6926 - model_1_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5015 - model_loss: 0.4228 - model_1_loss: 0.6925 - model_1_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5051 - model_loss: 0.4189 - model_1_loss: 0.6923 - model_1_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5061 - model_loss: 0.4206 - model_1_loss: 0.6925 - model_1_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5062 - model_loss: 0.4203 - model_1_loss: 0.6925 - model_1_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5076 - model_loss: 0.4182 - model_1_loss: 0.6924 - model_1_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9271 - model_1_loss: 0.6929 - model_1_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5041 - model_loss: 0.4182 - model_1_loss: 0.6921 - model_1_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5070 - model_loss: 0.4163 - model_1_loss: 0.6923 - model_1_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5036 - model_loss: 0.4178 - model_1_loss: 0.6921 - model_1_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5064 - model_loss: 0.4154 - model_1_loss: 0.6922 - model_1_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5060 - model_loss: 0.4163 - model_1_loss: 0.6922 - model_1_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9208 - model_1_loss: 0.6922 - model_1_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5014 - model_loss: 0.4148 - model_1_loss: 0.6916 - model_1_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5022 - model_loss: 0.4136 - model_1_loss: 0.6917 - model_1_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4997 - model_loss: 0.4151 - model_1_loss: 0.6917 - model_1_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4994 - model_loss: 0.4137 - model_1_loss: 0.6914 - model_1_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5004 - model_loss: 0.4140 - model_1_loss: 0.6914 - model_1_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9112 - model_1_loss: 0.6917 - model_1_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4975 - model_loss: 0.4147 - model_1_loss: 0.6916 - model_1_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4963 - model_loss: 0.4159 - model_1_loss: 0.6914 - model_1_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4937 - model_loss: 0.4158 - model_1_loss: 0.6912 - model_1_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4972 - model_loss: 0.4173 - model_1_loss: 0.6917 - model_1_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4984 - model_loss: 0.4165 - model_1_loss: 0.6918 - model_1_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9159 - model_1_loss: 0.6925 - model_1_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4995 - model_loss: 0.4161 - model_1_loss: 0.6916 - model_1_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5028 - model_loss: 0.4176 - model_1_loss: 0.6927 - model_1_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5006 - model_loss: 0.4202 - model_1_loss: 0.6921 - model_1_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5064 - model_loss: 0.4177 - model_1_loss: 0.6925 - model_1_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5037 - model_loss: 0.4206 - model_1_loss: 0.6924 - model_1_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9271 - model_1_loss: 0.6924 - model_1_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5091 - model_loss: 0.4194 - model_1_loss: 0.6927 - model_1_1_loss: 0.6930\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5059 - model_loss: 0.4216 - model_1_loss: 0.6925 - model_1_1_loss: 0.6930\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5087 - model_loss: 0.4211 - model_1_loss: 0.6929 - model_1_1_loss: 0.6930\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5093 - model_loss: 0.4235 - model_1_loss: 0.6932 - model_1_1_loss: 0.6934\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5115 - model_loss: 0.4229 - model_1_loss: 0.6932 - model_1_1_loss: 0.6937\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9306 - model_1_loss: 0.6920 - model_1_1_loss: 0.6936\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5074 - model_loss: 0.4211 - model_1_loss: 0.6926 - model_1_1_loss: 0.6930\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5038 - model_loss: 0.4244 - model_1_loss: 0.6926 - model_1_1_loss: 0.6931\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5052 - model_loss: 0.4226 - model_1_loss: 0.6925 - model_1_1_loss: 0.6931\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5039 - model_loss: 0.4233 - model_1_loss: 0.6925 - model_1_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5049 - model_loss: 0.4228 - model_1_loss: 0.6925 - model_1_1_loss: 0.6930\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9254 - model_1_loss: 0.6928 - model_1_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4986 - model_loss: 0.4226 - model_1_loss: 0.6920 - model_1_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4987 - model_loss: 0.4229 - model_1_loss: 0.6920 - model_1_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4971 - model_loss: 0.4240 - model_1_loss: 0.6921 - model_1_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4993 - model_loss: 0.4210 - model_1_loss: 0.6919 - model_1_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4978 - model_loss: 0.4219 - model_1_loss: 0.6917 - model_1_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9225 - model_1_loss: 0.6925 - model_1_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4984 - model_loss: 0.4208 - model_1_loss: 0.6919 - model_1_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4993 - model_loss: 0.4196 - model_1_loss: 0.6918 - model_1_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5018 - model_loss: 0.4192 - model_1_loss: 0.6920 - model_1_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5027 - model_loss: 0.4176 - model_1_loss: 0.6918 - model_1_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5043 - model_loss: 0.4154 - model_1_loss: 0.6918 - model_1_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9201 - model_1_loss: 0.6921 - model_1_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5040 - model_loss: 0.4160 - model_1_loss: 0.6919 - model_1_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5040 - model_loss: 0.4148 - model_1_loss: 0.6917 - model_1_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5068 - model_loss: 0.4128 - model_1_loss: 0.6919 - model_1_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5107 - model_loss: 0.4116 - model_1_loss: 0.6923 - model_1_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5100 - model_loss: 0.4098 - model_1_loss: 0.6918 - model_1_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9176 - model_1_loss: 0.6922 - model_1_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5057 - model_loss: 0.4094 - model_1_loss: 0.6915 - model_1_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5040 - model_loss: 0.4116 - model_1_loss: 0.6917 - model_1_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5091 - model_loss: 0.4082 - model_1_loss: 0.6917 - model_1_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5051 - model_loss: 0.4099 - model_1_loss: 0.6916 - model_1_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5095 - model_loss: 0.4088 - model_1_loss: 0.6920 - model_1_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9133 - model_1_loss: 0.6913 - model_1_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5033 - model_loss: 0.4078 - model_1_loss: 0.6914 - model_1_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5027 - model_loss: 0.4083 - model_1_loss: 0.6913 - model_1_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5042 - model_loss: 0.4086 - model_1_loss: 0.6916 - model_1_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5055 - model_loss: 0.4087 - model_1_loss: 0.6916 - model_1_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5068 - model_loss: 0.4088 - model_1_loss: 0.6919 - model_1_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9169 - model_1_loss: 0.6921 - model_1_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5014 - model_loss: 0.4110 - model_1_loss: 0.6917 - model_1_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5017 - model_loss: 0.4118 - model_1_loss: 0.6916 - model_1_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5044 - model_loss: 0.4116 - model_1_loss: 0.6918 - model_1_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5023 - model_loss: 0.4138 - model_1_loss: 0.6917 - model_1_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5052 - model_loss: 0.4142 - model_1_loss: 0.6920 - model_1_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9252 - model_1_loss: 0.6924 - model_1_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5056 - model_loss: 0.4161 - model_1_loss: 0.6924 - model_1_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5072 - model_loss: 0.4157 - model_1_loss: 0.6924 - model_1_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5080 - model_loss: 0.4183 - model_1_loss: 0.6927 - model_1_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5128 - model_loss: 0.4179 - model_1_loss: 0.6930 - model_1_1_loss: 0.6932\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5101 - model_loss: 0.4191 - model_1_loss: 0.6927 - model_1_1_loss: 0.6931\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9341 - model_1_loss: 0.6947 - model_1_1_loss: 0.6932\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5106 - model_loss: 0.4207 - model_1_loss: 0.6931 - model_1_1_loss: 0.6931\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5118 - model_loss: 0.4211 - model_1_loss: 0.6932 - model_1_1_loss: 0.6934\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5122 - model_loss: 0.4217 - model_1_loss: 0.6934 - model_1_1_loss: 0.6934\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5134 - model_loss: 0.4229 - model_1_loss: 0.6936 - model_1_1_loss: 0.6937\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5148 - model_loss: 0.4202 - model_1_loss: 0.6933 - model_1_1_loss: 0.6937\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9338 - model_1_loss: 0.6933 - model_1_1_loss: 0.6934\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5087 - model_loss: 0.4216 - model_1_loss: 0.6931 - model_1_1_loss: 0.6930\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5113 - model_loss: 0.4192 - model_1_loss: 0.6930 - model_1_1_loss: 0.6931\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5084 - model_loss: 0.4210 - model_1_loss: 0.6929 - model_1_1_loss: 0.6930\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5065 - model_loss: 0.4226 - model_1_loss: 0.6928 - model_1_1_loss: 0.6930\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5103 - model_loss: 0.4187 - model_1_loss: 0.6929 - model_1_1_loss: 0.6929\n",
      "For Attention Module: 0.2\n",
      "features X: 30940 samples, 76 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.3725 - model_5_loss: 0.6595 - model_5_1_loss: 0.6147\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0042 - model_4_loss: 0.3710 - model_5_loss: 0.6599 - model_5_1_loss: 0.6151\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0170 - model_4_loss: 0.3690 - model_5_loss: 0.6600 - model_5_1_loss: 0.6172\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0223 - model_4_loss: 0.3696 - model_5_loss: 0.6593 - model_5_1_loss: 0.6191\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0406 - model_4_loss: 0.3709 - model_5_loss: 0.6615 - model_5_1_loss: 0.6208\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0561 - model_4_loss: 0.3702 - model_5_loss: 0.6613 - model_5_1_loss: 0.6239\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.4351 - model_5_loss: 0.6623 - model_5_1_loss: 0.6256\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0645 - model_4_loss: 0.3690 - model_5_loss: 0.6617 - model_5_1_loss: 0.6250\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0770 - model_4_loss: 0.3710 - model_5_loss: 0.6622 - model_5_1_loss: 0.6274\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0844 - model_4_loss: 0.3716 - model_5_loss: 0.6622 - model_5_1_loss: 0.6290\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1091 - model_4_loss: 0.3721 - model_5_loss: 0.6644 - model_5_1_loss: 0.6319\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1185 - model_4_loss: 0.3728 - model_5_loss: 0.6643 - model_5_1_loss: 0.6339\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.5022 - model_5_loss: 0.6654 - model_5_1_loss: 0.6354\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1290 - model_4_loss: 0.3730 - model_5_loss: 0.6652 - model_5_1_loss: 0.6352\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1354 - model_4_loss: 0.3737 - model_5_loss: 0.6653 - model_5_1_loss: 0.6365\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1436 - model_4_loss: 0.3755 - model_5_loss: 0.6651 - model_5_1_loss: 0.6387\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1559 - model_4_loss: 0.3756 - model_5_loss: 0.6655 - model_5_1_loss: 0.6408\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1832 - model_4_loss: 0.3760 - model_5_loss: 0.6675 - model_5_1_loss: 0.6443\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.5615 - model_5_loss: 0.6682 - model_5_1_loss: 0.6447\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1748 - model_4_loss: 0.3779 - model_5_loss: 0.6670 - model_5_1_loss: 0.6435\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1952 - model_4_loss: 0.3786 - model_5_loss: 0.6674 - model_5_1_loss: 0.6473\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2118 - model_4_loss: 0.3794 - model_5_loss: 0.6694 - model_5_1_loss: 0.6488\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2179 - model_4_loss: 0.3796 - model_5_loss: 0.6686 - model_5_1_loss: 0.6509\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2409 - model_4_loss: 0.3820 - model_5_loss: 0.6709 - model_5_1_loss: 0.6537\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.6307 - model_5_loss: 0.6731 - model_5_1_loss: 0.6536\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2421 - model_4_loss: 0.3839 - model_5_loss: 0.6710 - model_5_1_loss: 0.6542\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2497 - model_4_loss: 0.3839 - model_5_loss: 0.6712 - model_5_1_loss: 0.6555\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2602 - model_4_loss: 0.3873 - model_5_loss: 0.6723 - model_5_1_loss: 0.6572\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2627 - model_4_loss: 0.3860 - model_5_loss: 0.6710 - model_5_1_loss: 0.6588\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2796 - model_4_loss: 0.3893 - model_5_loss: 0.6732 - model_5_1_loss: 0.6606\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.6908 - model_5_loss: 0.6748 - model_5_1_loss: 0.6627\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2856 - model_4_loss: 0.3904 - model_5_loss: 0.6737 - model_5_1_loss: 0.6615\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2972 - model_4_loss: 0.3925 - model_5_loss: 0.6736 - model_5_1_loss: 0.6644\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3202 - model_4_loss: 0.3942 - model_5_loss: 0.6761 - model_5_1_loss: 0.6668\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3270 - model_4_loss: 0.3947 - model_5_loss: 0.6768 - model_5_1_loss: 0.6676\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3355 - model_4_loss: 0.3968 - model_5_loss: 0.6764 - model_5_1_loss: 0.6700\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.7456 - model_5_loss: 0.6781 - model_5_1_loss: 0.67120s - loss: 6.7573 - model_5_loss: 0.6793 - model_5_1_loss: 0.672\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3358 - model_4_loss: 0.4008 - model_5_loss: 0.6772 - model_5_1_loss: 0.6701\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3494 - model_4_loss: 0.4004 - model_5_loss: 0.6783 - model_5_1_loss: 0.6717\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3631 - model_4_loss: 0.4016 - model_5_loss: 0.6793 - model_5_1_loss: 0.6736\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3682 - model_4_loss: 0.4050 - model_5_loss: 0.6796 - model_5_1_loss: 0.6750\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3748 - model_4_loss: 0.4076 - model_5_loss: 0.6807 - model_5_1_loss: 0.6758\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.7953 - model_5_loss: 0.6814 - model_5_1_loss: 0.6776\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3806 - model_4_loss: 0.4086 - model_5_loss: 0.6807 - model_5_1_loss: 0.6771\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3849 - model_4_loss: 0.4092 - model_5_loss: 0.6809 - model_5_1_loss: 0.6779\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3948 - model_4_loss: 0.4116 - model_5_loss: 0.6823 - model_5_1_loss: 0.6789\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4076 - model_4_loss: 0.4149 - model_5_loss: 0.6840 - model_5_1_loss: 0.6805\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4179 - model_4_loss: 0.4167 - model_5_loss: 0.6836 - model_5_1_loss: 0.6833\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.8346 - model_5_loss: 0.6848 - model_5_1_loss: 0.6823\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4112 - model_4_loss: 0.4194 - model_5_loss: 0.6834 - model_5_1_loss: 0.6827\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4165 - model_4_loss: 0.4193 - model_5_loss: 0.6838 - model_5_1_loss: 0.6834\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4297 - model_4_loss: 0.4221 - model_5_loss: 0.6853 - model_5_1_loss: 0.6850\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4370 - model_4_loss: 0.4229 - model_5_loss: 0.6861 - model_5_1_loss: 0.6859\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4421 - model_4_loss: 0.4247 - model_5_loss: 0.6870 - model_5_1_loss: 0.6863\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.8730 - model_5_loss: 0.6882 - model_5_1_loss: 0.6868\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4419 - model_4_loss: 0.4292 - model_5_loss: 0.6871 - model_5_1_loss: 0.6871\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4483 - model_4_loss: 0.4290 - model_5_loss: 0.6874 - model_5_1_loss: 0.6880\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4529 - model_4_loss: 0.4324 - model_5_loss: 0.6883 - model_5_1_loss: 0.6887\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4605 - model_4_loss: 0.4316 - model_5_loss: 0.6888 - model_5_1_loss: 0.6896\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4635 - model_4_loss: 0.4353 - model_5_loss: 0.6892 - model_5_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.8962 - model_5_loss: 0.6899 - model_5_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4582 - model_4_loss: 0.4392 - model_5_loss: 0.6891 - model_5_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4580 - model_4_loss: 0.4402 - model_5_loss: 0.6894 - model_5_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4597 - model_4_loss: 0.4411 - model_5_loss: 0.6897 - model_5_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4617 - model_4_loss: 0.4435 - model_5_loss: 0.6902 - model_5_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4649 - model_4_loss: 0.4455 - model_5_loss: 0.6909 - model_5_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9140 - model_5_loss: 0.6909 - model_5_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4646 - model_4_loss: 0.4461 - model_5_loss: 0.6907 - model_5_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4618 - model_4_loss: 0.4490 - model_5_loss: 0.6906 - model_5_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4631 - model_4_loss: 0.4475 - model_5_loss: 0.6908 - model_5_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4646 - model_4_loss: 0.4504 - model_5_loss: 0.6914 - model_5_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4680 - model_4_loss: 0.4513 - model_5_loss: 0.6915 - model_5_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9190 - model_5_loss: 0.6926 - model_5_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4654 - model_4_loss: 0.4523 - model_5_loss: 0.6917 - model_5_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4644 - model_4_loss: 0.4510 - model_5_loss: 0.6914 - model_5_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4682 - model_4_loss: 0.4518 - model_5_loss: 0.6918 - model_5_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4671 - model_4_loss: 0.4529 - model_5_loss: 0.6918 - model_5_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4696 - model_4_loss: 0.4526 - model_5_loss: 0.6921 - model_5_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9220 - model_5_loss: 0.6915 - model_5_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4736 - model_4_loss: 0.4511 - model_5_loss: 0.6927 - model_5_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4730 - model_4_loss: 0.4514 - model_5_loss: 0.6927 - model_5_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4758 - model_4_loss: 0.4509 - model_5_loss: 0.6930 - model_5_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4745 - model_4_loss: 0.4528 - model_5_loss: 0.6927 - model_5_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4740 - model_4_loss: 0.4518 - model_5_loss: 0.6926 - model_5_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9245 - model_5_loss: 0.6929 - model_5_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4728 - model_4_loss: 0.4516 - model_5_loss: 0.6923 - model_5_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4745 - model_4_loss: 0.4511 - model_5_loss: 0.6923 - model_5_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4747 - model_4_loss: 0.4505 - model_5_loss: 0.6922 - model_5_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4747 - model_4_loss: 0.4493 - model_5_loss: 0.6920 - model_5_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4781 - model_4_loss: 0.4486 - model_5_loss: 0.6925 - model_5_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9261 - model_5_loss: 0.6928 - model_5_1_loss: 0.6930\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4739 - model_4_loss: 0.4472 - model_5_loss: 0.6918 - model_5_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4792 - model_4_loss: 0.4462 - model_5_loss: 0.6924 - model_5_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4768 - model_4_loss: 0.4455 - model_5_loss: 0.6919 - model_5_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4781 - model_4_loss: 0.4459 - model_5_loss: 0.6922 - model_5_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4791 - model_4_loss: 0.4443 - model_5_loss: 0.6922 - model_5_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9239 - model_5_loss: 0.6924 - model_5_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4769 - model_4_loss: 0.4446 - model_5_loss: 0.6922 - model_5_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4807 - model_4_loss: 0.4419 - model_5_loss: 0.6923 - model_5_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4812 - model_4_loss: 0.4427 - model_5_loss: 0.6922 - model_5_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4804 - model_4_loss: 0.4420 - model_5_loss: 0.6921 - model_5_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4796 - model_4_loss: 0.4421 - model_5_loss: 0.6922 - model_5_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9232 - model_5_loss: 0.6926 - model_5_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4799 - model_4_loss: 0.4386 - model_5_loss: 0.6918 - model_5_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4791 - model_4_loss: 0.4404 - model_5_loss: 0.6918 - model_5_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4789 - model_4_loss: 0.4414 - model_5_loss: 0.6922 - model_5_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4799 - model_4_loss: 0.4387 - model_5_loss: 0.6919 - model_5_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4820 - model_4_loss: 0.4371 - model_5_loss: 0.6922 - model_5_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9206 - model_5_loss: 0.6929 - model_5_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4814 - model_4_loss: 0.4364 - model_5_loss: 0.6920 - model_5_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4783 - model_4_loss: 0.4389 - model_5_loss: 0.6918 - model_5_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4759 - model_4_loss: 0.4383 - model_5_loss: 0.6917 - model_5_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4810 - model_4_loss: 0.4354 - model_5_loss: 0.6921 - model_5_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4780 - model_4_loss: 0.4376 - model_5_loss: 0.6918 - model_5_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9184 - model_5_loss: 0.6937 - model_5_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4811 - model_4_loss: 0.4362 - model_5_loss: 0.6921 - model_5_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4823 - model_4_loss: 0.4345 - model_5_loss: 0.6920 - model_5_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4831 - model_4_loss: 0.4359 - model_5_loss: 0.6922 - model_5_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4806 - model_4_loss: 0.4366 - model_5_loss: 0.6920 - model_5_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4822 - model_4_loss: 0.4354 - model_5_loss: 0.6920 - model_5_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9186 - model_5_loss: 0.6917 - model_5_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4851 - model_4_loss: 0.4350 - model_5_loss: 0.6921 - model_5_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4823 - model_4_loss: 0.4349 - model_5_loss: 0.6917 - model_5_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4850 - model_4_loss: 0.4359 - model_5_loss: 0.6921 - model_5_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4822 - model_4_loss: 0.4371 - model_5_loss: 0.6920 - model_5_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4846 - model_4_loss: 0.4366 - model_5_loss: 0.6921 - model_5_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9241 - model_5_loss: 0.6930 - model_5_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4816 - model_4_loss: 0.4375 - model_5_loss: 0.6920 - model_5_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4859 - model_4_loss: 0.4354 - model_5_loss: 0.6922 - model_5_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4847 - model_4_loss: 0.4372 - model_5_loss: 0.6924 - model_5_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4846 - model_4_loss: 0.4380 - model_5_loss: 0.6922 - model_5_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4840 - model_4_loss: 0.4361 - model_5_loss: 0.6920 - model_5_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9259 - model_5_loss: 0.6929 - model_5_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4854 - model_4_loss: 0.4364 - model_5_loss: 0.6921 - model_5_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4851 - model_4_loss: 0.4369 - model_5_loss: 0.6922 - model_5_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4881 - model_4_loss: 0.4357 - model_5_loss: 0.6924 - model_5_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4891 - model_4_loss: 0.4347 - model_5_loss: 0.6923 - model_5_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4866 - model_4_loss: 0.4377 - model_5_loss: 0.6924 - model_5_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9230 - model_5_loss: 0.6923 - model_5_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4881 - model_4_loss: 0.4352 - model_5_loss: 0.6923 - model_5_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4851 - model_4_loss: 0.4371 - model_5_loss: 0.6922 - model_5_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4881 - model_4_loss: 0.4348 - model_5_loss: 0.6924 - model_5_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4872 - model_4_loss: 0.4365 - model_5_loss: 0.6923 - model_5_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4848 - model_4_loss: 0.4372 - model_5_loss: 0.6922 - model_5_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9239 - model_5_loss: 0.6934 - model_5_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4837 - model_4_loss: 0.4373 - model_5_loss: 0.6923 - model_5_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4848 - model_4_loss: 0.4352 - model_5_loss: 0.6921 - model_5_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4859 - model_4_loss: 0.4352 - model_5_loss: 0.6921 - model_5_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4854 - model_4_loss: 0.4364 - model_5_loss: 0.6923 - model_5_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4848 - model_4_loss: 0.4362 - model_5_loss: 0.6922 - model_5_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9216 - model_5_loss: 0.6924 - model_5_1_loss: 0.69190s - loss: 6.9374 - model_5_loss: 0.6975 - model_5_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4840 - model_4_loss: 0.4348 - model_5_loss: 0.6919 - model_5_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4874 - model_4_loss: 0.4339 - model_5_loss: 0.6921 - model_5_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4850 - model_4_loss: 0.4339 - model_5_loss: 0.6921 - model_5_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4851 - model_4_loss: 0.4342 - model_5_loss: 0.6923 - model_5_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4850 - model_4_loss: 0.4335 - model_5_loss: 0.6918 - model_5_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9216 - model_5_loss: 0.6920 - model_5_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4854 - model_4_loss: 0.4337 - model_5_loss: 0.6921 - model_5_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4854 - model_4_loss: 0.4341 - model_5_loss: 0.6920 - model_5_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4888 - model_4_loss: 0.4322 - model_5_loss: 0.6924 - model_5_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4860 - model_4_loss: 0.4351 - model_5_loss: 0.6921 - model_5_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4847 - model_4_loss: 0.4365 - model_5_loss: 0.6923 - model_5_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9221 - model_5_loss: 0.6925 - model_5_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4861 - model_4_loss: 0.4347 - model_5_loss: 0.6922 - model_5_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4857 - model_4_loss: 0.4336 - model_5_loss: 0.6922 - model_5_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4867 - model_4_loss: 0.4351 - model_5_loss: 0.6924 - model_5_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4881 - model_4_loss: 0.4338 - model_5_loss: 0.6922 - model_5_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4859 - model_4_loss: 0.4353 - model_5_loss: 0.6922 - model_5_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9237 - model_5_loss: 0.6931 - model_5_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4884 - model_4_loss: 0.4333 - model_5_loss: 0.6923 - model_5_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4879 - model_4_loss: 0.4343 - model_5_loss: 0.6923 - model_5_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4886 - model_4_loss: 0.4338 - model_5_loss: 0.6922 - model_5_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4886 - model_4_loss: 0.4342 - model_5_loss: 0.6923 - model_5_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4900 - model_4_loss: 0.4324 - model_5_loss: 0.6925 - model_5_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9230 - model_5_loss: 0.6922 - model_5_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4928 - model_4_loss: 0.4325 - model_5_loss: 0.6926 - model_5_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4935 - model_4_loss: 0.4312 - model_5_loss: 0.6924 - model_5_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4950 - model_4_loss: 0.4302 - model_5_loss: 0.6925 - model_5_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4932 - model_4_loss: 0.4313 - model_5_loss: 0.6924 - model_5_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4939 - model_4_loss: 0.4301 - model_5_loss: 0.6924 - model_5_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9230 - model_5_loss: 0.6924 - model_5_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4885 - model_4_loss: 0.4306 - model_5_loss: 0.6921 - model_5_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4892 - model_4_loss: 0.4303 - model_5_loss: 0.6923 - model_5_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4897 - model_4_loss: 0.4296 - model_5_loss: 0.6924 - model_5_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4878 - model_4_loss: 0.4308 - model_5_loss: 0.6924 - model_5_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4881 - model_4_loss: 0.4297 - model_5_loss: 0.6923 - model_5_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9204 - model_5_loss: 0.6923 - model_5_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4884 - model_4_loss: 0.4305 - model_5_loss: 0.6924 - model_5_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4871 - model_4_loss: 0.4314 - model_5_loss: 0.6924 - model_5_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4918 - model_4_loss: 0.4288 - model_5_loss: 0.6925 - model_5_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4880 - model_4_loss: 0.4306 - model_5_loss: 0.6923 - model_5_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4889 - model_4_loss: 0.4316 - model_5_loss: 0.6925 - model_5_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9225 - model_5_loss: 0.6925 - model_5_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4862 - model_4_loss: 0.4311 - model_5_loss: 0.6925 - model_5_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4870 - model_4_loss: 0.4302 - model_5_loss: 0.6923 - model_5_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4883 - model_4_loss: 0.4313 - model_5_loss: 0.6924 - model_5_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4892 - model_4_loss: 0.4311 - model_5_loss: 0.6924 - model_5_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4865 - model_4_loss: 0.4341 - model_5_loss: 0.6924 - model_5_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9223 - model_5_loss: 0.6918 - model_5_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4892 - model_4_loss: 0.4321 - model_5_loss: 0.6924 - model_5_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4894 - model_4_loss: 0.4330 - model_5_loss: 0.6924 - model_5_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4908 - model_4_loss: 0.4307 - model_5_loss: 0.6923 - model_5_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4891 - model_4_loss: 0.4328 - model_5_loss: 0.6924 - model_5_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4922 - model_4_loss: 0.4304 - model_5_loss: 0.6924 - model_5_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9235 - model_5_loss: 0.6939 - model_5_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4906 - model_4_loss: 0.4302 - model_5_loss: 0.6922 - model_5_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4916 - model_4_loss: 0.4308 - model_5_loss: 0.6922 - model_5_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4905 - model_4_loss: 0.4310 - model_5_loss: 0.6923 - model_5_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4901 - model_4_loss: 0.4302 - model_5_loss: 0.6920 - model_5_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4929 - model_4_loss: 0.4294 - model_5_loss: 0.6922 - model_5_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9228 - model_5_loss: 0.6924 - model_5_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4911 - model_4_loss: 0.4286 - model_5_loss: 0.6922 - model_5_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4901 - model_4_loss: 0.4294 - model_5_loss: 0.6921 - model_5_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4919 - model_4_loss: 0.4289 - model_5_loss: 0.6922 - model_5_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4893 - model_4_loss: 0.4299 - model_5_loss: 0.6920 - model_5_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4894 - model_4_loss: 0.4294 - model_5_loss: 0.6920 - model_5_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9193 - model_5_loss: 0.6916 - model_5_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4871 - model_4_loss: 0.4296 - model_5_loss: 0.6919 - model_5_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4915 - model_4_loss: 0.4280 - model_5_loss: 0.6921 - model_5_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4897 - model_4_loss: 0.4293 - model_5_loss: 0.6921 - model_5_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4885 - model_4_loss: 0.4292 - model_5_loss: 0.6920 - model_5_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4910 - model_4_loss: 0.4278 - model_5_loss: 0.6922 - model_5_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9177 - model_5_loss: 0.6927 - model_5_1_loss: 0.69160s - loss: 6.9306 - model_5_loss: 0.6951 - model_5_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4909 - model_4_loss: 0.4284 - model_5_loss: 0.6921 - model_5_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4950 - model_4_loss: 0.4279 - model_5_loss: 0.6924 - model_5_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4872 - model_4_loss: 0.4307 - model_5_loss: 0.6918 - model_5_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4923 - model_4_loss: 0.4304 - model_5_loss: 0.6924 - model_5_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4878 - model_4_loss: 0.4332 - model_5_loss: 0.6919 - model_5_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9251 - model_5_loss: 0.6921 - model_5_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4940 - model_4_loss: 0.4293 - model_5_loss: 0.6924 - model_5_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4893 - model_4_loss: 0.4319 - model_5_loss: 0.6922 - model_5_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4906 - model_4_loss: 0.4320 - model_5_loss: 0.6923 - model_5_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4903 - model_4_loss: 0.4332 - model_5_loss: 0.6924 - model_5_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4888 - model_4_loss: 0.4337 - model_5_loss: 0.6924 - model_5_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9244 - model_5_loss: 0.6928 - model_5_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4872 - model_4_loss: 0.4347 - model_5_loss: 0.6923 - model_5_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4895 - model_4_loss: 0.4336 - model_5_loss: 0.6924 - model_5_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4908 - model_4_loss: 0.4324 - model_5_loss: 0.6926 - model_5_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4873 - model_4_loss: 0.4358 - model_5_loss: 0.6925 - model_5_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4904 - model_4_loss: 0.4328 - model_5_loss: 0.6926 - model_5_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9239 - model_5_loss: 0.6925 - model_5_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4891 - model_4_loss: 0.4339 - model_5_loss: 0.6928 - model_5_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4897 - model_4_loss: 0.4344 - model_5_loss: 0.6926 - model_5_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4895 - model_4_loss: 0.4336 - model_5_loss: 0.6926 - model_5_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4915 - model_4_loss: 0.4310 - model_5_loss: 0.6926 - model_5_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4909 - model_4_loss: 0.4315 - model_5_loss: 0.6925 - model_5_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9230 - model_5_loss: 0.6927 - model_5_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4902 - model_4_loss: 0.4307 - model_5_loss: 0.6925 - model_5_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4892 - model_4_loss: 0.4297 - model_5_loss: 0.6923 - model_5_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4884 - model_4_loss: 0.4303 - model_5_loss: 0.6922 - model_5_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4895 - model_4_loss: 0.4293 - model_5_loss: 0.6924 - model_5_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4906 - model_4_loss: 0.4281 - model_5_loss: 0.6923 - model_5_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9195 - model_5_loss: 0.6928 - model_5_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4881 - model_4_loss: 0.4287 - model_5_loss: 0.6922 - model_5_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4896 - model_4_loss: 0.4278 - model_5_loss: 0.6922 - model_5_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4893 - model_4_loss: 0.4266 - model_5_loss: 0.6918 - model_5_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4894 - model_4_loss: 0.4265 - model_5_loss: 0.6921 - model_5_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4908 - model_4_loss: 0.4265 - model_5_loss: 0.6922 - model_5_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9185 - model_5_loss: 0.6924 - model_5_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4916 - model_4_loss: 0.4262 - model_5_loss: 0.6922 - model_5_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4896 - model_4_loss: 0.4268 - model_5_loss: 0.6920 - model_5_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4926 - model_4_loss: 0.4260 - model_5_loss: 0.6922 - model_5_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4913 - model_4_loss: 0.4271 - model_5_loss: 0.6922 - model_5_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4951 - model_4_loss: 0.4251 - model_5_loss: 0.6921 - model_5_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9207 - model_5_loss: 0.6931 - model_5_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4932 - model_4_loss: 0.4265 - model_5_loss: 0.6921 - model_5_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4925 - model_4_loss: 0.4264 - model_5_loss: 0.6919 - model_5_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4911 - model_4_loss: 0.4292 - model_5_loss: 0.6920 - model_5_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4909 - model_4_loss: 0.4299 - model_5_loss: 0.6921 - model_5_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4925 - model_4_loss: 0.4302 - model_5_loss: 0.6922 - model_5_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9216 - model_5_loss: 0.6916 - model_5_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4943 - model_4_loss: 0.4272 - model_5_loss: 0.6920 - model_5_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4909 - model_4_loss: 0.4289 - model_5_loss: 0.6918 - model_5_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4901 - model_4_loss: 0.4308 - model_5_loss: 0.6920 - model_5_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4903 - model_4_loss: 0.4307 - model_5_loss: 0.6920 - model_5_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4918 - model_4_loss: 0.4298 - model_5_loss: 0.6919 - model_5_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9236 - model_5_loss: 0.6934 - model_5_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4905 - model_4_loss: 0.4304 - model_5_loss: 0.6919 - model_5_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4884 - model_4_loss: 0.4335 - model_5_loss: 0.6920 - model_5_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4898 - model_4_loss: 0.4316 - model_5_loss: 0.6920 - model_5_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4890 - model_4_loss: 0.4328 - model_5_loss: 0.6922 - model_5_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4902 - model_4_loss: 0.4304 - model_5_loss: 0.6919 - model_5_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9224 - model_5_loss: 0.6917 - model_5_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4905 - model_4_loss: 0.4320 - model_5_loss: 0.6923 - model_5_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4922 - model_4_loss: 0.4300 - model_5_loss: 0.6923 - model_5_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4913 - model_4_loss: 0.4320 - model_5_loss: 0.6925 - model_5_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4916 - model_4_loss: 0.4305 - model_5_loss: 0.6922 - model_5_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4912 - model_4_loss: 0.4304 - model_5_loss: 0.6922 - model_5_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9249 - model_5_loss: 0.6914 - model_5_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4914 - model_4_loss: 0.4300 - model_5_loss: 0.6920 - model_5_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4903 - model_4_loss: 0.4309 - model_5_loss: 0.6922 - model_5_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4918 - model_4_loss: 0.4294 - model_5_loss: 0.6922 - model_5_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4927 - model_4_loss: 0.4291 - model_5_loss: 0.6922 - model_5_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4917 - model_4_loss: 0.4287 - model_5_loss: 0.6921 - model_5_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9219 - model_5_loss: 0.6921 - model_5_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4935 - model_4_loss: 0.4281 - model_5_loss: 0.6922 - model_5_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4920 - model_4_loss: 0.4295 - model_5_loss: 0.6922 - model_5_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4960 - model_4_loss: 0.4266 - model_5_loss: 0.6922 - model_5_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4936 - model_4_loss: 0.4278 - model_5_loss: 0.6922 - model_5_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4968 - model_4_loss: 0.4248 - model_5_loss: 0.6923 - model_5_1_loss: 0.6921\n",
      "For Attention Module: 0.30000000000000004\n",
      "features X: 30940 samples, 90 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.3830 - model_9_loss: 0.6554 - model_9_1_loss: 0.6217\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.0282 - model_8_loss: 0.3567 - model_9_loss: 0.6546 - model_9_1_loss: 0.6224\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0400 - model_8_loss: 0.3586 - model_9_loss: 0.6557 - model_9_1_loss: 0.6240\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0580 - model_8_loss: 0.3572 - model_9_loss: 0.6564 - model_9_1_loss: 0.6266\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0747 - model_8_loss: 0.3581 - model_9_loss: 0.6573 - model_9_1_loss: 0.6292\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0792 - model_8_loss: 0.3587 - model_9_loss: 0.6561 - model_9_1_loss: 0.6314\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.4614 - model_9_loss: 0.6568 - model_9_1_loss: 0.6346\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.0897 - model_8_loss: 0.3592 - model_9_loss: 0.6563 - model_9_1_loss: 0.6335\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1063 - model_8_loss: 0.3601 - model_9_loss: 0.6579 - model_9_1_loss: 0.6353\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1127 - model_8_loss: 0.3613 - model_9_loss: 0.6585 - model_9_1_loss: 0.6363\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1272 - model_8_loss: 0.3619 - model_9_loss: 0.6586 - model_9_1_loss: 0.6392\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1397 - model_8_loss: 0.3627 - model_9_loss: 0.6594 - model_9_1_loss: 0.6411\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.5241 - model_9_loss: 0.6607 - model_9_1_loss: 0.6439\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1489 - model_8_loss: 0.3640 - model_9_loss: 0.6598 - model_9_1_loss: 0.6428\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1674 - model_8_loss: 0.3652 - model_9_loss: 0.6610 - model_9_1_loss: 0.6455\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1762 - model_8_loss: 0.3652 - model_9_loss: 0.6610 - model_9_1_loss: 0.6473\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1897 - model_8_loss: 0.3666 - model_9_loss: 0.6614 - model_9_1_loss: 0.6498\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2109 - model_8_loss: 0.3681 - model_9_loss: 0.6623 - model_9_1_loss: 0.6535\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.5912 - model_9_loss: 0.6642 - model_9_1_loss: 0.6537\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2136 - model_8_loss: 0.3703 - model_9_loss: 0.6630 - model_9_1_loss: 0.6538\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2307 - model_8_loss: 0.3700 - model_9_loss: 0.6635 - model_9_1_loss: 0.6567\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2400 - model_8_loss: 0.3710 - model_9_loss: 0.6644 - model_9_1_loss: 0.6578\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2665 - model_8_loss: 0.3746 - model_9_loss: 0.6671 - model_9_1_loss: 0.6611\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2737 - model_8_loss: 0.3742 - model_9_loss: 0.6669 - model_9_1_loss: 0.6627\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.6506 - model_9_loss: 0.6667 - model_9_1_loss: 0.6634\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2777 - model_8_loss: 0.3768 - model_9_loss: 0.6672 - model_9_1_loss: 0.6637\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2863 - model_8_loss: 0.3766 - model_9_loss: 0.6675 - model_9_1_loss: 0.6651\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2979 - model_8_loss: 0.3798 - model_9_loss: 0.6687 - model_9_1_loss: 0.6669\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3094 - model_8_loss: 0.3803 - model_9_loss: 0.6685 - model_9_1_loss: 0.6695\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3200 - model_8_loss: 0.3845 - model_9_loss: 0.6698 - model_9_1_loss: 0.6711\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.7101 - model_9_loss: 0.6704 - model_9_1_loss: 0.6716\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3339 - model_8_loss: 0.3849 - model_9_loss: 0.6713 - model_9_1_loss: 0.6724\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3444 - model_8_loss: 0.3873 - model_9_loss: 0.6724 - model_9_1_loss: 0.6739\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3581 - model_8_loss: 0.3868 - model_9_loss: 0.6731 - model_9_1_loss: 0.6759\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3601 - model_8_loss: 0.3925 - model_9_loss: 0.6730 - model_9_1_loss: 0.6775\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3794 - model_8_loss: 0.3916 - model_9_loss: 0.6751 - model_9_1_loss: 0.6791\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.7766 - model_9_loss: 0.6763 - model_9_1_loss: 0.6787\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3719 - model_8_loss: 0.3947 - model_9_loss: 0.6745 - model_9_1_loss: 0.6788\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3763 - model_8_loss: 0.3968 - model_9_loss: 0.6742 - model_9_1_loss: 0.6804\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3968 - model_8_loss: 0.3981 - model_9_loss: 0.6773 - model_9_1_loss: 0.6817\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3978 - model_8_loss: 0.4029 - model_9_loss: 0.6770 - model_9_1_loss: 0.6831\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4091 - model_8_loss: 0.4030 - model_9_loss: 0.6779 - model_9_1_loss: 0.6845\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.8139 - model_9_loss: 0.6787 - model_9_1_loss: 0.68440s - loss: 6.8051 - model_9_loss: 0.6758 - model_9_1_loss: 0.685\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4043 - model_8_loss: 0.4059 - model_9_loss: 0.6778 - model_9_1_loss: 0.6842\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4132 - model_8_loss: 0.4083 - model_9_loss: 0.6792 - model_9_1_loss: 0.6851\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4134 - model_8_loss: 0.4118 - model_9_loss: 0.6795 - model_9_1_loss: 0.6856\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4213 - model_8_loss: 0.4140 - model_9_loss: 0.6801 - model_9_1_loss: 0.6870\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4292 - model_8_loss: 0.4165 - model_9_loss: 0.6814 - model_9_1_loss: 0.6878\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.8476 - model_9_loss: 0.6815 - model_9_1_loss: 0.6873\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4267 - model_8_loss: 0.4192 - model_9_loss: 0.6821 - model_9_1_loss: 0.6871\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4309 - model_8_loss: 0.4215 - model_9_loss: 0.6828 - model_9_1_loss: 0.6876\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4333 - model_8_loss: 0.4225 - model_9_loss: 0.6829 - model_9_1_loss: 0.6882\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4405 - model_8_loss: 0.4245 - model_9_loss: 0.6843 - model_9_1_loss: 0.6887\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4396 - model_8_loss: 0.4260 - model_9_loss: 0.6840 - model_9_1_loss: 0.6891\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.8706 - model_9_loss: 0.6849 - model_9_1_loss: 0.6893\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4399 - model_8_loss: 0.4279 - model_9_loss: 0.6852 - model_9_1_loss: 0.6884\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4354 - model_8_loss: 0.4319 - model_9_loss: 0.6847 - model_9_1_loss: 0.6888\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4454 - model_8_loss: 0.4334 - model_9_loss: 0.6863 - model_9_1_loss: 0.6894\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4421 - model_8_loss: 0.4354 - model_9_loss: 0.6859 - model_9_1_loss: 0.6896\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4482 - model_8_loss: 0.4347 - model_9_loss: 0.6868 - model_9_1_loss: 0.6898\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.8908 - model_9_loss: 0.6884 - model_9_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4471 - model_8_loss: 0.4365 - model_9_loss: 0.6867 - model_9_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4485 - model_8_loss: 0.4385 - model_9_loss: 0.6873 - model_9_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4522 - model_8_loss: 0.4401 - model_9_loss: 0.6879 - model_9_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4563 - model_8_loss: 0.4398 - model_9_loss: 0.6886 - model_9_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4579 - model_8_loss: 0.4435 - model_9_loss: 0.6891 - model_9_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9047 - model_9_loss: 0.6896 - model_9_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4604 - model_8_loss: 0.4413 - model_9_loss: 0.6888 - model_9_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4617 - model_8_loss: 0.4433 - model_9_loss: 0.6893 - model_9_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4656 - model_8_loss: 0.4443 - model_9_loss: 0.6900 - model_9_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4645 - model_8_loss: 0.4462 - model_9_loss: 0.6900 - model_9_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4679 - model_8_loss: 0.4474 - model_9_loss: 0.6905 - model_9_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9169 - model_9_loss: 0.6907 - model_9_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4653 - model_8_loss: 0.4479 - model_9_loss: 0.6904 - model_9_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4704 - model_8_loss: 0.4454 - model_9_loss: 0.6908 - model_9_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4703 - model_8_loss: 0.4462 - model_9_loss: 0.6906 - model_9_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4750 - model_8_loss: 0.4456 - model_9_loss: 0.6914 - model_9_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4785 - model_8_loss: 0.4438 - model_9_loss: 0.6914 - model_9_1_loss: 0.6931\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9237 - model_9_loss: 0.6912 - model_9_1_loss: 0.6933\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4721 - model_8_loss: 0.4469 - model_9_loss: 0.6910 - model_9_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4748 - model_8_loss: 0.4467 - model_9_loss: 0.6915 - model_9_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4793 - model_8_loss: 0.4441 - model_9_loss: 0.6918 - model_9_1_loss: 0.6929\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4802 - model_8_loss: 0.4424 - model_9_loss: 0.6914 - model_9_1_loss: 0.6932\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4800 - model_8_loss: 0.4427 - model_9_loss: 0.6914 - model_9_1_loss: 0.6931\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9256 - model_9_loss: 0.6913 - model_9_1_loss: 0.6933\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4773 - model_8_loss: 0.4438 - model_9_loss: 0.6913 - model_9_1_loss: 0.6929\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4782 - model_8_loss: 0.4436 - model_9_loss: 0.6915 - model_9_1_loss: 0.6929\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4804 - model_8_loss: 0.4420 - model_9_loss: 0.6915 - model_9_1_loss: 0.6930\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4820 - model_8_loss: 0.4412 - model_9_loss: 0.6917 - model_9_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4849 - model_8_loss: 0.4390 - model_9_loss: 0.6919 - model_9_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9222 - model_9_loss: 0.6916 - model_9_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4808 - model_8_loss: 0.4388 - model_9_loss: 0.6914 - model_9_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4795 - model_8_loss: 0.4386 - model_9_loss: 0.6911 - model_9_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4823 - model_8_loss: 0.4352 - model_9_loss: 0.6911 - model_9_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4788 - model_8_loss: 0.4377 - model_9_loss: 0.6910 - model_9_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4840 - model_8_loss: 0.4346 - model_9_loss: 0.6914 - model_9_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9151 - model_9_loss: 0.6907 - model_9_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4812 - model_8_loss: 0.4333 - model_9_loss: 0.6912 - model_9_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4809 - model_8_loss: 0.4326 - model_9_loss: 0.6912 - model_9_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4821 - model_8_loss: 0.4330 - model_9_loss: 0.6915 - model_9_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4801 - model_8_loss: 0.4320 - model_9_loss: 0.6912 - model_9_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4793 - model_8_loss: 0.4319 - model_9_loss: 0.6909 - model_9_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9141 - model_9_loss: 0.6914 - model_9_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4779 - model_8_loss: 0.4306 - model_9_loss: 0.6909 - model_9_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4817 - model_8_loss: 0.4297 - model_9_loss: 0.6913 - model_9_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4820 - model_8_loss: 0.4284 - model_9_loss: 0.6912 - model_9_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4829 - model_8_loss: 0.4272 - model_9_loss: 0.6911 - model_9_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4843 - model_8_loss: 0.4298 - model_9_loss: 0.6915 - model_9_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9158 - model_9_loss: 0.6908 - model_9_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4841 - model_8_loss: 0.4276 - model_9_loss: 0.6912 - model_9_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4816 - model_8_loss: 0.4287 - model_9_loss: 0.6911 - model_9_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4878 - model_8_loss: 0.4270 - model_9_loss: 0.6918 - model_9_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4880 - model_8_loss: 0.4275 - model_9_loss: 0.6917 - model_9_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4898 - model_8_loss: 0.4273 - model_9_loss: 0.6921 - model_9_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9173 - model_9_loss: 0.6916 - model_9_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4902 - model_8_loss: 0.4288 - model_9_loss: 0.6919 - model_9_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4912 - model_8_loss: 0.4290 - model_9_loss: 0.6920 - model_9_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4938 - model_8_loss: 0.4281 - model_9_loss: 0.6922 - model_9_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4943 - model_8_loss: 0.4288 - model_9_loss: 0.6924 - model_9_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4967 - model_8_loss: 0.4278 - model_9_loss: 0.6924 - model_9_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9258 - model_9_loss: 0.6918 - model_9_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4939 - model_8_loss: 0.4277 - model_9_loss: 0.6919 - model_9_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4907 - model_8_loss: 0.4303 - model_9_loss: 0.6916 - model_9_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4954 - model_8_loss: 0.4277 - model_9_loss: 0.6921 - model_9_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4954 - model_8_loss: 0.4292 - model_9_loss: 0.6921 - model_9_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4981 - model_8_loss: 0.4294 - model_9_loss: 0.6926 - model_9_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9251 - model_9_loss: 0.6924 - model_9_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4961 - model_8_loss: 0.4283 - model_9_loss: 0.6923 - model_9_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4930 - model_8_loss: 0.4298 - model_9_loss: 0.6919 - model_9_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4970 - model_8_loss: 0.4281 - model_9_loss: 0.6923 - model_9_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4962 - model_8_loss: 0.4294 - model_9_loss: 0.6922 - model_9_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4972 - model_8_loss: 0.4292 - model_9_loss: 0.6925 - model_9_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9253 - model_9_loss: 0.6921 - model_9_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4960 - model_8_loss: 0.4268 - model_9_loss: 0.6921 - model_9_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4933 - model_8_loss: 0.4301 - model_9_loss: 0.6922 - model_9_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4946 - model_8_loss: 0.4295 - model_9_loss: 0.6922 - model_9_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4961 - model_8_loss: 0.4271 - model_9_loss: 0.6922 - model_9_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4974 - model_8_loss: 0.4272 - model_9_loss: 0.6923 - model_9_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9242 - model_9_loss: 0.6914 - model_9_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4917 - model_8_loss: 0.4292 - model_9_loss: 0.6920 - model_9_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4935 - model_8_loss: 0.4264 - model_9_loss: 0.6918 - model_9_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4913 - model_8_loss: 0.4285 - model_9_loss: 0.6918 - model_9_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4965 - model_8_loss: 0.4249 - model_9_loss: 0.6921 - model_9_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4928 - model_8_loss: 0.4264 - model_9_loss: 0.6918 - model_9_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9195 - model_9_loss: 0.6922 - model_9_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4956 - model_8_loss: 0.4240 - model_9_loss: 0.6920 - model_9_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4924 - model_8_loss: 0.4256 - model_9_loss: 0.6917 - model_9_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4931 - model_8_loss: 0.4240 - model_9_loss: 0.6916 - model_9_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4911 - model_8_loss: 0.4247 - model_9_loss: 0.6914 - model_9_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4933 - model_8_loss: 0.4241 - model_9_loss: 0.6916 - model_9_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9182 - model_9_loss: 0.6917 - model_9_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4968 - model_8_loss: 0.4242 - model_9_loss: 0.6920 - model_9_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4959 - model_8_loss: 0.4233 - model_9_loss: 0.6917 - model_9_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4973 - model_8_loss: 0.4223 - model_9_loss: 0.6918 - model_9_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4990 - model_8_loss: 0.4212 - model_9_loss: 0.6918 - model_9_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5003 - model_8_loss: 0.4200 - model_9_loss: 0.6919 - model_9_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9209 - model_9_loss: 0.6922 - model_9_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4967 - model_8_loss: 0.4213 - model_9_loss: 0.6917 - model_9_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4987 - model_8_loss: 0.4186 - model_9_loss: 0.6916 - model_9_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5010 - model_8_loss: 0.4176 - model_9_loss: 0.6918 - model_9_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4984 - model_8_loss: 0.4212 - model_9_loss: 0.6920 - model_9_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4991 - model_8_loss: 0.4205 - model_9_loss: 0.6918 - model_9_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9198 - model_9_loss: 0.6925 - model_9_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4995 - model_8_loss: 0.4190 - model_9_loss: 0.6917 - model_9_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5001 - model_8_loss: 0.4190 - model_9_loss: 0.6918 - model_9_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4996 - model_8_loss: 0.4188 - model_9_loss: 0.6916 - model_9_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5014 - model_8_loss: 0.4195 - model_9_loss: 0.6920 - model_9_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5019 - model_8_loss: 0.4210 - model_9_loss: 0.6922 - model_9_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9230 - model_9_loss: 0.6926 - model_9_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5019 - model_8_loss: 0.4180 - model_9_loss: 0.6923 - model_9_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5012 - model_8_loss: 0.4192 - model_9_loss: 0.6924 - model_9_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4978 - model_8_loss: 0.4221 - model_9_loss: 0.6922 - model_9_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5002 - model_8_loss: 0.4219 - model_9_loss: 0.6924 - model_9_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5012 - model_8_loss: 0.4216 - model_9_loss: 0.6924 - model_9_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9230 - model_9_loss: 0.6927 - model_9_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5019 - model_8_loss: 0.4218 - model_9_loss: 0.6926 - model_9_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4982 - model_8_loss: 0.4239 - model_9_loss: 0.6926 - model_9_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5022 - model_8_loss: 0.4219 - model_9_loss: 0.6926 - model_9_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4986 - model_8_loss: 0.4245 - model_9_loss: 0.6924 - model_9_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5013 - model_8_loss: 0.4236 - model_9_loss: 0.6927 - model_9_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9237 - model_9_loss: 0.6922 - model_9_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4980 - model_8_loss: 0.4246 - model_9_loss: 0.6925 - model_9_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4970 - model_8_loss: 0.4250 - model_9_loss: 0.6925 - model_9_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4984 - model_8_loss: 0.4240 - model_9_loss: 0.6925 - model_9_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4975 - model_8_loss: 0.4252 - model_9_loss: 0.6926 - model_9_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5000 - model_8_loss: 0.4242 - model_9_loss: 0.6927 - model_9_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9254 - model_9_loss: 0.6930 - model_9_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4992 - model_8_loss: 0.4242 - model_9_loss: 0.6928 - model_9_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4982 - model_8_loss: 0.4250 - model_9_loss: 0.6925 - model_9_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4985 - model_8_loss: 0.4261 - model_9_loss: 0.6929 - model_9_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5002 - model_8_loss: 0.4239 - model_9_loss: 0.6927 - model_9_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5010 - model_8_loss: 0.4236 - model_9_loss: 0.6927 - model_9_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9249 - model_9_loss: 0.6926 - model_9_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5017 - model_8_loss: 0.4229 - model_9_loss: 0.6926 - model_9_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5029 - model_8_loss: 0.4232 - model_9_loss: 0.6927 - model_9_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5004 - model_8_loss: 0.4229 - model_9_loss: 0.6927 - model_9_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5029 - model_8_loss: 0.4217 - model_9_loss: 0.6925 - model_9_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5039 - model_8_loss: 0.4206 - model_9_loss: 0.6925 - model_9_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: 6.9254 - model_9_loss: 0.6924 - model_9_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5018 - model_8_loss: 0.4228 - model_9_loss: 0.6922 - model_9_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5024 - model_8_loss: 0.4205 - model_9_loss: 0.6923 - model_9_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5033 - model_8_loss: 0.4209 - model_9_loss: 0.6922 - model_9_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5051 - model_8_loss: 0.4205 - model_9_loss: 0.6924 - model_9_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5039 - model_8_loss: 0.4196 - model_9_loss: 0.6920 - model_9_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9244 - model_9_loss: 0.6922 - model_9_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5019 - model_8_loss: 0.4213 - model_9_loss: 0.6922 - model_9_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5042 - model_8_loss: 0.4193 - model_9_loss: 0.6920 - model_9_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5049 - model_8_loss: 0.4180 - model_9_loss: 0.6919 - model_9_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5065 - model_8_loss: 0.4193 - model_9_loss: 0.6922 - model_9_1_loss: 0.6930\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5056 - model_8_loss: 0.4182 - model_9_loss: 0.6920 - model_9_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9238 - model_9_loss: 0.6915 - model_9_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5055 - model_8_loss: 0.4156 - model_9_loss: 0.6917 - model_9_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5041 - model_8_loss: 0.4156 - model_9_loss: 0.6914 - model_9_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5034 - model_8_loss: 0.4168 - model_9_loss: 0.6916 - model_9_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5054 - model_8_loss: 0.4165 - model_9_loss: 0.6917 - model_9_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5051 - model_8_loss: 0.4163 - model_9_loss: 0.6917 - model_9_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9214 - model_9_loss: 0.6915 - model_9_1_loss: 0.69230s - loss: 6.9345 - model_9_loss: 0.6948 - model_9_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5041 - model_8_loss: 0.4151 - model_9_loss: 0.6917 - model_9_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5044 - model_8_loss: 0.4149 - model_9_loss: 0.6915 - model_9_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5041 - model_8_loss: 0.4149 - model_9_loss: 0.6916 - model_9_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5038 - model_8_loss: 0.4157 - model_9_loss: 0.6917 - model_9_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5035 - model_8_loss: 0.4145 - model_9_loss: 0.6914 - model_9_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9196 - model_9_loss: 0.6909 - model_9_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5027 - model_8_loss: 0.4168 - model_9_loss: 0.6919 - model_9_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5025 - model_8_loss: 0.4164 - model_9_loss: 0.6916 - model_9_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5023 - model_8_loss: 0.4154 - model_9_loss: 0.6916 - model_9_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5024 - model_8_loss: 0.4164 - model_9_loss: 0.6918 - model_9_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5032 - model_8_loss: 0.4158 - model_9_loss: 0.6917 - model_9_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9211 - model_9_loss: 0.6917 - model_9_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5061 - model_8_loss: 0.4148 - model_9_loss: 0.6921 - model_9_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5043 - model_8_loss: 0.4161 - model_9_loss: 0.6919 - model_9_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5052 - model_8_loss: 0.4160 - model_9_loss: 0.6921 - model_9_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5054 - model_8_loss: 0.4170 - model_9_loss: 0.6922 - model_9_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5021 - model_8_loss: 0.4194 - model_9_loss: 0.6920 - model_9_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9246 - model_9_loss: 0.6923 - model_9_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5059 - model_8_loss: 0.4188 - model_9_loss: 0.6924 - model_9_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5064 - model_8_loss: 0.4183 - model_9_loss: 0.6924 - model_9_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5056 - model_8_loss: 0.4197 - model_9_loss: 0.6924 - model_9_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5082 - model_8_loss: 0.4189 - model_9_loss: 0.6927 - model_9_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5063 - model_8_loss: 0.4208 - model_9_loss: 0.6927 - model_9_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9296 - model_9_loss: 0.6930 - model_9_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5055 - model_8_loss: 0.4212 - model_9_loss: 0.6927 - model_9_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5087 - model_8_loss: 0.4208 - model_9_loss: 0.6928 - model_9_1_loss: 0.6931\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5086 - model_8_loss: 0.4204 - model_9_loss: 0.6929 - model_9_1_loss: 0.6929\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5089 - model_8_loss: 0.4194 - model_9_loss: 0.6927 - model_9_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5085 - model_8_loss: 0.4199 - model_9_loss: 0.6928 - model_9_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9291 - model_9_loss: 0.6930 - model_9_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5070 - model_8_loss: 0.4204 - model_9_loss: 0.6929 - model_9_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5059 - model_8_loss: 0.4215 - model_9_loss: 0.6928 - model_9_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5073 - model_8_loss: 0.4193 - model_9_loss: 0.6928 - model_9_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5083 - model_8_loss: 0.4183 - model_9_loss: 0.6928 - model_9_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5055 - model_8_loss: 0.4203 - model_9_loss: 0.6928 - model_9_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9261 - model_9_loss: 0.6931 - model_9_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5059 - model_8_loss: 0.4195 - model_9_loss: 0.6928 - model_9_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5063 - model_8_loss: 0.4189 - model_9_loss: 0.6927 - model_9_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5063 - model_8_loss: 0.4174 - model_9_loss: 0.6926 - model_9_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5068 - model_8_loss: 0.4166 - model_9_loss: 0.6926 - model_9_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5092 - model_8_loss: 0.4165 - model_9_loss: 0.6925 - model_9_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9238 - model_9_loss: 0.6922 - model_9_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5081 - model_8_loss: 0.4154 - model_9_loss: 0.6925 - model_9_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5088 - model_8_loss: 0.4141 - model_9_loss: 0.6923 - model_9_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5104 - model_8_loss: 0.4133 - model_9_loss: 0.6924 - model_9_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5105 - model_8_loss: 0.4124 - model_9_loss: 0.6924 - model_9_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5085 - model_8_loss: 0.4151 - model_9_loss: 0.6923 - model_9_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9236 - model_9_loss: 0.6917 - model_9_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5061 - model_8_loss: 0.4145 - model_9_loss: 0.6920 - model_9_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5085 - model_8_loss: 0.4127 - model_9_loss: 0.6922 - model_9_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5083 - model_8_loss: 0.4123 - model_9_loss: 0.6921 - model_9_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5096 - model_8_loss: 0.4121 - model_9_loss: 0.6922 - model_9_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5078 - model_8_loss: 0.4131 - model_9_loss: 0.6919 - model_9_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9221 - model_9_loss: 0.6917 - model_9_1_loss: 0.692 - 0s 12us/sample - loss: 6.9222 - model_9_loss: 0.6920 - model_9_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5093 - model_8_loss: 0.4127 - model_9_loss: 0.6920 - model_9_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5097 - model_8_loss: 0.4127 - model_9_loss: 0.6922 - model_9_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5079 - model_8_loss: 0.4141 - model_9_loss: 0.6921 - model_9_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5095 - model_8_loss: 0.4121 - model_9_loss: 0.6919 - model_9_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5097 - model_8_loss: 0.4121 - model_9_loss: 0.6919 - model_9_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9217 - model_9_loss: 0.6917 - model_9_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5069 - model_8_loss: 0.4138 - model_9_loss: 0.6921 - model_9_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5061 - model_8_loss: 0.4120 - model_9_loss: 0.6916 - model_9_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5084 - model_8_loss: 0.4123 - model_9_loss: 0.6920 - model_9_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5086 - model_8_loss: 0.4122 - model_9_loss: 0.6920 - model_9_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5077 - model_8_loss: 0.4144 - model_9_loss: 0.6921 - model_9_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9212 - model_9_loss: 0.6926 - model_9_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5062 - model_8_loss: 0.4133 - model_9_loss: 0.6919 - model_9_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5064 - model_8_loss: 0.4133 - model_9_loss: 0.6918 - model_9_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5101 - model_8_loss: 0.4108 - model_9_loss: 0.6920 - model_9_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5055 - model_8_loss: 0.4137 - model_9_loss: 0.6918 - model_9_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5085 - model_8_loss: 0.4123 - model_9_loss: 0.6920 - model_9_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9214 - model_9_loss: 0.6914 - model_9_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5052 - model_8_loss: 0.4145 - model_9_loss: 0.6918 - model_9_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5067 - model_8_loss: 0.4136 - model_9_loss: 0.6917 - model_9_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5089 - model_8_loss: 0.4141 - model_9_loss: 0.6922 - model_9_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5083 - model_8_loss: 0.4117 - model_9_loss: 0.6918 - model_9_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5088 - model_8_loss: 0.4132 - model_9_loss: 0.6921 - model_9_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9257 - model_9_loss: 0.6929 - model_9_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5097 - model_8_loss: 0.4136 - model_9_loss: 0.6922 - model_9_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5082 - model_8_loss: 0.4154 - model_9_loss: 0.6922 - model_9_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5110 - model_8_loss: 0.4123 - model_9_loss: 0.6922 - model_9_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5099 - model_8_loss: 0.4143 - model_9_loss: 0.6924 - model_9_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.5136 - model_8_loss: 0.4125 - model_9_loss: 0.6925 - model_9_1_loss: 0.6927\n",
      "For Attention Module: 0.4\n",
      "features X: 30940 samples, 76 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.3958 - model_13_loss: 0.6604 - model_13_1_loss: 0.6189\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.0270 - model_12_loss: 0.3706 - model_13_loss: 0.6599 - model_13_1_loss: 0.6197\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0323 - model_12_loss: 0.3696 - model_13_loss: 0.6594 - model_13_1_loss: 0.6210\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0518 - model_12_loss: 0.3690 - model_13_loss: 0.6607 - model_13_1_loss: 0.6235\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0513 - model_12_loss: 0.3703 - model_13_loss: 0.6599 - model_13_1_loss: 0.6244\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0692 - model_12_loss: 0.3689 - model_13_loss: 0.6614 - model_13_1_loss: 0.6263\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.4525 - model_13_loss: 0.6619 - model_13_1_loss: 0.6287\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0816 - model_12_loss: 0.3718 - model_13_loss: 0.6606 - model_13_1_loss: 0.6301\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0928 - model_12_loss: 0.3726 - model_13_loss: 0.6621 - model_13_1_loss: 0.6310\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1034 - model_12_loss: 0.3735 - model_13_loss: 0.6622 - model_13_1_loss: 0.6332\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1145 - model_12_loss: 0.3728 - model_13_loss: 0.6626 - model_13_1_loss: 0.6348\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1365 - model_12_loss: 0.3720 - model_13_loss: 0.6638 - model_13_1_loss: 0.6379\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.5159 - model_13_loss: 0.6648 - model_13_1_loss: 0.6381\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1426 - model_12_loss: 0.3737 - model_13_loss: 0.6643 - model_13_1_loss: 0.6390\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1562 - model_12_loss: 0.3742 - model_13_loss: 0.6649 - model_13_1_loss: 0.6412\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1631 - model_12_loss: 0.3743 - model_13_loss: 0.6647 - model_13_1_loss: 0.6428\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1796 - model_12_loss: 0.3767 - model_13_loss: 0.6663 - model_13_1_loss: 0.6450\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1862 - model_12_loss: 0.3771 - model_13_loss: 0.6656 - model_13_1_loss: 0.6471\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.5858 - model_13_loss: 0.6682 - model_13_1_loss: 0.6488\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1908 - model_12_loss: 0.3801 - model_13_loss: 0.6661 - model_13_1_loss: 0.6481\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2026 - model_12_loss: 0.3797 - model_13_loss: 0.6663 - model_13_1_loss: 0.6502\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2258 - model_12_loss: 0.3827 - model_13_loss: 0.6675 - model_13_1_loss: 0.6542\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2401 - model_12_loss: 0.3829 - model_13_loss: 0.6693 - model_13_1_loss: 0.6553\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2510 - model_12_loss: 0.3840 - model_13_loss: 0.6698 - model_13_1_loss: 0.6572\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.6533 - model_13_loss: 0.6723 - model_13_1_loss: 0.6587\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2622 - model_12_loss: 0.3863 - model_13_loss: 0.6712 - model_13_1_loss: 0.6585\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2650 - model_12_loss: 0.3882 - model_13_loss: 0.6712 - model_13_1_loss: 0.6594\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2816 - model_12_loss: 0.3904 - model_13_loss: 0.6724 - model_13_1_loss: 0.6620\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2977 - model_12_loss: 0.3906 - model_13_loss: 0.6737 - model_13_1_loss: 0.6639\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3029 - model_12_loss: 0.3933 - model_13_loss: 0.6737 - model_13_1_loss: 0.6655\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.7110 - model_13_loss: 0.6756 - model_13_1_loss: 0.6678\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3104 - model_12_loss: 0.3965 - model_13_loss: 0.6741 - model_13_1_loss: 0.6673\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3170 - model_12_loss: 0.3959 - model_13_loss: 0.6750 - model_13_1_loss: 0.6676\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3305 - model_12_loss: 0.4010 - model_13_loss: 0.6761 - model_13_1_loss: 0.6702\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3440 - model_12_loss: 0.4031 - model_13_loss: 0.6776 - model_13_1_loss: 0.6718\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3475 - model_12_loss: 0.4050 - model_13_loss: 0.6769 - model_13_1_loss: 0.6736\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.7584 - model_13_loss: 0.6777 - model_13_1_loss: 0.6739\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3542 - model_12_loss: 0.4056 - model_13_loss: 0.6789 - model_13_1_loss: 0.6731\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3600 - model_12_loss: 0.4105 - model_13_loss: 0.6799 - model_13_1_loss: 0.6742\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3803 - model_12_loss: 0.4104 - model_13_loss: 0.6816 - model_13_1_loss: 0.6765\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3851 - model_12_loss: 0.4114 - model_13_loss: 0.6818 - model_13_1_loss: 0.6775\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3963 - model_12_loss: 0.4149 - model_13_loss: 0.6833 - model_13_1_loss: 0.6789\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.8099 - model_13_loss: 0.6821 - model_13_1_loss: 0.6795\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3854 - model_12_loss: 0.4159 - model_13_loss: 0.6807 - model_13_1_loss: 0.6796\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3972 - model_12_loss: 0.4161 - model_13_loss: 0.6819 - model_13_1_loss: 0.6807\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4042 - model_12_loss: 0.4184 - model_13_loss: 0.6829 - model_13_1_loss: 0.6816\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4091 - model_12_loss: 0.4205 - model_13_loss: 0.6834 - model_13_1_loss: 0.6825\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4125 - model_12_loss: 0.4224 - model_13_loss: 0.6834 - model_13_1_loss: 0.6836\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.8376 - model_13_loss: 0.6841 - model_13_1_loss: 0.683 - 0s 12us/sample - loss: 6.8401 - model_13_loss: 0.6849 - model_13_1_loss: 0.6832\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4091 - model_12_loss: 0.4237 - model_13_loss: 0.6837 - model_13_1_loss: 0.6828\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4165 - model_12_loss: 0.4244 - model_13_loss: 0.6839 - model_13_1_loss: 0.6843\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4203 - model_12_loss: 0.4280 - model_13_loss: 0.6850 - model_13_1_loss: 0.6847\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4287 - model_12_loss: 0.4297 - model_13_loss: 0.6856 - model_13_1_loss: 0.6860\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4379 - model_12_loss: 0.4307 - model_13_loss: 0.6865 - model_13_1_loss: 0.6872\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.8704 - model_13_loss: 0.6881 - model_13_1_loss: 0.6876\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4343 - model_12_loss: 0.4318 - model_13_loss: 0.6860 - model_13_1_loss: 0.6872\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4435 - model_12_loss: 0.4352 - model_13_loss: 0.6873 - model_13_1_loss: 0.6885\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4498 - model_12_loss: 0.4339 - model_13_loss: 0.6878 - model_13_1_loss: 0.6889\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4409 - model_12_loss: 0.4398 - model_13_loss: 0.6876 - model_13_1_loss: 0.6886\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4550 - model_12_loss: 0.4421 - model_13_loss: 0.6888 - model_13_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.8997 - model_13_loss: 0.6893 - model_13_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4562 - model_12_loss: 0.4419 - model_13_loss: 0.6890 - model_13_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4530 - model_12_loss: 0.4447 - model_13_loss: 0.6890 - model_13_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4580 - model_12_loss: 0.4466 - model_13_loss: 0.6899 - model_13_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4622 - model_12_loss: 0.4475 - model_13_loss: 0.6905 - model_13_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4597 - model_12_loss: 0.4503 - model_13_loss: 0.6900 - model_13_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9182 - model_13_loss: 0.6918 - model_13_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4633 - model_12_loss: 0.4512 - model_13_loss: 0.6910 - model_13_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4653 - model_12_loss: 0.4521 - model_13_loss: 0.6912 - model_13_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4666 - model_12_loss: 0.4533 - model_13_loss: 0.6914 - model_13_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4656 - model_12_loss: 0.4565 - model_13_loss: 0.6916 - model_13_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4688 - model_12_loss: 0.4583 - model_13_loss: 0.6921 - model_13_1_loss: 0.6933\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9199 - model_13_loss: 0.6914 - model_13_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4698 - model_12_loss: 0.4570 - model_13_loss: 0.6922 - model_13_1_loss: 0.6931\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4659 - model_12_loss: 0.4627 - model_13_loss: 0.6923 - model_13_1_loss: 0.6934\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4727 - model_12_loss: 0.4609 - model_13_loss: 0.6931 - model_13_1_loss: 0.6937\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4755 - model_12_loss: 0.4610 - model_13_loss: 0.6937 - model_13_1_loss: 0.6936\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4783 - model_12_loss: 0.4626 - model_13_loss: 0.6940 - model_13_1_loss: 0.6942\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9270 - model_13_loss: 0.6922 - model_13_1_loss: 0.6932\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4659 - model_12_loss: 0.4623 - model_13_loss: 0.6926 - model_13_1_loss: 0.6931\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4636 - model_12_loss: 0.4625 - model_13_loss: 0.6924 - model_13_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4658 - model_12_loss: 0.4622 - model_13_loss: 0.6926 - model_13_1_loss: 0.6930\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4665 - model_12_loss: 0.4634 - model_13_loss: 0.6930 - model_13_1_loss: 0.6930\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4672 - model_12_loss: 0.4609 - model_13_loss: 0.6927 - model_13_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9299 - model_13_loss: 0.6926 - model_13_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4674 - model_12_loss: 0.4602 - model_13_loss: 0.6928 - model_13_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4694 - model_12_loss: 0.4604 - model_13_loss: 0.6933 - model_13_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4671 - model_12_loss: 0.4597 - model_13_loss: 0.6927 - model_13_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4693 - model_12_loss: 0.4579 - model_13_loss: 0.6927 - model_13_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4702 - model_12_loss: 0.4567 - model_13_loss: 0.6928 - model_13_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9282 - model_13_loss: 0.6938 - model_13_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4680 - model_12_loss: 0.4566 - model_13_loss: 0.6925 - model_13_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4713 - model_12_loss: 0.4554 - model_13_loss: 0.6930 - model_13_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4700 - model_12_loss: 0.4530 - model_13_loss: 0.6927 - model_13_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4723 - model_12_loss: 0.4527 - model_13_loss: 0.6928 - model_13_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4674 - model_12_loss: 0.4541 - model_13_loss: 0.6924 - model_13_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9241 - model_13_loss: 0.6934 - model_13_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4748 - model_12_loss: 0.4482 - model_13_loss: 0.6925 - model_13_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4756 - model_12_loss: 0.4470 - model_13_loss: 0.6925 - model_13_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4723 - model_12_loss: 0.4496 - model_13_loss: 0.6922 - model_13_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4748 - model_12_loss: 0.4490 - model_13_loss: 0.6926 - model_13_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4755 - model_12_loss: 0.4462 - model_13_loss: 0.6924 - model_13_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9202 - model_13_loss: 0.6924 - model_13_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4710 - model_12_loss: 0.4460 - model_13_loss: 0.6919 - model_13_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4744 - model_12_loss: 0.4453 - model_13_loss: 0.6924 - model_13_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4758 - model_12_loss: 0.4421 - model_13_loss: 0.6922 - model_13_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4737 - model_12_loss: 0.4428 - model_13_loss: 0.6919 - model_13_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4771 - model_12_loss: 0.4417 - model_13_loss: 0.6920 - model_13_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9160 - model_13_loss: 0.6915 - model_13_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4759 - model_12_loss: 0.4404 - model_13_loss: 0.6920 - model_13_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4761 - model_12_loss: 0.4406 - model_13_loss: 0.6921 - model_13_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4780 - model_12_loss: 0.4400 - model_13_loss: 0.6925 - model_13_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4788 - model_12_loss: 0.4394 - model_13_loss: 0.6922 - model_13_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4784 - model_12_loss: 0.4378 - model_13_loss: 0.6922 - model_13_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9185 - model_13_loss: 0.6924 - model_13_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4788 - model_12_loss: 0.4367 - model_13_loss: 0.6918 - model_13_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4789 - model_12_loss: 0.4367 - model_13_loss: 0.6918 - model_13_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4797 - model_12_loss: 0.4368 - model_13_loss: 0.6922 - model_13_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4784 - model_12_loss: 0.4373 - model_13_loss: 0.6919 - model_13_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4800 - model_12_loss: 0.4366 - model_13_loss: 0.6921 - model_13_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9179 - model_13_loss: 0.6922 - model_13_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4805 - model_12_loss: 0.4361 - model_13_loss: 0.6922 - model_13_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4793 - model_12_loss: 0.4357 - model_13_loss: 0.6919 - model_13_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4770 - model_12_loss: 0.4378 - model_13_loss: 0.6921 - model_13_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4783 - model_12_loss: 0.4377 - model_13_loss: 0.6921 - model_13_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4798 - model_12_loss: 0.4373 - model_13_loss: 0.6922 - model_13_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9174 - model_13_loss: 0.6924 - model_13_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4808 - model_12_loss: 0.4363 - model_13_loss: 0.6918 - model_13_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4785 - model_12_loss: 0.4397 - model_13_loss: 0.6922 - model_13_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4770 - model_12_loss: 0.4389 - model_13_loss: 0.6919 - model_13_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4820 - model_12_loss: 0.4375 - model_13_loss: 0.6922 - model_13_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4797 - model_12_loss: 0.4403 - model_13_loss: 0.6923 - model_13_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9228 - model_13_loss: 0.6919 - model_13_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4823 - model_12_loss: 0.4390 - model_13_loss: 0.6922 - model_13_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4825 - model_12_loss: 0.4389 - model_13_loss: 0.6924 - model_13_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4821 - model_12_loss: 0.4401 - model_13_loss: 0.6923 - model_13_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4825 - model_12_loss: 0.4410 - model_13_loss: 0.6926 - model_13_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4841 - model_12_loss: 0.4398 - model_13_loss: 0.6926 - model_13_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9205 - model_13_loss: 0.6922 - model_13_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4801 - model_12_loss: 0.4412 - model_13_loss: 0.6922 - model_13_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4816 - model_12_loss: 0.4407 - model_13_loss: 0.6923 - model_13_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4816 - model_12_loss: 0.4406 - model_13_loss: 0.6925 - model_13_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4815 - model_12_loss: 0.4402 - model_13_loss: 0.6923 - model_13_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4816 - model_12_loss: 0.4413 - model_13_loss: 0.6925 - model_13_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9241 - model_13_loss: 0.6924 - model_13_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4818 - model_12_loss: 0.4416 - model_13_loss: 0.6923 - model_13_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4821 - model_12_loss: 0.4403 - model_13_loss: 0.6924 - model_13_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4831 - model_12_loss: 0.4398 - model_13_loss: 0.6925 - model_13_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4840 - model_12_loss: 0.4390 - model_13_loss: 0.6925 - model_13_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4839 - model_12_loss: 0.4383 - model_13_loss: 0.6924 - model_13_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9222 - model_13_loss: 0.6925 - model_13_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4835 - model_12_loss: 0.4372 - model_13_loss: 0.6921 - model_13_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4831 - model_12_loss: 0.4377 - model_13_loss: 0.6922 - model_13_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4853 - model_12_loss: 0.4356 - model_13_loss: 0.6923 - model_13_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4847 - model_12_loss: 0.4349 - model_13_loss: 0.6923 - model_13_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4833 - model_12_loss: 0.4357 - model_13_loss: 0.6921 - model_13_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9200 - model_13_loss: 0.6922 - model_13_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4853 - model_12_loss: 0.4333 - model_13_loss: 0.6922 - model_13_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4847 - model_12_loss: 0.4332 - model_13_loss: 0.6922 - model_13_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4840 - model_12_loss: 0.4324 - model_13_loss: 0.6922 - model_13_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4844 - model_12_loss: 0.4329 - model_13_loss: 0.6922 - model_13_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4844 - model_12_loss: 0.4322 - model_13_loss: 0.6921 - model_13_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9167 - model_13_loss: 0.6928 - model_13_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4859 - model_12_loss: 0.4297 - model_13_loss: 0.6920 - model_13_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4831 - model_12_loss: 0.4318 - model_13_loss: 0.6919 - model_13_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4818 - model_12_loss: 0.4316 - model_13_loss: 0.6918 - model_13_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4852 - model_12_loss: 0.4294 - model_13_loss: 0.6920 - model_13_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4838 - model_12_loss: 0.4299 - model_13_loss: 0.6919 - model_13_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9123 - model_13_loss: 0.6920 - model_13_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4849 - model_12_loss: 0.4300 - model_13_loss: 0.6920 - model_13_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4824 - model_12_loss: 0.4305 - model_13_loss: 0.6918 - model_13_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4840 - model_12_loss: 0.4295 - model_13_loss: 0.6918 - model_13_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4859 - model_12_loss: 0.4294 - model_13_loss: 0.6919 - model_13_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4851 - model_12_loss: 0.4300 - model_13_loss: 0.6921 - model_13_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9140 - model_13_loss: 0.6922 - model_13_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4848 - model_12_loss: 0.4297 - model_13_loss: 0.6918 - model_13_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4845 - model_12_loss: 0.4310 - model_13_loss: 0.6922 - model_13_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4856 - model_12_loss: 0.4291 - model_13_loss: 0.6917 - model_13_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4842 - model_12_loss: 0.4300 - model_13_loss: 0.6918 - model_13_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4852 - model_12_loss: 0.4287 - model_13_loss: 0.6918 - model_13_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9167 - model_13_loss: 0.6921 - model_13_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4884 - model_12_loss: 0.4290 - model_13_loss: 0.6920 - model_13_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4868 - model_12_loss: 0.4299 - model_13_loss: 0.6920 - model_13_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4881 - model_12_loss: 0.4314 - model_13_loss: 0.6921 - model_13_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4883 - model_12_loss: 0.4291 - model_13_loss: 0.6922 - model_13_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4892 - model_12_loss: 0.4315 - model_13_loss: 0.6921 - model_13_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9215 - model_13_loss: 0.6922 - model_13_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4895 - model_12_loss: 0.4320 - model_13_loss: 0.6922 - model_13_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4920 - model_12_loss: 0.4321 - model_13_loss: 0.6923 - model_13_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4913 - model_12_loss: 0.4320 - model_13_loss: 0.6922 - model_13_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4882 - model_12_loss: 0.4353 - model_13_loss: 0.6922 - model_13_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4930 - model_12_loss: 0.4338 - model_13_loss: 0.6925 - model_13_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9213 - model_13_loss: 0.6923 - model_13_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4849 - model_12_loss: 0.4332 - model_13_loss: 0.6919 - model_13_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4832 - model_12_loss: 0.4368 - model_13_loss: 0.6921 - model_13_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4871 - model_12_loss: 0.4338 - model_13_loss: 0.6921 - model_13_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4868 - model_12_loss: 0.4353 - model_13_loss: 0.6921 - model_13_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4846 - model_12_loss: 0.4371 - model_13_loss: 0.6920 - model_13_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9217 - model_13_loss: 0.6920 - model_13_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4856 - model_12_loss: 0.4366 - model_13_loss: 0.6919 - model_13_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4862 - model_12_loss: 0.4359 - model_13_loss: 0.6921 - model_13_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4849 - model_12_loss: 0.4369 - model_13_loss: 0.6919 - model_13_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4868 - model_12_loss: 0.4363 - model_13_loss: 0.6920 - model_13_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4850 - model_12_loss: 0.4362 - model_13_loss: 0.6918 - model_13_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9223 - model_13_loss: 0.6925 - model_13_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4845 - model_12_loss: 0.4352 - model_13_loss: 0.6918 - model_13_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4842 - model_12_loss: 0.4364 - model_13_loss: 0.6919 - model_13_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4839 - model_12_loss: 0.4349 - model_13_loss: 0.6916 - model_13_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4849 - model_12_loss: 0.4346 - model_13_loss: 0.6916 - model_13_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4858 - model_12_loss: 0.4332 - model_13_loss: 0.6917 - model_13_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9227 - model_13_loss: 0.6930 - model_13_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4871 - model_12_loss: 0.4334 - model_13_loss: 0.6920 - model_13_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4898 - model_12_loss: 0.4313 - model_13_loss: 0.6922 - model_13_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4884 - model_12_loss: 0.4313 - model_13_loss: 0.6918 - model_13_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4907 - model_12_loss: 0.4289 - model_13_loss: 0.6919 - model_13_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4941 - model_12_loss: 0.4274 - model_13_loss: 0.6920 - model_13_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9180 - model_13_loss: 0.6921 - model_13_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4934 - model_12_loss: 0.4256 - model_13_loss: 0.6920 - model_13_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4908 - model_12_loss: 0.4259 - model_13_loss: 0.6918 - model_13_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4890 - model_12_loss: 0.4275 - model_13_loss: 0.6919 - model_13_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4907 - model_12_loss: 0.4270 - model_13_loss: 0.6918 - model_13_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4927 - model_12_loss: 0.4251 - model_13_loss: 0.6918 - model_13_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9113 - model_13_loss: 0.6917 - model_13_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4858 - model_12_loss: 0.4248 - model_13_loss: 0.6918 - model_13_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4863 - model_12_loss: 0.4245 - model_13_loss: 0.6920 - model_13_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4842 - model_12_loss: 0.4247 - model_13_loss: 0.6919 - model_13_1_loss: 0.6899\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4849 - model_12_loss: 0.4255 - model_13_loss: 0.6920 - model_13_1_loss: 0.6901\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4887 - model_12_loss: 0.4242 - model_13_loss: 0.6921 - model_13_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9120 - model_13_loss: 0.6915 - model_13_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4868 - model_12_loss: 0.4233 - model_13_loss: 0.6916 - model_13_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4858 - model_12_loss: 0.4244 - model_13_loss: 0.6914 - model_13_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4897 - model_12_loss: 0.4248 - model_13_loss: 0.6919 - model_13_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4899 - model_12_loss: 0.4259 - model_13_loss: 0.6917 - model_13_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4919 - model_12_loss: 0.4252 - model_13_loss: 0.6917 - model_13_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9185 - model_13_loss: 0.6927 - model_13_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4937 - model_12_loss: 0.4238 - model_13_loss: 0.6920 - model_13_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4904 - model_12_loss: 0.4272 - model_13_loss: 0.6919 - model_13_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4922 - model_12_loss: 0.4268 - model_13_loss: 0.6921 - model_13_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4916 - model_12_loss: 0.4289 - model_13_loss: 0.6920 - model_13_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4967 - model_12_loss: 0.4280 - model_13_loss: 0.6923 - model_13_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9197 - model_13_loss: 0.6921 - model_13_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4838 - model_12_loss: 0.4303 - model_13_loss: 0.6915 - model_13_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4822 - model_12_loss: 0.4313 - model_13_loss: 0.6917 - model_13_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4826 - model_12_loss: 0.4320 - model_13_loss: 0.6916 - model_13_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4821 - model_12_loss: 0.4332 - model_13_loss: 0.6915 - model_13_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4819 - model_12_loss: 0.4355 - model_13_loss: 0.6918 - model_13_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9162 - model_13_loss: 0.6917 - model_13_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4813 - model_12_loss: 0.4354 - model_13_loss: 0.6918 - model_13_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4823 - model_12_loss: 0.4360 - model_13_loss: 0.6919 - model_13_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4853 - model_12_loss: 0.4354 - model_13_loss: 0.6922 - model_13_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4847 - model_12_loss: 0.4358 - model_13_loss: 0.6918 - model_13_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4902 - model_12_loss: 0.4354 - model_13_loss: 0.6925 - model_13_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9242 - model_13_loss: 0.6926 - model_13_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4835 - model_12_loss: 0.4360 - model_13_loss: 0.6924 - model_13_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4873 - model_12_loss: 0.4356 - model_13_loss: 0.6928 - model_13_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4877 - model_12_loss: 0.4367 - model_13_loss: 0.6928 - model_13_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4914 - model_12_loss: 0.4346 - model_13_loss: 0.6930 - model_13_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4890 - model_12_loss: 0.4365 - model_13_loss: 0.6927 - model_13_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9291 - model_13_loss: 0.6927 - model_13_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4920 - model_12_loss: 0.4348 - model_13_loss: 0.6927 - model_13_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4898 - model_12_loss: 0.4348 - model_13_loss: 0.6925 - model_13_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4907 - model_12_loss: 0.4348 - model_13_loss: 0.6926 - model_13_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4904 - model_12_loss: 0.4352 - model_13_loss: 0.6926 - model_13_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4946 - model_12_loss: 0.4337 - model_13_loss: 0.6927 - model_13_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9242 - model_13_loss: 0.6921 - model_13_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4878 - model_12_loss: 0.4313 - model_13_loss: 0.6923 - model_13_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4855 - model_12_loss: 0.4317 - model_13_loss: 0.6922 - model_13_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4828 - model_12_loss: 0.4323 - model_13_loss: 0.6921 - model_13_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4836 - model_12_loss: 0.4318 - model_13_loss: 0.6920 - model_13_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4867 - model_12_loss: 0.4305 - model_13_loss: 0.6923 - model_13_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9152 - model_13_loss: 0.6917 - model_13_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4851 - model_12_loss: 0.4310 - model_13_loss: 0.6923 - model_13_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4856 - model_12_loss: 0.4292 - model_13_loss: 0.6922 - model_13_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4888 - model_12_loss: 0.4267 - model_13_loss: 0.6923 - model_13_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4856 - model_12_loss: 0.4288 - model_13_loss: 0.6920 - model_13_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4900 - model_12_loss: 0.4280 - model_13_loss: 0.6922 - model_13_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9214 - model_13_loss: 0.6927 - model_13_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4890 - model_12_loss: 0.4290 - model_13_loss: 0.6923 - model_13_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4915 - model_12_loss: 0.4287 - model_13_loss: 0.6924 - model_13_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4935 - model_12_loss: 0.4285 - model_13_loss: 0.6924 - model_13_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4951 - model_12_loss: 0.4285 - model_13_loss: 0.6923 - model_13_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4952 - model_12_loss: 0.4301 - model_13_loss: 0.6925 - model_13_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9245 - model_13_loss: 0.6932 - model_13_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4945 - model_12_loss: 0.4290 - model_13_loss: 0.6925 - model_13_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4918 - model_12_loss: 0.4299 - model_13_loss: 0.6922 - model_13_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4940 - model_12_loss: 0.4300 - model_13_loss: 0.6922 - model_13_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4932 - model_12_loss: 0.4306 - model_13_loss: 0.6922 - model_13_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4923 - model_12_loss: 0.4308 - model_13_loss: 0.6922 - model_13_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9223 - model_13_loss: 0.6924 - model_13_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4882 - model_12_loss: 0.4295 - model_13_loss: 0.6920 - model_13_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4872 - model_12_loss: 0.4308 - model_13_loss: 0.6920 - model_13_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4852 - model_12_loss: 0.4322 - model_13_loss: 0.6919 - model_13_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4886 - model_12_loss: 0.4301 - model_13_loss: 0.6920 - model_13_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4862 - model_12_loss: 0.4328 - model_13_loss: 0.6922 - model_13_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9201 - model_13_loss: 0.6914 - model_13_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4874 - model_12_loss: 0.4312 - model_13_loss: 0.6920 - model_13_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4887 - model_12_loss: 0.4298 - model_13_loss: 0.6919 - model_13_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4890 - model_12_loss: 0.4308 - model_13_loss: 0.6922 - model_13_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4925 - model_12_loss: 0.4291 - model_13_loss: 0.6921 - model_13_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4931 - model_12_loss: 0.4302 - model_13_loss: 0.6925 - model_13_1_loss: 0.6922\n",
      "For Attention Module: 0.5\n",
      "features X: 30940 samples, 76 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.3616 - model_17_loss: 0.6585 - model_17_1_loss: 0.6138\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -5.9931 - model_16_loss: 0.3698 - model_17_loss: 0.6594 - model_17_1_loss: 0.6132\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0042 - model_16_loss: 0.3699 - model_17_loss: 0.6598 - model_17_1_loss: 0.6150\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0220 - model_16_loss: 0.3702 - model_17_loss: 0.6605 - model_17_1_loss: 0.6180\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0394 - model_16_loss: 0.3695 - model_17_loss: 0.6611 - model_17_1_loss: 0.6206\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0499 - model_16_loss: 0.3700 - model_17_loss: 0.6616 - model_17_1_loss: 0.6223\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.4363 - model_17_loss: 0.6617 - model_17_1_loss: 0.6255\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0612 - model_16_loss: 0.3709 - model_17_loss: 0.6621 - model_17_1_loss: 0.6243\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0760 - model_16_loss: 0.3705 - model_17_loss: 0.6626 - model_17_1_loss: 0.6267\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0976 - model_16_loss: 0.3720 - model_17_loss: 0.6636 - model_17_1_loss: 0.6303\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0920 - model_16_loss: 0.3728 - model_17_loss: 0.6619 - model_17_1_loss: 0.6310\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1143 - model_16_loss: 0.3730 - model_17_loss: 0.6643 - model_17_1_loss: 0.6331\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.4972 - model_17_loss: 0.6636 - model_17_1_loss: 0.6355\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1210 - model_16_loss: 0.3729 - model_17_loss: 0.6638 - model_17_1_loss: 0.6350\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1368 - model_16_loss: 0.3741 - model_17_loss: 0.6655 - model_17_1_loss: 0.6366\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1436 - model_16_loss: 0.3743 - model_17_loss: 0.6652 - model_17_1_loss: 0.6383\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1534 - model_16_loss: 0.3765 - model_17_loss: 0.6653 - model_17_1_loss: 0.6407\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1731 - model_16_loss: 0.3777 - model_17_loss: 0.6663 - model_17_1_loss: 0.6438\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.5717 - model_17_loss: 0.6678 - model_17_1_loss: 0.6456\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1837 - model_16_loss: 0.3808 - model_17_loss: 0.6676 - model_17_1_loss: 0.6453\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1971 - model_16_loss: 0.3819 - model_17_loss: 0.6683 - model_17_1_loss: 0.6475\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2118 - model_16_loss: 0.3837 - model_17_loss: 0.6694 - model_17_1_loss: 0.6497\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2284 - model_16_loss: 0.3838 - model_17_loss: 0.6697 - model_17_1_loss: 0.6528\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2371 - model_16_loss: 0.3862 - model_17_loss: 0.6701 - model_17_1_loss: 0.6546\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.6463 - model_17_loss: 0.6718 - model_17_1_loss: 0.6575\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2496 - model_16_loss: 0.3871 - model_17_loss: 0.6710 - model_17_1_loss: 0.6563\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2585 - model_16_loss: 0.3895 - model_17_loss: 0.6720 - model_17_1_loss: 0.6576\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2702 - model_16_loss: 0.3925 - model_17_loss: 0.6728 - model_17_1_loss: 0.6598\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2868 - model_16_loss: 0.3939 - model_17_loss: 0.6733 - model_17_1_loss: 0.6629\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.2989 - model_16_loss: 0.3955 - model_17_loss: 0.6734 - model_17_1_loss: 0.6654\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.7105 - model_17_loss: 0.6751 - model_17_1_loss: 0.6665\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3158 - model_16_loss: 0.3961 - model_17_loss: 0.6754 - model_17_1_loss: 0.6670\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3282 - model_16_loss: 0.3994 - model_17_loss: 0.6771 - model_17_1_loss: 0.6685\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3375 - model_16_loss: 0.4018 - model_17_loss: 0.6774 - model_17_1_loss: 0.6704\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3495 - model_16_loss: 0.4040 - model_17_loss: 0.6783 - model_17_1_loss: 0.6724\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3617 - model_16_loss: 0.4071 - model_17_loss: 0.6793 - model_17_1_loss: 0.6745\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.7810 - model_17_loss: 0.6804 - model_17_1_loss: 0.6760\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3621 - model_16_loss: 0.4092 - model_17_loss: 0.6789 - model_17_1_loss: 0.6753\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3640 - model_16_loss: 0.4126 - model_17_loss: 0.6793 - model_17_1_loss: 0.6760\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3774 - model_16_loss: 0.4145 - model_17_loss: 0.6807 - model_17_1_loss: 0.6777\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.3975 - model_16_loss: 0.4170 - model_17_loss: 0.6822 - model_17_1_loss: 0.6807\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3958 - model_16_loss: 0.4211 - model_17_loss: 0.6819 - model_17_1_loss: 0.6815\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.8323 - model_17_loss: 0.6831 - model_17_1_loss: 0.6829\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4021 - model_16_loss: 0.4224 - model_17_loss: 0.6831 - model_17_1_loss: 0.6818\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4108 - model_16_loss: 0.4243 - model_17_loss: 0.6839 - model_17_1_loss: 0.6831\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4117 - model_16_loss: 0.4277 - model_17_loss: 0.6834 - model_17_1_loss: 0.6844\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4191 - model_16_loss: 0.4293 - model_17_loss: 0.6836 - model_17_1_loss: 0.6860\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4212 - model_16_loss: 0.4332 - model_17_loss: 0.6844 - model_17_1_loss: 0.6865\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.8619 - model_17_loss: 0.6855 - model_17_1_loss: 0.6870\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4244 - model_16_loss: 0.4346 - model_17_loss: 0.6853 - model_17_1_loss: 0.6865\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4282 - model_16_loss: 0.4376 - model_17_loss: 0.6859 - model_17_1_loss: 0.6872\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4368 - model_16_loss: 0.4397 - model_17_loss: 0.6863 - model_17_1_loss: 0.6890\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4389 - model_16_loss: 0.4430 - model_17_loss: 0.6870 - model_17_1_loss: 0.6894\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4437 - model_16_loss: 0.4451 - model_17_loss: 0.6876 - model_17_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.8929 - model_17_loss: 0.6887 - model_17_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4362 - model_16_loss: 0.4486 - model_17_loss: 0.6875 - model_17_1_loss: 0.6894\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4395 - model_16_loss: 0.4499 - model_17_loss: 0.6880 - model_17_1_loss: 0.6899\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4408 - model_16_loss: 0.4512 - model_17_loss: 0.6880 - model_17_1_loss: 0.6904\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4472 - model_16_loss: 0.4523 - model_17_loss: 0.6892 - model_17_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4406 - model_16_loss: 0.4561 - model_17_loss: 0.6883 - model_17_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9013 - model_17_loss: 0.6896 - model_17_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4441 - model_16_loss: 0.4581 - model_17_loss: 0.6897 - model_17_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4476 - model_16_loss: 0.4577 - model_17_loss: 0.6899 - model_17_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4506 - model_16_loss: 0.4570 - model_17_loss: 0.6904 - model_17_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4534 - model_16_loss: 0.4574 - model_17_loss: 0.6909 - model_17_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4495 - model_16_loss: 0.4597 - model_17_loss: 0.6905 - model_17_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9108 - model_17_loss: 0.6906 - model_17_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4484 - model_16_loss: 0.4583 - model_17_loss: 0.6902 - model_17_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4509 - model_16_loss: 0.4583 - model_17_loss: 0.6906 - model_17_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4535 - model_16_loss: 0.4573 - model_17_loss: 0.6906 - model_17_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4552 - model_16_loss: 0.4563 - model_17_loss: 0.6910 - model_17_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4580 - model_16_loss: 0.4557 - model_17_loss: 0.6912 - model_17_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9131 - model_17_loss: 0.6911 - model_17_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4596 - model_16_loss: 0.4534 - model_17_loss: 0.6911 - model_17_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4623 - model_16_loss: 0.4512 - model_17_loss: 0.6911 - model_17_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4601 - model_16_loss: 0.4534 - model_17_loss: 0.6911 - model_17_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4640 - model_16_loss: 0.4525 - model_17_loss: 0.6912 - model_17_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4675 - model_16_loss: 0.4512 - model_17_loss: 0.6917 - model_17_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9184 - model_17_loss: 0.6918 - model_17_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4706 - model_16_loss: 0.4497 - model_17_loss: 0.6918 - model_17_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4685 - model_16_loss: 0.4484 - model_17_loss: 0.6913 - model_17_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4706 - model_16_loss: 0.4462 - model_17_loss: 0.6911 - model_17_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4699 - model_16_loss: 0.4477 - model_17_loss: 0.6917 - model_17_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4738 - model_16_loss: 0.4449 - model_17_loss: 0.6915 - model_17_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9321 - model_17_loss: 0.6956 - model_17_1_loss: 0.690 - 0s 12us/sample - loss: 6.9185 - model_17_loss: 0.6915 - model_17_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4717 - model_16_loss: 0.4446 - model_17_loss: 0.6920 - model_17_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4718 - model_16_loss: 0.4439 - model_17_loss: 0.6915 - model_17_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4770 - model_16_loss: 0.4420 - model_17_loss: 0.6922 - model_17_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4730 - model_16_loss: 0.4429 - model_17_loss: 0.6920 - model_17_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4775 - model_16_loss: 0.4394 - model_17_loss: 0.6916 - model_17_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9201 - model_17_loss: 0.6921 - model_17_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4723 - model_16_loss: 0.4421 - model_17_loss: 0.6917 - model_17_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4721 - model_16_loss: 0.4407 - model_17_loss: 0.6915 - model_17_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4783 - model_16_loss: 0.4384 - model_17_loss: 0.6915 - model_17_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4774 - model_16_loss: 0.4397 - model_17_loss: 0.6917 - model_17_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4805 - model_16_loss: 0.4388 - model_17_loss: 0.6924 - model_17_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9165 - model_17_loss: 0.6929 - model_17_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4770 - model_16_loss: 0.4389 - model_17_loss: 0.6921 - model_17_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4727 - model_16_loss: 0.4403 - model_17_loss: 0.6917 - model_17_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4735 - model_16_loss: 0.4401 - model_17_loss: 0.6917 - model_17_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4774 - model_16_loss: 0.4400 - model_17_loss: 0.6922 - model_17_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4782 - model_16_loss: 0.4404 - model_17_loss: 0.6925 - model_17_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9219 - model_17_loss: 0.6923 - model_17_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4772 - model_16_loss: 0.4417 - model_17_loss: 0.6923 - model_17_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4754 - model_16_loss: 0.4429 - model_17_loss: 0.6924 - model_17_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4762 - model_16_loss: 0.4430 - model_17_loss: 0.6925 - model_17_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4743 - model_16_loss: 0.4450 - model_17_loss: 0.6924 - model_17_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4767 - model_16_loss: 0.4440 - model_17_loss: 0.6921 - model_17_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9249 - model_17_loss: 0.6930 - model_17_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4784 - model_16_loss: 0.4462 - model_17_loss: 0.6925 - model_17_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4777 - model_16_loss: 0.4462 - model_17_loss: 0.6925 - model_17_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4810 - model_16_loss: 0.4474 - model_17_loss: 0.6928 - model_17_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4814 - model_16_loss: 0.4462 - model_17_loss: 0.6928 - model_17_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4828 - model_16_loss: 0.4448 - model_17_loss: 0.6927 - model_17_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9293 - model_17_loss: 0.6940 - model_17_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4787 - model_16_loss: 0.4469 - model_17_loss: 0.6926 - model_17_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4780 - model_16_loss: 0.4497 - model_17_loss: 0.6929 - model_17_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4797 - model_16_loss: 0.4476 - model_17_loss: 0.6929 - model_17_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4832 - model_16_loss: 0.4457 - model_17_loss: 0.6928 - model_17_1_loss: 0.6930\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4808 - model_16_loss: 0.4457 - model_17_loss: 0.6925 - model_17_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9279 - model_17_loss: 0.6920 - model_17_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4815 - model_16_loss: 0.4466 - model_17_loss: 0.6930 - model_17_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4844 - model_16_loss: 0.4445 - model_17_loss: 0.6930 - model_17_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4824 - model_16_loss: 0.4450 - model_17_loss: 0.6928 - model_17_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4846 - model_16_loss: 0.4431 - model_17_loss: 0.6928 - model_17_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4824 - model_16_loss: 0.4462 - model_17_loss: 0.6929 - model_17_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9264 - model_17_loss: 0.6926 - model_17_1_loss: 0.69240s - loss: 6.8981 - model_17_loss: 0.6892 - model_17_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4825 - model_16_loss: 0.4444 - model_17_loss: 0.6927 - model_17_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4826 - model_16_loss: 0.4419 - model_17_loss: 0.6924 - model_17_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4855 - model_16_loss: 0.4400 - model_17_loss: 0.6925 - model_17_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4830 - model_16_loss: 0.4425 - model_17_loss: 0.6927 - model_17_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4850 - model_16_loss: 0.4422 - model_17_loss: 0.6928 - model_17_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9263 - model_17_loss: 0.6924 - model_17_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4837 - model_16_loss: 0.4421 - model_17_loss: 0.6927 - model_17_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4812 - model_16_loss: 0.4432 - model_17_loss: 0.6925 - model_17_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4853 - model_16_loss: 0.4402 - model_17_loss: 0.6925 - model_17_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4845 - model_16_loss: 0.4427 - model_17_loss: 0.6928 - model_17_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4863 - model_16_loss: 0.4402 - model_17_loss: 0.6926 - model_17_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9266 - model_17_loss: 0.6926 - model_17_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4829 - model_16_loss: 0.4403 - model_17_loss: 0.6925 - model_17_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4838 - model_16_loss: 0.4396 - model_17_loss: 0.6924 - model_17_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4813 - model_16_loss: 0.4422 - model_17_loss: 0.6925 - model_17_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4817 - model_16_loss: 0.4416 - model_17_loss: 0.6924 - model_17_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4839 - model_16_loss: 0.4394 - model_17_loss: 0.6925 - model_17_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9224 - model_17_loss: 0.6922 - model_17_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4839 - model_16_loss: 0.4379 - model_17_loss: 0.6923 - model_17_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4858 - model_16_loss: 0.4364 - model_17_loss: 0.6923 - model_17_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4844 - model_16_loss: 0.4380 - model_17_loss: 0.6925 - model_17_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4804 - model_16_loss: 0.4392 - model_17_loss: 0.6920 - model_17_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4815 - model_16_loss: 0.4392 - model_17_loss: 0.6922 - model_17_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9218 - model_17_loss: 0.6928 - model_17_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4831 - model_16_loss: 0.4366 - model_17_loss: 0.6922 - model_17_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4826 - model_16_loss: 0.4379 - model_17_loss: 0.6924 - model_17_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4835 - model_16_loss: 0.4375 - model_17_loss: 0.6925 - model_17_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4839 - model_16_loss: 0.4379 - model_17_loss: 0.6925 - model_17_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4849 - model_16_loss: 0.4352 - model_17_loss: 0.6923 - model_17_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9229 - model_17_loss: 0.6922 - model_17_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4883 - model_16_loss: 0.4374 - model_17_loss: 0.6926 - model_17_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4912 - model_16_loss: 0.4349 - model_17_loss: 0.6926 - model_17_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4895 - model_16_loss: 0.4355 - model_17_loss: 0.6925 - model_17_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4918 - model_16_loss: 0.4351 - model_17_loss: 0.6928 - model_17_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4898 - model_16_loss: 0.4355 - model_17_loss: 0.6924 - model_17_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9221 - model_17_loss: 0.6930 - model_17_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4831 - model_16_loss: 0.4369 - model_17_loss: 0.6925 - model_17_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4857 - model_16_loss: 0.4357 - model_17_loss: 0.6926 - model_17_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4846 - model_16_loss: 0.4364 - model_17_loss: 0.6927 - model_17_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4854 - model_16_loss: 0.4344 - model_17_loss: 0.6924 - model_17_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4869 - model_16_loss: 0.4343 - model_17_loss: 0.6926 - model_17_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9184 - model_17_loss: 0.6920 - model_17_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4847 - model_16_loss: 0.4323 - model_17_loss: 0.6924 - model_17_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4834 - model_16_loss: 0.4343 - model_17_loss: 0.6922 - model_17_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4878 - model_16_loss: 0.4322 - model_17_loss: 0.6924 - model_17_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4829 - model_16_loss: 0.4325 - model_17_loss: 0.6921 - model_17_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4851 - model_16_loss: 0.4321 - model_17_loss: 0.6921 - model_17_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9201 - model_17_loss: 0.6923 - model_17_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4886 - model_16_loss: 0.4347 - model_17_loss: 0.6926 - model_17_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4891 - model_16_loss: 0.4340 - model_17_loss: 0.6928 - model_17_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4884 - model_16_loss: 0.4346 - model_17_loss: 0.6927 - model_17_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4885 - model_16_loss: 0.4354 - model_17_loss: 0.6927 - model_17_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4898 - model_16_loss: 0.4343 - model_17_loss: 0.6924 - model_17_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9223 - model_17_loss: 0.6927 - model_17_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4858 - model_16_loss: 0.4351 - model_17_loss: 0.6921 - model_17_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4883 - model_16_loss: 0.4351 - model_17_loss: 0.6922 - model_17_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4865 - model_16_loss: 0.4360 - model_17_loss: 0.6923 - model_17_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4867 - model_16_loss: 0.4362 - model_17_loss: 0.6923 - model_17_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4889 - model_16_loss: 0.4363 - model_17_loss: 0.6923 - model_17_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9234 - model_17_loss: 0.6923 - model_17_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4891 - model_16_loss: 0.4367 - model_17_loss: 0.6923 - model_17_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4906 - model_16_loss: 0.4368 - model_17_loss: 0.6922 - model_17_1_loss: 0.6932\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4897 - model_16_loss: 0.4367 - model_17_loss: 0.6923 - model_17_1_loss: 0.6930\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4927 - model_16_loss: 0.4341 - model_17_loss: 0.6925 - model_17_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4897 - model_16_loss: 0.4376 - model_17_loss: 0.6923 - model_17_1_loss: 0.6932\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9231 - model_17_loss: 0.6917 - model_17_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4855 - model_16_loss: 0.4367 - model_17_loss: 0.6921 - model_17_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4869 - model_16_loss: 0.4360 - model_17_loss: 0.6920 - model_17_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4880 - model_16_loss: 0.4335 - model_17_loss: 0.6919 - model_17_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4861 - model_16_loss: 0.4348 - model_17_loss: 0.6918 - model_17_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4883 - model_16_loss: 0.4325 - model_17_loss: 0.6918 - model_17_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9184 - model_17_loss: 0.6919 - model_17_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4868 - model_16_loss: 0.4324 - model_17_loss: 0.6918 - model_17_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4837 - model_16_loss: 0.4337 - model_17_loss: 0.6916 - model_17_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4878 - model_16_loss: 0.4307 - model_17_loss: 0.6918 - model_17_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4882 - model_16_loss: 0.4299 - model_17_loss: 0.6917 - model_17_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4887 - model_16_loss: 0.4295 - model_17_loss: 0.6918 - model_17_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9217 - model_17_loss: 0.6920 - model_17_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4865 - model_16_loss: 0.4295 - model_17_loss: 0.6916 - model_17_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4859 - model_16_loss: 0.4300 - model_17_loss: 0.6917 - model_17_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4886 - model_16_loss: 0.4276 - model_17_loss: 0.6916 - model_17_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4881 - model_16_loss: 0.4276 - model_17_loss: 0.6917 - model_17_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4889 - model_16_loss: 0.4252 - model_17_loss: 0.6915 - model_17_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9143 - model_17_loss: 0.6924 - model_17_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4845 - model_16_loss: 0.4260 - model_17_loss: 0.6915 - model_17_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4854 - model_16_loss: 0.4251 - model_17_loss: 0.6916 - model_17_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4814 - model_16_loss: 0.4265 - model_17_loss: 0.6910 - model_17_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4824 - model_16_loss: 0.4260 - model_17_loss: 0.6914 - model_17_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4883 - model_16_loss: 0.4245 - model_17_loss: 0.6919 - model_17_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9139 - model_17_loss: 0.6923 - model_17_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4863 - model_16_loss: 0.4252 - model_17_loss: 0.6914 - model_17_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4885 - model_16_loss: 0.4266 - model_17_loss: 0.6919 - model_17_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4873 - model_16_loss: 0.4273 - model_17_loss: 0.6917 - model_17_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4883 - model_16_loss: 0.4271 - model_17_loss: 0.6917 - model_17_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4896 - model_16_loss: 0.4295 - model_17_loss: 0.6921 - model_17_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9178 - model_17_loss: 0.6915 - model_17_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4880 - model_16_loss: 0.4293 - model_17_loss: 0.6920 - model_17_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4898 - model_16_loss: 0.4294 - model_17_loss: 0.6921 - model_17_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4877 - model_16_loss: 0.4328 - model_17_loss: 0.6920 - model_17_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4882 - model_16_loss: 0.4322 - model_17_loss: 0.6918 - model_17_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4924 - model_16_loss: 0.4310 - model_17_loss: 0.6920 - model_17_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9268 - model_17_loss: 0.6926 - model_17_1_loss: 0.6931\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4886 - model_16_loss: 0.4346 - model_17_loss: 0.6922 - model_17_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4901 - model_16_loss: 0.4338 - model_17_loss: 0.6920 - model_17_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4877 - model_16_loss: 0.4358 - model_17_loss: 0.6920 - model_17_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4885 - model_16_loss: 0.4362 - model_17_loss: 0.6922 - model_17_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4887 - model_16_loss: 0.4354 - model_17_loss: 0.6919 - model_17_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9258 - model_17_loss: 0.6927 - model_17_1_loss: 0.6931\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4878 - model_16_loss: 0.4367 - model_17_loss: 0.6921 - model_17_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4897 - model_16_loss: 0.4349 - model_17_loss: 0.6921 - model_17_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4894 - model_16_loss: 0.4354 - model_17_loss: 0.6923 - model_17_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4899 - model_16_loss: 0.4357 - model_17_loss: 0.6922 - model_17_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4901 - model_16_loss: 0.4345 - model_17_loss: 0.6921 - model_17_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9253 - model_17_loss: 0.6922 - model_17_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4867 - model_16_loss: 0.4359 - model_17_loss: 0.6919 - model_17_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4870 - model_16_loss: 0.4362 - model_17_loss: 0.6921 - model_17_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4875 - model_16_loss: 0.4340 - model_17_loss: 0.6920 - model_17_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4894 - model_16_loss: 0.4330 - model_17_loss: 0.6921 - model_17_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4883 - model_16_loss: 0.4339 - model_17_loss: 0.6921 - model_17_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9237 - model_17_loss: 0.6919 - model_17_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4886 - model_16_loss: 0.4319 - model_17_loss: 0.6920 - model_17_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4893 - model_16_loss: 0.4316 - model_17_loss: 0.6922 - model_17_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4891 - model_16_loss: 0.4309 - model_17_loss: 0.6920 - model_17_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4902 - model_16_loss: 0.4304 - model_17_loss: 0.6923 - model_17_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4903 - model_16_loss: 0.4304 - model_17_loss: 0.6920 - model_17_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9202 - model_17_loss: 0.6923 - model_17_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4925 - model_16_loss: 0.4299 - model_17_loss: 0.6924 - model_17_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4958 - model_16_loss: 0.4276 - model_17_loss: 0.6926 - model_17_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4933 - model_16_loss: 0.4279 - model_17_loss: 0.6924 - model_17_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4950 - model_16_loss: 0.4255 - model_17_loss: 0.6922 - model_17_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4932 - model_16_loss: 0.4272 - model_17_loss: 0.6923 - model_17_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9190 - model_17_loss: 0.6918 - model_17_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4920 - model_16_loss: 0.4254 - model_17_loss: 0.6918 - model_17_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4896 - model_16_loss: 0.4268 - model_17_loss: 0.6919 - model_17_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4916 - model_16_loss: 0.4253 - model_17_loss: 0.6919 - model_17_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4908 - model_16_loss: 0.4263 - model_17_loss: 0.6920 - model_17_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4909 - model_16_loss: 0.4267 - model_17_loss: 0.6921 - model_17_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9167 - model_17_loss: 0.6921 - model_17_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4895 - model_16_loss: 0.4249 - model_17_loss: 0.6918 - model_17_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4894 - model_16_loss: 0.4251 - model_17_loss: 0.6920 - model_17_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4889 - model_16_loss: 0.4249 - model_17_loss: 0.6918 - model_17_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4867 - model_16_loss: 0.4282 - model_17_loss: 0.6919 - model_17_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4893 - model_16_loss: 0.4262 - model_17_loss: 0.6921 - model_17_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9188 - model_17_loss: 0.6922 - model_17_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4933 - model_16_loss: 0.4260 - model_17_loss: 0.6923 - model_17_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4909 - model_16_loss: 0.4276 - model_17_loss: 0.6921 - model_17_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4926 - model_16_loss: 0.4279 - model_17_loss: 0.6923 - model_17_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4907 - model_16_loss: 0.4285 - model_17_loss: 0.6922 - model_17_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4898 - model_16_loss: 0.4297 - model_17_loss: 0.6921 - model_17_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9219 - model_17_loss: 0.6925 - model_17_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4933 - model_16_loss: 0.4277 - model_17_loss: 0.6922 - model_17_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4902 - model_16_loss: 0.4290 - model_17_loss: 0.6920 - model_17_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4933 - model_16_loss: 0.4297 - model_17_loss: 0.6922 - model_17_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4940 - model_16_loss: 0.4299 - model_17_loss: 0.6924 - model_17_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4919 - model_16_loss: 0.4322 - model_17_loss: 0.6922 - model_17_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9262 - model_17_loss: 0.6925 - model_17_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4908 - model_16_loss: 0.4337 - model_17_loss: 0.6925 - model_17_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4904 - model_16_loss: 0.4338 - model_17_loss: 0.6924 - model_17_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4902 - model_16_loss: 0.4337 - model_17_loss: 0.6923 - model_17_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4921 - model_16_loss: 0.4327 - model_17_loss: 0.6923 - model_17_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4900 - model_16_loss: 0.4339 - model_17_loss: 0.6921 - model_17_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: 6.9277 - model_17_loss: 0.6927 - model_17_1_loss: 0.6932\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4915 - model_16_loss: 0.4336 - model_17_loss: 0.6923 - model_17_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4906 - model_16_loss: 0.4341 - model_17_loss: 0.6923 - model_17_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4890 - model_16_loss: 0.4341 - model_17_loss: 0.6921 - model_17_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4919 - model_16_loss: 0.4324 - model_17_loss: 0.6922 - model_17_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4938 - model_16_loss: 0.4310 - model_17_loss: 0.6923 - model_17_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9247 - model_17_loss: 0.6924 - model_17_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4912 - model_16_loss: 0.4302 - model_17_loss: 0.6921 - model_17_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4926 - model_16_loss: 0.4293 - model_17_loss: 0.6921 - model_17_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4921 - model_16_loss: 0.4299 - model_17_loss: 0.6921 - model_17_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4950 - model_16_loss: 0.4277 - model_17_loss: 0.6921 - model_17_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4946 - model_16_loss: 0.4273 - model_17_loss: 0.6920 - model_17_1_loss: 0.6923\n",
      "For Attention Module: 0.6\n",
      "features X: 30940 samples, 76 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.3527 - model_21_loss: 0.6583 - model_21_1_loss: 0.6130\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -5.9891 - model_20_loss: 0.3723 - model_21_loss: 0.6593 - model_21_1_loss: 0.6130\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -5.9989 - model_20_loss: 0.3699 - model_21_loss: 0.6595 - model_21_1_loss: 0.6143\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0192 - model_20_loss: 0.3691 - model_21_loss: 0.6611 - model_21_1_loss: 0.6166\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0233 - model_20_loss: 0.3700 - model_21_loss: 0.6607 - model_21_1_loss: 0.6180\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0301 - model_20_loss: 0.3723 - model_21_loss: 0.6601 - model_21_1_loss: 0.6204\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.4185 - model_21_loss: 0.6615 - model_21_1_loss: 0.6227\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0387 - model_20_loss: 0.3691 - model_21_loss: 0.6606 - model_21_1_loss: 0.6209\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0625 - model_20_loss: 0.3699 - model_21_loss: 0.6616 - model_21_1_loss: 0.6249\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0681 - model_20_loss: 0.3707 - model_21_loss: 0.6612 - model_21_1_loss: 0.6266\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.0889 - model_20_loss: 0.3722 - model_21_loss: 0.6631 - model_21_1_loss: 0.6292\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1027 - model_20_loss: 0.3721 - model_21_loss: 0.6630 - model_21_1_loss: 0.6320\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.4896 - model_21_loss: 0.6633 - model_21_1_loss: 0.6341\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1128 - model_20_loss: 0.3731 - model_21_loss: 0.6637 - model_21_1_loss: 0.6335\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1173 - model_20_loss: 0.3741 - model_21_loss: 0.6633 - model_21_1_loss: 0.6350\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1412 - model_20_loss: 0.3744 - model_21_loss: 0.6645 - model_21_1_loss: 0.6386\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.1634 - model_20_loss: 0.3747 - model_21_loss: 0.6651 - model_21_1_loss: 0.6425\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1668 - model_20_loss: 0.3769 - model_21_loss: 0.6655 - model_21_1_loss: 0.6432\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.5599 - model_21_loss: 0.6666 - model_21_1_loss: 0.6453\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1902 - model_20_loss: 0.3772 - model_21_loss: 0.6668 - model_21_1_loss: 0.6467\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1973 - model_20_loss: 0.3776 - model_21_loss: 0.6676 - model_21_1_loss: 0.6474\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2154 - model_20_loss: 0.3795 - model_21_loss: 0.6677 - model_21_1_loss: 0.6513\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2205 - model_20_loss: 0.3808 - model_21_loss: 0.6677 - model_21_1_loss: 0.6526\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2476 - model_20_loss: 0.3813 - model_21_loss: 0.6694 - model_21_1_loss: 0.6564\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.6373 - model_21_loss: 0.6708 - model_21_1_loss: 0.6578\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2486 - model_20_loss: 0.3855 - model_21_loss: 0.6694 - model_21_1_loss: 0.6574\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2639 - model_20_loss: 0.3846 - model_21_loss: 0.6710 - model_21_1_loss: 0.6587\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2777 - model_20_loss: 0.3865 - model_21_loss: 0.6714 - model_21_1_loss: 0.6614\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2908 - model_20_loss: 0.3905 - model_21_loss: 0.6712 - model_21_1_loss: 0.6651\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3178 - model_20_loss: 0.3910 - model_21_loss: 0.6743 - model_21_1_loss: 0.6674\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.7156 - model_21_loss: 0.6744 - model_21_1_loss: 0.6689\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3147 - model_20_loss: 0.3925 - model_21_loss: 0.6734 - model_21_1_loss: 0.6680\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3330 - model_20_loss: 0.3951 - model_21_loss: 0.6752 - model_21_1_loss: 0.6704\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3512 - model_20_loss: 0.3954 - model_21_loss: 0.6770 - model_21_1_loss: 0.6723\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3462 - model_20_loss: 0.4006 - model_21_loss: 0.6754 - model_21_1_loss: 0.6740\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3776 - model_20_loss: 0.4032 - model_21_loss: 0.6785 - model_21_1_loss: 0.6777\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.7806 - model_21_loss: 0.6787 - model_21_1_loss: 0.6769\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3714 - model_20_loss: 0.4035 - model_21_loss: 0.6784 - model_21_1_loss: 0.6765\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3750 - model_20_loss: 0.4054 - model_21_loss: 0.6787 - model_21_1_loss: 0.6774\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3906 - model_20_loss: 0.4085 - model_21_loss: 0.6808 - model_21_1_loss: 0.6790\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3896 - model_20_loss: 0.4101 - model_21_loss: 0.6803 - model_21_1_loss: 0.6796\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4054 - model_20_loss: 0.4128 - model_21_loss: 0.6809 - model_21_1_loss: 0.6827\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.8211 - model_21_loss: 0.6818 - model_21_1_loss: 0.6820\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4009 - model_20_loss: 0.4170 - model_21_loss: 0.6821 - model_21_1_loss: 0.6815\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4063 - model_20_loss: 0.4191 - model_21_loss: 0.6826 - model_21_1_loss: 0.6825\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4212 - model_20_loss: 0.4210 - model_21_loss: 0.6842 - model_21_1_loss: 0.6842\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4231 - model_20_loss: 0.4235 - model_21_loss: 0.6843 - model_21_1_loss: 0.6850\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4314 - model_20_loss: 0.4260 - model_21_loss: 0.6850 - model_21_1_loss: 0.6864\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.8588 - model_21_loss: 0.6851 - model_21_1_loss: 0.6861\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4268 - model_20_loss: 0.4283 - model_21_loss: 0.6853 - model_21_1_loss: 0.6858\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4311 - model_20_loss: 0.4310 - model_21_loss: 0.6860 - model_21_1_loss: 0.6864\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4365 - model_20_loss: 0.4349 - model_21_loss: 0.6865 - model_21_1_loss: 0.6878\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4419 - model_20_loss: 0.4385 - model_21_loss: 0.6874 - model_21_1_loss: 0.6887\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4457 - model_20_loss: 0.4394 - model_21_loss: 0.6878 - model_21_1_loss: 0.6892\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.8851 - model_21_loss: 0.6884 - model_21_1_loss: 0.6886\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4434 - model_20_loss: 0.4438 - model_21_loss: 0.6891 - model_21_1_loss: 0.6883\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4446 - model_20_loss: 0.4432 - model_21_loss: 0.6885 - model_21_1_loss: 0.6891\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4467 - model_20_loss: 0.4465 - model_21_loss: 0.6887 - model_21_1_loss: 0.6900\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4560 - model_20_loss: 0.4501 - model_21_loss: 0.6907 - model_21_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4512 - model_20_loss: 0.4542 - model_21_loss: 0.6899 - model_21_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9054 - model_21_loss: 0.6909 - model_21_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4446 - model_20_loss: 0.4556 - model_21_loss: 0.6904 - model_21_1_loss: 0.6897\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4460 - model_20_loss: 0.4579 - model_21_loss: 0.6906 - model_21_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4486 - model_20_loss: 0.4594 - model_21_loss: 0.6912 - model_21_1_loss: 0.6904\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4466 - model_20_loss: 0.4614 - model_21_loss: 0.6909 - model_21_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4498 - model_20_loss: 0.4628 - model_21_loss: 0.6917 - model_21_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9130 - model_21_loss: 0.6916 - model_21_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4477 - model_20_loss: 0.4622 - model_21_loss: 0.6918 - model_21_1_loss: 0.6901\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4517 - model_20_loss: 0.4626 - model_21_loss: 0.6918 - model_21_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4522 - model_20_loss: 0.4625 - model_21_loss: 0.6922 - model_21_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4515 - model_20_loss: 0.4622 - model_21_loss: 0.6920 - model_21_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4534 - model_20_loss: 0.4626 - model_21_loss: 0.6922 - model_21_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9203 - model_21_loss: 0.6924 - model_21_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4541 - model_20_loss: 0.4623 - model_21_loss: 0.6924 - model_21_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4565 - model_20_loss: 0.4611 - model_21_loss: 0.6926 - model_21_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4586 - model_20_loss: 0.4608 - model_21_loss: 0.6922 - model_21_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4620 - model_20_loss: 0.4601 - model_21_loss: 0.6927 - model_21_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4645 - model_20_loss: 0.4613 - model_21_loss: 0.6932 - model_21_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9265 - model_21_loss: 0.6928 - model_21_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4680 - model_20_loss: 0.4588 - model_21_loss: 0.6929 - model_21_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4685 - model_20_loss: 0.4568 - model_21_loss: 0.6927 - model_21_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4708 - model_20_loss: 0.4548 - model_21_loss: 0.6925 - model_21_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4701 - model_20_loss: 0.4564 - model_21_loss: 0.6926 - model_21_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4731 - model_20_loss: 0.4529 - model_21_loss: 0.6926 - model_21_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9255 - model_21_loss: 0.6935 - model_21_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4696 - model_20_loss: 0.4547 - model_21_loss: 0.6923 - model_21_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4763 - model_20_loss: 0.4496 - model_21_loss: 0.6927 - model_21_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4767 - model_20_loss: 0.4508 - model_21_loss: 0.6928 - model_21_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4755 - model_20_loss: 0.4494 - model_21_loss: 0.6925 - model_21_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4767 - model_20_loss: 0.4484 - model_21_loss: 0.6925 - model_21_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9248 - model_21_loss: 0.6927 - model_21_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4746 - model_20_loss: 0.4468 - model_21_loss: 0.6922 - model_21_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4745 - model_20_loss: 0.4469 - model_21_loss: 0.6923 - model_21_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4739 - model_20_loss: 0.4469 - model_21_loss: 0.6918 - model_21_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4732 - model_20_loss: 0.4469 - model_21_loss: 0.6919 - model_21_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4739 - model_20_loss: 0.4449 - model_21_loss: 0.6921 - model_21_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9177 - model_21_loss: 0.6919 - model_21_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4688 - model_20_loss: 0.4443 - model_21_loss: 0.6914 - model_21_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4653 - model_20_loss: 0.4447 - model_21_loss: 0.6910 - model_21_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4680 - model_20_loss: 0.4425 - model_21_loss: 0.6909 - model_21_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4665 - model_20_loss: 0.4445 - model_21_loss: 0.6917 - model_21_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4707 - model_20_loss: 0.4428 - model_21_loss: 0.6915 - model_21_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9091 - model_21_loss: 0.6906 - model_21_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4663 - model_20_loss: 0.4433 - model_21_loss: 0.6910 - model_21_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4666 - model_20_loss: 0.4432 - model_21_loss: 0.6911 - model_21_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4685 - model_20_loss: 0.4431 - model_21_loss: 0.6912 - model_21_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4669 - model_20_loss: 0.4434 - model_21_loss: 0.6913 - model_21_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4717 - model_20_loss: 0.4409 - model_21_loss: 0.6914 - model_21_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9038 - model_21_loss: 0.6906 - model_21_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4592 - model_20_loss: 0.4408 - model_21_loss: 0.6901 - model_21_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4597 - model_20_loss: 0.4414 - model_21_loss: 0.6899 - model_21_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4635 - model_20_loss: 0.4403 - model_21_loss: 0.6905 - model_21_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4682 - model_20_loss: 0.4383 - model_21_loss: 0.6909 - model_21_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4706 - model_20_loss: 0.4392 - model_21_loss: 0.6911 - model_21_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9051 - model_21_loss: 0.6907 - model_21_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4598 - model_20_loss: 0.4381 - model_21_loss: 0.6897 - model_21_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4615 - model_20_loss: 0.4375 - model_21_loss: 0.6897 - model_21_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4611 - model_20_loss: 0.4391 - model_21_loss: 0.6902 - model_21_1_loss: 0.6898\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4653 - model_20_loss: 0.4365 - model_21_loss: 0.6903 - model_21_1_loss: 0.6901\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4680 - model_20_loss: 0.4386 - model_21_loss: 0.6909 - model_21_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9041 - model_21_loss: 0.6907 - model_21_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4578 - model_20_loss: 0.4377 - model_21_loss: 0.6892 - model_21_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4665 - model_20_loss: 0.4368 - model_21_loss: 0.6903 - model_21_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4656 - model_20_loss: 0.4357 - model_21_loss: 0.6900 - model_21_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4716 - model_20_loss: 0.4351 - model_21_loss: 0.6906 - model_21_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4731 - model_20_loss: 0.4369 - model_21_loss: 0.6912 - model_21_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9049 - model_21_loss: 0.6903 - model_21_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4728 - model_20_loss: 0.4360 - model_21_loss: 0.6906 - model_21_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4772 - model_20_loss: 0.4344 - model_21_loss: 0.6911 - model_21_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4760 - model_20_loss: 0.4382 - model_21_loss: 0.6911 - model_21_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4772 - model_20_loss: 0.4357 - model_21_loss: 0.6909 - model_21_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4807 - model_20_loss: 0.4344 - model_21_loss: 0.6915 - model_21_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9119 - model_21_loss: 0.6908 - model_21_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4709 - model_20_loss: 0.4369 - model_21_loss: 0.6904 - model_21_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4736 - model_20_loss: 0.4362 - model_21_loss: 0.6905 - model_21_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4720 - model_20_loss: 0.4374 - model_21_loss: 0.6905 - model_21_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4710 - model_20_loss: 0.4372 - model_21_loss: 0.6902 - model_21_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4732 - model_20_loss: 0.4378 - model_21_loss: 0.6905 - model_21_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9137 - model_21_loss: 0.6910 - model_21_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4710 - model_20_loss: 0.4387 - model_21_loss: 0.6905 - model_21_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4716 - model_20_loss: 0.4374 - model_21_loss: 0.6902 - model_21_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4736 - model_20_loss: 0.4367 - model_21_loss: 0.6904 - model_21_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4725 - model_20_loss: 0.4376 - model_21_loss: 0.6901 - model_21_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4764 - model_20_loss: 0.4360 - model_21_loss: 0.6908 - model_21_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9122 - model_21_loss: 0.6906 - model_21_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4698 - model_20_loss: 0.4370 - model_21_loss: 0.6901 - model_21_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4727 - model_20_loss: 0.4360 - model_21_loss: 0.6900 - model_21_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4725 - model_20_loss: 0.4365 - model_21_loss: 0.6905 - model_21_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4814 - model_20_loss: 0.4334 - model_21_loss: 0.6912 - model_21_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4763 - model_20_loss: 0.4334 - model_21_loss: 0.6903 - model_21_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9139 - model_21_loss: 0.6907 - model_21_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4770 - model_20_loss: 0.4346 - model_21_loss: 0.6908 - model_21_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4778 - model_20_loss: 0.4335 - model_21_loss: 0.6909 - model_21_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4806 - model_20_loss: 0.4346 - model_21_loss: 0.6913 - model_21_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4829 - model_20_loss: 0.4325 - model_21_loss: 0.6915 - model_21_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4815 - model_20_loss: 0.4334 - model_21_loss: 0.6913 - model_21_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9156 - model_21_loss: 0.6911 - model_21_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4804 - model_20_loss: 0.4316 - model_21_loss: 0.6910 - model_21_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4786 - model_20_loss: 0.4333 - model_21_loss: 0.6913 - model_21_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4779 - model_20_loss: 0.4341 - model_21_loss: 0.6910 - model_21_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4829 - model_20_loss: 0.4307 - model_21_loss: 0.6911 - model_21_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4846 - model_20_loss: 0.4310 - model_21_loss: 0.6917 - model_21_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9180 - model_21_loss: 0.6916 - model_21_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4800 - model_20_loss: 0.4315 - model_21_loss: 0.6912 - model_21_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4813 - model_20_loss: 0.4309 - model_21_loss: 0.6915 - model_21_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4825 - model_20_loss: 0.4312 - model_21_loss: 0.6915 - model_21_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4845 - model_20_loss: 0.4295 - model_21_loss: 0.6913 - model_21_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4827 - model_20_loss: 0.4325 - model_21_loss: 0.6917 - model_21_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9169 - model_21_loss: 0.6925 - model_21_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4816 - model_20_loss: 0.4337 - model_21_loss: 0.6917 - model_21_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4819 - model_20_loss: 0.4328 - model_21_loss: 0.6916 - model_21_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4885 - model_20_loss: 0.4293 - model_21_loss: 0.6918 - model_21_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4851 - model_20_loss: 0.4336 - model_21_loss: 0.6919 - model_21_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4875 - model_20_loss: 0.4324 - model_21_loss: 0.6921 - model_21_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9188 - model_21_loss: 0.6916 - model_21_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4830 - model_20_loss: 0.4306 - model_21_loss: 0.6917 - model_21_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4829 - model_20_loss: 0.4318 - model_21_loss: 0.6919 - model_21_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4850 - model_20_loss: 0.4309 - model_21_loss: 0.6918 - model_21_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4826 - model_20_loss: 0.4328 - model_21_loss: 0.6917 - model_21_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4819 - model_20_loss: 0.4328 - model_21_loss: 0.6916 - model_21_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9175 - model_21_loss: 0.6926 - model_21_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4784 - model_20_loss: 0.4342 - model_21_loss: 0.6916 - model_21_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4829 - model_20_loss: 0.4333 - model_21_loss: 0.6918 - model_21_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4838 - model_20_loss: 0.4325 - model_21_loss: 0.6918 - model_21_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4828 - model_20_loss: 0.4341 - model_21_loss: 0.6918 - model_21_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4822 - model_20_loss: 0.4343 - model_21_loss: 0.6919 - model_21_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9184 - model_21_loss: 0.6922 - model_21_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4862 - model_20_loss: 0.4355 - model_21_loss: 0.6924 - model_21_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4880 - model_20_loss: 0.4343 - model_21_loss: 0.6925 - model_21_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4871 - model_20_loss: 0.4346 - model_21_loss: 0.6924 - model_21_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4870 - model_20_loss: 0.4344 - model_21_loss: 0.6921 - model_21_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4896 - model_20_loss: 0.4340 - model_21_loss: 0.6924 - model_21_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9240 - model_21_loss: 0.6918 - model_21_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4850 - model_20_loss: 0.4351 - model_21_loss: 0.6920 - model_21_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4833 - model_20_loss: 0.4359 - model_21_loss: 0.6919 - model_21_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4883 - model_20_loss: 0.4353 - model_21_loss: 0.6924 - model_21_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4833 - model_20_loss: 0.4377 - model_21_loss: 0.6918 - model_21_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4837 - model_20_loss: 0.4381 - model_21_loss: 0.6920 - model_21_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9237 - model_21_loss: 0.6921 - model_21_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4846 - model_20_loss: 0.4379 - model_21_loss: 0.6922 - model_21_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4877 - model_20_loss: 0.4364 - model_21_loss: 0.6925 - model_21_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4870 - model_20_loss: 0.4366 - model_21_loss: 0.6924 - model_21_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4849 - model_20_loss: 0.4378 - model_21_loss: 0.6922 - model_21_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4872 - model_20_loss: 0.4357 - model_21_loss: 0.6922 - model_21_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9252 - model_21_loss: 0.6932 - model_21_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4849 - model_20_loss: 0.4371 - model_21_loss: 0.6922 - model_21_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4872 - model_20_loss: 0.4352 - model_21_loss: 0.6922 - model_21_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4845 - model_20_loss: 0.4381 - model_21_loss: 0.6921 - model_21_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4864 - model_20_loss: 0.4381 - model_21_loss: 0.6924 - model_21_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4861 - model_20_loss: 0.4370 - model_21_loss: 0.6922 - model_21_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9255 - model_21_loss: 0.6925 - model_21_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4875 - model_20_loss: 0.4367 - model_21_loss: 0.6925 - model_21_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4870 - model_20_loss: 0.4375 - model_21_loss: 0.6925 - model_21_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4863 - model_20_loss: 0.4360 - model_21_loss: 0.6923 - model_21_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4878 - model_20_loss: 0.4360 - model_21_loss: 0.6924 - model_21_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4879 - model_20_loss: 0.4348 - model_21_loss: 0.6922 - model_21_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9243 - model_21_loss: 0.6926 - model_21_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4861 - model_20_loss: 0.4369 - model_21_loss: 0.6925 - model_21_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4888 - model_20_loss: 0.4363 - model_21_loss: 0.6927 - model_21_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4871 - model_20_loss: 0.4359 - model_21_loss: 0.6924 - model_21_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4897 - model_20_loss: 0.4326 - model_21_loss: 0.6924 - model_21_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4869 - model_20_loss: 0.4346 - model_21_loss: 0.6923 - model_21_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9243 - model_21_loss: 0.6925 - model_21_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4878 - model_20_loss: 0.4338 - model_21_loss: 0.6923 - model_21_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4876 - model_20_loss: 0.4332 - model_21_loss: 0.6922 - model_21_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4889 - model_20_loss: 0.4309 - model_21_loss: 0.6921 - model_21_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4880 - model_20_loss: 0.4331 - model_21_loss: 0.6923 - model_21_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4877 - model_20_loss: 0.4331 - model_21_loss: 0.6923 - model_21_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9224 - model_21_loss: 0.6929 - model_21_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4899 - model_20_loss: 0.4312 - model_21_loss: 0.6923 - model_21_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4888 - model_20_loss: 0.4313 - model_21_loss: 0.6923 - model_21_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4892 - model_20_loss: 0.4340 - model_21_loss: 0.6926 - model_21_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4879 - model_20_loss: 0.4326 - model_21_loss: 0.6923 - model_21_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4904 - model_20_loss: 0.4303 - model_21_loss: 0.6922 - model_21_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9216 - model_21_loss: 0.6922 - model_21_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4879 - model_20_loss: 0.4310 - model_21_loss: 0.6922 - model_21_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4867 - model_20_loss: 0.4312 - model_21_loss: 0.6920 - model_21_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4887 - model_20_loss: 0.4313 - model_21_loss: 0.6920 - model_21_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4870 - model_20_loss: 0.4321 - model_21_loss: 0.6920 - model_21_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4896 - model_20_loss: 0.4295 - model_21_loss: 0.6921 - model_21_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9205 - model_21_loss: 0.6921 - model_21_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4861 - model_20_loss: 0.4301 - model_21_loss: 0.6916 - model_21_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4899 - model_20_loss: 0.4285 - model_21_loss: 0.6923 - model_21_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4911 - model_20_loss: 0.4282 - model_21_loss: 0.6923 - model_21_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4913 - model_20_loss: 0.4276 - model_21_loss: 0.6923 - model_21_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4880 - model_20_loss: 0.4299 - model_21_loss: 0.6921 - model_21_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9211 - model_21_loss: 0.6923 - model_21_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4912 - model_20_loss: 0.4283 - model_21_loss: 0.6922 - model_21_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4922 - model_20_loss: 0.4283 - model_21_loss: 0.6924 - model_21_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4907 - model_20_loss: 0.4291 - model_21_loss: 0.6924 - model_21_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4884 - model_20_loss: 0.4304 - model_21_loss: 0.6919 - model_21_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4898 - model_20_loss: 0.4308 - model_21_loss: 0.6924 - model_21_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9206 - model_21_loss: 0.6921 - model_21_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4915 - model_20_loss: 0.4290 - model_21_loss: 0.6924 - model_21_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4882 - model_20_loss: 0.4316 - model_21_loss: 0.6921 - model_21_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4904 - model_20_loss: 0.4300 - model_21_loss: 0.6921 - model_21_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4899 - model_20_loss: 0.4305 - model_21_loss: 0.6920 - model_21_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4876 - model_20_loss: 0.4318 - model_21_loss: 0.6921 - model_21_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9246 - model_21_loss: 0.6928 - model_21_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4891 - model_20_loss: 0.4316 - model_21_loss: 0.6923 - model_21_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4932 - model_20_loss: 0.4307 - model_21_loss: 0.6926 - model_21_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4929 - model_20_loss: 0.4295 - model_21_loss: 0.6925 - model_21_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4896 - model_20_loss: 0.4323 - model_21_loss: 0.6923 - model_21_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4916 - model_20_loss: 0.4308 - model_21_loss: 0.6925 - model_21_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9248 - model_21_loss: 0.6932 - model_21_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4886 - model_20_loss: 0.4339 - model_21_loss: 0.6923 - model_21_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4933 - model_20_loss: 0.4313 - model_21_loss: 0.6925 - model_21_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4907 - model_20_loss: 0.4306 - model_21_loss: 0.6921 - model_21_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4930 - model_20_loss: 0.4305 - model_21_loss: 0.6924 - model_21_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4920 - model_20_loss: 0.4308 - model_21_loss: 0.6923 - model_21_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9241 - model_21_loss: 0.6923 - model_21_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4927 - model_20_loss: 0.4308 - model_21_loss: 0.6923 - model_21_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4917 - model_20_loss: 0.4320 - model_21_loss: 0.6925 - model_21_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4881 - model_20_loss: 0.4325 - model_21_loss: 0.6920 - model_21_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4907 - model_20_loss: 0.4301 - model_21_loss: 0.6920 - model_21_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4912 - model_20_loss: 0.4323 - model_21_loss: 0.6925 - model_21_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9230 - model_21_loss: 0.6913 - model_21_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4959 - model_20_loss: 0.4310 - model_21_loss: 0.6928 - model_21_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4916 - model_20_loss: 0.4329 - model_21_loss: 0.6924 - model_21_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4951 - model_20_loss: 0.4308 - model_21_loss: 0.6925 - model_21_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4943 - model_20_loss: 0.4304 - model_21_loss: 0.6924 - model_21_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4952 - model_20_loss: 0.4311 - model_21_loss: 0.6928 - model_21_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9263 - model_21_loss: 0.6935 - model_21_1_loss: 0.69240s - loss: 6.9119 - model_21_loss: 0.6898 - model_21_1_loss: 0.692\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4916 - model_20_loss: 0.4306 - model_21_loss: 0.6922 - model_21_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4910 - model_20_loss: 0.4300 - model_21_loss: 0.6920 - model_21_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4964 - model_20_loss: 0.4292 - model_21_loss: 0.6927 - model_21_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4935 - model_20_loss: 0.4291 - model_21_loss: 0.6926 - model_21_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4926 - model_20_loss: 0.4298 - model_21_loss: 0.6922 - model_21_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9246 - model_21_loss: 0.6929 - model_21_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4907 - model_20_loss: 0.4312 - model_21_loss: 0.6922 - model_21_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4920 - model_20_loss: 0.4308 - model_21_loss: 0.6925 - model_21_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4934 - model_20_loss: 0.4290 - model_21_loss: 0.6924 - model_21_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4940 - model_20_loss: 0.4286 - model_21_loss: 0.6926 - model_21_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4928 - model_20_loss: 0.4292 - model_21_loss: 0.6924 - model_21_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9229 - model_21_loss: 0.6932 - model_21_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4929 - model_20_loss: 0.4284 - model_21_loss: 0.6924 - model_21_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4925 - model_20_loss: 0.4294 - model_21_loss: 0.6925 - model_21_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4932 - model_20_loss: 0.4281 - model_21_loss: 0.6925 - model_21_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4952 - model_20_loss: 0.4275 - model_21_loss: 0.6924 - model_21_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4945 - model_20_loss: 0.4269 - model_21_loss: 0.6926 - model_21_1_loss: 0.6917\n",
      "For Attention Module: 0.7000000000000001\n",
      "features X: 30940 samples, 71 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.3621 - model_25_loss: 0.6604 - model_25_1_loss: 0.6128\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -5.9770 - model_24_loss: 0.3772 - model_25_loss: 0.6589 - model_25_1_loss: 0.6120\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -5.9975 - model_24_loss: 0.3769 - model_25_loss: 0.6599 - model_25_1_loss: 0.6150\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -5.9990 - model_24_loss: 0.3792 - model_25_loss: 0.6605 - model_25_1_loss: 0.6151\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0056 - model_24_loss: 0.3778 - model_25_loss: 0.6594 - model_25_1_loss: 0.6173\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.0184 - model_24_loss: 0.3762 - model_25_loss: 0.6606 - model_25_1_loss: 0.6184\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.4144 - model_25_loss: 0.6618 - model_25_1_loss: 0.6209\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0265 - model_24_loss: 0.3772 - model_25_loss: 0.6607 - model_25_1_loss: 0.6200\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0345 - model_24_loss: 0.3795 - model_25_loss: 0.6603 - model_25_1_loss: 0.6225\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0505 - model_24_loss: 0.3771 - model_25_loss: 0.6619 - model_25_1_loss: 0.6236\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0538 - model_24_loss: 0.3783 - model_25_loss: 0.6610 - model_25_1_loss: 0.6254\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0668 - model_24_loss: 0.3798 - model_25_loss: 0.6627 - model_25_1_loss: 0.6267\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.4569 - model_25_loss: 0.6619 - model_25_1_loss: 0.6299\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0756 - model_24_loss: 0.3798 - model_25_loss: 0.6630 - model_25_1_loss: 0.6281\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.0865 - model_24_loss: 0.3805 - model_25_loss: 0.6636 - model_25_1_loss: 0.6298\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0869 - model_24_loss: 0.3815 - model_25_loss: 0.6625 - model_25_1_loss: 0.6312\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1154 - model_24_loss: 0.3825 - model_25_loss: 0.6643 - model_25_1_loss: 0.6352\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1247 - model_24_loss: 0.3835 - model_25_loss: 0.6640 - model_25_1_loss: 0.6376\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.5152 - model_25_loss: 0.6652 - model_25_1_loss: 0.6379\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1312 - model_24_loss: 0.3858 - model_25_loss: 0.6656 - model_25_1_loss: 0.6378\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1412 - model_24_loss: 0.3858 - model_25_loss: 0.6660 - model_25_1_loss: 0.6394\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1435 - model_24_loss: 0.3885 - model_25_loss: 0.6659 - model_25_1_loss: 0.6404\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1563 - model_24_loss: 0.3883 - model_25_loss: 0.6669 - model_25_1_loss: 0.6420\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1771 - model_24_loss: 0.3915 - model_25_loss: 0.6676 - model_25_1_loss: 0.6461\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.5739 - model_25_loss: 0.6681 - model_25_1_loss: 0.6470\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1773 - model_24_loss: 0.3927 - model_25_loss: 0.6668 - model_25_1_loss: 0.6472\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.1962 - model_24_loss: 0.3964 - model_25_loss: 0.6690 - model_25_1_loss: 0.6495\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2202 - model_24_loss: 0.3968 - model_25_loss: 0.6708 - model_25_1_loss: 0.6526\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2220 - model_24_loss: 0.3973 - model_25_loss: 0.6702 - model_25_1_loss: 0.6537\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2440 - model_24_loss: 0.4007 - model_25_loss: 0.6718 - model_25_1_loss: 0.6571\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.6460 - model_25_loss: 0.6723 - model_25_1_loss: 0.6568\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2385 - model_24_loss: 0.4026 - model_25_loss: 0.6709 - model_25_1_loss: 0.6574\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2464 - model_24_loss: 0.4057 - model_25_loss: 0.6724 - model_25_1_loss: 0.6580\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2563 - model_24_loss: 0.4077 - model_25_loss: 0.6726 - model_25_1_loss: 0.6602\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2686 - model_24_loss: 0.4109 - model_25_loss: 0.6738 - model_25_1_loss: 0.6621\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2846 - model_24_loss: 0.4127 - model_25_loss: 0.6753 - model_25_1_loss: 0.6642\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.7141 - model_25_loss: 0.6769 - model_25_1_loss: 0.6660\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2847 - model_24_loss: 0.4148 - model_25_loss: 0.6747 - model_25_1_loss: 0.6652\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2989 - model_24_loss: 0.4162 - model_25_loss: 0.6762 - model_25_1_loss: 0.6668\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3056 - model_24_loss: 0.4196 - model_25_loss: 0.6762 - model_25_1_loss: 0.6689\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3165 - model_24_loss: 0.4217 - model_25_loss: 0.6773 - model_25_1_loss: 0.6704\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3210 - model_24_loss: 0.4254 - model_25_loss: 0.6778 - model_25_1_loss: 0.6714\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.7560 - model_25_loss: 0.6783 - model_25_1_loss: 0.6726\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3282 - model_24_loss: 0.4267 - model_25_loss: 0.6791 - model_25_1_loss: 0.6719\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3373 - model_24_loss: 0.4281 - model_25_loss: 0.6793 - model_25_1_loss: 0.6738\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3466 - model_24_loss: 0.4327 - model_25_loss: 0.6806 - model_25_1_loss: 0.6752\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3538 - model_24_loss: 0.4352 - model_25_loss: 0.6811 - model_25_1_loss: 0.6767\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3653 - model_24_loss: 0.4359 - model_25_loss: 0.6821 - model_25_1_loss: 0.6782\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.8043 - model_25_loss: 0.6827 - model_25_1_loss: 0.6784\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3606 - model_24_loss: 0.4379 - model_25_loss: 0.6817 - model_25_1_loss: 0.6780\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3703 - model_24_loss: 0.4393 - model_25_loss: 0.6820 - model_25_1_loss: 0.6799\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3789 - model_24_loss: 0.4463 - model_25_loss: 0.6841 - model_25_1_loss: 0.6809\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3762 - model_24_loss: 0.4471 - model_25_loss: 0.6829 - model_25_1_loss: 0.6818\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3865 - model_24_loss: 0.4504 - model_25_loss: 0.6844 - model_25_1_loss: 0.6830\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.8391 - model_25_loss: 0.6849 - model_25_1_loss: 0.6829\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3875 - model_24_loss: 0.4527 - model_25_loss: 0.6852 - model_25_1_loss: 0.6828\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3853 - model_24_loss: 0.4554 - model_25_loss: 0.6848 - model_25_1_loss: 0.6833\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3949 - model_24_loss: 0.4580 - model_25_loss: 0.6861 - model_25_1_loss: 0.6845\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3973 - model_24_loss: 0.4575 - model_25_loss: 0.6861 - model_25_1_loss: 0.6849\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4003 - model_24_loss: 0.4626 - model_25_loss: 0.6864 - model_25_1_loss: 0.6862\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.8696 - model_25_loss: 0.6877 - model_25_1_loss: 0.6862\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4012 - model_24_loss: 0.4659 - model_25_loss: 0.6875 - model_25_1_loss: 0.6859\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4097 - model_24_loss: 0.4688 - model_25_loss: 0.6887 - model_25_1_loss: 0.6870\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4080 - model_24_loss: 0.4688 - model_25_loss: 0.6883 - model_25_1_loss: 0.6870\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4103 - model_24_loss: 0.4699 - model_25_loss: 0.6882 - model_25_1_loss: 0.6878\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4081 - model_24_loss: 0.4743 - model_25_loss: 0.6885 - model_25_1_loss: 0.6880\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.8854 - model_25_loss: 0.6886 - model_25_1_loss: 0.6885\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4086 - model_24_loss: 0.4795 - model_25_loss: 0.6891 - model_25_1_loss: 0.6885\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4142 - model_24_loss: 0.4778 - model_25_loss: 0.6892 - model_25_1_loss: 0.6892\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4135 - model_24_loss: 0.4787 - model_25_loss: 0.6895 - model_25_1_loss: 0.6889\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4182 - model_24_loss: 0.4812 - model_25_loss: 0.6902 - model_25_1_loss: 0.6897\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4214 - model_24_loss: 0.4828 - model_25_loss: 0.6910 - model_25_1_loss: 0.6898\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9054 - model_25_loss: 0.6907 - model_25_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4151 - model_24_loss: 0.4845 - model_25_loss: 0.6901 - model_25_1_loss: 0.6898\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4182 - model_24_loss: 0.4854 - model_25_loss: 0.6902 - model_25_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4227 - model_24_loss: 0.4838 - model_25_loss: 0.6908 - model_25_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4247 - model_24_loss: 0.4864 - model_25_loss: 0.6910 - model_25_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4267 - model_24_loss: 0.4887 - model_25_loss: 0.6917 - model_25_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9148 - model_25_loss: 0.6920 - model_25_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4262 - model_24_loss: 0.4874 - model_25_loss: 0.6914 - model_25_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4272 - model_24_loss: 0.4879 - model_25_loss: 0.6914 - model_25_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4280 - model_24_loss: 0.4873 - model_25_loss: 0.6913 - model_25_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4258 - model_24_loss: 0.4887 - model_25_loss: 0.6911 - model_25_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4272 - model_24_loss: 0.4894 - model_25_loss: 0.6912 - model_25_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9216 - model_25_loss: 0.6921 - model_25_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4295 - model_24_loss: 0.4882 - model_25_loss: 0.6916 - model_25_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4288 - model_24_loss: 0.4878 - model_25_loss: 0.6912 - model_25_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4285 - model_24_loss: 0.4886 - model_25_loss: 0.6915 - model_25_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4337 - model_24_loss: 0.4873 - model_25_loss: 0.6918 - model_25_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4349 - model_24_loss: 0.4871 - model_25_loss: 0.6919 - model_25_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9228 - model_25_loss: 0.6928 - model_25_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4357 - model_24_loss: 0.4863 - model_25_loss: 0.6921 - model_25_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4312 - model_24_loss: 0.4868 - model_25_loss: 0.6914 - model_25_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4367 - model_24_loss: 0.4860 - model_25_loss: 0.6921 - model_25_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4348 - model_24_loss: 0.4853 - model_25_loss: 0.6916 - model_25_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4380 - model_24_loss: 0.4837 - model_25_loss: 0.6918 - model_25_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9192 - model_25_loss: 0.6912 - model_25_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4334 - model_24_loss: 0.4831 - model_25_loss: 0.6912 - model_25_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4333 - model_24_loss: 0.4838 - model_25_loss: 0.6912 - model_25_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4351 - model_24_loss: 0.4832 - model_25_loss: 0.6916 - model_25_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4322 - model_24_loss: 0.4833 - model_25_loss: 0.6911 - model_25_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4339 - model_24_loss: 0.4820 - model_25_loss: 0.6913 - model_25_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9126 - model_25_loss: 0.6907 - model_25_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4268 - model_24_loss: 0.4834 - model_25_loss: 0.6904 - model_25_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4290 - model_24_loss: 0.4819 - model_25_loss: 0.6904 - model_25_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4342 - model_24_loss: 0.4792 - model_25_loss: 0.6911 - model_25_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4287 - model_24_loss: 0.4813 - model_25_loss: 0.6905 - model_25_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4301 - model_24_loss: 0.4814 - model_25_loss: 0.6905 - model_25_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9111 - model_25_loss: 0.6908 - model_25_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4301 - model_24_loss: 0.4794 - model_25_loss: 0.6900 - model_25_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4295 - model_24_loss: 0.4784 - model_25_loss: 0.6900 - model_25_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4302 - model_24_loss: 0.4789 - model_25_loss: 0.6900 - model_25_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4289 - model_24_loss: 0.4797 - model_25_loss: 0.6900 - model_25_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4292 - model_24_loss: 0.4796 - model_25_loss: 0.6902 - model_25_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9090 - model_25_loss: 0.6894 - model_25_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4325 - model_24_loss: 0.4754 - model_25_loss: 0.6901 - model_25_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4292 - model_24_loss: 0.4782 - model_25_loss: 0.6903 - model_25_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4312 - model_24_loss: 0.4772 - model_25_loss: 0.6906 - model_25_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4314 - model_24_loss: 0.4767 - model_25_loss: 0.6903 - model_25_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4340 - model_24_loss: 0.4751 - model_25_loss: 0.6907 - model_25_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9107 - model_25_loss: 0.6910 - model_25_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4318 - model_24_loss: 0.4764 - model_25_loss: 0.6904 - model_25_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4308 - model_24_loss: 0.4759 - model_25_loss: 0.6902 - model_25_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4317 - model_24_loss: 0.4755 - model_25_loss: 0.6900 - model_25_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4362 - model_24_loss: 0.4752 - model_25_loss: 0.6910 - model_25_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4362 - model_24_loss: 0.4722 - model_25_loss: 0.6906 - model_25_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9115 - model_25_loss: 0.6905 - model_25_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4348 - model_24_loss: 0.4740 - model_25_loss: 0.6905 - model_25_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4368 - model_24_loss: 0.4735 - model_25_loss: 0.6910 - model_25_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4408 - model_24_loss: 0.4739 - model_25_loss: 0.6914 - model_25_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4439 - model_24_loss: 0.4733 - model_25_loss: 0.6920 - model_25_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4396 - model_24_loss: 0.4747 - model_25_loss: 0.6915 - model_25_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9160 - model_25_loss: 0.6924 - model_25_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4369 - model_24_loss: 0.4746 - model_25_loss: 0.6913 - model_25_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4387 - model_24_loss: 0.4736 - model_25_loss: 0.6915 - model_25_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4432 - model_24_loss: 0.4738 - model_25_loss: 0.6922 - model_25_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4391 - model_24_loss: 0.4739 - model_25_loss: 0.6917 - model_25_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4426 - model_24_loss: 0.4738 - model_25_loss: 0.6923 - model_25_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9144 - model_25_loss: 0.6926 - model_25_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4396 - model_24_loss: 0.4727 - model_25_loss: 0.6915 - model_25_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4410 - model_24_loss: 0.4724 - model_25_loss: 0.6918 - model_25_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4388 - model_24_loss: 0.4745 - model_25_loss: 0.6917 - model_25_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4389 - model_24_loss: 0.4747 - model_25_loss: 0.6917 - model_25_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4413 - model_24_loss: 0.4735 - model_25_loss: 0.6919 - model_25_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9143 - model_25_loss: 0.6914 - model_25_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4380 - model_24_loss: 0.4723 - model_25_loss: 0.6915 - model_25_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4416 - model_24_loss: 0.4738 - model_25_loss: 0.6918 - model_25_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4417 - model_24_loss: 0.4725 - model_25_loss: 0.6919 - model_25_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4420 - model_24_loss: 0.4717 - model_25_loss: 0.6916 - model_25_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4433 - model_24_loss: 0.4718 - model_25_loss: 0.6917 - model_25_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9160 - model_25_loss: 0.6917 - model_25_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4461 - model_24_loss: 0.4708 - model_25_loss: 0.6919 - model_25_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4464 - model_24_loss: 0.4717 - model_25_loss: 0.6919 - model_25_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4449 - model_24_loss: 0.4728 - model_25_loss: 0.6919 - model_25_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4456 - model_24_loss: 0.4713 - model_25_loss: 0.6918 - model_25_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4488 - model_24_loss: 0.4699 - model_25_loss: 0.6920 - model_25_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9184 - model_25_loss: 0.6919 - model_25_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4471 - model_24_loss: 0.4701 - model_25_loss: 0.6920 - model_25_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4444 - model_24_loss: 0.4702 - model_25_loss: 0.6917 - model_25_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4456 - model_24_loss: 0.4706 - model_25_loss: 0.6918 - model_25_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4469 - model_24_loss: 0.4712 - model_25_loss: 0.6920 - model_25_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4521 - model_24_loss: 0.4695 - model_25_loss: 0.6924 - model_25_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9171 - model_25_loss: 0.6917 - model_25_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4442 - model_24_loss: 0.4726 - model_25_loss: 0.6921 - model_25_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4458 - model_24_loss: 0.4703 - model_25_loss: 0.6919 - model_25_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4469 - model_24_loss: 0.4718 - model_25_loss: 0.6921 - model_25_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4454 - model_24_loss: 0.4702 - model_25_loss: 0.6921 - model_25_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4472 - model_24_loss: 0.4707 - model_25_loss: 0.6923 - model_25_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9173 - model_25_loss: 0.6910 - model_25_1_loss: 0.69160s - loss: 6.9127 - model_25_loss: 0.6910 - model_25_1_loss: 0.691\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4459 - model_24_loss: 0.4700 - model_25_loss: 0.6919 - model_25_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4455 - model_24_loss: 0.4696 - model_25_loss: 0.6919 - model_25_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4465 - model_24_loss: 0.4705 - model_25_loss: 0.6920 - model_25_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4453 - model_24_loss: 0.4717 - model_25_loss: 0.6923 - model_25_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4424 - model_24_loss: 0.4708 - model_25_loss: 0.6916 - model_25_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9170 - model_25_loss: 0.6923 - model_25_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4465 - model_24_loss: 0.4697 - model_25_loss: 0.6920 - model_25_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4473 - model_24_loss: 0.4702 - model_25_loss: 0.6920 - model_25_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4467 - model_24_loss: 0.4696 - model_25_loss: 0.6920 - model_25_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4478 - model_24_loss: 0.4701 - model_25_loss: 0.6918 - model_25_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4491 - model_24_loss: 0.4690 - model_25_loss: 0.6921 - model_25_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9201 - model_25_loss: 0.6922 - model_25_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4470 - model_24_loss: 0.4713 - model_25_loss: 0.6920 - model_25_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4503 - model_24_loss: 0.4696 - model_25_loss: 0.6924 - model_25_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4471 - model_24_loss: 0.4719 - model_25_loss: 0.6920 - model_25_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4479 - model_24_loss: 0.4703 - model_25_loss: 0.6920 - model_25_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4490 - model_24_loss: 0.4712 - model_25_loss: 0.6921 - model_25_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9245 - model_25_loss: 0.6919 - model_25_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4511 - model_24_loss: 0.4715 - model_25_loss: 0.6925 - model_25_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4482 - model_24_loss: 0.4711 - model_25_loss: 0.6921 - model_25_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4490 - model_24_loss: 0.4718 - model_25_loss: 0.6923 - model_25_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4498 - model_24_loss: 0.4714 - model_25_loss: 0.6924 - model_25_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4484 - model_24_loss: 0.4716 - model_25_loss: 0.6920 - model_25_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9227 - model_25_loss: 0.6925 - model_25_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4506 - model_24_loss: 0.4709 - model_25_loss: 0.6923 - model_25_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4498 - model_24_loss: 0.4718 - model_25_loss: 0.6924 - model_25_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4511 - model_24_loss: 0.4692 - model_25_loss: 0.6922 - model_25_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4531 - model_24_loss: 0.4696 - model_25_loss: 0.6926 - model_25_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4488 - model_24_loss: 0.4720 - model_25_loss: 0.6922 - model_25_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9233 - model_25_loss: 0.6923 - model_25_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4471 - model_24_loss: 0.4736 - model_25_loss: 0.6925 - model_25_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4478 - model_24_loss: 0.4710 - model_25_loss: 0.6922 - model_25_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4475 - model_24_loss: 0.4714 - model_25_loss: 0.6923 - model_25_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4487 - model_24_loss: 0.4709 - model_25_loss: 0.6923 - model_25_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4495 - model_24_loss: 0.4699 - model_25_loss: 0.6923 - model_25_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9214 - model_25_loss: 0.6920 - model_25_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4479 - model_24_loss: 0.4714 - model_25_loss: 0.6922 - model_25_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4475 - model_24_loss: 0.4710 - model_25_loss: 0.6923 - model_25_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4492 - model_24_loss: 0.4703 - model_25_loss: 0.6924 - model_25_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4482 - model_24_loss: 0.4698 - model_25_loss: 0.6921 - model_25_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4490 - model_24_loss: 0.4690 - model_25_loss: 0.6922 - model_25_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9184 - model_25_loss: 0.6922 - model_25_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4513 - model_24_loss: 0.4658 - model_25_loss: 0.6920 - model_25_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4481 - model_24_loss: 0.4663 - model_25_loss: 0.6917 - model_25_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4507 - model_24_loss: 0.4650 - model_25_loss: 0.6919 - model_25_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4510 - model_24_loss: 0.4656 - model_25_loss: 0.6922 - model_25_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4479 - model_24_loss: 0.4671 - model_25_loss: 0.6918 - model_25_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9169 - model_25_loss: 0.6935 - model_25_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4504 - model_24_loss: 0.4668 - model_25_loss: 0.6921 - model_25_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4508 - model_24_loss: 0.4643 - model_25_loss: 0.6920 - model_25_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4528 - model_24_loss: 0.4649 - model_25_loss: 0.6922 - model_25_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4511 - model_24_loss: 0.4651 - model_25_loss: 0.6920 - model_25_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4550 - model_24_loss: 0.4642 - model_25_loss: 0.6923 - model_25_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9165 - model_25_loss: 0.6921 - model_25_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4496 - model_24_loss: 0.4648 - model_25_loss: 0.6917 - model_25_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4503 - model_24_loss: 0.4641 - model_25_loss: 0.6920 - model_25_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4522 - model_24_loss: 0.4643 - model_25_loss: 0.6923 - model_25_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4533 - model_24_loss: 0.4647 - model_25_loss: 0.6924 - model_25_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4522 - model_24_loss: 0.4647 - model_25_loss: 0.6922 - model_25_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9189 - model_25_loss: 0.6916 - model_25_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4524 - model_24_loss: 0.4654 - model_25_loss: 0.6923 - model_25_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4527 - model_24_loss: 0.4655 - model_25_loss: 0.6924 - model_25_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4537 - model_24_loss: 0.4643 - model_25_loss: 0.6924 - model_25_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4527 - model_24_loss: 0.4648 - model_25_loss: 0.6922 - model_25_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4518 - model_24_loss: 0.4663 - model_25_loss: 0.6923 - model_25_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9193 - model_25_loss: 0.6931 - model_25_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4495 - model_24_loss: 0.4666 - model_25_loss: 0.6922 - model_25_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 2us/sample - loss: -6.4522 - model_24_loss: 0.4666 - model_25_loss: 0.6925 - model_25_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4519 - model_24_loss: 0.4664 - model_25_loss: 0.6923 - model_25_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4537 - model_24_loss: 0.4662 - model_25_loss: 0.6925 - model_25_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4538 - model_24_loss: 0.4658 - model_25_loss: 0.6923 - model_25_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9204 - model_25_loss: 0.6917 - model_25_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4516 - model_24_loss: 0.4658 - model_25_loss: 0.6924 - model_25_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4507 - model_24_loss: 0.4678 - model_25_loss: 0.6926 - model_25_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4502 - model_24_loss: 0.4684 - model_25_loss: 0.6925 - model_25_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4518 - model_24_loss: 0.4663 - model_25_loss: 0.6923 - model_25_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4493 - model_24_loss: 0.4666 - model_25_loss: 0.6922 - model_25_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9206 - model_25_loss: 0.6931 - model_25_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4505 - model_24_loss: 0.4660 - model_25_loss: 0.6922 - model_25_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4493 - model_24_loss: 0.4673 - model_25_loss: 0.6921 - model_25_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4530 - model_24_loss: 0.4659 - model_25_loss: 0.6925 - model_25_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4517 - model_24_loss: 0.4664 - model_25_loss: 0.6922 - model_25_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4556 - model_24_loss: 0.4646 - model_25_loss: 0.6926 - model_25_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9194 - model_25_loss: 0.6931 - model_25_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4544 - model_24_loss: 0.4659 - model_25_loss: 0.6925 - model_25_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4537 - model_24_loss: 0.4659 - model_25_loss: 0.6923 - model_25_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4552 - model_24_loss: 0.4650 - model_25_loss: 0.6923 - model_25_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4544 - model_24_loss: 0.4662 - model_25_loss: 0.6922 - model_25_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4557 - model_24_loss: 0.4651 - model_25_loss: 0.6924 - model_25_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9218 - model_25_loss: 0.6924 - model_25_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4545 - model_24_loss: 0.4669 - model_25_loss: 0.6926 - model_25_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4535 - model_24_loss: 0.4659 - model_25_loss: 0.6920 - model_25_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4535 - model_24_loss: 0.4658 - model_25_loss: 0.6922 - model_25_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4539 - model_24_loss: 0.4653 - model_25_loss: 0.6923 - model_25_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4563 - model_24_loss: 0.4655 - model_25_loss: 0.6926 - model_25_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9210 - model_25_loss: 0.6927 - model_25_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4537 - model_24_loss: 0.4656 - model_25_loss: 0.6923 - model_25_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4560 - model_24_loss: 0.4651 - model_25_loss: 0.6926 - model_25_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4567 - model_24_loss: 0.4641 - model_25_loss: 0.6924 - model_25_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4527 - model_24_loss: 0.4666 - model_25_loss: 0.6921 - model_25_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4549 - model_24_loss: 0.4667 - model_25_loss: 0.6927 - model_25_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9194 - model_25_loss: 0.6926 - model_25_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4514 - model_24_loss: 0.4661 - model_25_loss: 0.6920 - model_25_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4555 - model_24_loss: 0.4645 - model_25_loss: 0.6923 - model_25_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4542 - model_24_loss: 0.4646 - model_25_loss: 0.6922 - model_25_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4548 - model_24_loss: 0.4650 - model_25_loss: 0.6922 - model_25_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4524 - model_24_loss: 0.4661 - model_25_loss: 0.6919 - model_25_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9206 - model_25_loss: 0.6925 - model_25_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4538 - model_24_loss: 0.4647 - model_25_loss: 0.6923 - model_25_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4535 - model_24_loss: 0.4639 - model_25_loss: 0.6918 - model_25_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4516 - model_24_loss: 0.4662 - model_25_loss: 0.6920 - model_25_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4525 - model_24_loss: 0.4649 - model_25_loss: 0.6919 - model_25_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4542 - model_24_loss: 0.4650 - model_25_loss: 0.6922 - model_25_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9208 - model_25_loss: 0.6923 - model_25_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4546 - model_24_loss: 0.4654 - model_25_loss: 0.6923 - model_25_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4527 - model_24_loss: 0.4668 - model_25_loss: 0.6921 - model_25_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4527 - model_24_loss: 0.4676 - model_25_loss: 0.6924 - model_25_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4554 - model_24_loss: 0.4665 - model_25_loss: 0.6924 - model_25_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4565 - model_24_loss: 0.4654 - model_25_loss: 0.6926 - model_25_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9207 - model_25_loss: 0.6932 - model_25_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4543 - model_24_loss: 0.4657 - model_25_loss: 0.6925 - model_25_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4522 - model_24_loss: 0.4681 - model_25_loss: 0.6922 - model_25_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4520 - model_24_loss: 0.4666 - model_25_loss: 0.6922 - model_25_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4570 - model_24_loss: 0.4640 - model_25_loss: 0.6926 - model_25_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4538 - model_24_loss: 0.4652 - model_25_loss: 0.6921 - model_25_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.9228 - model_25_loss: 0.6926 - model_25_1_loss: 0.69170s - loss: 6.9321 - model_25_loss: 0.6946 - model_25_1_loss: 0.691\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4562 - model_24_loss: 0.4646 - model_25_loss: 0.6927 - model_25_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4562 - model_24_loss: 0.4659 - model_25_loss: 0.6927 - model_25_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4542 - model_24_loss: 0.4657 - model_25_loss: 0.6923 - model_25_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4549 - model_24_loss: 0.4652 - model_25_loss: 0.6924 - model_25_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4560 - model_24_loss: 0.4663 - model_25_loss: 0.6926 - model_25_1_loss: 0.6918\n",
      "For Attention Module: 0.8\n",
      "features X: 30940 samples, 90 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.3829 - model_29_loss: 0.6569 - model_29_1_loss: 0.6199\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.0200 - model_28_loss: 0.3591 - model_29_loss: 0.6556 - model_29_1_loss: 0.6203\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0374 - model_28_loss: 0.3572 - model_29_loss: 0.6572 - model_29_1_loss: 0.6217\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0425 - model_28_loss: 0.3581 - model_29_loss: 0.6559 - model_29_1_loss: 0.6243\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.0533 - model_28_loss: 0.3588 - model_29_loss: 0.6554 - model_29_1_loss: 0.6270\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0817 - model_28_loss: 0.3586 - model_29_loss: 0.6585 - model_29_1_loss: 0.6295\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.4436 - model_29_loss: 0.6575 - model_29_1_loss: 0.6309\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0724 - model_28_loss: 0.3599 - model_29_loss: 0.6577 - model_29_1_loss: 0.6288\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.0902 - model_28_loss: 0.3606 - model_29_loss: 0.6585 - model_29_1_loss: 0.6317\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0987 - model_28_loss: 0.3639 - model_29_loss: 0.6589 - model_29_1_loss: 0.6336\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1112 - model_28_loss: 0.3627 - model_29_loss: 0.6595 - model_29_1_loss: 0.6353\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1230 - model_28_loss: 0.3658 - model_29_loss: 0.6595 - model_29_1_loss: 0.6382\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.5138 - model_29_loss: 0.6625 - model_29_1_loss: 0.6409\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1431 - model_28_loss: 0.3660 - model_29_loss: 0.6617 - model_29_1_loss: 0.6401\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1522 - model_28_loss: 0.3654 - model_29_loss: 0.6613 - model_29_1_loss: 0.6422\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1598 - model_28_loss: 0.3676 - model_29_loss: 0.6617 - model_29_1_loss: 0.6438\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1788 - model_28_loss: 0.3681 - model_29_loss: 0.6630 - model_29_1_loss: 0.6464\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1978 - model_28_loss: 0.3659 - model_29_loss: 0.6640 - model_29_1_loss: 0.6487\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.5848 - model_29_loss: 0.6665 - model_29_1_loss: 0.6506\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1997 - model_28_loss: 0.3699 - model_29_loss: 0.6640 - model_29_1_loss: 0.6499\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2153 - model_28_loss: 0.3706 - model_29_loss: 0.6653 - model_29_1_loss: 0.6519\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2252 - model_28_loss: 0.3746 - model_29_loss: 0.6665 - model_29_1_loss: 0.6535\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2408 - model_28_loss: 0.3741 - model_29_loss: 0.6668 - model_29_1_loss: 0.6562\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2571 - model_28_loss: 0.3750 - model_29_loss: 0.6678 - model_29_1_loss: 0.6586\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.6371 - model_29_loss: 0.6672 - model_29_1_loss: 0.6597\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2626 - model_28_loss: 0.3771 - model_29_loss: 0.6684 - model_29_1_loss: 0.6595\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2746 - model_28_loss: 0.3773 - model_29_loss: 0.6691 - model_29_1_loss: 0.6613\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.2869 - model_28_loss: 0.3803 - model_29_loss: 0.6701 - model_29_1_loss: 0.6633\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3098 - model_28_loss: 0.3786 - model_29_loss: 0.6715 - model_29_1_loss: 0.6662\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3147 - model_28_loss: 0.3819 - model_29_loss: 0.6721 - model_29_1_loss: 0.6673\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: 6.7091 - model_29_loss: 0.6741 - model_29_1_loss: 0.6691\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3133 - model_28_loss: 0.3847 - model_29_loss: 0.6718 - model_29_1_loss: 0.6678\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3288 - model_28_loss: 0.3855 - model_29_loss: 0.6728 - model_29_1_loss: 0.6700\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3418 - model_28_loss: 0.3867 - model_29_loss: 0.6740 - model_29_1_loss: 0.6717\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3462 - model_28_loss: 0.3914 - model_29_loss: 0.6744 - model_29_1_loss: 0.6732\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3608 - model_28_loss: 0.3934 - model_29_loss: 0.6757 - model_29_1_loss: 0.6752\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.7666 - model_29_loss: 0.6777 - model_29_1_loss: 0.6761\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3675 - model_28_loss: 0.3961 - model_29_loss: 0.6767 - model_29_1_loss: 0.6761\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3779 - model_28_loss: 0.3970 - model_29_loss: 0.6775 - model_29_1_loss: 0.6775\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3843 - model_28_loss: 0.3982 - model_29_loss: 0.6778 - model_29_1_loss: 0.6787\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3985 - model_28_loss: 0.4016 - model_29_loss: 0.6796 - model_29_1_loss: 0.6805\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4010 - model_28_loss: 0.4020 - model_29_loss: 0.6789 - model_29_1_loss: 0.6817\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.8216 - model_29_loss: 0.6804 - model_29_1_loss: 0.6831\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4072 - model_28_loss: 0.4070 - model_29_loss: 0.6804 - model_29_1_loss: 0.6824\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4176 - model_28_loss: 0.4085 - model_29_loss: 0.6814 - model_29_1_loss: 0.6838\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4246 - model_28_loss: 0.4107 - model_29_loss: 0.6818 - model_29_1_loss: 0.6853\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4312 - model_28_loss: 0.4120 - model_29_loss: 0.6823 - model_29_1_loss: 0.6863\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4426 - model_28_loss: 0.4144 - model_29_loss: 0.6838 - model_29_1_loss: 0.6876\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.8580 - model_29_loss: 0.6842 - model_29_1_loss: 0.6878\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4372 - model_28_loss: 0.4191 - model_29_loss: 0.6838 - model_29_1_loss: 0.6875\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4400 - model_28_loss: 0.4204 - model_29_loss: 0.6842 - model_29_1_loss: 0.6879\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4424 - model_28_loss: 0.4232 - model_29_loss: 0.6843 - model_29_1_loss: 0.6888\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4537 - model_28_loss: 0.4236 - model_29_loss: 0.6858 - model_29_1_loss: 0.6896\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4619 - model_28_loss: 0.4270 - model_29_loss: 0.6872 - model_29_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.8847 - model_29_loss: 0.6866 - model_29_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4536 - model_28_loss: 0.4314 - model_29_loss: 0.6868 - model_29_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4601 - model_28_loss: 0.4329 - model_29_loss: 0.6879 - model_29_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4604 - model_28_loss: 0.4345 - model_29_loss: 0.6876 - model_29_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4671 - model_28_loss: 0.4388 - model_29_loss: 0.6895 - model_29_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4629 - model_28_loss: 0.4424 - model_29_loss: 0.6891 - model_29_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9094 - model_29_loss: 0.6895 - model_29_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4582 - model_28_loss: 0.4425 - model_29_loss: 0.6886 - model_29_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4634 - model_28_loss: 0.4427 - model_29_loss: 0.6893 - model_29_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4637 - model_28_loss: 0.4448 - model_29_loss: 0.6897 - model_29_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4674 - model_28_loss: 0.4466 - model_29_loss: 0.6904 - model_29_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4717 - model_28_loss: 0.4501 - model_29_loss: 0.6915 - model_29_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9236 - model_29_loss: 0.6929 - model_29_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4677 - model_28_loss: 0.4508 - model_29_loss: 0.6911 - model_29_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4694 - model_28_loss: 0.4527 - model_29_loss: 0.6917 - model_29_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4710 - model_28_loss: 0.4513 - model_29_loss: 0.6917 - model_29_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4753 - model_28_loss: 0.4509 - model_29_loss: 0.6923 - model_29_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4749 - model_28_loss: 0.4530 - model_29_loss: 0.6925 - model_29_1_loss: 0.6931\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9281 - model_29_loss: 0.6927 - model_29_1_loss: 0.6931\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4717 - model_28_loss: 0.4530 - model_29_loss: 0.6919 - model_29_1_loss: 0.6930\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4752 - model_28_loss: 0.4517 - model_29_loss: 0.6924 - model_29_1_loss: 0.6930\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4774 - model_28_loss: 0.4517 - model_29_loss: 0.6927 - model_29_1_loss: 0.6932\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4758 - model_28_loss: 0.4526 - model_29_loss: 0.6925 - model_29_1_loss: 0.6932\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4798 - model_28_loss: 0.4492 - model_29_loss: 0.6927 - model_29_1_loss: 0.6931\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9313 - model_29_loss: 0.6933 - model_29_1_loss: 0.6935\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4806 - model_28_loss: 0.4497 - model_29_loss: 0.6929 - model_29_1_loss: 0.6932\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4813 - model_28_loss: 0.4489 - model_29_loss: 0.6928 - model_29_1_loss: 0.6932\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4834 - model_28_loss: 0.4470 - model_29_loss: 0.6929 - model_29_1_loss: 0.6932\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4864 - model_28_loss: 0.4450 - model_29_loss: 0.6930 - model_29_1_loss: 0.6933\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4866 - model_28_loss: 0.4448 - model_29_loss: 0.6929 - model_29_1_loss: 0.6933\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9293 - model_29_loss: 0.6930 - model_29_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4816 - model_28_loss: 0.4435 - model_29_loss: 0.6924 - model_29_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4808 - model_28_loss: 0.4439 - model_29_loss: 0.6925 - model_29_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4838 - model_28_loss: 0.4403 - model_29_loss: 0.6924 - model_29_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4833 - model_28_loss: 0.4408 - model_29_loss: 0.6925 - model_29_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4862 - model_28_loss: 0.4374 - model_29_loss: 0.6923 - model_29_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9208 - model_29_loss: 0.6919 - model_29_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4796 - model_28_loss: 0.4360 - model_29_loss: 0.6920 - model_29_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4799 - model_28_loss: 0.4362 - model_29_loss: 0.6920 - model_29_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4810 - model_28_loss: 0.4336 - model_29_loss: 0.6918 - model_29_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4801 - model_28_loss: 0.4338 - model_29_loss: 0.6920 - model_29_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4808 - model_28_loss: 0.4315 - model_29_loss: 0.6917 - model_29_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9151 - model_29_loss: 0.6922 - model_29_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4801 - model_28_loss: 0.4314 - model_29_loss: 0.6916 - model_29_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4794 - model_28_loss: 0.4309 - model_29_loss: 0.6912 - model_29_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4836 - model_28_loss: 0.4286 - model_29_loss: 0.6916 - model_29_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4841 - model_28_loss: 0.4276 - model_29_loss: 0.6918 - model_29_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4832 - model_28_loss: 0.4274 - model_29_loss: 0.6914 - model_29_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9102 - model_29_loss: 0.6913 - model_29_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4828 - model_28_loss: 0.4268 - model_29_loss: 0.6912 - model_29_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4829 - model_28_loss: 0.4259 - model_29_loss: 0.6913 - model_29_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4853 - model_28_loss: 0.4255 - model_29_loss: 0.6913 - model_29_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4849 - model_28_loss: 0.4249 - model_29_loss: 0.6914 - model_29_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4863 - model_28_loss: 0.4260 - model_29_loss: 0.6914 - model_29_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9144 - model_29_loss: 0.6913 - model_29_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4862 - model_28_loss: 0.4247 - model_29_loss: 0.6912 - model_29_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4856 - model_28_loss: 0.4237 - model_29_loss: 0.6910 - model_29_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4891 - model_28_loss: 0.4225 - model_29_loss: 0.6910 - model_29_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4904 - model_28_loss: 0.4245 - model_29_loss: 0.6912 - model_29_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4925 - model_28_loss: 0.4241 - model_29_loss: 0.6917 - model_29_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9161 - model_29_loss: 0.6915 - model_29_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4869 - model_28_loss: 0.4243 - model_29_loss: 0.6910 - model_29_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4911 - model_28_loss: 0.4241 - model_29_loss: 0.6916 - model_29_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4932 - model_28_loss: 0.4248 - model_29_loss: 0.6915 - model_29_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4889 - model_28_loss: 0.4260 - model_29_loss: 0.6914 - model_29_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4903 - model_28_loss: 0.4264 - model_29_loss: 0.6913 - model_29_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9179 - model_29_loss: 0.6912 - model_29_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4884 - model_28_loss: 0.4271 - model_29_loss: 0.6914 - model_29_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4916 - model_28_loss: 0.4264 - model_29_loss: 0.6916 - model_29_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4894 - model_28_loss: 0.4281 - model_29_loss: 0.6915 - model_29_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4915 - model_28_loss: 0.4276 - model_29_loss: 0.6916 - model_29_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4911 - model_28_loss: 0.4278 - model_29_loss: 0.6917 - model_29_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9195 - model_29_loss: 0.6913 - model_29_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4905 - model_28_loss: 0.4276 - model_29_loss: 0.6917 - model_29_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4889 - model_28_loss: 0.4295 - model_29_loss: 0.6918 - model_29_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4877 - model_28_loss: 0.4298 - model_29_loss: 0.6915 - model_29_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4901 - model_28_loss: 0.4294 - model_29_loss: 0.6918 - model_29_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4888 - model_28_loss: 0.4292 - model_29_loss: 0.6915 - model_29_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9199 - model_29_loss: 0.6919 - model_29_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4892 - model_28_loss: 0.4303 - model_29_loss: 0.6919 - model_29_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4911 - model_28_loss: 0.4278 - model_29_loss: 0.6916 - model_29_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4887 - model_28_loss: 0.4305 - model_29_loss: 0.6916 - model_29_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4926 - model_28_loss: 0.4283 - model_29_loss: 0.6920 - model_29_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4924 - model_28_loss: 0.4289 - model_29_loss: 0.6919 - model_29_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9234 - model_29_loss: 0.6931 - model_29_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4933 - model_28_loss: 0.4295 - model_29_loss: 0.6919 - model_29_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4952 - model_28_loss: 0.4275 - model_29_loss: 0.6919 - model_29_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4952 - model_28_loss: 0.4284 - model_29_loss: 0.6919 - model_29_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4953 - model_28_loss: 0.4281 - model_29_loss: 0.6921 - model_29_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4956 - model_28_loss: 0.4279 - model_29_loss: 0.6920 - model_29_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9227 - model_29_loss: 0.6921 - model_29_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4920 - model_28_loss: 0.4275 - model_29_loss: 0.6919 - model_29_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4965 - model_28_loss: 0.4247 - model_29_loss: 0.6920 - model_29_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4950 - model_28_loss: 0.4258 - model_29_loss: 0.6919 - model_29_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4950 - model_28_loss: 0.4259 - model_29_loss: 0.6920 - model_29_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4969 - model_28_loss: 0.4241 - model_29_loss: 0.6920 - model_29_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9204 - model_29_loss: 0.6921 - model_29_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4956 - model_28_loss: 0.4243 - model_29_loss: 0.6915 - model_29_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4976 - model_28_loss: 0.4237 - model_29_loss: 0.6920 - model_29_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4977 - model_28_loss: 0.4223 - model_29_loss: 0.6917 - model_29_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4983 - model_28_loss: 0.4231 - model_29_loss: 0.6920 - model_29_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4951 - model_28_loss: 0.4238 - model_29_loss: 0.6916 - model_29_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9220 - model_29_loss: 0.6919 - model_29_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5000 - model_28_loss: 0.4219 - model_29_loss: 0.6921 - model_29_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4979 - model_28_loss: 0.4221 - model_29_loss: 0.6920 - model_29_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4954 - model_28_loss: 0.4246 - model_29_loss: 0.6919 - model_29_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4981 - model_28_loss: 0.4236 - model_29_loss: 0.6923 - model_29_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4965 - model_28_loss: 0.4238 - model_29_loss: 0.6919 - model_29_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9223 - model_29_loss: 0.6925 - model_29_1_loss: 0.69270s - loss: 6.9351 - model_29_loss: 0.6944 - model_29_1_loss: 0.692\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4966 - model_28_loss: 0.4231 - model_29_loss: 0.6919 - model_29_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4985 - model_28_loss: 0.4218 - model_29_loss: 0.6921 - model_29_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4986 - model_28_loss: 0.4225 - model_29_loss: 0.6921 - model_29_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4962 - model_28_loss: 0.4239 - model_29_loss: 0.6920 - model_29_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4991 - model_28_loss: 0.4207 - model_29_loss: 0.6919 - model_29_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9212 - model_29_loss: 0.6920 - model_29_1_loss: 0.69230s - loss: 6.9009 - model_29_loss: 0.6873 - model_29_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4977 - model_28_loss: 0.4211 - model_29_loss: 0.6919 - model_29_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5007 - model_28_loss: 0.4198 - model_29_loss: 0.6920 - model_29_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4997 - model_28_loss: 0.4191 - model_29_loss: 0.6918 - model_29_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5025 - model_28_loss: 0.4179 - model_29_loss: 0.6920 - model_29_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4999 - model_28_loss: 0.4198 - model_29_loss: 0.6918 - model_29_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9196 - model_29_loss: 0.6913 - model_29_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4997 - model_28_loss: 0.4203 - model_29_loss: 0.6918 - model_29_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4992 - model_28_loss: 0.4199 - model_29_loss: 0.6917 - model_29_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5006 - model_28_loss: 0.4187 - model_29_loss: 0.6915 - model_29_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4986 - model_28_loss: 0.4205 - model_29_loss: 0.6919 - model_29_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5013 - model_28_loss: 0.4185 - model_29_loss: 0.6917 - model_29_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9212 - model_29_loss: 0.6925 - model_29_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5046 - model_28_loss: 0.4193 - model_29_loss: 0.6921 - model_29_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5027 - model_28_loss: 0.4182 - model_29_loss: 0.6918 - model_29_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5025 - model_28_loss: 0.4191 - model_29_loss: 0.6918 - model_29_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5036 - model_28_loss: 0.4185 - model_29_loss: 0.6922 - model_29_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5066 - model_28_loss: 0.4182 - model_29_loss: 0.6922 - model_29_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9227 - model_29_loss: 0.6921 - model_29_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5009 - model_28_loss: 0.4196 - model_29_loss: 0.6919 - model_29_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5011 - model_28_loss: 0.4210 - model_29_loss: 0.6922 - model_29_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4986 - model_28_loss: 0.4223 - model_29_loss: 0.6918 - model_29_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5029 - model_28_loss: 0.4214 - model_29_loss: 0.6924 - model_29_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5023 - model_28_loss: 0.4203 - model_29_loss: 0.6920 - model_29_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9236 - model_29_loss: 0.6917 - model_29_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5016 - model_28_loss: 0.4216 - model_29_loss: 0.6920 - model_29_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5029 - model_28_loss: 0.4205 - model_29_loss: 0.6921 - model_29_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5002 - model_28_loss: 0.4237 - model_29_loss: 0.6920 - model_29_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5006 - model_28_loss: 0.4229 - model_29_loss: 0.6920 - model_29_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5007 - model_28_loss: 0.4227 - model_29_loss: 0.6920 - model_29_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9279 - model_29_loss: 0.6928 - model_29_1_loss: 0.6931\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5002 - model_28_loss: 0.4228 - model_29_loss: 0.6920 - model_29_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5029 - model_28_loss: 0.4232 - model_29_loss: 0.6925 - model_29_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4995 - model_28_loss: 0.4245 - model_29_loss: 0.6921 - model_29_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5035 - model_28_loss: 0.4220 - model_29_loss: 0.6924 - model_29_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5024 - model_28_loss: 0.4225 - model_29_loss: 0.6923 - model_29_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9247 - model_29_loss: 0.6917 - model_29_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5036 - model_28_loss: 0.4222 - model_29_loss: 0.6923 - model_29_1_loss: 0.6929\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5032 - model_28_loss: 0.4227 - model_29_loss: 0.6924 - model_29_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5049 - model_28_loss: 0.4188 - model_29_loss: 0.6921 - model_29_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5045 - model_28_loss: 0.4202 - model_29_loss: 0.6924 - model_29_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5070 - model_28_loss: 0.4189 - model_29_loss: 0.6925 - model_29_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9236 - model_29_loss: 0.6924 - model_29_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5043 - model_28_loss: 0.4201 - model_29_loss: 0.6923 - model_29_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5049 - model_28_loss: 0.4187 - model_29_loss: 0.6922 - model_29_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5062 - model_28_loss: 0.4191 - model_29_loss: 0.6926 - model_29_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5043 - model_28_loss: 0.4187 - model_29_loss: 0.6923 - model_29_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5023 - model_28_loss: 0.4206 - model_29_loss: 0.6922 - model_29_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9232 - model_29_loss: 0.6917 - model_29_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5058 - model_28_loss: 0.4193 - model_29_loss: 0.6925 - model_29_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5029 - model_28_loss: 0.4208 - model_29_loss: 0.6923 - model_29_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5050 - model_28_loss: 0.4187 - model_29_loss: 0.6924 - model_29_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5044 - model_28_loss: 0.4196 - model_29_loss: 0.6923 - model_29_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5047 - model_28_loss: 0.4195 - model_29_loss: 0.6924 - model_29_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9241 - model_29_loss: 0.6924 - model_29_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5084 - model_28_loss: 0.4175 - model_29_loss: 0.6924 - model_29_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5046 - model_28_loss: 0.4193 - model_29_loss: 0.6924 - model_29_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5066 - model_28_loss: 0.4179 - model_29_loss: 0.6925 - model_29_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.5055 - model_28_loss: 0.4191 - model_29_loss: 0.6925 - model_29_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5051 - model_28_loss: 0.4195 - model_29_loss: 0.6925 - model_29_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9260 - model_29_loss: 0.6924 - model_29_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5047 - model_28_loss: 0.4202 - model_29_loss: 0.6924 - model_29_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5078 - model_28_loss: 0.4174 - model_29_loss: 0.6922 - model_29_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5056 - model_28_loss: 0.4195 - model_29_loss: 0.6923 - model_29_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5089 - model_28_loss: 0.4178 - model_29_loss: 0.6926 - model_29_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5036 - model_28_loss: 0.4209 - model_29_loss: 0.6922 - model_29_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9249 - model_29_loss: 0.6921 - model_29_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5058 - model_28_loss: 0.4174 - model_29_loss: 0.6922 - model_29_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5070 - model_28_loss: 0.4168 - model_29_loss: 0.6922 - model_29_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5051 - model_28_loss: 0.4180 - model_29_loss: 0.6921 - model_29_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5068 - model_28_loss: 0.4180 - model_29_loss: 0.6924 - model_29_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5079 - model_28_loss: 0.4180 - model_29_loss: 0.6925 - model_29_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9252 - model_29_loss: 0.6924 - model_29_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5061 - model_28_loss: 0.4171 - model_29_loss: 0.6921 - model_29_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5069 - model_28_loss: 0.4170 - model_29_loss: 0.6923 - model_29_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5073 - model_28_loss: 0.4156 - model_29_loss: 0.6922 - model_29_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5073 - model_28_loss: 0.4167 - model_29_loss: 0.6921 - model_29_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5076 - model_28_loss: 0.4160 - model_29_loss: 0.6921 - model_29_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9235 - model_29_loss: 0.6922 - model_29_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5070 - model_28_loss: 0.4155 - model_29_loss: 0.6921 - model_29_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5100 - model_28_loss: 0.4133 - model_29_loss: 0.6922 - model_29_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5072 - model_28_loss: 0.4157 - model_29_loss: 0.6920 - model_29_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5075 - model_28_loss: 0.4133 - model_29_loss: 0.6918 - model_29_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5103 - model_28_loss: 0.4123 - model_29_loss: 0.6923 - model_29_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9168 - model_29_loss: 0.6913 - model_29_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5045 - model_28_loss: 0.4137 - model_29_loss: 0.6917 - model_29_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5052 - model_28_loss: 0.4127 - model_29_loss: 0.6916 - model_29_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5042 - model_28_loss: 0.4133 - model_29_loss: 0.6915 - model_29_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5042 - model_28_loss: 0.4126 - model_29_loss: 0.6915 - model_29_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5058 - model_28_loss: 0.4123 - model_29_loss: 0.6915 - model_29_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9194 - model_29_loss: 0.6920 - model_29_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5062 - model_28_loss: 0.4140 - model_29_loss: 0.6920 - model_29_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5107 - model_28_loss: 0.4103 - model_29_loss: 0.6919 - model_29_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5092 - model_28_loss: 0.4128 - model_29_loss: 0.6920 - model_29_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5093 - model_28_loss: 0.4158 - model_29_loss: 0.6923 - model_29_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5114 - model_28_loss: 0.4143 - model_29_loss: 0.6924 - model_29_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9272 - model_29_loss: 0.6920 - model_29_1_loss: 0.69300s - loss: 6.9286 - model_29_loss: 0.6924 - model_29_1_loss: 0.693\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5073 - model_28_loss: 0.4165 - model_29_loss: 0.6924 - model_29_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5074 - model_28_loss: 0.4186 - model_29_loss: 0.6927 - model_29_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5077 - model_28_loss: 0.4189 - model_29_loss: 0.6928 - model_29_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5102 - model_28_loss: 0.4169 - model_29_loss: 0.6928 - model_29_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5081 - model_28_loss: 0.4211 - model_29_loss: 0.6931 - model_29_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9277 - model_29_loss: 0.6935 - model_29_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5037 - model_28_loss: 0.4218 - model_29_loss: 0.6930 - model_29_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5018 - model_28_loss: 0.4233 - model_29_loss: 0.6929 - model_29_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5054 - model_28_loss: 0.4212 - model_29_loss: 0.6931 - model_29_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5044 - model_28_loss: 0.4204 - model_29_loss: 0.6928 - model_29_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5035 - model_28_loss: 0.4201 - model_29_loss: 0.6930 - model_29_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9244 - model_29_loss: 0.6931 - model_29_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5024 - model_28_loss: 0.4208 - model_29_loss: 0.6928 - model_29_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5036 - model_28_loss: 0.4205 - model_29_loss: 0.6927 - model_29_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5078 - model_28_loss: 0.4178 - model_29_loss: 0.6930 - model_29_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5048 - model_28_loss: 0.4180 - model_29_loss: 0.6927 - model_29_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5089 - model_28_loss: 0.4148 - model_29_loss: 0.6926 - model_29_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9246 - model_29_loss: 0.6935 - model_29_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5066 - model_28_loss: 0.4156 - model_29_loss: 0.6923 - model_29_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5089 - model_28_loss: 0.4134 - model_29_loss: 0.6923 - model_29_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5112 - model_28_loss: 0.4128 - model_29_loss: 0.6924 - model_29_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5099 - model_28_loss: 0.4142 - model_29_loss: 0.6924 - model_29_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5121 - model_28_loss: 0.4137 - model_29_loss: 0.6923 - model_29_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9249 - model_29_loss: 0.6933 - model_29_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5112 - model_28_loss: 0.4108 - model_29_loss: 0.6923 - model_29_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5097 - model_28_loss: 0.4105 - model_29_loss: 0.6917 - model_29_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5102 - model_28_loss: 0.4102 - model_29_loss: 0.6919 - model_29_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5100 - model_28_loss: 0.4103 - model_29_loss: 0.6917 - model_29_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5121 - model_28_loss: 0.4108 - model_29_loss: 0.6920 - model_29_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9205 - model_29_loss: 0.6914 - model_29_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5080 - model_28_loss: 0.4094 - model_29_loss: 0.6915 - model_29_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5046 - model_28_loss: 0.4118 - model_29_loss: 0.6914 - model_29_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5059 - model_28_loss: 0.4107 - model_29_loss: 0.6915 - model_29_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5072 - model_28_loss: 0.4102 - model_29_loss: 0.6915 - model_29_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.5076 - model_28_loss: 0.4114 - model_29_loss: 0.6917 - model_29_1_loss: 0.6920\n",
      "For Attention Module: 0.9\n",
      "features X: 30940 samples, 71 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.3490 - model_33_loss: 0.6614 - model_33_1_loss: 0.6080\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -5.9746 - model_32_loss: 0.3766 - model_33_loss: 0.6608 - model_33_1_loss: 0.6095\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -5.9838 - model_32_loss: 0.3746 - model_33_loss: 0.6606 - model_33_1_loss: 0.6111\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -5.9950 - model_32_loss: 0.3752 - model_33_loss: 0.6618 - model_33_1_loss: 0.6122\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0013 - model_32_loss: 0.3750 - model_33_loss: 0.6609 - model_33_1_loss: 0.6143\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0141 - model_32_loss: 0.3783 - model_33_loss: 0.6619 - model_33_1_loss: 0.6166\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.4079 - model_33_loss: 0.6634 - model_33_1_loss: 0.6187\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.0153 - model_32_loss: 0.3776 - model_33_loss: 0.6617 - model_33_1_loss: 0.6169\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.0411 - model_32_loss: 0.3770 - model_33_loss: 0.6634 - model_33_1_loss: 0.6202\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.0410 - model_32_loss: 0.3777 - model_33_loss: 0.6627 - model_33_1_loss: 0.6210\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.0563 - model_32_loss: 0.3768 - model_33_loss: 0.6637 - model_33_1_loss: 0.6229\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.0647 - model_32_loss: 0.3798 - model_33_loss: 0.6635 - model_33_1_loss: 0.6254\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.4554 - model_33_loss: 0.6644 - model_33_1_loss: 0.6263\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0788 - model_32_loss: 0.3822 - model_33_loss: 0.6650 - model_33_1_loss: 0.6272\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.0886 - model_32_loss: 0.3831 - model_33_loss: 0.6643 - model_33_1_loss: 0.6300\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0988 - model_32_loss: 0.3822 - model_33_loss: 0.6658 - model_33_1_loss: 0.6304\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1175 - model_32_loss: 0.3838 - model_33_loss: 0.6667 - model_33_1_loss: 0.6336\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1279 - model_32_loss: 0.3859 - model_33_loss: 0.6675 - model_33_1_loss: 0.6353\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.5250 - model_33_loss: 0.6690 - model_33_1_loss: 0.6358\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1368 - model_32_loss: 0.3870 - model_33_loss: 0.6677 - model_33_1_loss: 0.6371\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1471 - model_32_loss: 0.3888 - model_33_loss: 0.6686 - model_33_1_loss: 0.6386\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1487 - model_32_loss: 0.3888 - model_33_loss: 0.6674 - model_33_1_loss: 0.6401\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1620 - model_32_loss: 0.3915 - model_33_loss: 0.6689 - model_33_1_loss: 0.6418\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1944 - model_32_loss: 0.3926 - model_33_loss: 0.6708 - model_33_1_loss: 0.6466\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.5892 - model_33_loss: 0.6717 - model_33_1_loss: 0.6467\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1941 - model_32_loss: 0.3953 - model_33_loss: 0.6715 - model_33_1_loss: 0.6464\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2056 - model_32_loss: 0.3955 - model_33_loss: 0.6718 - model_33_1_loss: 0.6484\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2155 - model_32_loss: 0.3980 - model_33_loss: 0.6723 - model_33_1_loss: 0.6504\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2289 - model_32_loss: 0.4010 - model_33_loss: 0.6738 - model_33_1_loss: 0.6522\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2503 - model_32_loss: 0.4020 - model_33_loss: 0.6757 - model_33_1_loss: 0.6547\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.6512 - model_33_loss: 0.6763 - model_33_1_loss: 0.6552\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2535 - model_32_loss: 0.4029 - model_33_loss: 0.6755 - model_33_1_loss: 0.6558\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2540 - model_32_loss: 0.4049 - model_33_loss: 0.6750 - model_33_1_loss: 0.6568\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2729 - model_32_loss: 0.4090 - model_33_loss: 0.6764 - model_33_1_loss: 0.6600\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2973 - model_32_loss: 0.4115 - model_33_loss: 0.6794 - model_33_1_loss: 0.6623\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2932 - model_32_loss: 0.4136 - model_33_loss: 0.6788 - model_33_1_loss: 0.6626\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.7083 - model_33_loss: 0.6798 - model_33_1_loss: 0.6628\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2929 - model_32_loss: 0.4173 - model_33_loss: 0.6789 - model_33_1_loss: 0.6631\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3058 - model_32_loss: 0.4178 - model_33_loss: 0.6800 - model_33_1_loss: 0.6648\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3013 - model_32_loss: 0.4226 - model_33_loss: 0.6793 - model_33_1_loss: 0.6655\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3180 - model_32_loss: 0.4224 - model_33_loss: 0.6812 - model_33_1_loss: 0.6668\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3316 - model_32_loss: 0.4271 - model_33_loss: 0.6824 - model_33_1_loss: 0.6693\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.7595 - model_33_loss: 0.6822 - model_33_1_loss: 0.6702\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3289 - model_32_loss: 0.4265 - model_33_loss: 0.6820 - model_33_1_loss: 0.6691\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3282 - model_32_loss: 0.4324 - model_33_loss: 0.6818 - model_33_1_loss: 0.6703\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3504 - model_32_loss: 0.4347 - model_33_loss: 0.6833 - model_33_1_loss: 0.6737\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3525 - model_32_loss: 0.4372 - model_33_loss: 0.6835 - model_33_1_loss: 0.6745\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3668 - model_32_loss: 0.4420 - model_33_loss: 0.6855 - model_33_1_loss: 0.6763\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.8099 - model_33_loss: 0.6851 - model_33_1_loss: 0.6761\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3666 - model_32_loss: 0.4440 - model_33_loss: 0.6861 - model_33_1_loss: 0.6760\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3711 - model_32_loss: 0.4478 - model_33_loss: 0.6865 - model_33_1_loss: 0.6773\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3723 - model_32_loss: 0.4490 - model_33_loss: 0.6862 - model_33_1_loss: 0.6781\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3812 - model_32_loss: 0.4546 - model_33_loss: 0.6870 - model_33_1_loss: 0.6801\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3852 - model_32_loss: 0.4572 - model_33_loss: 0.6880 - model_33_1_loss: 0.6805\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.8468 - model_33_loss: 0.6884 - model_33_1_loss: 0.6811\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3859 - model_32_loss: 0.4602 - model_33_loss: 0.6881 - model_33_1_loss: 0.6811\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.3901 - model_32_loss: 0.4627 - model_33_loss: 0.6885 - model_33_1_loss: 0.6821\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3900 - model_32_loss: 0.4672 - model_33_loss: 0.6888 - model_33_1_loss: 0.6826\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3976 - model_32_loss: 0.4687 - model_33_loss: 0.6892 - model_33_1_loss: 0.6840\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3981 - model_32_loss: 0.4718 - model_33_loss: 0.6892 - model_33_1_loss: 0.6847\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.8775 - model_33_loss: 0.6897 - model_33_1_loss: 0.6859\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3968 - model_32_loss: 0.4752 - model_33_loss: 0.6895 - model_33_1_loss: 0.6849\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4017 - model_32_loss: 0.4797 - model_33_loss: 0.6901 - model_33_1_loss: 0.6861\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4054 - model_32_loss: 0.4786 - model_33_loss: 0.6902 - model_33_1_loss: 0.6866\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4064 - model_32_loss: 0.4813 - model_33_loss: 0.6906 - model_33_1_loss: 0.6870\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4107 - model_32_loss: 0.4828 - model_33_loss: 0.6907 - model_33_1_loss: 0.6880\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.8983 - model_33_loss: 0.6913 - model_33_1_loss: 0.6881\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4113 - model_32_loss: 0.4837 - model_33_loss: 0.6909 - model_33_1_loss: 0.6881\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4150 - model_32_loss: 0.4868 - model_33_loss: 0.6917 - model_33_1_loss: 0.6887\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4174 - model_32_loss: 0.4850 - model_33_loss: 0.6917 - model_33_1_loss: 0.6888\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4215 - model_32_loss: 0.4859 - model_33_loss: 0.6921 - model_33_1_loss: 0.6893\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4187 - model_32_loss: 0.4880 - model_33_loss: 0.6922 - model_33_1_loss: 0.6892\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9091 - model_33_loss: 0.6923 - model_33_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4214 - model_32_loss: 0.4883 - model_33_loss: 0.6924 - model_33_1_loss: 0.6896\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4271 - model_32_loss: 0.4893 - model_33_loss: 0.6929 - model_33_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4243 - model_32_loss: 0.4910 - model_33_loss: 0.6926 - model_33_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4304 - model_32_loss: 0.4893 - model_33_loss: 0.6934 - model_33_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4309 - model_32_loss: 0.4897 - model_33_loss: 0.6930 - model_33_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9166 - model_33_loss: 0.6924 - model_33_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4285 - model_32_loss: 0.4898 - model_33_loss: 0.6930 - model_33_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4304 - model_32_loss: 0.4885 - model_33_loss: 0.6932 - model_33_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4281 - model_32_loss: 0.4883 - model_33_loss: 0.6927 - model_33_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4321 - model_32_loss: 0.4855 - model_33_loss: 0.6929 - model_33_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4361 - model_32_loss: 0.4846 - model_33_loss: 0.6932 - model_33_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9213 - model_33_loss: 0.6929 - model_33_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4327 - model_32_loss: 0.4842 - model_33_loss: 0.6925 - model_33_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4361 - model_32_loss: 0.4820 - model_33_loss: 0.6925 - model_33_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4359 - model_32_loss: 0.4835 - model_33_loss: 0.6930 - model_33_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4378 - model_32_loss: 0.4821 - model_33_loss: 0.6928 - model_33_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4391 - model_32_loss: 0.4812 - model_33_loss: 0.6933 - model_33_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9191 - model_33_loss: 0.6927 - model_33_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4366 - model_32_loss: 0.4799 - model_33_loss: 0.6923 - model_33_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4387 - model_32_loss: 0.4800 - model_33_loss: 0.6927 - model_33_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4368 - model_32_loss: 0.4796 - model_33_loss: 0.6924 - model_33_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4410 - model_32_loss: 0.4786 - model_33_loss: 0.6926 - model_33_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4400 - model_32_loss: 0.4785 - model_33_loss: 0.6925 - model_33_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9135 - model_33_loss: 0.6927 - model_33_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4383 - model_32_loss: 0.4762 - model_33_loss: 0.6924 - model_33_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4358 - model_32_loss: 0.4764 - model_33_loss: 0.6923 - model_33_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4385 - model_32_loss: 0.4768 - model_33_loss: 0.6926 - model_33_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4388 - model_32_loss: 0.4759 - model_33_loss: 0.6926 - model_33_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4405 - model_32_loss: 0.4742 - model_33_loss: 0.6926 - model_33_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9129 - model_33_loss: 0.6930 - model_33_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4363 - model_32_loss: 0.4733 - model_33_loss: 0.6919 - model_33_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4366 - model_32_loss: 0.4731 - model_33_loss: 0.6917 - model_33_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4367 - model_32_loss: 0.4723 - model_33_loss: 0.6919 - model_33_1_loss: 0.6899\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4340 - model_32_loss: 0.4743 - model_33_loss: 0.6916 - model_33_1_loss: 0.6901\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4398 - model_32_loss: 0.4738 - model_33_loss: 0.6924 - model_33_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9069 - model_33_loss: 0.6915 - model_33_1_loss: 0.68970s - loss: 6.8982 - model_33_loss: 0.6898 - model_33_1_loss: 0.689\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4316 - model_32_loss: 0.4741 - model_33_loss: 0.6914 - model_33_1_loss: 0.6897\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4369 - model_32_loss: 0.4718 - model_33_loss: 0.6917 - model_33_1_loss: 0.6900\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4337 - model_32_loss: 0.4742 - model_33_loss: 0.6915 - model_33_1_loss: 0.6901\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4397 - model_32_loss: 0.4718 - model_33_loss: 0.6921 - model_33_1_loss: 0.6902\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4351 - model_32_loss: 0.4730 - model_33_loss: 0.6915 - model_33_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9094 - model_33_loss: 0.6920 - model_33_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4351 - model_32_loss: 0.4716 - model_33_loss: 0.6912 - model_33_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4361 - model_32_loss: 0.4745 - model_33_loss: 0.6915 - model_33_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4344 - model_32_loss: 0.4727 - model_33_loss: 0.6913 - model_33_1_loss: 0.6901\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4331 - model_32_loss: 0.4752 - model_33_loss: 0.6912 - model_33_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4372 - model_32_loss: 0.4738 - model_33_loss: 0.6918 - model_33_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9087 - model_33_loss: 0.6910 - model_33_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4343 - model_32_loss: 0.4735 - model_33_loss: 0.6912 - model_33_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4327 - model_32_loss: 0.4757 - model_33_loss: 0.6909 - model_33_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4331 - model_32_loss: 0.4773 - model_33_loss: 0.6914 - model_33_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4347 - model_32_loss: 0.4765 - model_33_loss: 0.6914 - model_33_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4370 - model_32_loss: 0.4758 - model_33_loss: 0.6915 - model_33_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9148 - model_33_loss: 0.6921 - model_33_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4339 - model_32_loss: 0.4771 - model_33_loss: 0.6911 - model_33_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4341 - model_32_loss: 0.4778 - model_33_loss: 0.6911 - model_33_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4376 - model_32_loss: 0.4780 - model_33_loss: 0.6917 - model_33_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4353 - model_32_loss: 0.4771 - model_33_loss: 0.6911 - model_33_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4372 - model_32_loss: 0.4799 - model_33_loss: 0.6915 - model_33_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9190 - model_33_loss: 0.6913 - model_33_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4353 - model_32_loss: 0.4783 - model_33_loss: 0.6910 - model_33_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4325 - model_32_loss: 0.4821 - model_33_loss: 0.6914 - model_33_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4396 - model_32_loss: 0.4784 - model_33_loss: 0.6917 - model_33_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4378 - model_32_loss: 0.4811 - model_33_loss: 0.6918 - model_33_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4385 - model_32_loss: 0.4802 - model_33_loss: 0.6916 - model_33_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9183 - model_33_loss: 0.6921 - model_33_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4342 - model_32_loss: 0.4800 - model_33_loss: 0.6913 - model_33_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4406 - model_32_loss: 0.4790 - model_33_loss: 0.6919 - model_33_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4375 - model_32_loss: 0.4807 - model_33_loss: 0.6916 - model_33_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4383 - model_32_loss: 0.4791 - model_33_loss: 0.6919 - model_33_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4387 - model_32_loss: 0.4797 - model_33_loss: 0.6919 - model_33_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9209 - model_33_loss: 0.6915 - model_33_1_loss: 0.69230s - loss: 6.8899 - model_33_loss: 0.6863 - model_33_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4389 - model_32_loss: 0.4785 - model_33_loss: 0.6916 - model_33_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4396 - model_32_loss: 0.4785 - model_33_loss: 0.6918 - model_33_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4398 - model_32_loss: 0.4789 - model_33_loss: 0.6919 - model_33_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4424 - model_32_loss: 0.4772 - model_33_loss: 0.6922 - model_33_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4416 - model_32_loss: 0.4783 - model_33_loss: 0.6922 - model_33_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9214 - model_33_loss: 0.6918 - model_33_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4388 - model_32_loss: 0.4781 - model_33_loss: 0.6918 - model_33_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4423 - model_32_loss: 0.4761 - model_33_loss: 0.6920 - model_33_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4414 - model_32_loss: 0.4769 - model_33_loss: 0.6921 - model_33_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4417 - model_32_loss: 0.4769 - model_33_loss: 0.6921 - model_33_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4397 - model_32_loss: 0.4763 - model_33_loss: 0.6917 - model_33_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9171 - model_33_loss: 0.6923 - model_33_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4423 - model_32_loss: 0.4723 - model_33_loss: 0.6917 - model_33_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4428 - model_32_loss: 0.4733 - model_33_loss: 0.6917 - model_33_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4434 - model_32_loss: 0.4709 - model_33_loss: 0.6917 - model_33_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4422 - model_32_loss: 0.4729 - model_33_loss: 0.6918 - model_33_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4414 - model_32_loss: 0.4723 - model_33_loss: 0.6917 - model_33_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9145 - model_33_loss: 0.6920 - model_33_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4443 - model_32_loss: 0.4703 - model_33_loss: 0.6917 - model_33_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4419 - model_32_loss: 0.4717 - model_33_loss: 0.6916 - model_33_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4412 - model_32_loss: 0.4722 - model_33_loss: 0.6914 - model_33_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4430 - model_32_loss: 0.4710 - model_33_loss: 0.6915 - model_33_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4446 - model_32_loss: 0.4703 - model_33_loss: 0.6916 - model_33_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9149 - model_33_loss: 0.6917 - model_33_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4437 - model_32_loss: 0.4715 - model_33_loss: 0.6918 - model_33_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4478 - model_32_loss: 0.4706 - model_33_loss: 0.6921 - model_33_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4501 - model_32_loss: 0.4712 - model_33_loss: 0.6926 - model_33_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4468 - model_32_loss: 0.4706 - model_33_loss: 0.6919 - model_33_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4511 - model_32_loss: 0.4683 - model_33_loss: 0.6923 - model_33_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9169 - model_33_loss: 0.6917 - model_33_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4457 - model_32_loss: 0.4713 - model_33_loss: 0.6916 - model_33_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4539 - model_32_loss: 0.4693 - model_33_loss: 0.6927 - model_33_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4483 - model_32_loss: 0.4710 - model_33_loss: 0.6920 - model_33_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4494 - model_32_loss: 0.4703 - model_33_loss: 0.6919 - model_33_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4487 - model_32_loss: 0.4699 - model_33_loss: 0.6921 - model_33_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9198 - model_33_loss: 0.6924 - model_33_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4442 - model_32_loss: 0.4719 - model_33_loss: 0.6917 - model_33_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4460 - model_32_loss: 0.4720 - model_33_loss: 0.6924 - model_33_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4452 - model_32_loss: 0.4719 - model_33_loss: 0.6920 - model_33_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4466 - model_32_loss: 0.4707 - model_33_loss: 0.6923 - model_33_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4472 - model_32_loss: 0.4712 - model_33_loss: 0.6921 - model_33_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9207 - model_33_loss: 0.6923 - model_33_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4417 - model_32_loss: 0.4718 - model_33_loss: 0.6918 - model_33_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4483 - model_32_loss: 0.4710 - model_33_loss: 0.6921 - model_33_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4477 - model_32_loss: 0.4697 - model_33_loss: 0.6922 - model_33_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4469 - model_32_loss: 0.4720 - model_33_loss: 0.6920 - model_33_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4503 - model_32_loss: 0.4695 - model_33_loss: 0.6921 - model_33_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9206 - model_33_loss: 0.6920 - model_33_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4446 - model_32_loss: 0.4710 - model_33_loss: 0.6917 - model_33_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4498 - model_32_loss: 0.4706 - model_33_loss: 0.6922 - model_33_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4488 - model_32_loss: 0.4701 - model_33_loss: 0.6921 - model_33_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4482 - model_32_loss: 0.4700 - model_33_loss: 0.6919 - model_33_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4481 - model_32_loss: 0.4723 - model_33_loss: 0.6924 - model_33_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9231 - model_33_loss: 0.6918 - model_33_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4515 - model_32_loss: 0.4691 - model_33_loss: 0.6921 - model_33_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4477 - model_32_loss: 0.4727 - model_33_loss: 0.6921 - model_33_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4483 - model_32_loss: 0.4712 - model_33_loss: 0.6922 - model_33_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4490 - model_32_loss: 0.4708 - model_33_loss: 0.6919 - model_33_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4488 - model_32_loss: 0.4709 - model_33_loss: 0.6922 - model_33_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9204 - model_33_loss: 0.6920 - model_33_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4500 - model_32_loss: 0.4718 - model_33_loss: 0.6925 - model_33_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4500 - model_32_loss: 0.4716 - model_33_loss: 0.6924 - model_33_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4533 - model_32_loss: 0.4704 - model_33_loss: 0.6930 - model_33_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4515 - model_32_loss: 0.4677 - model_33_loss: 0.6921 - model_33_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4490 - model_32_loss: 0.4703 - model_33_loss: 0.6922 - model_33_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9203 - model_33_loss: 0.6923 - model_33_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4515 - model_32_loss: 0.4684 - model_33_loss: 0.6924 - model_33_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4492 - model_32_loss: 0.4676 - model_33_loss: 0.6919 - model_33_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4506 - model_32_loss: 0.4663 - model_33_loss: 0.6921 - model_33_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4513 - model_32_loss: 0.4676 - model_33_loss: 0.6923 - model_33_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4529 - model_32_loss: 0.4655 - model_33_loss: 0.6922 - model_33_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9179 - model_33_loss: 0.6925 - model_33_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4499 - model_32_loss: 0.4666 - model_33_loss: 0.6921 - model_33_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4487 - model_32_loss: 0.4657 - model_33_loss: 0.6919 - model_33_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4523 - model_32_loss: 0.4644 - model_33_loss: 0.6923 - model_33_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4480 - model_32_loss: 0.4666 - model_33_loss: 0.6920 - model_33_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4509 - model_32_loss: 0.4649 - model_33_loss: 0.6921 - model_33_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9162 - model_33_loss: 0.6928 - model_33_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4504 - model_32_loss: 0.4648 - model_33_loss: 0.6921 - model_33_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4511 - model_32_loss: 0.4653 - model_33_loss: 0.6924 - model_33_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4511 - model_32_loss: 0.4662 - model_33_loss: 0.6924 - model_33_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4519 - model_32_loss: 0.4637 - model_33_loss: 0.6922 - model_33_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4514 - model_32_loss: 0.4653 - model_33_loss: 0.6924 - model_33_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9196 - model_33_loss: 0.6926 - model_33_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4525 - model_32_loss: 0.4636 - model_33_loss: 0.6921 - model_33_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4512 - model_32_loss: 0.4657 - model_33_loss: 0.6921 - model_33_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4536 - model_32_loss: 0.4643 - model_33_loss: 0.6923 - model_33_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4549 - model_32_loss: 0.4644 - model_33_loss: 0.6922 - model_33_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4524 - model_32_loss: 0.4650 - model_33_loss: 0.6922 - model_33_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9175 - model_33_loss: 0.6923 - model_33_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4484 - model_32_loss: 0.4662 - model_33_loss: 0.6918 - model_33_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4492 - model_32_loss: 0.4660 - model_33_loss: 0.6916 - model_33_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4527 - model_32_loss: 0.4656 - model_33_loss: 0.6922 - model_33_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4495 - model_32_loss: 0.4677 - model_33_loss: 0.6922 - model_33_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4548 - model_32_loss: 0.4664 - model_33_loss: 0.6929 - model_33_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9196 - model_33_loss: 0.6945 - model_33_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4545 - model_32_loss: 0.4633 - model_33_loss: 0.6921 - model_33_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4528 - model_32_loss: 0.4652 - model_33_loss: 0.6923 - model_33_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4539 - model_32_loss: 0.4657 - model_33_loss: 0.6928 - model_33_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4527 - model_32_loss: 0.4660 - model_33_loss: 0.6925 - model_33_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4572 - model_32_loss: 0.4626 - model_33_loss: 0.6926 - model_33_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9205 - model_33_loss: 0.6929 - model_33_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4566 - model_32_loss: 0.4627 - model_33_loss: 0.6924 - model_33_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4558 - model_32_loss: 0.4630 - model_33_loss: 0.6923 - model_33_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4535 - model_32_loss: 0.4636 - model_33_loss: 0.6921 - model_33_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4565 - model_32_loss: 0.4642 - model_33_loss: 0.6924 - model_33_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4552 - model_32_loss: 0.4662 - model_33_loss: 0.6927 - model_33_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9205 - model_33_loss: 0.6917 - model_33_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4526 - model_32_loss: 0.4639 - model_33_loss: 0.6919 - model_33_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4490 - model_32_loss: 0.4665 - model_33_loss: 0.6919 - model_33_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4501 - model_32_loss: 0.4649 - model_33_loss: 0.6920 - model_33_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4513 - model_32_loss: 0.4665 - model_33_loss: 0.6920 - model_33_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4509 - model_32_loss: 0.4662 - model_33_loss: 0.6921 - model_33_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9169 - model_33_loss: 0.6922 - model_33_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4476 - model_32_loss: 0.4676 - model_33_loss: 0.6916 - model_33_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4515 - model_32_loss: 0.4663 - model_33_loss: 0.6918 - model_33_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4496 - model_32_loss: 0.4684 - model_33_loss: 0.6920 - model_33_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4491 - model_32_loss: 0.4684 - model_33_loss: 0.6916 - model_33_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4523 - model_32_loss: 0.4677 - model_33_loss: 0.6921 - model_33_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9204 - model_33_loss: 0.6915 - model_33_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4522 - model_32_loss: 0.4691 - model_33_loss: 0.6923 - model_33_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4519 - model_32_loss: 0.4690 - model_33_loss: 0.6920 - model_33_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4521 - model_32_loss: 0.4701 - model_33_loss: 0.6923 - model_33_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4555 - model_32_loss: 0.4703 - model_33_loss: 0.6927 - model_33_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4535 - model_32_loss: 0.4705 - model_33_loss: 0.6925 - model_33_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9268 - model_33_loss: 0.6921 - model_33_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4559 - model_32_loss: 0.4713 - model_33_loss: 0.6929 - model_33_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4550 - model_32_loss: 0.4708 - model_33_loss: 0.6926 - model_33_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4552 - model_32_loss: 0.4733 - model_33_loss: 0.6930 - model_33_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4566 - model_32_loss: 0.4702 - model_33_loss: 0.6927 - model_33_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4575 - model_32_loss: 0.4701 - model_33_loss: 0.6928 - model_33_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9264 - model_33_loss: 0.6925 - model_33_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4569 - model_32_loss: 0.4679 - model_33_loss: 0.6925 - model_33_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4531 - model_32_loss: 0.4705 - model_33_loss: 0.6926 - model_33_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4545 - model_32_loss: 0.4692 - model_33_loss: 0.6927 - model_33_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4535 - model_32_loss: 0.4683 - model_33_loss: 0.6925 - model_33_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4546 - model_32_loss: 0.4680 - model_33_loss: 0.6927 - model_33_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9198 - model_33_loss: 0.6924 - model_33_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4516 - model_32_loss: 0.4664 - model_33_loss: 0.6924 - model_33_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4540 - model_32_loss: 0.4657 - model_33_loss: 0.6927 - model_33_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4531 - model_32_loss: 0.4649 - model_33_loss: 0.6925 - model_33_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4529 - model_32_loss: 0.4634 - model_33_loss: 0.6926 - model_33_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4532 - model_32_loss: 0.4629 - model_33_loss: 0.6924 - model_33_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9129 - model_33_loss: 0.6918 - model_33_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4512 - model_32_loss: 0.4620 - model_33_loss: 0.6924 - model_33_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4501 - model_32_loss: 0.4620 - model_33_loss: 0.6922 - model_33_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4500 - model_32_loss: 0.4596 - model_33_loss: 0.6918 - model_33_1_loss: 0.6901\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4519 - model_32_loss: 0.4594 - model_33_loss: 0.6920 - model_33_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4496 - model_32_loss: 0.4595 - model_33_loss: 0.6918 - model_33_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9144 - model_33_loss: 0.6918 - model_33_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4569 - model_32_loss: 0.4578 - model_33_loss: 0.6922 - model_33_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4552 - model_32_loss: 0.4578 - model_33_loss: 0.6919 - model_33_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 3us/sample - loss: -6.4553 - model_32_loss: 0.4601 - model_33_loss: 0.6921 - model_33_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4550 - model_32_loss: 0.4603 - model_33_loss: 0.6920 - model_33_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4588 - model_32_loss: 0.4576 - model_33_loss: 0.6922 - model_33_1_loss: 0.6910\n",
      "For Attention Module: 1.0\n",
      "features X: 30940 samples, 76 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.3961 - model_37_loss: 0.6582 - model_37_1_loss: 0.6208\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0306 - model_36_loss: 0.3720 - model_37_loss: 0.6590 - model_37_1_loss: 0.6215\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.0369 - model_36_loss: 0.3715 - model_37_loss: 0.6598 - model_37_1_loss: 0.6219\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.0541 - model_36_loss: 0.3727 - model_37_loss: 0.6611 - model_37_1_loss: 0.6243\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0618 - model_36_loss: 0.3735 - model_37_loss: 0.6597 - model_37_1_loss: 0.6274\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0847 - model_36_loss: 0.3719 - model_37_loss: 0.6616 - model_37_1_loss: 0.6297\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.4698 - model_37_loss: 0.6625 - model_37_1_loss: 0.6325\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1041 - model_36_loss: 0.3724 - model_37_loss: 0.6629 - model_37_1_loss: 0.6325\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1118 - model_36_loss: 0.3725 - model_37_loss: 0.6624 - model_37_1_loss: 0.6345\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1286 - model_36_loss: 0.3737 - model_37_loss: 0.6633 - model_37_1_loss: 0.6371\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1461 - model_36_loss: 0.3763 - model_37_loss: 0.6640 - model_37_1_loss: 0.6405\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1615 - model_36_loss: 0.3759 - model_37_loss: 0.6650 - model_37_1_loss: 0.6424\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.5423 - model_37_loss: 0.6647 - model_37_1_loss: 0.6436\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1593 - model_36_loss: 0.3772 - model_37_loss: 0.6654 - model_37_1_loss: 0.6419\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1769 - model_36_loss: 0.3780 - model_37_loss: 0.6655 - model_37_1_loss: 0.6455\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1843 - model_36_loss: 0.3782 - model_37_loss: 0.6653 - model_37_1_loss: 0.6472\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2049 - model_36_loss: 0.3804 - model_37_loss: 0.6672 - model_37_1_loss: 0.6499\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2130 - model_36_loss: 0.3837 - model_37_loss: 0.6668 - model_37_1_loss: 0.6525\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.6095 - model_37_loss: 0.6681 - model_37_1_loss: 0.6532\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2154 - model_36_loss: 0.3834 - model_37_loss: 0.6676 - model_37_1_loss: 0.6521\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2364 - model_36_loss: 0.3860 - model_37_loss: 0.6686 - model_37_1_loss: 0.6559\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2544 - model_36_loss: 0.3875 - model_37_loss: 0.6701 - model_37_1_loss: 0.6583\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2599 - model_36_loss: 0.3884 - model_37_loss: 0.6703 - model_37_1_loss: 0.6594\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2698 - model_36_loss: 0.3904 - model_37_loss: 0.6702 - model_37_1_loss: 0.6619\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.6810 - model_37_loss: 0.6723 - model_37_1_loss: 0.6637\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2816 - model_36_loss: 0.3927 - model_37_loss: 0.6711 - model_37_1_loss: 0.6637\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2944 - model_36_loss: 0.3942 - model_37_loss: 0.6726 - model_37_1_loss: 0.6651\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3078 - model_36_loss: 0.3947 - model_37_loss: 0.6731 - model_37_1_loss: 0.6674\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3098 - model_36_loss: 0.3973 - model_37_loss: 0.6739 - model_37_1_loss: 0.6676\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3235 - model_36_loss: 0.3996 - model_37_loss: 0.6752 - model_37_1_loss: 0.6695\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.7324 - model_37_loss: 0.6749 - model_37_1_loss: 0.6712\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3322 - model_36_loss: 0.4004 - model_37_loss: 0.6754 - model_37_1_loss: 0.6711\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3314 - model_36_loss: 0.4041 - model_37_loss: 0.6755 - model_37_1_loss: 0.6717\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3432 - model_36_loss: 0.4041 - model_37_loss: 0.6763 - model_37_1_loss: 0.6732\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3605 - model_36_loss: 0.4071 - model_37_loss: 0.6778 - model_37_1_loss: 0.6757\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3675 - model_36_loss: 0.4096 - model_37_loss: 0.6783 - model_37_1_loss: 0.6771\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.7803 - model_37_loss: 0.6787 - model_37_1_loss: 0.6772\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3731 - model_36_loss: 0.4107 - model_37_loss: 0.6793 - model_37_1_loss: 0.6774\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3778 - model_36_loss: 0.4125 - model_37_loss: 0.6805 - model_37_1_loss: 0.6775\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3890 - model_36_loss: 0.4131 - model_37_loss: 0.6814 - model_37_1_loss: 0.6790\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3926 - model_36_loss: 0.4155 - model_37_loss: 0.6816 - model_37_1_loss: 0.6800\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4041 - model_36_loss: 0.4158 - model_37_loss: 0.6827 - model_37_1_loss: 0.6813\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.8274 - model_37_loss: 0.6840 - model_37_1_loss: 0.6822\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4064 - model_36_loss: 0.4187 - model_37_loss: 0.6834 - model_37_1_loss: 0.6816\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4126 - model_36_loss: 0.4215 - model_37_loss: 0.6836 - model_37_1_loss: 0.6832\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4197 - model_36_loss: 0.4213 - model_37_loss: 0.6840 - model_37_1_loss: 0.6842\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4214 - model_36_loss: 0.4247 - model_37_loss: 0.6843 - model_37_1_loss: 0.6849\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4385 - model_36_loss: 0.4280 - model_37_loss: 0.6867 - model_37_1_loss: 0.6866\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.8603 - model_37_loss: 0.6853 - model_37_1_loss: 0.6866\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4305 - model_36_loss: 0.4267 - model_37_loss: 0.6853 - model_37_1_loss: 0.6861\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4377 - model_36_loss: 0.4296 - model_37_loss: 0.6865 - model_37_1_loss: 0.6870\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4372 - model_36_loss: 0.4332 - model_37_loss: 0.6866 - model_37_1_loss: 0.6875\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4472 - model_36_loss: 0.4357 - model_37_loss: 0.6886 - model_37_1_loss: 0.6880\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4467 - model_36_loss: 0.4364 - model_37_loss: 0.6880 - model_37_1_loss: 0.6886\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.8824 - model_37_loss: 0.6866 - model_37_1_loss: 0.6885\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4638 - model_36_loss: 0.4371 - model_37_loss: 0.6893 - model_37_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4668 - model_36_loss: 0.4424 - model_37_loss: 0.6901 - model_37_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4673 - model_36_loss: 0.4430 - model_37_loss: 0.6902 - model_37_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4770 - model_36_loss: 0.4448 - model_37_loss: 0.6919 - model_37_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4793 - model_36_loss: 0.4482 - model_37_loss: 0.6923 - model_37_1_loss: 0.6932\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9095 - model_37_loss: 0.6905 - model_37_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4605 - model_36_loss: 0.4498 - model_37_loss: 0.6905 - model_37_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4622 - model_36_loss: 0.4515 - model_37_loss: 0.6908 - model_37_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4640 - model_36_loss: 0.4544 - model_37_loss: 0.6916 - model_37_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4638 - model_36_loss: 0.4559 - model_37_loss: 0.6916 - model_37_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4607 - model_36_loss: 0.4590 - model_37_loss: 0.6915 - model_37_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9211 - model_37_loss: 0.6915 - model_37_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4633 - model_36_loss: 0.4572 - model_37_loss: 0.6916 - model_37_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4656 - model_36_loss: 0.4596 - model_37_loss: 0.6923 - model_37_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4635 - model_36_loss: 0.4605 - model_37_loss: 0.6920 - model_37_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4624 - model_36_loss: 0.4625 - model_37_loss: 0.6917 - model_37_1_loss: 0.6933\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4685 - model_36_loss: 0.4634 - model_37_loss: 0.6931 - model_37_1_loss: 0.6933\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9291 - model_37_loss: 0.6931 - model_37_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4686 - model_36_loss: 0.4618 - model_37_loss: 0.6928 - model_37_1_loss: 0.6933\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4665 - model_36_loss: 0.4621 - model_37_loss: 0.6929 - model_37_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4681 - model_36_loss: 0.4624 - model_37_loss: 0.6931 - model_37_1_loss: 0.6930\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4685 - model_36_loss: 0.4639 - model_37_loss: 0.6934 - model_37_1_loss: 0.6930\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4697 - model_36_loss: 0.4633 - model_37_loss: 0.6938 - model_37_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9308 - model_37_loss: 0.6934 - model_37_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4670 - model_36_loss: 0.4630 - model_37_loss: 0.6931 - model_37_1_loss: 0.6929\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4654 - model_36_loss: 0.4624 - model_37_loss: 0.6933 - model_37_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4664 - model_36_loss: 0.4621 - model_37_loss: 0.6933 - model_37_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4719 - model_36_loss: 0.4599 - model_37_loss: 0.6936 - model_37_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4708 - model_36_loss: 0.4590 - model_37_loss: 0.6933 - model_37_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9303 - model_37_loss: 0.6942 - model_37_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4691 - model_36_loss: 0.4579 - model_37_loss: 0.6931 - model_37_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4698 - model_36_loss: 0.4561 - model_37_loss: 0.6930 - model_37_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4704 - model_36_loss: 0.4563 - model_37_loss: 0.6929 - model_37_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4718 - model_36_loss: 0.4544 - model_37_loss: 0.6930 - model_37_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4733 - model_36_loss: 0.4540 - model_37_loss: 0.6932 - model_37_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: 6.9269 - model_37_loss: 0.6931 - model_37_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4731 - model_36_loss: 0.4536 - model_37_loss: 0.6930 - model_37_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4753 - model_36_loss: 0.4509 - model_37_loss: 0.6929 - model_37_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4730 - model_36_loss: 0.4518 - model_37_loss: 0.6927 - model_37_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4738 - model_36_loss: 0.4497 - model_37_loss: 0.6925 - model_37_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4763 - model_36_loss: 0.4483 - model_37_loss: 0.6926 - model_37_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9249 - model_37_loss: 0.6925 - model_37_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4781 - model_36_loss: 0.4458 - model_37_loss: 0.6925 - model_37_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4732 - model_36_loss: 0.4479 - model_37_loss: 0.6922 - model_37_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4784 - model_36_loss: 0.4447 - model_37_loss: 0.6924 - model_37_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4781 - model_36_loss: 0.4457 - model_37_loss: 0.6923 - model_37_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4768 - model_36_loss: 0.4446 - model_37_loss: 0.6922 - model_37_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9228 - model_37_loss: 0.6926 - model_37_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4774 - model_36_loss: 0.4426 - model_37_loss: 0.6922 - model_37_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4751 - model_36_loss: 0.4425 - model_37_loss: 0.6919 - model_37_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4789 - model_36_loss: 0.4411 - model_37_loss: 0.6921 - model_37_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4791 - model_36_loss: 0.4406 - model_37_loss: 0.6922 - model_37_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4791 - model_36_loss: 0.4398 - model_37_loss: 0.6917 - model_37_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9198 - model_37_loss: 0.6930 - model_37_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4825 - model_36_loss: 0.4385 - model_37_loss: 0.6921 - model_37_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4818 - model_36_loss: 0.4396 - model_37_loss: 0.6924 - model_37_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4796 - model_36_loss: 0.4405 - model_37_loss: 0.6920 - model_37_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4811 - model_36_loss: 0.4384 - model_37_loss: 0.6920 - model_37_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4804 - model_36_loss: 0.4389 - model_37_loss: 0.6921 - model_37_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9215 - model_37_loss: 0.6927 - model_37_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4805 - model_36_loss: 0.4377 - model_37_loss: 0.6921 - model_37_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4825 - model_36_loss: 0.4357 - model_37_loss: 0.6921 - model_37_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4807 - model_36_loss: 0.4377 - model_37_loss: 0.6920 - model_37_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4801 - model_36_loss: 0.4375 - model_37_loss: 0.6919 - model_37_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4766 - model_36_loss: 0.4401 - model_37_loss: 0.6918 - model_37_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9178 - model_37_loss: 0.6927 - model_37_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4796 - model_36_loss: 0.4372 - model_37_loss: 0.6919 - model_37_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4804 - model_36_loss: 0.4364 - model_37_loss: 0.6920 - model_37_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4799 - model_36_loss: 0.4383 - model_37_loss: 0.6921 - model_37_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4814 - model_36_loss: 0.4363 - model_37_loss: 0.6920 - model_37_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4787 - model_36_loss: 0.4375 - model_37_loss: 0.6920 - model_37_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9174 - model_37_loss: 0.6925 - model_37_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4818 - model_36_loss: 0.4352 - model_37_loss: 0.6921 - model_37_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4815 - model_36_loss: 0.4357 - model_37_loss: 0.6920 - model_37_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4821 - model_36_loss: 0.4332 - model_37_loss: 0.6917 - model_37_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4817 - model_36_loss: 0.4354 - model_37_loss: 0.6922 - model_37_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4822 - model_36_loss: 0.4360 - model_37_loss: 0.6921 - model_37_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9201 - model_37_loss: 0.6920 - model_37_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4816 - model_36_loss: 0.4350 - model_37_loss: 0.6922 - model_37_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4800 - model_36_loss: 0.4375 - model_37_loss: 0.6920 - model_37_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4822 - model_36_loss: 0.4370 - model_37_loss: 0.6923 - model_37_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4840 - model_36_loss: 0.4341 - model_37_loss: 0.6922 - model_37_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4822 - model_36_loss: 0.4354 - model_37_loss: 0.6919 - model_37_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9211 - model_37_loss: 0.6920 - model_37_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4828 - model_36_loss: 0.4360 - model_37_loss: 0.6919 - model_37_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4849 - model_36_loss: 0.4361 - model_37_loss: 0.6921 - model_37_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4848 - model_36_loss: 0.4378 - model_37_loss: 0.6923 - model_37_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4852 - model_36_loss: 0.4387 - model_37_loss: 0.6925 - model_37_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4858 - model_36_loss: 0.4394 - model_37_loss: 0.6924 - model_37_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9229 - model_37_loss: 0.6924 - model_37_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4824 - model_36_loss: 0.4376 - model_37_loss: 0.6921 - model_37_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4835 - model_36_loss: 0.4399 - model_37_loss: 0.6926 - model_37_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4804 - model_36_loss: 0.4397 - model_37_loss: 0.6922 - model_37_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4836 - model_36_loss: 0.4382 - model_37_loss: 0.6924 - model_37_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4834 - model_36_loss: 0.4394 - model_37_loss: 0.6925 - model_37_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9236 - model_37_loss: 0.6926 - model_37_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4802 - model_36_loss: 0.4412 - model_37_loss: 0.6923 - model_37_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4792 - model_36_loss: 0.4410 - model_37_loss: 0.6921 - model_37_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4829 - model_36_loss: 0.4391 - model_37_loss: 0.6922 - model_37_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4823 - model_36_loss: 0.4392 - model_37_loss: 0.6923 - model_37_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4839 - model_36_loss: 0.4376 - model_37_loss: 0.6921 - model_37_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9213 - model_37_loss: 0.6914 - model_37_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4797 - model_36_loss: 0.4372 - model_37_loss: 0.6919 - model_37_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4818 - model_36_loss: 0.4366 - model_37_loss: 0.6919 - model_37_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4816 - model_36_loss: 0.4390 - model_37_loss: 0.6921 - model_37_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4832 - model_36_loss: 0.4353 - model_37_loss: 0.6919 - model_37_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4867 - model_36_loss: 0.4339 - model_37_loss: 0.6919 - model_37_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9209 - model_37_loss: 0.6916 - model_37_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4817 - model_36_loss: 0.4367 - model_37_loss: 0.6918 - model_37_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4825 - model_36_loss: 0.4357 - model_37_loss: 0.6917 - model_37_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4839 - model_36_loss: 0.4344 - model_37_loss: 0.6918 - model_37_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4855 - model_36_loss: 0.4339 - model_37_loss: 0.6917 - model_37_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4866 - model_36_loss: 0.4331 - model_37_loss: 0.6919 - model_37_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9143 - model_37_loss: 0.6920 - model_37_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4839 - model_36_loss: 0.4321 - model_37_loss: 0.6918 - model_37_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4824 - model_36_loss: 0.4296 - model_37_loss: 0.6914 - model_37_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4835 - model_36_loss: 0.4314 - model_37_loss: 0.6918 - model_37_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4816 - model_36_loss: 0.4323 - model_37_loss: 0.6915 - model_37_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4830 - model_36_loss: 0.4295 - model_37_loss: 0.6915 - model_37_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9127 - model_37_loss: 0.6914 - model_37_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4786 - model_36_loss: 0.4320 - model_37_loss: 0.6911 - model_37_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4826 - model_36_loss: 0.4296 - model_37_loss: 0.6914 - model_37_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4839 - model_36_loss: 0.4293 - model_37_loss: 0.6915 - model_37_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4811 - model_36_loss: 0.4315 - model_37_loss: 0.6915 - model_37_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4815 - model_36_loss: 0.4318 - model_37_loss: 0.6915 - model_37_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9148 - model_37_loss: 0.6917 - model_37_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4789 - model_36_loss: 0.4314 - model_37_loss: 0.6913 - model_37_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4830 - model_36_loss: 0.4311 - model_37_loss: 0.6916 - model_37_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4814 - model_36_loss: 0.4302 - model_37_loss: 0.6913 - model_37_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4830 - model_36_loss: 0.4308 - model_37_loss: 0.6915 - model_37_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4840 - model_36_loss: 0.4306 - model_37_loss: 0.6914 - model_37_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9159 - model_37_loss: 0.6913 - model_37_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4823 - model_36_loss: 0.4308 - model_37_loss: 0.6915 - model_37_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4852 - model_36_loss: 0.4303 - model_37_loss: 0.6917 - model_37_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4881 - model_36_loss: 0.4308 - model_37_loss: 0.6921 - model_37_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4875 - model_36_loss: 0.4304 - model_37_loss: 0.6918 - model_37_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4883 - model_36_loss: 0.4295 - model_37_loss: 0.6919 - model_37_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9184 - model_37_loss: 0.6920 - model_37_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4839 - model_36_loss: 0.4332 - model_37_loss: 0.6916 - model_37_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4849 - model_36_loss: 0.4318 - model_37_loss: 0.6916 - model_37_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4871 - model_36_loss: 0.4294 - model_37_loss: 0.6915 - model_37_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4829 - model_36_loss: 0.4325 - model_37_loss: 0.6916 - model_37_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4867 - model_36_loss: 0.4307 - model_37_loss: 0.6918 - model_37_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9175 - model_37_loss: 0.6922 - model_37_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4888 - model_36_loss: 0.4297 - model_37_loss: 0.6921 - model_37_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4883 - model_36_loss: 0.4324 - model_37_loss: 0.6922 - model_37_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4889 - model_36_loss: 0.4309 - model_37_loss: 0.6921 - model_37_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4902 - model_36_loss: 0.4318 - model_37_loss: 0.6924 - model_37_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4904 - model_36_loss: 0.4308 - model_37_loss: 0.6922 - model_37_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9171 - model_37_loss: 0.6924 - model_37_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4861 - model_36_loss: 0.4313 - model_37_loss: 0.6917 - model_37_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4852 - model_36_loss: 0.4312 - model_37_loss: 0.6915 - model_37_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4882 - model_36_loss: 0.4281 - model_37_loss: 0.6915 - model_37_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4883 - model_36_loss: 0.4285 - model_37_loss: 0.6915 - model_37_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4894 - model_36_loss: 0.4282 - model_37_loss: 0.6917 - model_37_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9157 - model_37_loss: 0.6911 - model_37_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4878 - model_36_loss: 0.4274 - model_37_loss: 0.6914 - model_37_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4846 - model_36_loss: 0.4298 - model_37_loss: 0.6912 - model_37_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4886 - model_36_loss: 0.4272 - model_37_loss: 0.6916 - model_37_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4874 - model_36_loss: 0.4281 - model_37_loss: 0.6914 - model_37_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4896 - model_36_loss: 0.4268 - model_37_loss: 0.6917 - model_37_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9165 - model_37_loss: 0.6921 - model_37_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4824 - model_36_loss: 0.4281 - model_37_loss: 0.6911 - model_37_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4859 - model_36_loss: 0.4271 - model_37_loss: 0.6912 - model_37_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4832 - model_36_loss: 0.4294 - model_37_loss: 0.6915 - model_37_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4853 - model_36_loss: 0.4276 - model_37_loss: 0.6912 - model_37_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4840 - model_36_loss: 0.4266 - model_37_loss: 0.6907 - model_37_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9143 - model_37_loss: 0.6907 - model_37_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4828 - model_36_loss: 0.4280 - model_37_loss: 0.6909 - model_37_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4853 - model_36_loss: 0.4267 - model_37_loss: 0.6910 - model_37_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4820 - model_36_loss: 0.4308 - model_37_loss: 0.6912 - model_37_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4870 - model_36_loss: 0.4287 - model_37_loss: 0.6916 - model_37_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4903 - model_36_loss: 0.4290 - model_37_loss: 0.6922 - model_37_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9207 - model_37_loss: 0.6928 - model_37_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4862 - model_36_loss: 0.4313 - model_37_loss: 0.6918 - model_37_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4866 - model_36_loss: 0.4299 - model_37_loss: 0.6917 - model_37_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4940 - model_36_loss: 0.4296 - model_37_loss: 0.6927 - model_37_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4867 - model_36_loss: 0.4311 - model_37_loss: 0.6917 - model_37_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4907 - model_36_loss: 0.4319 - model_37_loss: 0.6926 - model_37_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9234 - model_37_loss: 0.6930 - model_37_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4886 - model_36_loss: 0.4315 - model_37_loss: 0.6927 - model_37_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4936 - model_36_loss: 0.4302 - model_37_loss: 0.6930 - model_37_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4893 - model_36_loss: 0.4338 - model_37_loss: 0.6927 - model_37_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4905 - model_36_loss: 0.4330 - model_37_loss: 0.6929 - model_37_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4916 - model_36_loss: 0.4330 - model_37_loss: 0.6931 - model_37_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9261 - model_37_loss: 0.6931 - model_37_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4902 - model_36_loss: 0.4333 - model_37_loss: 0.6930 - model_37_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4927 - model_36_loss: 0.4312 - model_37_loss: 0.6930 - model_37_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4913 - model_36_loss: 0.4331 - model_37_loss: 0.6929 - model_37_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4940 - model_36_loss: 0.4319 - model_37_loss: 0.6931 - model_37_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4928 - model_36_loss: 0.4330 - model_37_loss: 0.6932 - model_37_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9232 - model_37_loss: 0.6934 - model_37_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4926 - model_36_loss: 0.4314 - model_37_loss: 0.6930 - model_37_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4899 - model_36_loss: 0.4320 - model_37_loss: 0.6926 - model_37_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4893 - model_36_loss: 0.4334 - model_37_loss: 0.6927 - model_37_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4909 - model_36_loss: 0.4341 - model_37_loss: 0.6929 - model_37_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4910 - model_36_loss: 0.4339 - model_37_loss: 0.6929 - model_37_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9233 - model_37_loss: 0.6921 - model_37_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4871 - model_36_loss: 0.4331 - model_37_loss: 0.6921 - model_37_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4846 - model_36_loss: 0.4334 - model_37_loss: 0.6918 - model_37_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4872 - model_36_loss: 0.4328 - model_37_loss: 0.6919 - model_37_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4884 - model_36_loss: 0.4328 - model_37_loss: 0.6920 - model_37_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4887 - model_36_loss: 0.4323 - model_37_loss: 0.6920 - model_37_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9235 - model_37_loss: 0.6917 - model_37_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4863 - model_36_loss: 0.4337 - model_37_loss: 0.6915 - model_37_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4873 - model_36_loss: 0.4337 - model_37_loss: 0.6917 - model_37_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4887 - model_36_loss: 0.4320 - model_37_loss: 0.6916 - model_37_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4874 - model_36_loss: 0.4333 - model_37_loss: 0.6915 - model_37_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4894 - model_36_loss: 0.4332 - model_37_loss: 0.6919 - model_37_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9240 - model_37_loss: 0.6918 - model_37_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4895 - model_36_loss: 0.4317 - model_37_loss: 0.6922 - model_37_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4881 - model_36_loss: 0.4323 - model_37_loss: 0.6919 - model_37_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4891 - model_36_loss: 0.4310 - model_37_loss: 0.6920 - model_37_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4903 - model_36_loss: 0.4310 - model_37_loss: 0.6920 - model_37_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4888 - model_36_loss: 0.4319 - model_37_loss: 0.6921 - model_37_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9228 - model_37_loss: 0.6925 - model_37_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4886 - model_36_loss: 0.4311 - model_37_loss: 0.6921 - model_37_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4888 - model_36_loss: 0.4305 - model_37_loss: 0.6919 - model_37_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4902 - model_36_loss: 0.4308 - model_37_loss: 0.6922 - model_37_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4892 - model_36_loss: 0.4307 - model_37_loss: 0.6921 - model_37_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4914 - model_36_loss: 0.4289 - model_37_loss: 0.6921 - model_37_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9223 - model_37_loss: 0.6919 - model_37_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4905 - model_36_loss: 0.4284 - model_37_loss: 0.6921 - model_37_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4920 - model_36_loss: 0.4278 - model_37_loss: 0.6921 - model_37_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4907 - model_36_loss: 0.4293 - model_37_loss: 0.6921 - model_37_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4911 - model_36_loss: 0.4286 - model_37_loss: 0.6922 - model_37_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4898 - model_36_loss: 0.4293 - model_37_loss: 0.6921 - model_37_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9222 - model_37_loss: 0.6918 - model_37_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4926 - model_36_loss: 0.4271 - model_37_loss: 0.6921 - model_37_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4911 - model_36_loss: 0.4282 - model_37_loss: 0.6921 - model_37_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4909 - model_36_loss: 0.4282 - model_37_loss: 0.6920 - model_37_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4902 - model_36_loss: 0.4275 - model_37_loss: 0.6920 - model_37_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4937 - model_36_loss: 0.4282 - model_37_loss: 0.6926 - model_37_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9191 - model_37_loss: 0.6916 - model_37_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4924 - model_36_loss: 0.4262 - model_37_loss: 0.6924 - model_37_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4929 - model_36_loss: 0.4262 - model_37_loss: 0.6924 - model_37_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4920 - model_36_loss: 0.4263 - model_37_loss: 0.6921 - model_37_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4925 - model_36_loss: 0.4284 - model_37_loss: 0.6929 - model_37_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4932 - model_36_loss: 0.4276 - model_37_loss: 0.6927 - model_37_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9210 - model_37_loss: 0.6931 - model_37_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4913 - model_36_loss: 0.4268 - model_37_loss: 0.6925 - model_37_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4916 - model_36_loss: 0.4271 - model_37_loss: 0.6927 - model_37_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4896 - model_36_loss: 0.4294 - model_37_loss: 0.6925 - model_37_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4869 - model_36_loss: 0.4313 - model_37_loss: 0.6926 - model_37_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4947 - model_36_loss: 0.4279 - model_37_loss: 0.6929 - model_37_1_loss: 0.6917\n",
      "For Attention Module: 1.1\n",
      "features X: 30940 samples, 90 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.3804 - model_41_loss: 0.6555 - model_41_1_loss: 0.6206\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0216 - model_40_loss: 0.3597 - model_41_loss: 0.6561 - model_41_1_loss: 0.6202\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0393 - model_40_loss: 0.3588 - model_41_loss: 0.6570 - model_41_1_loss: 0.6226\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0513 - model_40_loss: 0.3591 - model_41_loss: 0.6569 - model_41_1_loss: 0.6252\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.0684 - model_40_loss: 0.3590 - model_41_loss: 0.6583 - model_41_1_loss: 0.6272\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0834 - model_40_loss: 0.3602 - model_41_loss: 0.6591 - model_41_1_loss: 0.6296\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.4501 - model_41_loss: 0.6582 - model_41_1_loss: 0.6312\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0874 - model_40_loss: 0.3602 - model_41_loss: 0.6578 - model_41_1_loss: 0.6318\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1081 - model_40_loss: 0.3615 - model_41_loss: 0.6600 - model_41_1_loss: 0.6339\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1168 - model_40_loss: 0.3622 - model_41_loss: 0.6605 - model_41_1_loss: 0.6353\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1308 - model_40_loss: 0.3611 - model_41_loss: 0.6613 - model_41_1_loss: 0.6371\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1470 - model_40_loss: 0.3636 - model_41_loss: 0.6618 - model_41_1_loss: 0.6403\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.5176 - model_41_loss: 0.6620 - model_41_1_loss: 0.6418\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1602 - model_40_loss: 0.3617 - model_41_loss: 0.6625 - model_41_1_loss: 0.6419\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1734 - model_40_loss: 0.3631 - model_41_loss: 0.6631 - model_41_1_loss: 0.6442\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.1889 - model_40_loss: 0.3644 - model_41_loss: 0.6634 - model_41_1_loss: 0.6473\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1924 - model_40_loss: 0.3645 - model_41_loss: 0.6635 - model_41_1_loss: 0.6479\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2067 - model_40_loss: 0.3647 - model_41_loss: 0.6644 - model_41_1_loss: 0.6499\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.5856 - model_41_loss: 0.6659 - model_41_1_loss: 0.6515\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2238 - model_40_loss: 0.3680 - model_41_loss: 0.6661 - model_41_1_loss: 0.6523\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2303 - model_40_loss: 0.3707 - model_41_loss: 0.6658 - model_41_1_loss: 0.6544\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2411 - model_40_loss: 0.3709 - model_41_loss: 0.6668 - model_41_1_loss: 0.6556\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2549 - model_40_loss: 0.3721 - model_41_loss: 0.6673 - model_41_1_loss: 0.6581\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2577 - model_40_loss: 0.3741 - model_41_loss: 0.6674 - model_41_1_loss: 0.6589\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.6511 - model_41_loss: 0.6697 - model_41_1_loss: 0.6609\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2673 - model_40_loss: 0.3745 - model_41_loss: 0.6680 - model_41_1_loss: 0.6603\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2837 - model_40_loss: 0.3752 - model_41_loss: 0.6698 - model_41_1_loss: 0.6619\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.2955 - model_40_loss: 0.3746 - model_41_loss: 0.6702 - model_41_1_loss: 0.6638\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3025 - model_40_loss: 0.3790 - model_41_loss: 0.6707 - model_41_1_loss: 0.6655\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3248 - model_40_loss: 0.3797 - model_41_loss: 0.6727 - model_41_1_loss: 0.6682\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.6979 - model_41_loss: 0.6722 - model_41_1_loss: 0.6678\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3229 - model_40_loss: 0.3814 - model_41_loss: 0.6732 - model_41_1_loss: 0.6677\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3328 - model_40_loss: 0.3852 - model_41_loss: 0.6745 - model_41_1_loss: 0.6691\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3383 - model_40_loss: 0.3865 - model_41_loss: 0.6743 - model_41_1_loss: 0.6707\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3510 - model_40_loss: 0.3871 - model_41_loss: 0.6754 - model_41_1_loss: 0.6722\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3607 - model_40_loss: 0.3901 - model_41_loss: 0.6759 - model_41_1_loss: 0.6742\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.7541 - model_41_loss: 0.6755 - model_41_1_loss: 0.6745\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3646 - model_40_loss: 0.3907 - model_41_loss: 0.6756 - model_41_1_loss: 0.6754\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3729 - model_40_loss: 0.3942 - model_41_loss: 0.6777 - model_41_1_loss: 0.6757\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3813 - model_40_loss: 0.3946 - model_41_loss: 0.6772 - model_41_1_loss: 0.6779\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.3894 - model_40_loss: 0.3961 - model_41_loss: 0.6786 - model_41_1_loss: 0.6785\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3886 - model_40_loss: 0.4002 - model_41_loss: 0.6783 - model_41_1_loss: 0.6794\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.8018 - model_41_loss: 0.6799 - model_41_1_loss: 0.68050s - loss: 6.7784 - model_41_loss: 0.6759 - model_41_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3978 - model_40_loss: 0.4011 - model_41_loss: 0.6798 - model_41_1_loss: 0.6800\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3975 - model_40_loss: 0.4044 - model_41_loss: 0.6791 - model_41_1_loss: 0.6813\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4069 - model_40_loss: 0.4024 - model_41_loss: 0.6798 - model_41_1_loss: 0.6820\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4142 - model_40_loss: 0.4071 - model_41_loss: 0.6809 - model_41_1_loss: 0.6834\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4160 - model_40_loss: 0.4095 - model_41_loss: 0.6810 - model_41_1_loss: 0.6841\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.8340 - model_41_loss: 0.6818 - model_41_1_loss: 0.6847\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4290 - model_40_loss: 0.4116 - model_41_loss: 0.6826 - model_41_1_loss: 0.6856\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4325 - model_40_loss: 0.4146 - model_41_loss: 0.6835 - model_41_1_loss: 0.6859\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4362 - model_40_loss: 0.4137 - model_41_loss: 0.6830 - model_41_1_loss: 0.6870\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4423 - model_40_loss: 0.4170 - model_41_loss: 0.6842 - model_41_1_loss: 0.6876\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4479 - model_40_loss: 0.4178 - model_41_loss: 0.6847 - model_41_1_loss: 0.6885\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.8669 - model_41_loss: 0.6848 - model_41_1_loss: 0.6888\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4426 - model_40_loss: 0.4199 - model_41_loss: 0.6840 - model_41_1_loss: 0.6884\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4486 - model_40_loss: 0.4238 - model_41_loss: 0.6854 - model_41_1_loss: 0.6891\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4569 - model_40_loss: 0.4224 - model_41_loss: 0.6863 - model_41_1_loss: 0.6896\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4574 - model_40_loss: 0.4280 - model_41_loss: 0.6869 - model_41_1_loss: 0.6902\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4631 - model_40_loss: 0.4270 - model_41_loss: 0.6875 - model_41_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.8961 - model_41_loss: 0.6877 - model_41_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4647 - model_40_loss: 0.4283 - model_41_loss: 0.6877 - model_41_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4617 - model_40_loss: 0.4313 - model_41_loss: 0.6872 - model_41_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4696 - model_40_loss: 0.4322 - model_41_loss: 0.6885 - model_41_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4710 - model_40_loss: 0.4352 - model_41_loss: 0.6889 - model_41_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4718 - model_40_loss: 0.4363 - model_41_loss: 0.6886 - model_41_1_loss: 0.6930\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9114 - model_41_loss: 0.6904 - model_41_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4718 - model_40_loss: 0.4368 - model_41_loss: 0.6896 - model_41_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4749 - model_40_loss: 0.4396 - model_41_loss: 0.6904 - model_41_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4728 - model_40_loss: 0.4414 - model_41_loss: 0.6901 - model_41_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4740 - model_40_loss: 0.4423 - model_41_loss: 0.6904 - model_41_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4770 - model_40_loss: 0.4428 - model_41_loss: 0.6909 - model_41_1_loss: 0.6930\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9182 - model_41_loss: 0.6912 - model_41_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4711 - model_40_loss: 0.4433 - model_41_loss: 0.6904 - model_41_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4728 - model_40_loss: 0.4423 - model_41_loss: 0.6904 - model_41_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4726 - model_40_loss: 0.4431 - model_41_loss: 0.6904 - model_41_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4751 - model_40_loss: 0.4433 - model_41_loss: 0.6910 - model_41_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4754 - model_40_loss: 0.4427 - model_41_loss: 0.6908 - model_41_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9177 - model_41_loss: 0.6903 - model_41_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4725 - model_40_loss: 0.4433 - model_41_loss: 0.6904 - model_41_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 4us/sample - loss: -6.4739 - model_40_loss: 0.4426 - model_41_loss: 0.6904 - model_41_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4757 - model_40_loss: 0.4418 - model_41_loss: 0.6906 - model_41_1_loss: 0.6929\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4772 - model_40_loss: 0.4407 - model_41_loss: 0.6907 - model_41_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4763 - model_40_loss: 0.4401 - model_41_loss: 0.6903 - model_41_1_loss: 0.6930\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9128 - model_41_loss: 0.6894 - model_41_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4702 - model_40_loss: 0.4395 - model_41_loss: 0.6897 - model_41_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4698 - model_40_loss: 0.4387 - model_41_loss: 0.6894 - model_41_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4705 - model_40_loss: 0.4382 - model_41_loss: 0.6895 - model_41_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4710 - model_40_loss: 0.4398 - model_41_loss: 0.6897 - model_41_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4742 - model_40_loss: 0.4367 - model_41_loss: 0.6898 - model_41_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9105 - model_41_loss: 0.6898 - model_41_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4652 - model_40_loss: 0.4369 - model_41_loss: 0.6886 - model_41_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4700 - model_40_loss: 0.4330 - model_41_loss: 0.6886 - model_41_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4687 - model_40_loss: 0.4350 - model_41_loss: 0.6889 - model_41_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4729 - model_40_loss: 0.4324 - model_41_loss: 0.6892 - model_41_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4734 - model_40_loss: 0.4315 - model_41_loss: 0.6890 - model_41_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9021 - model_41_loss: 0.6888 - model_41_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4633 - model_40_loss: 0.4323 - model_41_loss: 0.6880 - model_41_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4698 - model_40_loss: 0.4289 - model_41_loss: 0.6885 - model_41_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4668 - model_40_loss: 0.4290 - model_41_loss: 0.6880 - model_41_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4701 - model_40_loss: 0.4279 - model_41_loss: 0.6885 - model_41_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4714 - model_40_loss: 0.4265 - model_41_loss: 0.6884 - model_41_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9004 - model_41_loss: 0.6888 - model_41_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4639 - model_40_loss: 0.4280 - model_41_loss: 0.6883 - model_41_1_loss: 0.6901\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4650 - model_40_loss: 0.4261 - model_41_loss: 0.6881 - model_41_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4659 - model_40_loss: 0.4261 - model_41_loss: 0.6882 - model_41_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4702 - model_40_loss: 0.4234 - model_41_loss: 0.6885 - model_41_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4734 - model_40_loss: 0.4244 - model_41_loss: 0.6891 - model_41_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.8989 - model_41_loss: 0.6898 - model_41_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4736 - model_40_loss: 0.4223 - model_41_loss: 0.6892 - model_41_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4719 - model_40_loss: 0.4229 - model_41_loss: 0.6889 - model_41_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4754 - model_40_loss: 0.4228 - model_41_loss: 0.6895 - model_41_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4753 - model_40_loss: 0.4227 - model_41_loss: 0.6894 - model_41_1_loss: 0.6901\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4780 - model_40_loss: 0.4227 - model_41_loss: 0.6897 - model_41_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9094 - model_41_loss: 0.6915 - model_41_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4834 - model_40_loss: 0.4195 - model_41_loss: 0.6901 - model_41_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4813 - model_40_loss: 0.4214 - model_41_loss: 0.6901 - model_41_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4849 - model_40_loss: 0.4226 - model_41_loss: 0.6904 - model_41_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4851 - model_40_loss: 0.4206 - model_41_loss: 0.6903 - model_41_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4871 - model_40_loss: 0.4223 - model_41_loss: 0.6908 - model_41_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9101 - model_41_loss: 0.6912 - model_41_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4839 - model_40_loss: 0.4217 - model_41_loss: 0.6904 - model_41_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4859 - model_40_loss: 0.4228 - model_41_loss: 0.6909 - model_41_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4843 - model_40_loss: 0.4233 - model_41_loss: 0.6903 - model_41_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4886 - model_40_loss: 0.4230 - model_41_loss: 0.6913 - model_41_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4874 - model_40_loss: 0.4253 - model_41_loss: 0.6911 - model_41_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9149 - model_41_loss: 0.6917 - model_41_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4884 - model_40_loss: 0.4255 - model_41_loss: 0.6907 - model_41_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4905 - model_40_loss: 0.4243 - model_41_loss: 0.6911 - model_41_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4928 - model_40_loss: 0.4256 - model_41_loss: 0.6912 - model_41_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4898 - model_40_loss: 0.4263 - model_41_loss: 0.6912 - model_41_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4875 - model_40_loss: 0.4299 - model_41_loss: 0.6911 - model_41_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9208 - model_41_loss: 0.6916 - model_41_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4877 - model_40_loss: 0.4283 - model_41_loss: 0.6912 - model_41_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4947 - model_40_loss: 0.4277 - model_41_loss: 0.6923 - model_41_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4932 - model_40_loss: 0.4283 - model_41_loss: 0.6921 - model_41_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4916 - model_40_loss: 0.4285 - model_41_loss: 0.6918 - model_41_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4919 - model_40_loss: 0.4299 - model_41_loss: 0.6920 - model_41_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9254 - model_41_loss: 0.6924 - model_41_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4940 - model_40_loss: 0.4299 - model_41_loss: 0.6926 - model_41_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4982 - model_40_loss: 0.4288 - model_41_loss: 0.6929 - model_41_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4976 - model_40_loss: 0.4296 - model_41_loss: 0.6931 - model_41_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4967 - model_40_loss: 0.4302 - model_41_loss: 0.6930 - model_41_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4971 - model_40_loss: 0.4299 - model_41_loss: 0.6928 - model_41_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9310 - model_41_loss: 0.6932 - model_41_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4940 - model_40_loss: 0.4301 - model_41_loss: 0.6927 - model_41_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4930 - model_40_loss: 0.4310 - model_41_loss: 0.6928 - model_41_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4946 - model_40_loss: 0.4286 - model_41_loss: 0.6926 - model_41_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4961 - model_40_loss: 0.4281 - model_41_loss: 0.6927 - model_41_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4954 - model_40_loss: 0.4281 - model_41_loss: 0.6927 - model_41_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9248 - model_41_loss: 0.6922 - model_41_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4932 - model_40_loss: 0.4246 - model_41_loss: 0.6919 - model_41_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4947 - model_40_loss: 0.4238 - model_41_loss: 0.6920 - model_41_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4937 - model_40_loss: 0.4248 - model_41_loss: 0.6920 - model_41_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4959 - model_40_loss: 0.4233 - model_41_loss: 0.6921 - model_41_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4955 - model_40_loss: 0.4246 - model_41_loss: 0.6919 - model_41_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9209 - model_41_loss: 0.6918 - model_41_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4958 - model_40_loss: 0.4216 - model_41_loss: 0.6917 - model_41_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4996 - model_40_loss: 0.4204 - model_41_loss: 0.6918 - model_41_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5013 - model_40_loss: 0.4218 - model_41_loss: 0.6925 - model_41_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5051 - model_40_loss: 0.4205 - model_41_loss: 0.6924 - model_41_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.5087 - model_40_loss: 0.4192 - model_41_loss: 0.6928 - model_41_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9204 - model_41_loss: 0.6922 - model_41_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4983 - model_40_loss: 0.4194 - model_41_loss: 0.6915 - model_41_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4980 - model_40_loss: 0.4202 - model_41_loss: 0.6913 - model_41_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5026 - model_40_loss: 0.4164 - model_41_loss: 0.6915 - model_41_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5004 - model_40_loss: 0.4181 - model_41_loss: 0.6913 - model_41_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5020 - model_40_loss: 0.4187 - model_41_loss: 0.6915 - model_41_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9167 - model_41_loss: 0.6906 - model_41_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4954 - model_40_loss: 0.4201 - model_41_loss: 0.6911 - model_41_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4957 - model_40_loss: 0.4169 - model_41_loss: 0.6907 - model_41_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4936 - model_40_loss: 0.4199 - model_41_loss: 0.6908 - model_41_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4951 - model_40_loss: 0.4191 - model_41_loss: 0.6909 - model_41_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4971 - model_40_loss: 0.4180 - model_41_loss: 0.6909 - model_41_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9155 - model_41_loss: 0.6905 - model_41_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4969 - model_40_loss: 0.4173 - model_41_loss: 0.6913 - model_41_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4899 - model_40_loss: 0.4225 - model_41_loss: 0.6909 - model_41_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4941 - model_40_loss: 0.4200 - model_41_loss: 0.6910 - model_41_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4961 - model_40_loss: 0.4205 - model_41_loss: 0.6913 - model_41_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4974 - model_40_loss: 0.4190 - model_41_loss: 0.6913 - model_41_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9150 - model_41_loss: 0.6911 - model_41_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4954 - model_40_loss: 0.4201 - model_41_loss: 0.6911 - model_41_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4957 - model_40_loss: 0.4208 - model_41_loss: 0.6914 - model_41_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4950 - model_40_loss: 0.4217 - model_41_loss: 0.6912 - model_41_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4965 - model_40_loss: 0.4210 - model_41_loss: 0.6913 - model_41_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5000 - model_40_loss: 0.4209 - model_41_loss: 0.6916 - model_41_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9230 - model_41_loss: 0.6927 - model_41_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5004 - model_40_loss: 0.4221 - model_41_loss: 0.6920 - model_41_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4998 - model_40_loss: 0.4211 - model_41_loss: 0.6917 - model_41_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5010 - model_40_loss: 0.4219 - model_41_loss: 0.6920 - model_41_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5048 - model_40_loss: 0.4216 - model_41_loss: 0.6925 - model_41_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5045 - model_40_loss: 0.4225 - model_41_loss: 0.6927 - model_41_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9255 - model_41_loss: 0.6921 - model_41_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.5029 - model_40_loss: 0.4222 - model_41_loss: 0.6926 - model_41_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5008 - model_40_loss: 0.4245 - model_41_loss: 0.6925 - model_41_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5041 - model_40_loss: 0.4233 - model_41_loss: 0.6927 - model_41_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5002 - model_40_loss: 0.4261 - model_41_loss: 0.6928 - model_41_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5046 - model_40_loss: 0.4236 - model_41_loss: 0.6930 - model_41_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9257 - model_41_loss: 0.6928 - model_41_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.5000 - model_40_loss: 0.4230 - model_41_loss: 0.6925 - model_41_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4976 - model_40_loss: 0.4252 - model_41_loss: 0.6929 - model_41_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5009 - model_40_loss: 0.4220 - model_41_loss: 0.6929 - model_41_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4988 - model_40_loss: 0.4231 - model_41_loss: 0.6929 - model_41_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4997 - model_40_loss: 0.4234 - model_41_loss: 0.6929 - model_41_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9236 - model_41_loss: 0.6936 - model_41_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4986 - model_40_loss: 0.4235 - model_41_loss: 0.6927 - model_41_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4964 - model_40_loss: 0.4240 - model_41_loss: 0.6924 - model_41_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5004 - model_40_loss: 0.4232 - model_41_loss: 0.6927 - model_41_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4991 - model_40_loss: 0.4230 - model_41_loss: 0.6927 - model_41_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5019 - model_40_loss: 0.4205 - model_41_loss: 0.6926 - model_41_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9236 - model_41_loss: 0.6928 - model_41_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4990 - model_40_loss: 0.4213 - model_41_loss: 0.6924 - model_41_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4980 - model_40_loss: 0.4224 - model_41_loss: 0.6923 - model_41_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5021 - model_40_loss: 0.4220 - model_41_loss: 0.6926 - model_41_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5026 - model_40_loss: 0.4194 - model_41_loss: 0.6923 - model_41_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5021 - model_40_loss: 0.4197 - model_41_loss: 0.6923 - model_41_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9246 - model_41_loss: 0.6927 - model_41_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5033 - model_40_loss: 0.4191 - model_41_loss: 0.6922 - model_41_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5042 - model_40_loss: 0.4185 - model_41_loss: 0.6923 - model_41_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5043 - model_40_loss: 0.4179 - model_41_loss: 0.6923 - model_41_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5043 - model_40_loss: 0.4185 - model_41_loss: 0.6921 - model_41_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5059 - model_40_loss: 0.4184 - model_41_loss: 0.6923 - model_41_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9236 - model_41_loss: 0.6924 - model_41_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.5030 - model_40_loss: 0.4185 - model_41_loss: 0.6921 - model_41_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5013 - model_40_loss: 0.4201 - model_41_loss: 0.6918 - model_41_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5013 - model_40_loss: 0.4187 - model_41_loss: 0.6919 - model_41_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5036 - model_40_loss: 0.4174 - model_41_loss: 0.6920 - model_41_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5057 - model_40_loss: 0.4165 - model_41_loss: 0.6920 - model_41_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9239 - model_41_loss: 0.6922 - model_41_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.5025 - model_40_loss: 0.4183 - model_41_loss: 0.6920 - model_41_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5026 - model_40_loss: 0.4172 - model_41_loss: 0.6920 - model_41_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5031 - model_40_loss: 0.4167 - model_41_loss: 0.6919 - model_41_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5010 - model_40_loss: 0.4191 - model_41_loss: 0.6920 - model_41_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5026 - model_40_loss: 0.4171 - model_41_loss: 0.6920 - model_41_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9216 - model_41_loss: 0.6926 - model_41_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4990 - model_40_loss: 0.4194 - model_41_loss: 0.6916 - model_41_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5015 - model_40_loss: 0.4179 - model_41_loss: 0.6920 - model_41_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5011 - model_40_loss: 0.4187 - model_41_loss: 0.6919 - model_41_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5034 - model_40_loss: 0.4183 - model_41_loss: 0.6921 - model_41_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5017 - model_40_loss: 0.4186 - model_41_loss: 0.6919 - model_41_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9214 - model_41_loss: 0.6921 - model_41_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.5043 - model_40_loss: 0.4182 - model_41_loss: 0.6922 - model_41_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5031 - model_40_loss: 0.4190 - model_41_loss: 0.6921 - model_41_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5066 - model_40_loss: 0.4166 - model_41_loss: 0.6920 - model_41_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5041 - model_40_loss: 0.4192 - model_41_loss: 0.6921 - model_41_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5067 - model_40_loss: 0.4190 - model_41_loss: 0.6922 - model_41_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9264 - model_41_loss: 0.6933 - model_41_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.5046 - model_40_loss: 0.4190 - model_41_loss: 0.6922 - model_41_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5053 - model_40_loss: 0.4199 - model_41_loss: 0.6923 - model_41_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5066 - model_40_loss: 0.4201 - model_41_loss: 0.6926 - model_41_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5068 - model_40_loss: 0.4204 - model_41_loss: 0.6925 - model_41_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5055 - model_40_loss: 0.4216 - model_41_loss: 0.6926 - model_41_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: 6.9283 - model_41_loss: 0.6932 - model_41_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.5044 - model_40_loss: 0.4220 - model_41_loss: 0.6925 - model_41_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5057 - model_40_loss: 0.4217 - model_41_loss: 0.6927 - model_41_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5060 - model_40_loss: 0.4223 - model_41_loss: 0.6929 - model_41_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5055 - model_40_loss: 0.4218 - model_41_loss: 0.6927 - model_41_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5049 - model_40_loss: 0.4223 - model_41_loss: 0.6926 - model_41_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9249 - model_41_loss: 0.6919 - model_41_1_loss: 0.693 - 0s 16us/sample - loss: 6.9274 - model_41_loss: 0.6927 - model_41_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5068 - model_40_loss: 0.4204 - model_41_loss: 0.6928 - model_41_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5052 - model_40_loss: 0.4222 - model_41_loss: 0.6928 - model_41_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5053 - model_40_loss: 0.4217 - model_41_loss: 0.6927 - model_41_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5061 - model_40_loss: 0.4209 - model_41_loss: 0.6929 - model_41_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5072 - model_40_loss: 0.4204 - model_41_loss: 0.6928 - model_41_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9272 - model_41_loss: 0.6934 - model_41_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5032 - model_40_loss: 0.4222 - model_41_loss: 0.6925 - model_41_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5074 - model_40_loss: 0.4188 - model_41_loss: 0.6927 - model_41_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5054 - model_40_loss: 0.4199 - model_41_loss: 0.6925 - model_41_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5071 - model_40_loss: 0.4186 - model_41_loss: 0.6927 - model_41_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5040 - model_40_loss: 0.4194 - model_41_loss: 0.6923 - model_41_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9247 - model_41_loss: 0.6926 - model_41_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5049 - model_40_loss: 0.4182 - model_41_loss: 0.6925 - model_41_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5073 - model_40_loss: 0.4160 - model_41_loss: 0.6924 - model_41_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5059 - model_40_loss: 0.4156 - model_41_loss: 0.6924 - model_41_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5056 - model_40_loss: 0.4168 - model_41_loss: 0.6924 - model_41_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5052 - model_40_loss: 0.4163 - model_41_loss: 0.6922 - model_41_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9240 - model_41_loss: 0.6928 - model_41_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.5072 - model_40_loss: 0.4137 - model_41_loss: 0.6922 - model_41_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5047 - model_40_loss: 0.4149 - model_41_loss: 0.6921 - model_41_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5066 - model_40_loss: 0.4131 - model_41_loss: 0.6922 - model_41_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5047 - model_40_loss: 0.4140 - model_41_loss: 0.6920 - model_41_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5061 - model_40_loss: 0.4128 - model_41_loss: 0.6919 - model_41_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9224 - model_41_loss: 0.6922 - model_41_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5069 - model_40_loss: 0.4125 - model_41_loss: 0.6920 - model_41_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5082 - model_40_loss: 0.4106 - model_41_loss: 0.6919 - model_41_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5060 - model_40_loss: 0.4128 - model_41_loss: 0.6919 - model_41_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5060 - model_40_loss: 0.4146 - model_41_loss: 0.6921 - model_41_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5078 - model_40_loss: 0.4121 - model_41_loss: 0.6920 - model_41_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9222 - model_41_loss: 0.6922 - model_41_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.5068 - model_40_loss: 0.4135 - model_41_loss: 0.6920 - model_41_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5065 - model_40_loss: 0.4127 - model_41_loss: 0.6919 - model_41_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5092 - model_40_loss: 0.4127 - model_41_loss: 0.6922 - model_41_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5048 - model_40_loss: 0.4153 - model_41_loss: 0.6919 - model_41_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5064 - model_40_loss: 0.4157 - model_41_loss: 0.6920 - model_41_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9257 - model_41_loss: 0.6926 - model_41_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5084 - model_40_loss: 0.4141 - model_41_loss: 0.6921 - model_41_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.5076 - model_40_loss: 0.4150 - model_41_loss: 0.6921 - model_41_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5095 - model_40_loss: 0.4147 - model_41_loss: 0.6922 - model_41_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5082 - model_40_loss: 0.4151 - model_41_loss: 0.6921 - model_41_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.5075 - model_40_loss: 0.4171 - model_41_loss: 0.6923 - model_41_1_loss: 0.6926\n",
      "For Attention Module: 1.2000000000000002\n",
      "features X: 30940 samples, 75 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.3727 - model_45_loss: 0.6598 - model_45_1_loss: 0.6140\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0006 - model_44_loss: 0.3704 - model_45_loss: 0.6600 - model_45_1_loss: 0.6142\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0170 - model_44_loss: 0.3694 - model_45_loss: 0.6608 - model_45_1_loss: 0.6164\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0223 - model_44_loss: 0.3712 - model_45_loss: 0.6604 - model_45_1_loss: 0.6183\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0318 - model_44_loss: 0.3719 - model_45_loss: 0.6614 - model_45_1_loss: 0.6193\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0580 - model_44_loss: 0.3716 - model_45_loss: 0.6630 - model_45_1_loss: 0.6229\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.4346 - model_45_loss: 0.6628 - model_45_1_loss: 0.6234\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0448 - model_44_loss: 0.3716 - model_45_loss: 0.6614 - model_45_1_loss: 0.6219\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0717 - model_44_loss: 0.3734 - model_45_loss: 0.6636 - model_45_1_loss: 0.6255\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0790 - model_44_loss: 0.3739 - model_45_loss: 0.6635 - model_45_1_loss: 0.6271\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0957 - model_44_loss: 0.3737 - model_45_loss: 0.6647 - model_45_1_loss: 0.6292\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1062 - model_44_loss: 0.3753 - model_45_loss: 0.6654 - model_45_1_loss: 0.6309\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.4851 - model_45_loss: 0.6645 - model_45_1_loss: 0.6314\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1115 - model_44_loss: 0.3756 - model_45_loss: 0.6648 - model_45_1_loss: 0.6326\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1288 - model_44_loss: 0.3769 - model_45_loss: 0.6665 - model_45_1_loss: 0.6347\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1390 - model_44_loss: 0.3774 - model_45_loss: 0.6669 - model_45_1_loss: 0.6364\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1483 - model_44_loss: 0.3781 - model_45_loss: 0.6676 - model_45_1_loss: 0.6377\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1680 - model_44_loss: 0.3796 - model_45_loss: 0.6686 - model_45_1_loss: 0.6409\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.5646 - model_45_loss: 0.6697 - model_45_1_loss: 0.6433\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1681 - model_44_loss: 0.3803 - model_45_loss: 0.6680 - model_45_1_loss: 0.6416\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.1855 - model_44_loss: 0.3820 - model_45_loss: 0.6693 - model_45_1_loss: 0.6442\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2056 - model_44_loss: 0.3831 - model_45_loss: 0.6710 - model_45_1_loss: 0.6468\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2171 - model_44_loss: 0.3828 - model_45_loss: 0.6717 - model_45_1_loss: 0.6483\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2297 - model_44_loss: 0.3848 - model_45_loss: 0.6719 - model_45_1_loss: 0.6510\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.6427 - model_45_loss: 0.6739 - model_45_1_loss: 0.6551\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2457 - model_44_loss: 0.3858 - model_45_loss: 0.6724 - model_45_1_loss: 0.6539\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2548 - model_44_loss: 0.3890 - model_45_loss: 0.6737 - model_45_1_loss: 0.6551\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.2760 - model_44_loss: 0.3890 - model_45_loss: 0.6745 - model_45_1_loss: 0.6585\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3001 - model_44_loss: 0.3915 - model_45_loss: 0.6767 - model_45_1_loss: 0.6617\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3007 - model_44_loss: 0.3923 - model_45_loss: 0.6763 - model_45_1_loss: 0.6623\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.7081 - model_45_loss: 0.6769 - model_45_1_loss: 0.6640\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3091 - model_44_loss: 0.3924 - model_45_loss: 0.6766 - model_45_1_loss: 0.6637\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3184 - model_44_loss: 0.3960 - model_45_loss: 0.6777 - model_45_1_loss: 0.6651\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3384 - model_44_loss: 0.3961 - model_45_loss: 0.6785 - model_45_1_loss: 0.6684\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3504 - model_44_loss: 0.3989 - model_45_loss: 0.6799 - model_45_1_loss: 0.6699\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3630 - model_44_loss: 0.4003 - model_45_loss: 0.6805 - model_45_1_loss: 0.6721\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.7678 - model_45_loss: 0.6808 - model_45_1_loss: 0.6722\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3650 - model_44_loss: 0.4010 - model_45_loss: 0.6811 - model_45_1_loss: 0.6721\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3830 - model_44_loss: 0.4038 - model_45_loss: 0.6821 - model_45_1_loss: 0.6753\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.3830 - model_44_loss: 0.4052 - model_45_loss: 0.6825 - model_45_1_loss: 0.6752\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3947 - model_44_loss: 0.4077 - model_45_loss: 0.6834 - model_45_1_loss: 0.6770\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4108 - model_44_loss: 0.4118 - model_45_loss: 0.6847 - model_45_1_loss: 0.6799\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.8231 - model_45_loss: 0.6839 - model_45_1_loss: 0.67980s - loss: 6.8272 - model_45_loss: 0.6850 - model_45_1_loss: 0.680\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4090 - model_44_loss: 0.4126 - model_45_loss: 0.6853 - model_45_1_loss: 0.6790\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4093 - model_44_loss: 0.4150 - model_45_loss: 0.6852 - model_45_1_loss: 0.6796\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4196 - model_44_loss: 0.4161 - model_45_loss: 0.6859 - model_45_1_loss: 0.6813\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4187 - model_44_loss: 0.4193 - model_45_loss: 0.6855 - model_45_1_loss: 0.6821\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4335 - model_44_loss: 0.4215 - model_45_loss: 0.6872 - model_45_1_loss: 0.6838\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.8597 - model_45_loss: 0.6877 - model_45_1_loss: 0.6841\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4329 - model_44_loss: 0.4235 - model_45_loss: 0.6873 - model_45_1_loss: 0.6840\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4405 - model_44_loss: 0.4262 - model_45_loss: 0.6883 - model_45_1_loss: 0.6850\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4466 - model_44_loss: 0.4294 - model_45_loss: 0.6887 - model_45_1_loss: 0.6865\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4490 - model_44_loss: 0.4302 - model_45_loss: 0.6889 - model_45_1_loss: 0.6869\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4603 - model_44_loss: 0.4335 - model_45_loss: 0.6900 - model_45_1_loss: 0.6888\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.8865 - model_45_loss: 0.6906 - model_45_1_loss: 0.6877\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4459 - model_44_loss: 0.4387 - model_45_loss: 0.6897 - model_45_1_loss: 0.6873\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4531 - model_44_loss: 0.4397 - model_45_loss: 0.6899 - model_45_1_loss: 0.6887\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4576 - model_44_loss: 0.4409 - model_45_loss: 0.6908 - model_45_1_loss: 0.6889\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4602 - model_44_loss: 0.4447 - model_45_loss: 0.6915 - model_45_1_loss: 0.6895\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4603 - model_44_loss: 0.4454 - model_45_loss: 0.6911 - model_45_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9166 - model_45_loss: 0.6921 - model_45_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4584 - model_44_loss: 0.4487 - model_45_loss: 0.6913 - model_45_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4591 - model_44_loss: 0.4515 - model_45_loss: 0.6917 - model_45_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4627 - model_44_loss: 0.4497 - model_45_loss: 0.6918 - model_45_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4649 - model_44_loss: 0.4540 - model_45_loss: 0.6923 - model_45_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4691 - model_44_loss: 0.4514 - model_45_loss: 0.6921 - model_45_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9218 - model_45_loss: 0.6930 - model_45_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4679 - model_44_loss: 0.4550 - model_45_loss: 0.6926 - model_45_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4675 - model_44_loss: 0.4569 - model_45_loss: 0.6927 - model_45_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4702 - model_44_loss: 0.4550 - model_45_loss: 0.6926 - model_45_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4683 - model_44_loss: 0.4572 - model_45_loss: 0.6928 - model_45_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4662 - model_44_loss: 0.4575 - model_45_loss: 0.6926 - model_45_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9264 - model_45_loss: 0.6937 - model_45_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4678 - model_44_loss: 0.4579 - model_45_loss: 0.6927 - model_45_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4673 - model_44_loss: 0.4576 - model_45_loss: 0.6927 - model_45_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4703 - model_44_loss: 0.4567 - model_45_loss: 0.6928 - model_45_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4685 - model_44_loss: 0.4600 - model_45_loss: 0.6930 - model_45_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4664 - model_44_loss: 0.4587 - model_45_loss: 0.6927 - model_45_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9277 - model_45_loss: 0.6926 - model_45_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4715 - model_44_loss: 0.4551 - model_45_loss: 0.6928 - model_45_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4695 - model_44_loss: 0.4561 - model_45_loss: 0.6926 - model_45_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4729 - model_44_loss: 0.4538 - model_45_loss: 0.6929 - model_45_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4761 - model_44_loss: 0.4523 - model_45_loss: 0.6927 - model_45_1_loss: 0.6930\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4748 - model_44_loss: 0.4511 - model_45_loss: 0.6928 - model_45_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9281 - model_45_loss: 0.6925 - model_45_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4766 - model_44_loss: 0.4499 - model_45_loss: 0.6925 - model_45_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4735 - model_44_loss: 0.4501 - model_45_loss: 0.6924 - model_45_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4779 - model_44_loss: 0.4466 - model_45_loss: 0.6925 - model_45_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4781 - model_44_loss: 0.4463 - model_45_loss: 0.6924 - model_45_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4775 - model_44_loss: 0.4461 - model_45_loss: 0.6924 - model_45_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9225 - model_45_loss: 0.6917 - model_45_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4782 - model_44_loss: 0.4438 - model_45_loss: 0.6924 - model_45_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4800 - model_44_loss: 0.4423 - model_45_loss: 0.6924 - model_45_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4777 - model_44_loss: 0.4420 - model_45_loss: 0.6922 - model_45_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4780 - model_44_loss: 0.4422 - model_45_loss: 0.6921 - model_45_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4799 - model_44_loss: 0.4408 - model_45_loss: 0.6923 - model_45_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9181 - model_45_loss: 0.6918 - model_45_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4821 - model_44_loss: 0.4368 - model_45_loss: 0.6921 - model_45_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4791 - model_44_loss: 0.4388 - model_45_loss: 0.6920 - model_45_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4779 - model_44_loss: 0.4383 - model_45_loss: 0.6917 - model_45_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4801 - model_44_loss: 0.4365 - model_45_loss: 0.6919 - model_45_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4772 - model_44_loss: 0.4373 - model_45_loss: 0.6917 - model_45_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9186 - model_45_loss: 0.6924 - model_45_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4801 - model_44_loss: 0.4368 - model_45_loss: 0.6920 - model_45_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4787 - model_44_loss: 0.4362 - model_45_loss: 0.6918 - model_45_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4761 - model_44_loss: 0.4360 - model_45_loss: 0.6917 - model_45_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4781 - model_44_loss: 0.4359 - model_45_loss: 0.6915 - model_45_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4777 - model_44_loss: 0.4373 - model_45_loss: 0.6918 - model_45_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9156 - model_45_loss: 0.6923 - model_45_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4816 - model_44_loss: 0.4346 - model_45_loss: 0.6917 - model_45_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4792 - model_44_loss: 0.4365 - model_45_loss: 0.6915 - model_45_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4796 - model_44_loss: 0.4360 - model_45_loss: 0.6917 - model_45_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4845 - model_44_loss: 0.4333 - model_45_loss: 0.6917 - model_45_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4813 - model_44_loss: 0.4359 - model_45_loss: 0.6917 - model_45_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9163 - model_45_loss: 0.6912 - model_45_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4812 - model_44_loss: 0.4353 - model_45_loss: 0.6920 - model_45_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4826 - model_44_loss: 0.4364 - model_45_loss: 0.6921 - model_45_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4844 - model_44_loss: 0.4356 - model_45_loss: 0.6922 - model_45_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4810 - model_44_loss: 0.4366 - model_45_loss: 0.6921 - model_45_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4833 - model_44_loss: 0.4356 - model_45_loss: 0.6920 - model_45_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9205 - model_45_loss: 0.6917 - model_45_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4839 - model_44_loss: 0.4354 - model_45_loss: 0.6921 - model_45_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4788 - model_44_loss: 0.4397 - model_45_loss: 0.6919 - model_45_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4844 - model_44_loss: 0.4362 - model_45_loss: 0.6922 - model_45_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4812 - model_44_loss: 0.4398 - model_45_loss: 0.6921 - model_45_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4850 - model_44_loss: 0.4381 - model_45_loss: 0.6924 - model_45_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9245 - model_45_loss: 0.6924 - model_45_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4817 - model_44_loss: 0.4400 - model_45_loss: 0.6923 - model_45_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4814 - model_44_loss: 0.4403 - model_45_loss: 0.6924 - model_45_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4839 - model_44_loss: 0.4394 - model_45_loss: 0.6924 - model_45_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4874 - model_44_loss: 0.4375 - model_45_loss: 0.6926 - model_45_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4845 - model_44_loss: 0.4390 - model_45_loss: 0.6924 - model_45_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9245 - model_45_loss: 0.6927 - model_45_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4839 - model_44_loss: 0.4405 - model_45_loss: 0.6925 - model_45_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4869 - model_44_loss: 0.4395 - model_45_loss: 0.6927 - model_45_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4868 - model_44_loss: 0.4399 - model_45_loss: 0.6928 - model_45_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4859 - model_44_loss: 0.4392 - model_45_loss: 0.6926 - model_45_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4855 - model_44_loss: 0.4403 - model_45_loss: 0.6928 - model_45_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9255 - model_45_loss: 0.6918 - model_45_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4843 - model_44_loss: 0.4413 - model_45_loss: 0.6927 - model_45_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4854 - model_44_loss: 0.4403 - model_45_loss: 0.6927 - model_45_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4868 - model_44_loss: 0.4388 - model_45_loss: 0.6928 - model_45_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4855 - model_44_loss: 0.4393 - model_45_loss: 0.6926 - model_45_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4864 - model_44_loss: 0.4387 - model_45_loss: 0.6926 - model_45_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9244 - model_45_loss: 0.6924 - model_45_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4855 - model_44_loss: 0.4393 - model_45_loss: 0.6928 - model_45_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4853 - model_44_loss: 0.4369 - model_45_loss: 0.6926 - model_45_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4850 - model_44_loss: 0.4383 - model_45_loss: 0.6927 - model_45_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4841 - model_44_loss: 0.4378 - model_45_loss: 0.6927 - model_45_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4851 - model_44_loss: 0.4384 - model_45_loss: 0.6927 - model_45_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9250 - model_45_loss: 0.6926 - model_45_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4827 - model_44_loss: 0.4378 - model_45_loss: 0.6927 - model_45_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4866 - model_44_loss: 0.4369 - model_45_loss: 0.6928 - model_45_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4850 - model_44_loss: 0.4373 - model_45_loss: 0.6929 - model_45_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4849 - model_44_loss: 0.4356 - model_45_loss: 0.6926 - model_45_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4882 - model_44_loss: 0.4349 - model_45_loss: 0.6928 - model_45_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9215 - model_45_loss: 0.6926 - model_45_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4834 - model_44_loss: 0.4367 - model_45_loss: 0.6925 - model_45_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4846 - model_44_loss: 0.4356 - model_45_loss: 0.6927 - model_45_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4856 - model_44_loss: 0.4355 - model_45_loss: 0.6926 - model_45_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4865 - model_44_loss: 0.4341 - model_45_loss: 0.6927 - model_45_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4866 - model_44_loss: 0.4353 - model_45_loss: 0.6926 - model_45_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9229 - model_45_loss: 0.6927 - model_45_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4848 - model_44_loss: 0.4357 - model_45_loss: 0.6923 - model_45_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4879 - model_44_loss: 0.4353 - model_45_loss: 0.6926 - model_45_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4864 - model_44_loss: 0.4350 - model_45_loss: 0.6923 - model_45_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4885 - model_44_loss: 0.4346 - model_45_loss: 0.6927 - model_45_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4882 - model_44_loss: 0.4341 - model_45_loss: 0.6923 - model_45_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9228 - model_45_loss: 0.6927 - model_45_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4903 - model_44_loss: 0.4336 - model_45_loss: 0.6926 - model_45_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4887 - model_44_loss: 0.4331 - model_45_loss: 0.6924 - model_45_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4854 - model_44_loss: 0.4365 - model_45_loss: 0.6922 - model_45_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4890 - model_44_loss: 0.4343 - model_45_loss: 0.6924 - model_45_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4886 - model_44_loss: 0.4357 - model_45_loss: 0.6924 - model_45_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9245 - model_45_loss: 0.6923 - model_45_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4871 - model_44_loss: 0.4344 - model_45_loss: 0.6921 - model_45_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4917 - model_44_loss: 0.4323 - model_45_loss: 0.6924 - model_45_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4909 - model_44_loss: 0.4335 - model_45_loss: 0.6926 - model_45_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4867 - model_44_loss: 0.4354 - model_45_loss: 0.6920 - model_45_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4898 - model_44_loss: 0.4346 - model_45_loss: 0.6924 - model_45_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: 6.9247 - model_45_loss: 0.6924 - model_45_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4894 - model_44_loss: 0.4341 - model_45_loss: 0.6922 - model_45_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4873 - model_44_loss: 0.4363 - model_45_loss: 0.6922 - model_45_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4890 - model_44_loss: 0.4347 - model_45_loss: 0.6922 - model_45_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4900 - model_44_loss: 0.4340 - model_45_loss: 0.6923 - model_45_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4886 - model_44_loss: 0.4338 - model_45_loss: 0.6922 - model_45_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9233 - model_45_loss: 0.6927 - model_45_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4849 - model_44_loss: 0.4358 - model_45_loss: 0.6921 - model_45_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4870 - model_44_loss: 0.4348 - model_45_loss: 0.6923 - model_45_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4876 - model_44_loss: 0.4340 - model_45_loss: 0.6921 - model_45_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4895 - model_44_loss: 0.4322 - model_45_loss: 0.6923 - model_45_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4885 - model_44_loss: 0.4338 - model_45_loss: 0.6924 - model_45_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9213 - model_45_loss: 0.6930 - model_45_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4853 - model_44_loss: 0.4353 - model_45_loss: 0.6922 - model_45_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4898 - model_44_loss: 0.4316 - model_45_loss: 0.6922 - model_45_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4885 - model_44_loss: 0.4325 - model_45_loss: 0.6923 - model_45_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4881 - model_44_loss: 0.4326 - model_45_loss: 0.6922 - model_45_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4889 - model_44_loss: 0.4322 - model_45_loss: 0.6922 - model_45_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9228 - model_45_loss: 0.6928 - model_45_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4894 - model_44_loss: 0.4316 - model_45_loss: 0.6922 - model_45_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4889 - model_44_loss: 0.4328 - model_45_loss: 0.6923 - model_45_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4884 - model_44_loss: 0.4336 - model_45_loss: 0.6925 - model_45_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4922 - model_44_loss: 0.4312 - model_45_loss: 0.6926 - model_45_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4883 - model_44_loss: 0.4329 - model_45_loss: 0.6925 - model_45_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9252 - model_45_loss: 0.6936 - model_45_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4867 - model_44_loss: 0.4351 - model_45_loss: 0.6925 - model_45_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4888 - model_44_loss: 0.4329 - model_45_loss: 0.6926 - model_45_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4922 - model_44_loss: 0.4324 - model_45_loss: 0.6929 - model_45_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4909 - model_44_loss: 0.4326 - model_45_loss: 0.6927 - model_45_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4885 - model_44_loss: 0.4335 - model_45_loss: 0.6927 - model_45_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9254 - model_45_loss: 0.6925 - model_45_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4879 - model_44_loss: 0.4337 - model_45_loss: 0.6926 - model_45_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4905 - model_44_loss: 0.4329 - model_45_loss: 0.6929 - model_45_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4892 - model_44_loss: 0.4336 - model_45_loss: 0.6928 - model_45_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4895 - model_44_loss: 0.4333 - model_45_loss: 0.6927 - model_45_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4879 - model_44_loss: 0.4335 - model_45_loss: 0.6927 - model_45_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9235 - model_45_loss: 0.6926 - model_45_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4883 - model_44_loss: 0.4344 - model_45_loss: 0.6927 - model_45_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4896 - model_44_loss: 0.4322 - model_45_loss: 0.6928 - model_45_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4904 - model_44_loss: 0.4315 - model_45_loss: 0.6928 - model_45_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4895 - model_44_loss: 0.4320 - model_45_loss: 0.6927 - model_45_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4910 - model_44_loss: 0.4315 - model_45_loss: 0.6929 - model_45_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9230 - model_45_loss: 0.6928 - model_45_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4880 - model_44_loss: 0.4327 - model_45_loss: 0.6927 - model_45_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4888 - model_44_loss: 0.4326 - model_45_loss: 0.6929 - model_45_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4888 - model_44_loss: 0.4321 - model_45_loss: 0.6927 - model_45_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4901 - model_44_loss: 0.4315 - model_45_loss: 0.6927 - model_45_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4891 - model_44_loss: 0.4306 - model_45_loss: 0.6925 - model_45_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9224 - model_45_loss: 0.6922 - model_45_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4892 - model_44_loss: 0.4315 - model_45_loss: 0.6924 - model_45_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4896 - model_44_loss: 0.4315 - model_45_loss: 0.6925 - model_45_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4895 - model_44_loss: 0.4305 - model_45_loss: 0.6924 - model_45_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4913 - model_44_loss: 0.4300 - model_45_loss: 0.6924 - model_45_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4943 - model_44_loss: 0.4277 - model_45_loss: 0.6924 - model_45_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9225 - model_45_loss: 0.6924 - model_45_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4902 - model_44_loss: 0.4290 - model_45_loss: 0.6920 - model_45_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4925 - model_44_loss: 0.4289 - model_45_loss: 0.6923 - model_45_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4935 - model_44_loss: 0.4278 - model_45_loss: 0.6922 - model_45_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4907 - model_44_loss: 0.4292 - model_45_loss: 0.6921 - model_45_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4915 - model_44_loss: 0.4286 - model_45_loss: 0.6921 - model_45_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9183 - model_45_loss: 0.6914 - model_45_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4903 - model_44_loss: 0.4281 - model_45_loss: 0.6920 - model_45_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4906 - model_44_loss: 0.4279 - model_45_loss: 0.6922 - model_45_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4895 - model_44_loss: 0.4290 - model_45_loss: 0.6920 - model_45_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4906 - model_44_loss: 0.4278 - model_45_loss: 0.6921 - model_45_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4909 - model_44_loss: 0.4275 - model_45_loss: 0.6922 - model_45_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9202 - model_45_loss: 0.6918 - model_45_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4892 - model_44_loss: 0.4290 - model_45_loss: 0.6920 - model_45_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4901 - model_44_loss: 0.4276 - model_45_loss: 0.6920 - model_45_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4912 - model_44_loss: 0.4290 - model_45_loss: 0.6922 - model_45_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4892 - model_44_loss: 0.4301 - model_45_loss: 0.6920 - model_45_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4900 - model_44_loss: 0.4308 - model_45_loss: 0.6922 - model_45_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9220 - model_45_loss: 0.6929 - model_45_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4902 - model_44_loss: 0.4307 - model_45_loss: 0.6922 - model_45_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4920 - model_44_loss: 0.4298 - model_45_loss: 0.6922 - model_45_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4893 - model_44_loss: 0.4322 - model_45_loss: 0.6923 - model_45_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4890 - model_44_loss: 0.4330 - model_45_loss: 0.6923 - model_45_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4918 - model_44_loss: 0.4322 - model_45_loss: 0.6925 - model_45_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9249 - model_45_loss: 0.6918 - model_45_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4933 - model_44_loss: 0.4313 - model_45_loss: 0.6926 - model_45_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4896 - model_44_loss: 0.4336 - model_45_loss: 0.6924 - model_45_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4927 - model_44_loss: 0.4316 - model_45_loss: 0.6925 - model_45_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4908 - model_44_loss: 0.4339 - model_45_loss: 0.6925 - model_45_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4911 - model_44_loss: 0.4328 - model_45_loss: 0.6924 - model_45_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9249 - model_45_loss: 0.6925 - model_45_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4932 - model_44_loss: 0.4326 - model_45_loss: 0.6926 - model_45_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4936 - model_44_loss: 0.4327 - model_45_loss: 0.6927 - model_45_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4932 - model_44_loss: 0.4318 - model_45_loss: 0.6925 - model_45_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4920 - model_44_loss: 0.4325 - model_45_loss: 0.6925 - model_45_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4929 - model_44_loss: 0.4318 - model_45_loss: 0.6926 - model_45_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9230 - model_45_loss: 0.6923 - model_45_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4923 - model_44_loss: 0.4300 - model_45_loss: 0.6923 - model_45_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4903 - model_44_loss: 0.4310 - model_45_loss: 0.6924 - model_45_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4929 - model_44_loss: 0.4278 - model_45_loss: 0.6923 - model_45_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4908 - model_44_loss: 0.4300 - model_45_loss: 0.6923 - model_45_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4910 - model_44_loss: 0.4296 - model_45_loss: 0.6923 - model_45_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9202 - model_45_loss: 0.6923 - model_45_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4907 - model_44_loss: 0.4271 - model_45_loss: 0.6921 - model_45_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4911 - model_44_loss: 0.4262 - model_45_loss: 0.6921 - model_45_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4918 - model_44_loss: 0.4258 - model_45_loss: 0.6922 - model_45_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4919 - model_44_loss: 0.4268 - model_45_loss: 0.6922 - model_45_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4924 - model_44_loss: 0.4269 - model_45_loss: 0.6924 - model_45_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9163 - model_45_loss: 0.6919 - model_45_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4923 - model_44_loss: 0.4264 - model_45_loss: 0.6922 - model_45_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4910 - model_44_loss: 0.4250 - model_45_loss: 0.6919 - model_45_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4956 - model_44_loss: 0.4223 - model_45_loss: 0.6923 - model_45_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4908 - model_44_loss: 0.4258 - model_45_loss: 0.6921 - model_45_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4901 - model_44_loss: 0.4266 - model_45_loss: 0.6918 - model_45_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9178 - model_45_loss: 0.6927 - model_45_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4935 - model_44_loss: 0.4244 - model_45_loss: 0.6920 - model_45_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4932 - model_44_loss: 0.4241 - model_45_loss: 0.6919 - model_45_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4905 - model_44_loss: 0.4264 - model_45_loss: 0.6918 - model_45_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4903 - model_44_loss: 0.4280 - model_45_loss: 0.6920 - model_45_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4927 - model_44_loss: 0.4256 - model_45_loss: 0.6921 - model_45_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9199 - model_45_loss: 0.6915 - model_45_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4926 - model_44_loss: 0.4274 - model_45_loss: 0.6923 - model_45_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4895 - model_44_loss: 0.4292 - model_45_loss: 0.6920 - model_45_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4924 - model_44_loss: 0.4288 - model_45_loss: 0.6923 - model_45_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4949 - model_44_loss: 0.4264 - model_45_loss: 0.6922 - model_45_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4928 - model_44_loss: 0.4284 - model_45_loss: 0.6924 - model_45_1_loss: 0.6919\n",
      "For Attention Module: 1.3000000000000003\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.3312 - model_49_loss: 0.6591 - model_49_1_loss: 0.6068\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -5.9551 - model_48_loss: 0.3769 - model_49_loss: 0.6590 - model_49_1_loss: 0.6075\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -5.9678 - model_48_loss: 0.3789 - model_49_loss: 0.6601 - model_49_1_loss: 0.6093\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -5.9772 - model_48_loss: 0.3768 - model_49_loss: 0.6607 - model_49_1_loss: 0.6101\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -5.9855 - model_48_loss: 0.3804 - model_49_loss: 0.6603 - model_49_1_loss: 0.6129\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -5.9977 - model_48_loss: 0.3784 - model_49_loss: 0.6607 - model_49_1_loss: 0.6145\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.3772 - model_49_loss: 0.6611 - model_49_1_loss: 0.6145\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0000 - model_48_loss: 0.3798 - model_49_loss: 0.6612 - model_49_1_loss: 0.6148\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0078 - model_48_loss: 0.3835 - model_49_loss: 0.6632 - model_49_1_loss: 0.6151\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0184 - model_48_loss: 0.3836 - model_49_loss: 0.6613 - model_49_1_loss: 0.6191\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0221 - model_48_loss: 0.3850 - model_49_loss: 0.6617 - model_49_1_loss: 0.6197\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.0393 - model_48_loss: 0.3871 - model_49_loss: 0.6631 - model_49_1_loss: 0.6222\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.4294 - model_49_loss: 0.6643 - model_49_1_loss: 0.6219\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0402 - model_48_loss: 0.3878 - model_49_loss: 0.6648 - model_49_1_loss: 0.6208\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0443 - model_48_loss: 0.3919 - model_49_loss: 0.6642 - model_49_1_loss: 0.6230\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0532 - model_48_loss: 0.3921 - model_49_loss: 0.6645 - model_49_1_loss: 0.6245\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0669 - model_48_loss: 0.3936 - model_49_loss: 0.6654 - model_49_1_loss: 0.6267\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0666 - model_48_loss: 0.3958 - model_49_loss: 0.6644 - model_49_1_loss: 0.6281\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.4877 - model_49_loss: 0.6661 - model_49_1_loss: 0.6309\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0903 - model_48_loss: 0.3955 - model_49_loss: 0.6667 - model_49_1_loss: 0.6305\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0912 - model_48_loss: 0.3994 - model_49_loss: 0.6671 - model_49_1_loss: 0.6310\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1093 - model_48_loss: 0.3998 - model_49_loss: 0.6676 - model_49_1_loss: 0.6342\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1227 - model_48_loss: 0.4021 - model_49_loss: 0.6695 - model_49_1_loss: 0.6355\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1310 - model_48_loss: 0.4049 - model_49_loss: 0.6695 - model_49_1_loss: 0.6377\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.5457 - model_49_loss: 0.6695 - model_49_1_loss: 0.6391\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1416 - model_48_loss: 0.4053 - model_49_loss: 0.6709 - model_49_1_loss: 0.6385\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1626 - model_48_loss: 0.4052 - model_49_loss: 0.6723 - model_49_1_loss: 0.6413\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1825 - model_48_loss: 0.4086 - model_49_loss: 0.6746 - model_49_1_loss: 0.6436\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1872 - model_48_loss: 0.4101 - model_49_loss: 0.6741 - model_49_1_loss: 0.6454\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1982 - model_48_loss: 0.4133 - model_49_loss: 0.6745 - model_49_1_loss: 0.6478\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.6225 - model_49_loss: 0.6751 - model_49_1_loss: 0.6499\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2044 - model_48_loss: 0.4152 - model_49_loss: 0.6738 - model_49_1_loss: 0.6501\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2202 - model_48_loss: 0.4160 - model_49_loss: 0.6755 - model_49_1_loss: 0.6517\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2313 - model_48_loss: 0.4183 - model_49_loss: 0.6755 - model_49_1_loss: 0.6544\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2404 - model_48_loss: 0.4224 - model_49_loss: 0.6762 - model_49_1_loss: 0.6564\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2616 - model_48_loss: 0.4210 - model_49_loss: 0.6783 - model_49_1_loss: 0.6583\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.6885 - model_49_loss: 0.6793 - model_49_1_loss: 0.6587\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2648 - model_48_loss: 0.4238 - model_49_loss: 0.6781 - model_49_1_loss: 0.6596\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2689 - model_48_loss: 0.4259 - model_49_loss: 0.6785 - model_49_1_loss: 0.6605\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2924 - model_48_loss: 0.4294 - model_49_loss: 0.6808 - model_49_1_loss: 0.6636\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2963 - model_48_loss: 0.4288 - model_49_loss: 0.6810 - model_49_1_loss: 0.6640\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3115 - model_48_loss: 0.4294 - model_49_loss: 0.6807 - model_49_1_loss: 0.6675\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.7441 - model_49_loss: 0.6812 - model_49_1_loss: 0.6668\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3051 - model_48_loss: 0.4333 - model_49_loss: 0.6817 - model_49_1_loss: 0.6660\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3093 - model_48_loss: 0.4346 - model_49_loss: 0.6809 - model_49_1_loss: 0.6678\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3220 - model_48_loss: 0.4372 - model_49_loss: 0.6826 - model_49_1_loss: 0.6693\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3238 - model_48_loss: 0.4389 - model_49_loss: 0.6821 - model_49_1_loss: 0.6704\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3349 - model_48_loss: 0.4397 - model_49_loss: 0.6833 - model_49_1_loss: 0.6716\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.7834 - model_49_loss: 0.6830 - model_49_1_loss: 0.6738\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3481 - model_48_loss: 0.4424 - model_49_loss: 0.6850 - model_49_1_loss: 0.6731\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3566 - model_48_loss: 0.4431 - model_49_loss: 0.6853 - model_49_1_loss: 0.6746\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3618 - model_48_loss: 0.4457 - model_49_loss: 0.6852 - model_49_1_loss: 0.6763\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3658 - model_48_loss: 0.4484 - model_49_loss: 0.6854 - model_49_1_loss: 0.6774\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3741 - model_48_loss: 0.4521 - model_49_loss: 0.6861 - model_49_1_loss: 0.6792\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.8274 - model_49_loss: 0.6854 - model_49_1_loss: 0.6794\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3790 - model_48_loss: 0.4526 - model_49_loss: 0.6865 - model_49_1_loss: 0.6798\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3758 - model_48_loss: 0.4548 - model_49_loss: 0.6865 - model_49_1_loss: 0.6796\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3917 - model_48_loss: 0.4575 - model_49_loss: 0.6876 - model_49_1_loss: 0.6823\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3853 - model_48_loss: 0.4603 - model_49_loss: 0.6871 - model_49_1_loss: 0.6820\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3941 - model_48_loss: 0.4603 - model_49_loss: 0.6878 - model_49_1_loss: 0.6831\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.8575 - model_49_loss: 0.6883 - model_49_1_loss: 0.6833\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3920 - model_48_loss: 0.4641 - model_49_loss: 0.6876 - model_49_1_loss: 0.6837\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4039 - model_48_loss: 0.4638 - model_49_loss: 0.6886 - model_49_1_loss: 0.6850\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4011 - model_48_loss: 0.4679 - model_49_loss: 0.6886 - model_49_1_loss: 0.6852\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4029 - model_48_loss: 0.4691 - model_49_loss: 0.6884 - model_49_1_loss: 0.6860\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4117 - model_48_loss: 0.4698 - model_49_loss: 0.6893 - model_49_1_loss: 0.6870\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.8839 - model_49_loss: 0.6892 - model_49_1_loss: 0.6873\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4081 - model_48_loss: 0.4726 - model_49_loss: 0.6892 - model_49_1_loss: 0.6870\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4142 - model_48_loss: 0.4741 - model_49_loss: 0.6899 - model_49_1_loss: 0.6878\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4124 - model_48_loss: 0.4793 - model_49_loss: 0.6903 - model_49_1_loss: 0.6881\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4124 - model_48_loss: 0.4784 - model_49_loss: 0.6895 - model_49_1_loss: 0.6886\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4146 - model_48_loss: 0.4801 - model_49_loss: 0.6902 - model_49_1_loss: 0.6888\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9066 - model_49_loss: 0.6914 - model_49_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4161 - model_48_loss: 0.4813 - model_49_loss: 0.6904 - model_49_1_loss: 0.6891\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4213 - model_48_loss: 0.4811 - model_49_loss: 0.6910 - model_49_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4188 - model_48_loss: 0.4824 - model_49_loss: 0.6904 - model_49_1_loss: 0.6898\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4178 - model_48_loss: 0.4870 - model_49_loss: 0.6908 - model_49_1_loss: 0.6902\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4209 - model_48_loss: 0.4868 - model_49_loss: 0.6910 - model_49_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9105 - model_49_loss: 0.6919 - model_49_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4185 - model_48_loss: 0.4883 - model_49_loss: 0.6912 - model_49_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4169 - model_48_loss: 0.4877 - model_49_loss: 0.6907 - model_49_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4178 - model_48_loss: 0.4893 - model_49_loss: 0.6909 - model_49_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4257 - model_48_loss: 0.4874 - model_49_loss: 0.6917 - model_49_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4251 - model_48_loss: 0.4912 - model_49_loss: 0.6919 - model_49_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9160 - model_49_loss: 0.6921 - model_49_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4228 - model_48_loss: 0.4892 - model_49_loss: 0.6911 - model_49_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4231 - model_48_loss: 0.4894 - model_49_loss: 0.6915 - model_49_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4261 - model_48_loss: 0.4908 - model_49_loss: 0.6919 - model_49_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4277 - model_48_loss: 0.4896 - model_49_loss: 0.6917 - model_49_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4228 - model_48_loss: 0.4920 - model_49_loss: 0.6917 - model_49_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9189 - model_49_loss: 0.6923 - model_49_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4223 - model_48_loss: 0.4913 - model_49_loss: 0.6915 - model_49_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4251 - model_48_loss: 0.4905 - model_49_loss: 0.6916 - model_49_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4245 - model_48_loss: 0.4917 - model_49_loss: 0.6916 - model_49_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4240 - model_48_loss: 0.4894 - model_49_loss: 0.6912 - model_49_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4270 - model_48_loss: 0.4909 - model_49_loss: 0.6918 - model_49_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9165 - model_49_loss: 0.6910 - model_49_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4252 - model_48_loss: 0.4906 - model_49_loss: 0.6915 - model_49_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4275 - model_48_loss: 0.4883 - model_49_loss: 0.6914 - model_49_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4247 - model_48_loss: 0.4888 - model_49_loss: 0.6912 - model_49_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4312 - model_48_loss: 0.4866 - model_49_loss: 0.6918 - model_49_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4310 - model_48_loss: 0.4846 - model_49_loss: 0.6914 - model_49_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9149 - model_49_loss: 0.6919 - model_49_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4284 - model_48_loss: 0.4860 - model_49_loss: 0.6912 - model_49_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4323 - model_48_loss: 0.4845 - model_49_loss: 0.6914 - model_49_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4279 - model_48_loss: 0.4842 - model_49_loss: 0.6909 - model_49_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4295 - model_48_loss: 0.4843 - model_49_loss: 0.6911 - model_49_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4308 - model_48_loss: 0.4841 - model_49_loss: 0.6913 - model_49_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9137 - model_49_loss: 0.6905 - model_49_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4280 - model_48_loss: 0.4830 - model_49_loss: 0.6909 - model_49_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4272 - model_48_loss: 0.4835 - model_49_loss: 0.6909 - model_49_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4283 - model_48_loss: 0.4826 - model_49_loss: 0.6908 - model_49_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4274 - model_48_loss: 0.4828 - model_49_loss: 0.6905 - model_49_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4316 - model_48_loss: 0.4817 - model_49_loss: 0.6914 - model_49_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9120 - model_49_loss: 0.6908 - model_49_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4317 - model_48_loss: 0.4808 - model_49_loss: 0.6913 - model_49_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4316 - model_48_loss: 0.4808 - model_49_loss: 0.6913 - model_49_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4300 - model_48_loss: 0.4804 - model_49_loss: 0.6907 - model_49_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4318 - model_48_loss: 0.4798 - model_49_loss: 0.6912 - model_49_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4315 - model_48_loss: 0.4787 - model_49_loss: 0.6912 - model_49_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9122 - model_49_loss: 0.6912 - model_49_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4304 - model_48_loss: 0.4782 - model_49_loss: 0.6904 - model_49_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4318 - model_48_loss: 0.4763 - model_49_loss: 0.6905 - model_49_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4337 - model_48_loss: 0.4757 - model_49_loss: 0.6906 - model_49_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4326 - model_48_loss: 0.4746 - model_49_loss: 0.6903 - model_49_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4327 - model_48_loss: 0.4745 - model_49_loss: 0.6905 - model_49_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9096 - model_49_loss: 0.6905 - model_49_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4337 - model_48_loss: 0.4721 - model_49_loss: 0.6903 - model_49_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4351 - model_48_loss: 0.4724 - model_49_loss: 0.6908 - model_49_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4322 - model_48_loss: 0.4737 - model_49_loss: 0.6905 - model_49_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4338 - model_48_loss: 0.4739 - model_49_loss: 0.6909 - model_49_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4364 - model_48_loss: 0.4721 - model_49_loss: 0.6910 - model_49_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9072 - model_49_loss: 0.6916 - model_49_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4351 - model_48_loss: 0.4703 - model_49_loss: 0.6907 - model_49_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4343 - model_48_loss: 0.4704 - model_49_loss: 0.6906 - model_49_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4344 - model_48_loss: 0.4716 - model_49_loss: 0.6905 - model_49_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4361 - model_48_loss: 0.4697 - model_49_loss: 0.6908 - model_49_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4388 - model_48_loss: 0.4684 - model_49_loss: 0.6911 - model_49_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9065 - model_49_loss: 0.6911 - model_49_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4337 - model_48_loss: 0.4702 - model_49_loss: 0.6906 - model_49_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4337 - model_48_loss: 0.4696 - model_49_loss: 0.6906 - model_49_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4366 - model_48_loss: 0.4685 - model_49_loss: 0.6908 - model_49_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4367 - model_48_loss: 0.4693 - model_49_loss: 0.6909 - model_49_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4332 - model_48_loss: 0.4701 - model_49_loss: 0.6905 - model_49_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9072 - model_49_loss: 0.6908 - model_49_1_loss: 0.69090s - loss: 6.9252 - model_49_loss: 0.6936 - model_49_1_loss: 0.691\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4389 - model_48_loss: 0.4693 - model_49_loss: 0.6911 - model_49_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4350 - model_48_loss: 0.4709 - model_49_loss: 0.6911 - model_49_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4404 - model_48_loss: 0.4702 - model_49_loss: 0.6917 - model_49_1_loss: 0.6904\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4420 - model_48_loss: 0.4675 - model_49_loss: 0.6913 - model_49_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 5us/sample - loss: -6.4396 - model_48_loss: 0.4719 - model_49_loss: 0.6915 - model_49_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9132 - model_49_loss: 0.6914 - model_49_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4419 - model_48_loss: 0.4721 - model_49_loss: 0.6918 - model_49_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4428 - model_48_loss: 0.4705 - model_49_loss: 0.6916 - model_49_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4416 - model_48_loss: 0.4711 - model_49_loss: 0.6915 - model_49_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4440 - model_48_loss: 0.4713 - model_49_loss: 0.6920 - model_49_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4447 - model_48_loss: 0.4711 - model_49_loss: 0.6918 - model_49_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9165 - model_49_loss: 0.6925 - model_49_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4424 - model_48_loss: 0.4723 - model_49_loss: 0.6918 - model_49_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4452 - model_48_loss: 0.4717 - model_49_loss: 0.6921 - model_49_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4405 - model_48_loss: 0.4734 - model_49_loss: 0.6917 - model_49_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4449 - model_48_loss: 0.4714 - model_49_loss: 0.6920 - model_49_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4453 - model_48_loss: 0.4726 - model_49_loss: 0.6923 - model_49_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9164 - model_49_loss: 0.6926 - model_49_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4406 - model_48_loss: 0.4711 - model_49_loss: 0.6913 - model_49_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4465 - model_48_loss: 0.4707 - model_49_loss: 0.6921 - model_49_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4450 - model_48_loss: 0.4693 - model_49_loss: 0.6920 - model_49_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4421 - model_48_loss: 0.4713 - model_49_loss: 0.6917 - model_49_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4460 - model_48_loss: 0.4716 - model_49_loss: 0.6923 - model_49_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9163 - model_49_loss: 0.6917 - model_49_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4434 - model_48_loss: 0.4710 - model_49_loss: 0.6919 - model_49_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4436 - model_48_loss: 0.4705 - model_49_loss: 0.6916 - model_49_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4430 - model_48_loss: 0.4719 - model_49_loss: 0.6919 - model_49_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4466 - model_48_loss: 0.4706 - model_49_loss: 0.6922 - model_49_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4457 - model_48_loss: 0.4723 - model_49_loss: 0.6921 - model_49_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9148 - model_49_loss: 0.6911 - model_49_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4410 - model_48_loss: 0.4723 - model_49_loss: 0.6914 - model_49_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4436 - model_48_loss: 0.4716 - model_49_loss: 0.6917 - model_49_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4479 - model_48_loss: 0.4705 - model_49_loss: 0.6921 - model_49_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4456 - model_48_loss: 0.4722 - model_49_loss: 0.6920 - model_49_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4486 - model_48_loss: 0.4704 - model_49_loss: 0.6921 - model_49_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9206 - model_49_loss: 0.6918 - model_49_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4464 - model_48_loss: 0.4724 - model_49_loss: 0.6919 - model_49_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4492 - model_48_loss: 0.4719 - model_49_loss: 0.6922 - model_49_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4500 - model_48_loss: 0.4729 - model_49_loss: 0.6922 - model_49_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4514 - model_48_loss: 0.4729 - model_49_loss: 0.6922 - model_49_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4509 - model_48_loss: 0.4731 - model_49_loss: 0.6922 - model_49_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9263 - model_49_loss: 0.6928 - model_49_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4475 - model_48_loss: 0.4752 - model_49_loss: 0.6923 - model_49_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4498 - model_48_loss: 0.4756 - model_49_loss: 0.6927 - model_49_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4497 - model_48_loss: 0.4759 - model_49_loss: 0.6926 - model_49_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4480 - model_48_loss: 0.4752 - model_49_loss: 0.6923 - model_49_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4488 - model_48_loss: 0.4750 - model_49_loss: 0.6923 - model_49_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9245 - model_49_loss: 0.6930 - model_49_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4469 - model_48_loss: 0.4741 - model_49_loss: 0.6924 - model_49_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4472 - model_48_loss: 0.4756 - model_49_loss: 0.6928 - model_49_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4461 - model_48_loss: 0.4756 - model_49_loss: 0.6925 - model_49_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4449 - model_48_loss: 0.4748 - model_49_loss: 0.6923 - model_49_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4484 - model_48_loss: 0.4728 - model_49_loss: 0.6925 - model_49_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9188 - model_49_loss: 0.6930 - model_49_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4423 - model_48_loss: 0.4754 - model_49_loss: 0.6921 - model_49_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4490 - model_48_loss: 0.4715 - model_49_loss: 0.6926 - model_49_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4479 - model_48_loss: 0.4708 - model_49_loss: 0.6921 - model_49_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4438 - model_48_loss: 0.4720 - model_49_loss: 0.6919 - model_49_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4486 - model_48_loss: 0.4700 - model_49_loss: 0.6924 - model_49_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9181 - model_49_loss: 0.6921 - model_49_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4477 - model_48_loss: 0.4704 - model_49_loss: 0.6920 - model_49_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4485 - model_48_loss: 0.4697 - model_49_loss: 0.6922 - model_49_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4481 - model_48_loss: 0.4692 - model_49_loss: 0.6921 - model_49_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4480 - model_48_loss: 0.4694 - model_49_loss: 0.6921 - model_49_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4511 - model_48_loss: 0.4688 - model_49_loss: 0.6922 - model_49_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9192 - model_49_loss: 0.6922 - model_49_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4503 - model_48_loss: 0.4684 - model_49_loss: 0.6920 - model_49_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4496 - model_48_loss: 0.4686 - model_49_loss: 0.6923 - model_49_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4502 - model_48_loss: 0.4679 - model_49_loss: 0.6919 - model_49_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4520 - model_48_loss: 0.4673 - model_49_loss: 0.6922 - model_49_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4500 - model_48_loss: 0.4682 - model_49_loss: 0.6920 - model_49_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9184 - model_49_loss: 0.6923 - model_49_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4490 - model_48_loss: 0.4676 - model_49_loss: 0.6921 - model_49_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4462 - model_48_loss: 0.4699 - model_49_loss: 0.6921 - model_49_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4459 - model_48_loss: 0.4692 - model_49_loss: 0.6918 - model_49_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4483 - model_48_loss: 0.4680 - model_49_loss: 0.6920 - model_49_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4478 - model_48_loss: 0.4683 - model_49_loss: 0.6919 - model_49_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9148 - model_49_loss: 0.6911 - model_49_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4465 - model_48_loss: 0.4677 - model_49_loss: 0.6920 - model_49_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4461 - model_48_loss: 0.4689 - model_49_loss: 0.6919 - model_49_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4467 - model_48_loss: 0.4678 - model_49_loss: 0.6920 - model_49_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4464 - model_48_loss: 0.4685 - model_49_loss: 0.6921 - model_49_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4478 - model_48_loss: 0.4686 - model_49_loss: 0.6921 - model_49_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9167 - model_49_loss: 0.6916 - model_49_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4453 - model_48_loss: 0.4671 - model_49_loss: 0.6917 - model_49_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4491 - model_48_loss: 0.4676 - model_49_loss: 0.6921 - model_49_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4492 - model_48_loss: 0.4678 - model_49_loss: 0.6921 - model_49_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4525 - model_48_loss: 0.4689 - model_49_loss: 0.6925 - model_49_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4525 - model_48_loss: 0.4691 - model_49_loss: 0.6926 - model_49_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9239 - model_49_loss: 0.6924 - model_49_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4520 - model_48_loss: 0.4694 - model_49_loss: 0.6924 - model_49_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4516 - model_48_loss: 0.4686 - model_49_loss: 0.6924 - model_49_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4466 - model_48_loss: 0.4718 - model_49_loss: 0.6921 - model_49_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4483 - model_48_loss: 0.4715 - model_49_loss: 0.6923 - model_49_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4529 - model_48_loss: 0.4692 - model_49_loss: 0.6925 - model_49_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9233 - model_49_loss: 0.6922 - model_49_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4502 - model_48_loss: 0.4707 - model_49_loss: 0.6925 - model_49_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4485 - model_48_loss: 0.4707 - model_49_loss: 0.6922 - model_49_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4476 - model_48_loss: 0.4705 - model_49_loss: 0.6920 - model_49_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4475 - model_48_loss: 0.4724 - model_49_loss: 0.6922 - model_49_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4515 - model_48_loss: 0.4698 - model_49_loss: 0.6925 - model_49_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9173 - model_49_loss: 0.6923 - model_49_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4468 - model_48_loss: 0.4705 - model_49_loss: 0.6921 - model_49_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4444 - model_48_loss: 0.4698 - model_49_loss: 0.6922 - model_49_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4460 - model_48_loss: 0.4685 - model_49_loss: 0.6918 - model_49_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4455 - model_48_loss: 0.4699 - model_49_loss: 0.6922 - model_49_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4488 - model_48_loss: 0.4670 - model_49_loss: 0.6921 - model_49_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9173 - model_49_loss: 0.6924 - model_49_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4445 - model_48_loss: 0.4667 - model_49_loss: 0.6918 - model_49_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4474 - model_48_loss: 0.4678 - model_49_loss: 0.6918 - model_49_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4508 - model_48_loss: 0.4647 - model_49_loss: 0.6922 - model_49_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4518 - model_48_loss: 0.4642 - model_49_loss: 0.6922 - model_49_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4526 - model_48_loss: 0.4644 - model_49_loss: 0.6922 - model_49_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9164 - model_49_loss: 0.6924 - model_49_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4523 - model_48_loss: 0.4637 - model_49_loss: 0.6923 - model_49_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4540 - model_48_loss: 0.4637 - model_49_loss: 0.6923 - model_49_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4554 - model_48_loss: 0.4628 - model_49_loss: 0.6927 - model_49_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4565 - model_48_loss: 0.4628 - model_49_loss: 0.6925 - model_49_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4575 - model_48_loss: 0.4613 - model_49_loss: 0.6925 - model_49_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9145 - model_49_loss: 0.6915 - model_49_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4498 - model_48_loss: 0.4620 - model_49_loss: 0.6921 - model_49_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4527 - model_48_loss: 0.4610 - model_49_loss: 0.6918 - model_49_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4519 - model_48_loss: 0.4620 - model_49_loss: 0.6922 - model_49_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4480 - model_48_loss: 0.4630 - model_49_loss: 0.6919 - model_49_1_loss: 0.6902\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4463 - model_48_loss: 0.4650 - model_49_loss: 0.6919 - model_49_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9141 - model_49_loss: 0.6915 - model_49_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4502 - model_48_loss: 0.4646 - model_49_loss: 0.6922 - model_49_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4484 - model_48_loss: 0.4648 - model_49_loss: 0.6921 - model_49_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4468 - model_48_loss: 0.4664 - model_49_loss: 0.6919 - model_49_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4498 - model_48_loss: 0.4680 - model_49_loss: 0.6924 - model_49_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4469 - model_48_loss: 0.4696 - model_49_loss: 0.6922 - model_49_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: 6.9200 - model_49_loss: 0.6925 - model_49_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4467 - model_48_loss: 0.4696 - model_49_loss: 0.6920 - model_49_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4505 - model_48_loss: 0.4710 - model_49_loss: 0.6924 - model_49_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4504 - model_48_loss: 0.4721 - model_49_loss: 0.6924 - model_49_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4522 - model_48_loss: 0.4695 - model_49_loss: 0.6926 - model_49_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4511 - model_48_loss: 0.4725 - model_49_loss: 0.6928 - model_49_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9273 - model_49_loss: 0.6936 - model_49_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4520 - model_48_loss: 0.4722 - model_49_loss: 0.6924 - model_49_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4497 - model_48_loss: 0.4742 - model_49_loss: 0.6924 - model_49_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4525 - model_48_loss: 0.4733 - model_49_loss: 0.6925 - model_49_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4531 - model_48_loss: 0.4743 - model_49_loss: 0.6928 - model_49_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4528 - model_48_loss: 0.4733 - model_49_loss: 0.6925 - model_49_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9248 - model_49_loss: 0.6926 - model_49_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4494 - model_48_loss: 0.4727 - model_49_loss: 0.6922 - model_49_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4489 - model_48_loss: 0.4719 - model_49_loss: 0.6920 - model_49_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4497 - model_48_loss: 0.4715 - model_49_loss: 0.6923 - model_49_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4520 - model_48_loss: 0.4721 - model_49_loss: 0.6927 - model_49_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4497 - model_48_loss: 0.4713 - model_49_loss: 0.6923 - model_49_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9184 - model_49_loss: 0.6920 - model_49_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4447 - model_48_loss: 0.4709 - model_49_loss: 0.6920 - model_49_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4451 - model_48_loss: 0.4711 - model_49_loss: 0.6920 - model_49_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4457 - model_48_loss: 0.4692 - model_49_loss: 0.6919 - model_49_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4481 - model_48_loss: 0.4662 - model_49_loss: 0.6919 - model_49_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4459 - model_48_loss: 0.4663 - model_49_loss: 0.6915 - model_49_1_loss: 0.6909\n",
      "For Attention Module: 1.4000000000000001\n",
      "features X: 30940 samples, 75 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.3708 - model_53_loss: 0.6596 - model_53_1_loss: 0.6151\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -5.9939 - model_52_loss: 0.3733 - model_53_loss: 0.6591 - model_53_1_loss: 0.6143\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0041 - model_52_loss: 0.3723 - model_53_loss: 0.6591 - model_53_1_loss: 0.6161\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0257 - model_52_loss: 0.3712 - model_53_loss: 0.6607 - model_53_1_loss: 0.6186\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0371 - model_52_loss: 0.3718 - model_53_loss: 0.6604 - model_53_1_loss: 0.6213\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0594 - model_52_loss: 0.3736 - model_53_loss: 0.6627 - model_53_1_loss: 0.6239\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.4216 - model_53_loss: 0.6614 - model_53_1_loss: 0.6229\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0517 - model_52_loss: 0.3714 - model_53_loss: 0.6617 - model_53_1_loss: 0.6229\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0614 - model_52_loss: 0.3741 - model_53_loss: 0.6618 - model_53_1_loss: 0.6253\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0844 - model_52_loss: 0.3740 - model_53_loss: 0.6635 - model_53_1_loss: 0.6282\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0928 - model_52_loss: 0.3752 - model_53_loss: 0.6625 - model_53_1_loss: 0.6311\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1074 - model_52_loss: 0.3752 - model_53_loss: 0.6639 - model_53_1_loss: 0.6326\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.4927 - model_53_loss: 0.6632 - model_53_1_loss: 0.6346\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1212 - model_52_loss: 0.3754 - model_53_loss: 0.6648 - model_53_1_loss: 0.6345\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1444 - model_52_loss: 0.3754 - model_53_loss: 0.6664 - model_53_1_loss: 0.6375\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1498 - model_52_loss: 0.3781 - model_53_loss: 0.6661 - model_53_1_loss: 0.6394\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1653 - model_52_loss: 0.3804 - model_53_loss: 0.6670 - model_53_1_loss: 0.6421\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1815 - model_52_loss: 0.3770 - model_53_loss: 0.6670 - model_53_1_loss: 0.6447\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.5858 - model_53_loss: 0.6701 - model_53_1_loss: 0.6469\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1900 - model_52_loss: 0.3820 - model_53_loss: 0.6695 - model_53_1_loss: 0.6449\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2078 - model_52_loss: 0.3826 - model_53_loss: 0.6698 - model_53_1_loss: 0.6483\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2125 - model_52_loss: 0.3849 - model_53_loss: 0.6699 - model_53_1_loss: 0.6495\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2283 - model_52_loss: 0.3845 - model_53_loss: 0.6706 - model_53_1_loss: 0.6520\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2347 - model_52_loss: 0.3884 - model_53_loss: 0.6702 - model_53_1_loss: 0.6544\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.6315 - model_53_loss: 0.6711 - model_53_1_loss: 0.6545\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2409 - model_52_loss: 0.3890 - model_53_loss: 0.6721 - model_53_1_loss: 0.6539\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2644 - model_52_loss: 0.3913 - model_53_loss: 0.6732 - model_53_1_loss: 0.6580\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2683 - model_52_loss: 0.3922 - model_53_loss: 0.6740 - model_53_1_loss: 0.6581\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2920 - model_52_loss: 0.3938 - model_53_loss: 0.6761 - model_53_1_loss: 0.6611\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2900 - model_52_loss: 0.3937 - model_53_loss: 0.6743 - model_53_1_loss: 0.6624\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.7036 - model_53_loss: 0.6756 - model_53_1_loss: 0.6644\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3040 - model_52_loss: 0.3983 - model_53_loss: 0.6763 - model_53_1_loss: 0.6641\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3156 - model_52_loss: 0.4001 - model_53_loss: 0.6770 - model_53_1_loss: 0.6661\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3241 - model_52_loss: 0.4009 - model_53_loss: 0.6781 - model_53_1_loss: 0.6669\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3327 - model_52_loss: 0.4039 - model_53_loss: 0.6791 - model_53_1_loss: 0.6682\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3555 - model_52_loss: 0.4043 - model_53_loss: 0.6803 - model_53_1_loss: 0.6716\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.7587 - model_53_loss: 0.6796 - model_53_1_loss: 0.6725\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3564 - model_52_loss: 0.4080 - model_53_loss: 0.6802 - model_53_1_loss: 0.6727\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3574 - model_52_loss: 0.4074 - model_53_loss: 0.6797 - model_53_1_loss: 0.6733\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3707 - model_52_loss: 0.4097 - model_53_loss: 0.6809 - model_53_1_loss: 0.6752\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3823 - model_52_loss: 0.4131 - model_53_loss: 0.6825 - model_53_1_loss: 0.6766\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3853 - model_52_loss: 0.4150 - model_53_loss: 0.6831 - model_53_1_loss: 0.6770\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.8148 - model_53_loss: 0.6839 - model_53_1_loss: 0.6790\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3918 - model_52_loss: 0.4171 - model_53_loss: 0.6829 - model_53_1_loss: 0.6789\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3964 - model_52_loss: 0.4201 - model_53_loss: 0.6837 - model_53_1_loss: 0.6796\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4010 - model_52_loss: 0.4207 - model_53_loss: 0.6840 - model_53_1_loss: 0.6803\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4113 - model_52_loss: 0.4226 - model_53_loss: 0.6850 - model_53_1_loss: 0.6817\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4150 - model_52_loss: 0.4256 - model_53_loss: 0.6855 - model_53_1_loss: 0.6826\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.8575 - model_53_loss: 0.6874 - model_53_1_loss: 0.6845\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4208 - model_52_loss: 0.4272 - model_53_loss: 0.6857 - model_53_1_loss: 0.6839\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4263 - model_52_loss: 0.4295 - model_53_loss: 0.6867 - model_53_1_loss: 0.6845\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4362 - model_52_loss: 0.4296 - model_53_loss: 0.6875 - model_53_1_loss: 0.6856\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4371 - model_52_loss: 0.4320 - model_53_loss: 0.6872 - model_53_1_loss: 0.6866\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4384 - model_52_loss: 0.4356 - model_53_loss: 0.6885 - model_53_1_loss: 0.6863\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.8865 - model_53_loss: 0.6894 - model_53_1_loss: 0.6880\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4433 - model_52_loss: 0.4391 - model_53_loss: 0.6893 - model_53_1_loss: 0.6872\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4478 - model_52_loss: 0.4408 - model_53_loss: 0.6896 - model_53_1_loss: 0.6881\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4502 - model_52_loss: 0.4438 - model_53_loss: 0.6901 - model_53_1_loss: 0.6887\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4560 - model_52_loss: 0.4439 - model_53_loss: 0.6901 - model_53_1_loss: 0.6899\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4596 - model_52_loss: 0.4456 - model_53_loss: 0.6912 - model_53_1_loss: 0.6898\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9095 - model_53_loss: 0.6911 - model_53_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4580 - model_52_loss: 0.4462 - model_53_loss: 0.6909 - model_53_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4519 - model_52_loss: 0.4523 - model_53_loss: 0.6907 - model_53_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4574 - model_52_loss: 0.4529 - model_53_loss: 0.6913 - model_53_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4588 - model_52_loss: 0.4538 - model_53_loss: 0.6917 - model_53_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4575 - model_52_loss: 0.4552 - model_53_loss: 0.6914 - model_53_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9150 - model_53_loss: 0.6917 - model_53_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4622 - model_52_loss: 0.4565 - model_53_loss: 0.6923 - model_53_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4588 - model_52_loss: 0.4573 - model_53_loss: 0.6920 - model_53_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4582 - model_52_loss: 0.4584 - model_53_loss: 0.6923 - model_53_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4596 - model_52_loss: 0.4587 - model_53_loss: 0.6921 - model_53_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4644 - model_52_loss: 0.4601 - model_53_loss: 0.6926 - model_53_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9239 - model_53_loss: 0.6923 - model_53_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4657 - model_52_loss: 0.4590 - model_53_loss: 0.6930 - model_53_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4646 - model_52_loss: 0.4580 - model_53_loss: 0.6928 - model_53_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4648 - model_52_loss: 0.4595 - model_53_loss: 0.6927 - model_53_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4660 - model_52_loss: 0.4582 - model_53_loss: 0.6929 - model_53_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4682 - model_52_loss: 0.4574 - model_53_loss: 0.6931 - model_53_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9246 - model_53_loss: 0.6924 - model_53_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4661 - model_52_loss: 0.4574 - model_53_loss: 0.6928 - model_53_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4619 - model_52_loss: 0.4585 - model_53_loss: 0.6927 - model_53_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4666 - model_52_loss: 0.4562 - model_53_loss: 0.6926 - model_53_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4705 - model_52_loss: 0.4546 - model_53_loss: 0.6929 - model_53_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4676 - model_52_loss: 0.4531 - model_53_loss: 0.6925 - model_53_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9240 - model_53_loss: 0.6928 - model_53_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4691 - model_52_loss: 0.4538 - model_53_loss: 0.6925 - model_53_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4716 - model_52_loss: 0.4511 - model_53_loss: 0.6923 - model_53_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4700 - model_52_loss: 0.4512 - model_53_loss: 0.6924 - model_53_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4719 - model_52_loss: 0.4499 - model_53_loss: 0.6923 - model_53_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4707 - model_52_loss: 0.4484 - model_53_loss: 0.6920 - model_53_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9202 - model_53_loss: 0.6922 - model_53_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4700 - model_52_loss: 0.4482 - model_53_loss: 0.6920 - model_53_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4726 - model_52_loss: 0.4467 - model_53_loss: 0.6922 - model_53_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4750 - model_52_loss: 0.4448 - model_53_loss: 0.6920 - model_53_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4724 - model_52_loss: 0.4441 - model_53_loss: 0.6920 - model_53_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4755 - model_52_loss: 0.4434 - model_53_loss: 0.6923 - model_53_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9161 - model_53_loss: 0.6921 - model_53_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4746 - model_52_loss: 0.4430 - model_53_loss: 0.6920 - model_53_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4731 - model_52_loss: 0.4418 - model_53_loss: 0.6921 - model_53_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4741 - model_52_loss: 0.4403 - model_53_loss: 0.6919 - model_53_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4769 - model_52_loss: 0.4386 - model_53_loss: 0.6920 - model_53_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4791 - model_52_loss: 0.4380 - model_53_loss: 0.6920 - model_53_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9174 - model_53_loss: 0.6917 - model_53_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4753 - model_52_loss: 0.4367 - model_53_loss: 0.6918 - model_53_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4721 - model_52_loss: 0.4390 - model_53_loss: 0.6916 - model_53_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4786 - model_52_loss: 0.4382 - model_53_loss: 0.6924 - model_53_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4733 - model_52_loss: 0.4387 - model_53_loss: 0.6917 - model_53_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4782 - model_52_loss: 0.4370 - model_53_loss: 0.6921 - model_53_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9166 - model_53_loss: 0.6918 - model_53_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4783 - model_52_loss: 0.4381 - model_53_loss: 0.6921 - model_53_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4790 - model_52_loss: 0.4373 - model_53_loss: 0.6917 - model_53_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4789 - model_52_loss: 0.4381 - model_53_loss: 0.6921 - model_53_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4804 - model_52_loss: 0.4375 - model_53_loss: 0.6920 - model_53_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4811 - model_52_loss: 0.4388 - model_53_loss: 0.6923 - model_53_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9180 - model_53_loss: 0.6922 - model_53_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4776 - model_52_loss: 0.4395 - model_53_loss: 0.6922 - model_53_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4794 - model_52_loss: 0.4386 - model_53_loss: 0.6927 - model_53_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4811 - model_52_loss: 0.4392 - model_53_loss: 0.6926 - model_53_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4832 - model_52_loss: 0.4384 - model_53_loss: 0.6929 - model_53_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4805 - model_52_loss: 0.4416 - model_53_loss: 0.6928 - model_53_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9247 - model_53_loss: 0.6934 - model_53_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4818 - model_52_loss: 0.4421 - model_53_loss: 0.6929 - model_53_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4833 - model_52_loss: 0.4410 - model_53_loss: 0.6930 - model_53_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4830 - model_52_loss: 0.4416 - model_53_loss: 0.6929 - model_53_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4828 - model_52_loss: 0.4429 - model_53_loss: 0.6931 - model_53_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4827 - model_52_loss: 0.4430 - model_53_loss: 0.6932 - model_53_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9272 - model_53_loss: 0.6936 - model_53_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4793 - model_52_loss: 0.4440 - model_53_loss: 0.6928 - model_53_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4810 - model_52_loss: 0.4433 - model_53_loss: 0.6928 - model_53_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4805 - model_52_loss: 0.4436 - model_53_loss: 0.6928 - model_53_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4826 - model_52_loss: 0.4453 - model_53_loss: 0.6931 - model_53_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4795 - model_52_loss: 0.4471 - model_53_loss: 0.6931 - model_53_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9279 - model_53_loss: 0.6930 - model_53_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4836 - model_52_loss: 0.4435 - model_53_loss: 0.6929 - model_53_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4798 - model_52_loss: 0.4452 - model_53_loss: 0.6929 - model_53_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4818 - model_52_loss: 0.4439 - model_53_loss: 0.6930 - model_53_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4825 - model_52_loss: 0.4437 - model_53_loss: 0.6930 - model_53_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4839 - model_52_loss: 0.4422 - model_53_loss: 0.6931 - model_53_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9261 - model_53_loss: 0.6927 - model_53_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4822 - model_52_loss: 0.4429 - model_53_loss: 0.6928 - model_53_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4817 - model_52_loss: 0.4421 - model_53_loss: 0.6928 - model_53_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4807 - model_52_loss: 0.4408 - model_53_loss: 0.6928 - model_53_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4848 - model_52_loss: 0.4401 - model_53_loss: 0.6929 - model_53_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4816 - model_52_loss: 0.4411 - model_53_loss: 0.6930 - model_53_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9233 - model_53_loss: 0.6932 - model_53_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4831 - model_52_loss: 0.4389 - model_53_loss: 0.6926 - model_53_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4851 - model_52_loss: 0.4373 - model_53_loss: 0.6926 - model_53_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4853 - model_52_loss: 0.4388 - model_53_loss: 0.6927 - model_53_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4839 - model_52_loss: 0.4371 - model_53_loss: 0.6926 - model_53_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4805 - model_52_loss: 0.4382 - model_53_loss: 0.6925 - model_53_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9207 - model_53_loss: 0.6927 - model_53_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4854 - model_52_loss: 0.4361 - model_53_loss: 0.6926 - model_53_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4829 - model_52_loss: 0.4373 - model_53_loss: 0.6924 - model_53_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4833 - model_52_loss: 0.4354 - model_53_loss: 0.6926 - model_53_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4852 - model_52_loss: 0.4335 - model_53_loss: 0.6926 - model_53_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4865 - model_52_loss: 0.4331 - model_53_loss: 0.6926 - model_53_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9187 - model_53_loss: 0.6923 - model_53_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4848 - model_52_loss: 0.4335 - model_53_loss: 0.6922 - model_53_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4841 - model_52_loss: 0.4344 - model_53_loss: 0.6921 - model_53_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4863 - model_52_loss: 0.4321 - model_53_loss: 0.6923 - model_53_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4853 - model_52_loss: 0.4334 - model_53_loss: 0.6923 - model_53_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4880 - model_52_loss: 0.4317 - model_53_loss: 0.6922 - model_53_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9227 - model_53_loss: 0.6925 - model_53_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4842 - model_52_loss: 0.4333 - model_53_loss: 0.6920 - model_53_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4856 - model_52_loss: 0.4353 - model_53_loss: 0.6924 - model_53_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4865 - model_52_loss: 0.4353 - model_53_loss: 0.6924 - model_53_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4849 - model_52_loss: 0.4349 - model_53_loss: 0.6922 - model_53_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4841 - model_52_loss: 0.4350 - model_53_loss: 0.6922 - model_53_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9232 - model_53_loss: 0.6933 - model_53_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4841 - model_52_loss: 0.4360 - model_53_loss: 0.6920 - model_53_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4837 - model_52_loss: 0.4363 - model_53_loss: 0.6922 - model_53_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4876 - model_52_loss: 0.4354 - model_53_loss: 0.6925 - model_53_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4860 - model_52_loss: 0.4365 - model_53_loss: 0.6922 - model_53_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4863 - model_52_loss: 0.4364 - model_53_loss: 0.6924 - model_53_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9226 - model_53_loss: 0.6929 - model_53_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4874 - model_52_loss: 0.4359 - model_53_loss: 0.6928 - model_53_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4861 - model_52_loss: 0.4377 - model_53_loss: 0.6928 - model_53_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4883 - model_52_loss: 0.4361 - model_53_loss: 0.6928 - model_53_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4829 - model_52_loss: 0.4376 - model_53_loss: 0.6925 - model_53_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4871 - model_52_loss: 0.4367 - model_53_loss: 0.6926 - model_53_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9247 - model_53_loss: 0.6926 - model_53_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4873 - model_52_loss: 0.4361 - model_53_loss: 0.6926 - model_53_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4874 - model_52_loss: 0.4364 - model_53_loss: 0.6924 - model_53_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4889 - model_52_loss: 0.4343 - model_53_loss: 0.6924 - model_53_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4884 - model_52_loss: 0.4359 - model_53_loss: 0.6927 - model_53_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4902 - model_52_loss: 0.4354 - model_53_loss: 0.6928 - model_53_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9240 - model_53_loss: 0.6924 - model_53_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4885 - model_52_loss: 0.4369 - model_53_loss: 0.6926 - model_53_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4898 - model_52_loss: 0.4356 - model_53_loss: 0.6926 - model_53_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4871 - model_52_loss: 0.4367 - model_53_loss: 0.6925 - model_53_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4910 - model_52_loss: 0.4361 - model_53_loss: 0.6929 - model_53_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4887 - model_52_loss: 0.4356 - model_53_loss: 0.6926 - model_53_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9259 - model_53_loss: 0.6923 - model_53_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4872 - model_52_loss: 0.4354 - model_53_loss: 0.6925 - model_53_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4893 - model_52_loss: 0.4354 - model_53_loss: 0.6928 - model_53_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4885 - model_52_loss: 0.4357 - model_53_loss: 0.6927 - model_53_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4872 - model_52_loss: 0.4353 - model_53_loss: 0.6925 - model_53_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4889 - model_52_loss: 0.4339 - model_53_loss: 0.6923 - model_53_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9254 - model_53_loss: 0.6934 - model_53_1_loss: 0.69250s - loss: 6.9228 - model_53_loss: 0.6915 - model_53_1_loss: \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4868 - model_52_loss: 0.4348 - model_53_loss: 0.6924 - model_53_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4887 - model_52_loss: 0.4342 - model_53_loss: 0.6925 - model_53_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4885 - model_52_loss: 0.4333 - model_53_loss: 0.6922 - model_53_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4892 - model_52_loss: 0.4343 - model_53_loss: 0.6924 - model_53_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4885 - model_52_loss: 0.4326 - model_53_loss: 0.6922 - model_53_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9247 - model_53_loss: 0.6931 - model_53_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4900 - model_52_loss: 0.4323 - model_53_loss: 0.6923 - model_53_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4877 - model_52_loss: 0.4320 - model_53_loss: 0.6922 - model_53_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4919 - model_52_loss: 0.4295 - model_53_loss: 0.6924 - model_53_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4882 - model_52_loss: 0.4321 - model_53_loss: 0.6921 - model_53_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4862 - model_52_loss: 0.4312 - model_53_loss: 0.6920 - model_53_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9200 - model_53_loss: 0.6921 - model_53_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4894 - model_52_loss: 0.4299 - model_53_loss: 0.6922 - model_53_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4886 - model_52_loss: 0.4295 - model_53_loss: 0.6921 - model_53_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4882 - model_52_loss: 0.4305 - model_53_loss: 0.6921 - model_53_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4885 - model_52_loss: 0.4303 - model_53_loss: 0.6923 - model_53_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4881 - model_52_loss: 0.4300 - model_53_loss: 0.6919 - model_53_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9191 - model_53_loss: 0.6917 - model_53_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4895 - model_52_loss: 0.4294 - model_53_loss: 0.6922 - model_53_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4899 - model_52_loss: 0.4299 - model_53_loss: 0.6921 - model_53_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4870 - model_52_loss: 0.4309 - model_53_loss: 0.6918 - model_53_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4927 - model_52_loss: 0.4284 - model_53_loss: 0.6924 - model_53_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4893 - model_52_loss: 0.4306 - model_53_loss: 0.6921 - model_53_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9233 - model_53_loss: 0.6923 - model_53_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4909 - model_52_loss: 0.4301 - model_53_loss: 0.6923 - model_53_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4892 - model_52_loss: 0.4316 - model_53_loss: 0.6922 - model_53_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4902 - model_52_loss: 0.4313 - model_53_loss: 0.6924 - model_53_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4896 - model_52_loss: 0.4316 - model_53_loss: 0.6923 - model_53_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4883 - model_52_loss: 0.4336 - model_53_loss: 0.6922 - model_53_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9248 - model_53_loss: 0.6920 - model_53_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4883 - model_52_loss: 0.4337 - model_53_loss: 0.6923 - model_53_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4877 - model_52_loss: 0.4353 - model_53_loss: 0.6925 - model_53_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4887 - model_52_loss: 0.4335 - model_53_loss: 0.6924 - model_53_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4895 - model_52_loss: 0.4352 - model_53_loss: 0.6924 - model_53_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4929 - model_52_loss: 0.4325 - model_53_loss: 0.6927 - model_53_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9245 - model_53_loss: 0.6924 - model_53_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4881 - model_52_loss: 0.4355 - model_53_loss: 0.6923 - model_53_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4917 - model_52_loss: 0.4341 - model_53_loss: 0.6925 - model_53_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4907 - model_52_loss: 0.4348 - model_53_loss: 0.6926 - model_53_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4910 - model_52_loss: 0.4346 - model_53_loss: 0.6928 - model_53_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4903 - model_52_loss: 0.4342 - model_53_loss: 0.6925 - model_53_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9266 - model_53_loss: 0.6930 - model_53_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4878 - model_52_loss: 0.4357 - model_53_loss: 0.6928 - model_53_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4902 - model_52_loss: 0.4340 - model_53_loss: 0.6926 - model_53_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4917 - model_52_loss: 0.4342 - model_53_loss: 0.6925 - model_53_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4927 - model_52_loss: 0.4339 - model_53_loss: 0.6928 - model_53_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4901 - model_52_loss: 0.4344 - model_53_loss: 0.6926 - model_53_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9248 - model_53_loss: 0.6933 - model_53_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4886 - model_52_loss: 0.4333 - model_53_loss: 0.6924 - model_53_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4909 - model_52_loss: 0.4324 - model_53_loss: 0.6926 - model_53_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4884 - model_52_loss: 0.4330 - model_53_loss: 0.6924 - model_53_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4913 - model_52_loss: 0.4314 - model_53_loss: 0.6924 - model_53_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4917 - model_52_loss: 0.4310 - model_53_loss: 0.6926 - model_53_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9222 - model_53_loss: 0.6924 - model_53_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4894 - model_52_loss: 0.4305 - model_53_loss: 0.6923 - model_53_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4923 - model_52_loss: 0.4287 - model_53_loss: 0.6923 - model_53_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4905 - model_52_loss: 0.4299 - model_53_loss: 0.6923 - model_53_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4901 - model_52_loss: 0.4282 - model_53_loss: 0.6921 - model_53_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4917 - model_52_loss: 0.4271 - model_53_loss: 0.6922 - model_53_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9213 - model_53_loss: 0.6927 - model_53_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4896 - model_52_loss: 0.4280 - model_53_loss: 0.6921 - model_53_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4897 - model_52_loss: 0.4282 - model_53_loss: 0.6919 - model_53_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4903 - model_52_loss: 0.4277 - model_53_loss: 0.6920 - model_53_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4896 - model_52_loss: 0.4282 - model_53_loss: 0.6920 - model_53_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4917 - model_52_loss: 0.4277 - model_53_loss: 0.6922 - model_53_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9218 - model_53_loss: 0.6924 - model_53_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4922 - model_52_loss: 0.4271 - model_53_loss: 0.6919 - model_53_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4914 - model_52_loss: 0.4275 - model_53_loss: 0.6920 - model_53_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4927 - model_52_loss: 0.4260 - model_53_loss: 0.6919 - model_53_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4912 - model_52_loss: 0.4288 - model_53_loss: 0.6920 - model_53_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4897 - model_52_loss: 0.4303 - model_53_loss: 0.6922 - model_53_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9211 - model_53_loss: 0.6929 - model_53_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4905 - model_52_loss: 0.4297 - model_53_loss: 0.6920 - model_53_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4908 - model_52_loss: 0.4302 - model_53_loss: 0.6921 - model_53_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4922 - model_52_loss: 0.4281 - model_53_loss: 0.6919 - model_53_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4907 - model_52_loss: 0.4292 - model_53_loss: 0.6921 - model_53_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4938 - model_52_loss: 0.4288 - model_53_loss: 0.6923 - model_53_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9243 - model_53_loss: 0.6927 - model_53_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4905 - model_52_loss: 0.4307 - model_53_loss: 0.6921 - model_53_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4928 - model_52_loss: 0.4292 - model_53_loss: 0.6922 - model_53_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4911 - model_52_loss: 0.4301 - model_53_loss: 0.6919 - model_53_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4925 - model_52_loss: 0.4299 - model_53_loss: 0.6922 - model_53_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4916 - model_52_loss: 0.4308 - model_53_loss: 0.6923 - model_53_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9226 - model_53_loss: 0.6927 - model_53_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4921 - model_52_loss: 0.4288 - model_53_loss: 0.6922 - model_53_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4928 - model_52_loss: 0.4294 - model_53_loss: 0.6925 - model_53_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4931 - model_52_loss: 0.4291 - model_53_loss: 0.6925 - model_53_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4929 - model_52_loss: 0.4289 - model_53_loss: 0.6924 - model_53_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4927 - model_52_loss: 0.4290 - model_53_loss: 0.6924 - model_53_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9237 - model_53_loss: 0.6926 - model_53_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4908 - model_52_loss: 0.4298 - model_53_loss: 0.6924 - model_53_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4918 - model_52_loss: 0.4293 - model_53_loss: 0.6925 - model_53_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4911 - model_52_loss: 0.4294 - model_53_loss: 0.6923 - model_53_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4920 - model_52_loss: 0.4288 - model_53_loss: 0.6923 - model_53_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4916 - model_52_loss: 0.4311 - model_53_loss: 0.6926 - model_53_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9224 - model_53_loss: 0.6921 - model_53_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4901 - model_52_loss: 0.4298 - model_53_loss: 0.6922 - model_53_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4926 - model_52_loss: 0.4292 - model_53_loss: 0.6924 - model_53_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4912 - model_52_loss: 0.4291 - model_53_loss: 0.6924 - model_53_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4908 - model_52_loss: 0.4288 - model_53_loss: 0.6923 - model_53_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4919 - model_52_loss: 0.4291 - model_53_loss: 0.6925 - model_53_1_loss: 0.6917\n",
      "For Attention Module: 1.5000000000000002\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.3676 - model_57_loss: 0.6607 - model_57_1_loss: 0.6129\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -5.9774 - model_56_loss: 0.3782 - model_57_loss: 0.6592 - model_57_1_loss: 0.6120\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -5.9983 - model_56_loss: 0.3762 - model_57_loss: 0.6610 - model_57_1_loss: 0.6139\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -5.9929 - model_56_loss: 0.3794 - model_57_loss: 0.6596 - model_57_1_loss: 0.6148\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0184 - model_56_loss: 0.3788 - model_57_loss: 0.6618 - model_57_1_loss: 0.6177\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0253 - model_56_loss: 0.3782 - model_57_loss: 0.6611 - model_57_1_loss: 0.6196\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.4126 - model_57_loss: 0.6622 - model_57_1_loss: 0.6209\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0288 - model_56_loss: 0.3787 - model_57_loss: 0.6616 - model_57_1_loss: 0.6199\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0441 - model_56_loss: 0.3801 - model_57_loss: 0.6627 - model_57_1_loss: 0.6221\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0576 - model_56_loss: 0.3817 - model_57_loss: 0.6628 - model_57_1_loss: 0.6251\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0619 - model_56_loss: 0.3823 - model_57_loss: 0.6641 - model_57_1_loss: 0.6248\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0788 - model_56_loss: 0.3832 - model_57_loss: 0.6637 - model_57_1_loss: 0.6287\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.4649 - model_57_loss: 0.6642 - model_57_1_loss: 0.6284\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0870 - model_56_loss: 0.3842 - model_57_loss: 0.6649 - model_57_1_loss: 0.6294\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1006 - model_56_loss: 0.3837 - model_57_loss: 0.6656 - model_57_1_loss: 0.6313\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1067 - model_56_loss: 0.3852 - model_57_loss: 0.6653 - model_57_1_loss: 0.6330\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1277 - model_56_loss: 0.3890 - model_57_loss: 0.6669 - model_57_1_loss: 0.6364\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1324 - model_56_loss: 0.3904 - model_57_loss: 0.6661 - model_57_1_loss: 0.6385\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.5489 - model_57_loss: 0.6696 - model_57_1_loss: 0.6402\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1529 - model_56_loss: 0.3914 - model_57_loss: 0.6696 - model_57_1_loss: 0.6393\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1695 - model_56_loss: 0.3928 - model_57_loss: 0.6697 - model_57_1_loss: 0.6428\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1685 - model_56_loss: 0.3961 - model_57_loss: 0.6691 - model_57_1_loss: 0.6438\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1901 - model_56_loss: 0.3978 - model_57_loss: 0.6707 - model_57_1_loss: 0.6469\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2047 - model_56_loss: 0.4001 - model_57_loss: 0.6725 - model_57_1_loss: 0.6484\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.6204 - model_57_loss: 0.6740 - model_57_1_loss: 0.6501\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2092 - model_56_loss: 0.4018 - model_57_loss: 0.6717 - model_57_1_loss: 0.6504\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2177 - model_56_loss: 0.4048 - model_57_loss: 0.6733 - model_57_1_loss: 0.6512\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2333 - model_56_loss: 0.4074 - model_57_loss: 0.6747 - model_57_1_loss: 0.6535\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2537 - model_56_loss: 0.4077 - model_57_loss: 0.6761 - model_57_1_loss: 0.6562\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2623 - model_56_loss: 0.4097 - model_57_loss: 0.6755 - model_57_1_loss: 0.6589\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.6801 - model_57_loss: 0.6764 - model_57_1_loss: 0.6592\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2708 - model_56_loss: 0.4122 - model_57_loss: 0.6769 - model_57_1_loss: 0.6597\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2825 - model_56_loss: 0.4151 - model_57_loss: 0.6780 - model_57_1_loss: 0.6615\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2951 - model_56_loss: 0.4165 - model_57_loss: 0.6786 - model_57_1_loss: 0.6637\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2976 - model_56_loss: 0.4214 - model_57_loss: 0.6798 - model_57_1_loss: 0.6640\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3158 - model_56_loss: 0.4254 - model_57_loss: 0.6808 - model_57_1_loss: 0.6675\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.7470 - model_57_loss: 0.6807 - model_57_1_loss: 0.66860s - loss: 6.7242 - model_57_loss: 0.6768 - model_57_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3246 - model_56_loss: 0.4288 - model_57_loss: 0.6820 - model_57_1_loss: 0.6687\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3329 - model_56_loss: 0.4288 - model_57_loss: 0.6815 - model_57_1_loss: 0.6708\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3432 - model_56_loss: 0.4326 - model_57_loss: 0.6838 - model_57_1_loss: 0.6713\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3560 - model_56_loss: 0.4371 - model_57_loss: 0.6841 - model_57_1_loss: 0.6745\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3664 - model_56_loss: 0.4399 - model_57_loss: 0.6852 - model_57_1_loss: 0.6760\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.8133 - model_57_loss: 0.6859 - model_57_1_loss: 0.6768\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3689 - model_56_loss: 0.4427 - model_57_loss: 0.6856 - model_57_1_loss: 0.6767\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3778 - model_56_loss: 0.4454 - model_57_loss: 0.6862 - model_57_1_loss: 0.6785\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3889 - model_56_loss: 0.4495 - model_57_loss: 0.6870 - model_57_1_loss: 0.6807\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4021 - model_56_loss: 0.4526 - model_57_loss: 0.6877 - model_57_1_loss: 0.6833\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4002 - model_56_loss: 0.4551 - model_57_loss: 0.6881 - model_57_1_loss: 0.6830\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.8695 - model_57_loss: 0.6896 - model_57_1_loss: 0.6843\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4021 - model_56_loss: 0.4599 - model_57_loss: 0.6891 - model_57_1_loss: 0.6833\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4060 - model_56_loss: 0.4637 - model_57_loss: 0.6891 - model_57_1_loss: 0.6848\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4156 - model_56_loss: 0.4639 - model_57_loss: 0.6902 - model_57_1_loss: 0.6857\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4198 - model_56_loss: 0.4685 - model_57_loss: 0.6904 - model_57_1_loss: 0.6872\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4210 - model_56_loss: 0.4728 - model_57_loss: 0.6907 - model_57_1_loss: 0.6880\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.8988 - model_57_loss: 0.6904 - model_57_1_loss: 0.6890\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4192 - model_56_loss: 0.4760 - model_57_loss: 0.6907 - model_57_1_loss: 0.6883\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4217 - model_56_loss: 0.4786 - model_57_loss: 0.6911 - model_57_1_loss: 0.6890\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4218 - model_56_loss: 0.4832 - model_57_loss: 0.6916 - model_57_1_loss: 0.6894\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4235 - model_56_loss: 0.4856 - model_57_loss: 0.6918 - model_57_1_loss: 0.6900\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4249 - model_56_loss: 0.4890 - model_57_loss: 0.6921 - model_57_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9169 - model_57_loss: 0.6920 - model_57_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4191 - model_56_loss: 0.4927 - model_57_loss: 0.6922 - model_57_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4250 - model_56_loss: 0.4905 - model_57_loss: 0.6925 - model_57_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4222 - model_56_loss: 0.4955 - model_57_loss: 0.6927 - model_57_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4206 - model_56_loss: 0.4984 - model_57_loss: 0.6928 - model_57_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4215 - model_56_loss: 0.5002 - model_57_loss: 0.6932 - model_57_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9229 - model_57_loss: 0.6932 - model_57_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4212 - model_56_loss: 0.4995 - model_57_loss: 0.6931 - model_57_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4228 - model_56_loss: 0.5005 - model_57_loss: 0.6936 - model_57_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4226 - model_56_loss: 0.5011 - model_57_loss: 0.6933 - model_57_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4311 - model_56_loss: 0.4985 - model_57_loss: 0.6938 - model_57_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4304 - model_56_loss: 0.4985 - model_57_loss: 0.6938 - model_57_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9293 - model_57_loss: 0.6939 - model_57_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4291 - model_56_loss: 0.4972 - model_57_loss: 0.6935 - model_57_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4273 - model_56_loss: 0.4993 - model_57_loss: 0.6935 - model_57_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4324 - model_56_loss: 0.4956 - model_57_loss: 0.6939 - model_57_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4365 - model_56_loss: 0.4923 - model_57_loss: 0.6937 - model_57_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4349 - model_56_loss: 0.4928 - model_57_loss: 0.6937 - model_57_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9307 - model_57_loss: 0.6935 - model_57_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4365 - model_56_loss: 0.4924 - model_57_loss: 0.6936 - model_57_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4404 - model_56_loss: 0.4895 - model_57_loss: 0.6937 - model_57_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4382 - model_56_loss: 0.4906 - model_57_loss: 0.6935 - model_57_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4429 - model_56_loss: 0.4861 - model_57_loss: 0.6936 - model_57_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4435 - model_56_loss: 0.4857 - model_57_loss: 0.6937 - model_57_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9278 - model_57_loss: 0.6931 - model_57_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4386 - model_56_loss: 0.4834 - model_57_loss: 0.6929 - model_57_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4394 - model_56_loss: 0.4827 - model_57_loss: 0.6931 - model_57_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4404 - model_56_loss: 0.4797 - model_57_loss: 0.6929 - model_57_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4409 - model_56_loss: 0.4799 - model_57_loss: 0.6930 - model_57_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4397 - model_56_loss: 0.4778 - model_57_loss: 0.6930 - model_57_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9181 - model_57_loss: 0.6924 - model_57_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4393 - model_56_loss: 0.4758 - model_57_loss: 0.6923 - model_57_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4410 - model_56_loss: 0.4743 - model_57_loss: 0.6924 - model_57_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4394 - model_56_loss: 0.4730 - model_57_loss: 0.6920 - model_57_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4360 - model_56_loss: 0.4739 - model_57_loss: 0.6921 - model_57_1_loss: 0.6899\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4377 - model_56_loss: 0.4719 - model_57_loss: 0.6922 - model_57_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9093 - model_57_loss: 0.6923 - model_57_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4367 - model_56_loss: 0.4722 - model_57_loss: 0.6919 - model_57_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4369 - model_56_loss: 0.4718 - model_57_loss: 0.6920 - model_57_1_loss: 0.6897\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4376 - model_56_loss: 0.4714 - model_57_loss: 0.6919 - model_57_1_loss: 0.6899\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4370 - model_56_loss: 0.4712 - model_57_loss: 0.6916 - model_57_1_loss: 0.6901\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4352 - model_56_loss: 0.4707 - model_57_loss: 0.6918 - model_57_1_loss: 0.6894\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9102 - model_57_loss: 0.6924 - model_57_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4368 - model_56_loss: 0.4702 - model_57_loss: 0.6916 - model_57_1_loss: 0.6898\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4343 - model_56_loss: 0.4708 - model_57_loss: 0.6914 - model_57_1_loss: 0.6896\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4340 - model_56_loss: 0.4711 - model_57_loss: 0.6913 - model_57_1_loss: 0.6897\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4352 - model_56_loss: 0.4718 - model_57_loss: 0.6916 - model_57_1_loss: 0.6898\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4340 - model_56_loss: 0.4712 - model_57_loss: 0.6914 - model_57_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9091 - model_57_loss: 0.6916 - model_57_1_loss: 0.6898\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4344 - model_56_loss: 0.4704 - model_57_loss: 0.6915 - model_57_1_loss: 0.6894\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4337 - model_56_loss: 0.4711 - model_57_loss: 0.6912 - model_57_1_loss: 0.6897\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4335 - model_56_loss: 0.4727 - model_57_loss: 0.6916 - model_57_1_loss: 0.6896\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4320 - model_56_loss: 0.4726 - model_57_loss: 0.6913 - model_57_1_loss: 0.6896\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4362 - model_56_loss: 0.4693 - model_57_loss: 0.6913 - model_57_1_loss: 0.6898\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9101 - model_57_loss: 0.6917 - model_57_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4322 - model_56_loss: 0.4726 - model_57_loss: 0.6912 - model_57_1_loss: 0.6898\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4378 - model_56_loss: 0.4728 - model_57_loss: 0.6917 - model_57_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4404 - model_56_loss: 0.4716 - model_57_loss: 0.6917 - model_57_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4372 - model_56_loss: 0.4722 - model_57_loss: 0.6914 - model_57_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4393 - model_56_loss: 0.4712 - model_57_loss: 0.6915 - model_57_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9128 - model_57_loss: 0.6917 - model_57_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4378 - model_56_loss: 0.4714 - model_57_loss: 0.6915 - model_57_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4430 - model_56_loss: 0.4711 - model_57_loss: 0.6921 - model_57_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4415 - model_56_loss: 0.4727 - model_57_loss: 0.6921 - model_57_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4424 - model_56_loss: 0.4712 - model_57_loss: 0.6918 - model_57_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4431 - model_56_loss: 0.4728 - model_57_loss: 0.6921 - model_57_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9138 - model_57_loss: 0.6915 - model_57_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4429 - model_56_loss: 0.4725 - model_57_loss: 0.6923 - model_57_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4439 - model_56_loss: 0.4728 - model_57_loss: 0.6923 - model_57_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4434 - model_56_loss: 0.4736 - model_57_loss: 0.6922 - model_57_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4449 - model_56_loss: 0.4734 - model_57_loss: 0.6924 - model_57_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4445 - model_56_loss: 0.4742 - model_57_loss: 0.6922 - model_57_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9179 - model_57_loss: 0.6934 - model_57_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4420 - model_56_loss: 0.4738 - model_57_loss: 0.6921 - model_57_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4412 - model_56_loss: 0.4749 - model_57_loss: 0.6920 - model_57_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4439 - model_56_loss: 0.4738 - model_57_loss: 0.6919 - model_57_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4431 - model_56_loss: 0.4749 - model_57_loss: 0.6921 - model_57_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4465 - model_56_loss: 0.4760 - model_57_loss: 0.6927 - model_57_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9186 - model_57_loss: 0.6916 - model_57_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4439 - model_56_loss: 0.4741 - model_57_loss: 0.6921 - model_57_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4440 - model_56_loss: 0.4748 - model_57_loss: 0.6922 - model_57_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4469 - model_56_loss: 0.4751 - model_57_loss: 0.6927 - model_57_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4460 - model_56_loss: 0.4759 - model_57_loss: 0.6926 - model_57_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4470 - model_56_loss: 0.4761 - model_57_loss: 0.6928 - model_57_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9231 - model_57_loss: 0.6933 - model_57_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4440 - model_56_loss: 0.4776 - model_57_loss: 0.6926 - model_57_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4446 - model_56_loss: 0.4790 - model_57_loss: 0.6926 - model_57_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4452 - model_56_loss: 0.4766 - model_57_loss: 0.6927 - model_57_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4464 - model_56_loss: 0.4786 - model_57_loss: 0.6929 - model_57_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4445 - model_56_loss: 0.4786 - model_57_loss: 0.6927 - model_57_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9252 - model_57_loss: 0.6931 - model_57_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4456 - model_56_loss: 0.4789 - model_57_loss: 0.6926 - model_57_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4498 - model_56_loss: 0.4768 - model_57_loss: 0.6927 - model_57_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4495 - model_56_loss: 0.4767 - model_57_loss: 0.6929 - model_57_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4495 - model_56_loss: 0.4772 - model_57_loss: 0.6928 - model_57_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4527 - model_56_loss: 0.4734 - model_57_loss: 0.6927 - model_57_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9267 - model_57_loss: 0.6937 - model_57_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4452 - model_56_loss: 0.4784 - model_57_loss: 0.6926 - model_57_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4450 - model_56_loss: 0.4789 - model_57_loss: 0.6928 - model_57_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4463 - model_56_loss: 0.4773 - model_57_loss: 0.6927 - model_57_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4456 - model_56_loss: 0.4783 - model_57_loss: 0.6928 - model_57_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4439 - model_56_loss: 0.4773 - model_57_loss: 0.6925 - model_57_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9237 - model_57_loss: 0.6929 - model_57_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4461 - model_56_loss: 0.4746 - model_57_loss: 0.6924 - model_57_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4452 - model_56_loss: 0.4756 - model_57_loss: 0.6925 - model_57_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4468 - model_56_loss: 0.4747 - model_57_loss: 0.6928 - model_57_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4461 - model_56_loss: 0.4743 - model_57_loss: 0.6926 - model_57_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4474 - model_56_loss: 0.4718 - model_57_loss: 0.6925 - model_57_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9208 - model_57_loss: 0.6931 - model_57_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4476 - model_56_loss: 0.4707 - model_57_loss: 0.6927 - model_57_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4466 - model_56_loss: 0.4710 - model_57_loss: 0.6925 - model_57_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4471 - model_56_loss: 0.4693 - model_57_loss: 0.6925 - model_57_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4476 - model_56_loss: 0.4702 - model_57_loss: 0.6927 - model_57_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4487 - model_56_loss: 0.4687 - model_57_loss: 0.6925 - model_57_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9171 - model_57_loss: 0.6923 - model_57_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4472 - model_56_loss: 0.4691 - model_57_loss: 0.6927 - model_57_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4457 - model_56_loss: 0.4695 - model_57_loss: 0.6922 - model_57_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4467 - model_56_loss: 0.4670 - model_57_loss: 0.6922 - model_57_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4498 - model_56_loss: 0.4655 - model_57_loss: 0.6922 - model_57_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4494 - model_56_loss: 0.4670 - model_57_loss: 0.6922 - model_57_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9163 - model_57_loss: 0.6919 - model_57_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4528 - model_56_loss: 0.4637 - model_57_loss: 0.6922 - model_57_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4465 - model_56_loss: 0.4655 - model_57_loss: 0.6921 - model_57_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4500 - model_56_loss: 0.4649 - model_57_loss: 0.6923 - model_57_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4514 - model_56_loss: 0.4657 - model_57_loss: 0.6925 - model_57_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4524 - model_56_loss: 0.4658 - model_57_loss: 0.6927 - model_57_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9171 - model_57_loss: 0.6927 - model_57_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4511 - model_56_loss: 0.4652 - model_57_loss: 0.6925 - model_57_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4476 - model_56_loss: 0.4664 - model_57_loss: 0.6921 - model_57_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4486 - model_56_loss: 0.4677 - model_57_loss: 0.6922 - model_57_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4510 - model_56_loss: 0.4649 - model_57_loss: 0.6920 - model_57_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4501 - model_56_loss: 0.4675 - model_57_loss: 0.6922 - model_57_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9181 - model_57_loss: 0.6920 - model_57_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4529 - model_56_loss: 0.4656 - model_57_loss: 0.6924 - model_57_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4528 - model_56_loss: 0.4675 - model_57_loss: 0.6925 - model_57_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4524 - model_56_loss: 0.4678 - model_57_loss: 0.6922 - model_57_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4488 - model_56_loss: 0.4685 - model_57_loss: 0.6920 - model_57_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4522 - model_56_loss: 0.4676 - model_57_loss: 0.6922 - model_57_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9222 - model_57_loss: 0.6931 - model_57_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4526 - model_56_loss: 0.4677 - model_57_loss: 0.6924 - model_57_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4530 - model_56_loss: 0.4677 - model_57_loss: 0.6926 - model_57_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4552 - model_56_loss: 0.4658 - model_57_loss: 0.6925 - model_57_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4517 - model_56_loss: 0.4696 - model_57_loss: 0.6926 - model_57_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4496 - model_56_loss: 0.4705 - model_57_loss: 0.6923 - model_57_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9234 - model_57_loss: 0.6925 - model_57_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4504 - model_56_loss: 0.4702 - model_57_loss: 0.6925 - model_57_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4503 - model_56_loss: 0.4705 - model_57_loss: 0.6927 - model_57_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4508 - model_56_loss: 0.4710 - model_57_loss: 0.6927 - model_57_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4505 - model_56_loss: 0.4696 - model_57_loss: 0.6924 - model_57_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4509 - model_56_loss: 0.4704 - model_57_loss: 0.6924 - model_57_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9256 - model_57_loss: 0.6921 - model_57_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4529 - model_56_loss: 0.4717 - model_57_loss: 0.6928 - model_57_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4559 - model_56_loss: 0.4698 - model_57_loss: 0.6929 - model_57_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4536 - model_56_loss: 0.4717 - model_57_loss: 0.6928 - model_57_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4535 - model_56_loss: 0.4725 - model_57_loss: 0.6927 - model_57_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4563 - model_56_loss: 0.4721 - model_57_loss: 0.6931 - model_57_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9264 - model_57_loss: 0.6933 - model_57_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4527 - model_56_loss: 0.4710 - model_57_loss: 0.6928 - model_57_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4497 - model_56_loss: 0.4715 - model_57_loss: 0.6924 - model_57_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4518 - model_56_loss: 0.4721 - model_57_loss: 0.6927 - model_57_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4502 - model_56_loss: 0.4714 - model_57_loss: 0.6924 - model_57_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4532 - model_56_loss: 0.4705 - model_57_loss: 0.6926 - model_57_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9232 - model_57_loss: 0.6926 - model_57_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4486 - model_56_loss: 0.4722 - model_57_loss: 0.6924 - model_57_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4510 - model_56_loss: 0.4713 - model_57_loss: 0.6925 - model_57_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4518 - model_56_loss: 0.4674 - model_57_loss: 0.6922 - model_57_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4574 - model_56_loss: 0.4664 - model_57_loss: 0.6928 - model_57_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4536 - model_56_loss: 0.4671 - model_57_loss: 0.6926 - model_57_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9227 - model_57_loss: 0.6940 - model_57_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4521 - model_56_loss: 0.4688 - model_57_loss: 0.6927 - model_57_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4535 - model_56_loss: 0.4668 - model_57_loss: 0.6929 - model_57_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4513 - model_56_loss: 0.4638 - model_57_loss: 0.6922 - model_57_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4514 - model_56_loss: 0.4654 - model_57_loss: 0.6924 - model_57_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4513 - model_56_loss: 0.4639 - model_57_loss: 0.6924 - model_57_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9187 - model_57_loss: 0.6924 - model_57_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4522 - model_56_loss: 0.4635 - model_57_loss: 0.6922 - model_57_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4524 - model_56_loss: 0.4628 - model_57_loss: 0.6921 - model_57_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4555 - model_56_loss: 0.4632 - model_57_loss: 0.6927 - model_57_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4547 - model_56_loss: 0.4620 - model_57_loss: 0.6925 - model_57_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4551 - model_56_loss: 0.4620 - model_57_loss: 0.6926 - model_57_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9149 - model_57_loss: 0.6923 - model_57_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4490 - model_56_loss: 0.4608 - model_57_loss: 0.6915 - model_57_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4473 - model_56_loss: 0.4617 - model_57_loss: 0.6918 - model_57_1_loss: 0.6900\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4492 - model_56_loss: 0.4610 - model_57_loss: 0.6919 - model_57_1_loss: 0.6901\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4503 - model_56_loss: 0.4606 - model_57_loss: 0.6918 - model_57_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4508 - model_56_loss: 0.4622 - model_57_loss: 0.6921 - model_57_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9114 - model_57_loss: 0.6913 - model_57_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4485 - model_56_loss: 0.4618 - model_57_loss: 0.6917 - model_57_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4481 - model_56_loss: 0.4626 - model_57_loss: 0.6919 - model_57_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4525 - model_56_loss: 0.4636 - model_57_loss: 0.6923 - model_57_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4528 - model_56_loss: 0.4655 - model_57_loss: 0.6921 - model_57_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4535 - model_56_loss: 0.4631 - model_57_loss: 0.6919 - model_57_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9214 - model_57_loss: 0.6931 - model_57_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4526 - model_56_loss: 0.4677 - model_57_loss: 0.6922 - model_57_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4519 - model_56_loss: 0.4706 - model_57_loss: 0.6924 - model_57_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4565 - model_56_loss: 0.4673 - model_57_loss: 0.6927 - model_57_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4552 - model_56_loss: 0.4691 - model_57_loss: 0.6926 - model_57_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4552 - model_56_loss: 0.4697 - model_57_loss: 0.6925 - model_57_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9268 - model_57_loss: 0.6928 - model_57_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4507 - model_56_loss: 0.4721 - model_57_loss: 0.6926 - model_57_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4538 - model_56_loss: 0.4726 - model_57_loss: 0.6931 - model_57_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4517 - model_56_loss: 0.4720 - model_57_loss: 0.6926 - model_57_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4529 - model_56_loss: 0.4723 - model_57_loss: 0.6927 - model_57_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4545 - model_56_loss: 0.4722 - model_57_loss: 0.6928 - model_57_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9242 - model_57_loss: 0.6927 - model_57_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4513 - model_56_loss: 0.4726 - model_57_loss: 0.6926 - model_57_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4504 - model_56_loss: 0.4717 - model_57_loss: 0.6925 - model_57_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4504 - model_56_loss: 0.4711 - model_57_loss: 0.6926 - model_57_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4520 - model_56_loss: 0.4703 - model_57_loss: 0.6926 - model_57_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4534 - model_56_loss: 0.4679 - model_57_loss: 0.6925 - model_57_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9243 - model_57_loss: 0.6933 - model_57_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4532 - model_56_loss: 0.4673 - model_57_loss: 0.6926 - model_57_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4549 - model_56_loss: 0.4639 - model_57_loss: 0.6923 - model_57_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4546 - model_56_loss: 0.4640 - model_57_loss: 0.6924 - model_57_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4538 - model_56_loss: 0.4655 - model_57_loss: 0.6925 - model_57_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4541 - model_56_loss: 0.4647 - model_57_loss: 0.6924 - model_57_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9168 - model_57_loss: 0.6920 - model_57_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4520 - model_56_loss: 0.4623 - model_57_loss: 0.6922 - model_57_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4515 - model_56_loss: 0.4610 - model_57_loss: 0.6920 - model_57_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4546 - model_56_loss: 0.4577 - model_57_loss: 0.6922 - model_57_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4553 - model_56_loss: 0.4585 - model_57_loss: 0.6923 - model_57_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4536 - model_56_loss: 0.4595 - model_57_loss: 0.6921 - model_57_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9143 - model_57_loss: 0.6922 - model_57_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4517 - model_56_loss: 0.4598 - model_57_loss: 0.6921 - model_57_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4510 - model_56_loss: 0.4599 - model_57_loss: 0.6921 - model_57_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4488 - model_56_loss: 0.4600 - model_57_loss: 0.6920 - model_57_1_loss: 0.6898\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4536 - model_56_loss: 0.4583 - model_57_loss: 0.6921 - model_57_1_loss: 0.6902\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4511 - model_56_loss: 0.4586 - model_57_loss: 0.6921 - model_57_1_loss: 0.6898\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9136 - model_57_loss: 0.6922 - model_57_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4503 - model_56_loss: 0.4591 - model_57_loss: 0.6919 - model_57_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4522 - model_56_loss: 0.4600 - model_57_loss: 0.6923 - model_57_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4509 - model_56_loss: 0.4606 - model_57_loss: 0.6918 - model_57_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4534 - model_56_loss: 0.4615 - model_57_loss: 0.6922 - model_57_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4520 - model_56_loss: 0.4628 - model_57_loss: 0.6923 - model_57_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9143 - model_57_loss: 0.6917 - model_57_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4577 - model_56_loss: 0.4632 - model_57_loss: 0.6926 - model_57_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4559 - model_56_loss: 0.4637 - model_57_loss: 0.6921 - model_57_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4582 - model_56_loss: 0.4641 - model_57_loss: 0.6926 - model_57_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4591 - model_56_loss: 0.4661 - model_57_loss: 0.6927 - model_57_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4586 - model_56_loss: 0.4682 - model_57_loss: 0.6926 - model_57_1_loss: 0.6927\n",
      "For Attention Module: 1.6\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.3452 - model_61_loss: 0.6606 - model_61_1_loss: 0.6089\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -5.9593 - model_60_loss: 0.3759 - model_61_loss: 0.6592 - model_61_1_loss: 0.6078\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -5.9700 - model_60_loss: 0.3749 - model_61_loss: 0.6602 - model_61_1_loss: 0.6088\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -5.9811 - model_60_loss: 0.3765 - model_61_loss: 0.6607 - model_61_1_loss: 0.6108\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -5.9897 - model_60_loss: 0.3756 - model_61_loss: 0.6605 - model_61_1_loss: 0.6126\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0106 - model_60_loss: 0.3755 - model_61_loss: 0.6614 - model_61_1_loss: 0.6158\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.3900 - model_61_loss: 0.6617 - model_61_1_loss: 0.6164\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.0037 - model_60_loss: 0.3771 - model_61_loss: 0.6608 - model_61_1_loss: 0.6153\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0156 - model_60_loss: 0.3767 - model_61_loss: 0.6616 - model_61_1_loss: 0.6169\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0289 - model_60_loss: 0.3766 - model_61_loss: 0.6617 - model_61_1_loss: 0.6194\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0273 - model_60_loss: 0.3784 - model_61_loss: 0.6612 - model_61_1_loss: 0.6199\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0554 - model_60_loss: 0.3807 - model_61_loss: 0.6634 - model_61_1_loss: 0.6238\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.4447 - model_61_loss: 0.6642 - model_61_1_loss: 0.6245\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0733 - model_60_loss: 0.3837 - model_61_loss: 0.6656 - model_61_1_loss: 0.6257\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.0773 - model_60_loss: 0.3848 - model_61_loss: 0.6649 - model_61_1_loss: 0.6275\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0934 - model_60_loss: 0.3867 - model_61_loss: 0.6665 - model_61_1_loss: 0.6295\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1155 - model_60_loss: 0.3862 - model_61_loss: 0.6685 - model_61_1_loss: 0.6319\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1337 - model_60_loss: 0.3878 - model_61_loss: 0.6686 - model_61_1_loss: 0.6357\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.5188 - model_61_loss: 0.6694 - model_61_1_loss: 0.6349\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1181 - model_60_loss: 0.3898 - model_61_loss: 0.6676 - model_61_1_loss: 0.6340\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1421 - model_60_loss: 0.3937 - model_61_loss: 0.6699 - model_61_1_loss: 0.6373\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1440 - model_60_loss: 0.3940 - model_61_loss: 0.6688 - model_61_1_loss: 0.6388\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1656 - model_60_loss: 0.3987 - model_61_loss: 0.6715 - model_61_1_loss: 0.6413\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1744 - model_60_loss: 0.4010 - model_61_loss: 0.6720 - model_61_1_loss: 0.6430\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.5895 - model_61_loss: 0.6730 - model_61_1_loss: 0.6452\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1811 - model_60_loss: 0.4050 - model_61_loss: 0.6724 - model_61_1_loss: 0.6448\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.1913 - model_60_loss: 0.4079 - model_61_loss: 0.6736 - model_61_1_loss: 0.6462\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2128 - model_60_loss: 0.4090 - model_61_loss: 0.6749 - model_61_1_loss: 0.6494\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2385 - model_60_loss: 0.4096 - model_61_loss: 0.6768 - model_61_1_loss: 0.6528\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2438 - model_60_loss: 0.4113 - model_61_loss: 0.6763 - model_61_1_loss: 0.6547\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.6704 - model_61_loss: 0.6776 - model_61_1_loss: 0.6563\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2502 - model_60_loss: 0.4156 - model_61_loss: 0.6766 - model_61_1_loss: 0.6565\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2645 - model_60_loss: 0.4167 - model_61_loss: 0.6784 - model_61_1_loss: 0.6578\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2770 - model_60_loss: 0.4192 - model_61_loss: 0.6790 - model_61_1_loss: 0.6603\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.2805 - model_60_loss: 0.4235 - model_61_loss: 0.6799 - model_61_1_loss: 0.6609\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2994 - model_60_loss: 0.4221 - model_61_loss: 0.6806 - model_61_1_loss: 0.6637\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.7377 - model_61_loss: 0.6813 - model_61_1_loss: 0.6658\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3121 - model_60_loss: 0.4239 - model_61_loss: 0.6812 - model_61_1_loss: 0.6660\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3151 - model_60_loss: 0.4283 - model_61_loss: 0.6815 - model_61_1_loss: 0.6672\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3267 - model_60_loss: 0.4295 - model_61_loss: 0.6826 - model_61_1_loss: 0.6687\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3444 - model_60_loss: 0.4314 - model_61_loss: 0.6836 - model_61_1_loss: 0.6716\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3464 - model_60_loss: 0.4335 - model_61_loss: 0.6841 - model_61_1_loss: 0.6719\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.7913 - model_61_loss: 0.6847 - model_61_1_loss: 0.6735\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3522 - model_60_loss: 0.4364 - model_61_loss: 0.6851 - model_61_1_loss: 0.6726\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3644 - model_60_loss: 0.4392 - model_61_loss: 0.6855 - model_61_1_loss: 0.6752\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3656 - model_60_loss: 0.4385 - model_61_loss: 0.6855 - model_61_1_loss: 0.6753\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3747 - model_60_loss: 0.4414 - model_61_loss: 0.6861 - model_61_1_loss: 0.6771\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3827 - model_60_loss: 0.4444 - model_61_loss: 0.6869 - model_61_1_loss: 0.6786\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.8277 - model_61_loss: 0.6865 - model_61_1_loss: 0.6784\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3730 - model_60_loss: 0.4476 - model_61_loss: 0.6866 - model_61_1_loss: 0.6775\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3818 - model_60_loss: 0.4484 - model_61_loss: 0.6871 - model_61_1_loss: 0.6790\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3884 - model_60_loss: 0.4507 - model_61_loss: 0.6877 - model_61_1_loss: 0.6801\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3907 - model_60_loss: 0.4522 - model_61_loss: 0.6883 - model_61_1_loss: 0.6803\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4019 - model_60_loss: 0.4517 - model_61_loss: 0.6887 - model_61_1_loss: 0.6820\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.8577 - model_61_loss: 0.6891 - model_61_1_loss: 0.6828\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4027 - model_60_loss: 0.4554 - model_61_loss: 0.6893 - model_61_1_loss: 0.6823\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4061 - model_60_loss: 0.4571 - model_61_loss: 0.6895 - model_61_1_loss: 0.6831\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4059 - model_60_loss: 0.4583 - model_61_loss: 0.6894 - model_61_1_loss: 0.6834\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4096 - model_60_loss: 0.4630 - model_61_loss: 0.6903 - model_61_1_loss: 0.6842\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4166 - model_60_loss: 0.4633 - model_61_loss: 0.6903 - model_61_1_loss: 0.6857\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.8813 - model_61_loss: 0.6906 - model_61_1_loss: 0.6851\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4134 - model_60_loss: 0.4660 - model_61_loss: 0.6905 - model_61_1_loss: 0.6854\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4198 - model_60_loss: 0.4670 - model_61_loss: 0.6909 - model_61_1_loss: 0.6865\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4201 - model_60_loss: 0.4688 - model_61_loss: 0.6914 - model_61_1_loss: 0.6864\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4276 - model_60_loss: 0.4718 - model_61_loss: 0.6921 - model_61_1_loss: 0.6877\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4255 - model_60_loss: 0.4722 - model_61_loss: 0.6916 - model_61_1_loss: 0.6879\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9026 - model_61_loss: 0.6926 - model_61_1_loss: 0.6881\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4246 - model_60_loss: 0.4740 - model_61_loss: 0.6917 - model_61_1_loss: 0.6880\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4245 - model_60_loss: 0.4773 - model_61_loss: 0.6919 - model_61_1_loss: 0.6884\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4232 - model_60_loss: 0.4780 - model_61_loss: 0.6919 - model_61_1_loss: 0.6883\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4281 - model_60_loss: 0.4796 - model_61_loss: 0.6924 - model_61_1_loss: 0.6891\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4286 - model_60_loss: 0.4813 - model_61_loss: 0.6923 - model_61_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9125 - model_61_loss: 0.6923 - model_61_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4296 - model_60_loss: 0.4820 - model_61_loss: 0.6924 - model_61_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4291 - model_60_loss: 0.4826 - model_61_loss: 0.6928 - model_61_1_loss: 0.6896\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4300 - model_60_loss: 0.4844 - model_61_loss: 0.6928 - model_61_1_loss: 0.6901\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4320 - model_60_loss: 0.4844 - model_61_loss: 0.6928 - model_61_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4337 - model_60_loss: 0.4837 - model_61_loss: 0.6932 - model_61_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9173 - model_61_loss: 0.6938 - model_61_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4344 - model_60_loss: 0.4850 - model_61_loss: 0.6931 - model_61_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4320 - model_60_loss: 0.4840 - model_61_loss: 0.6926 - model_61_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4343 - model_60_loss: 0.4842 - model_61_loss: 0.6927 - model_61_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4328 - model_60_loss: 0.4832 - model_61_loss: 0.6927 - model_61_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4337 - model_60_loss: 0.4860 - model_61_loss: 0.6930 - model_61_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9196 - model_61_loss: 0.6933 - model_61_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4388 - model_60_loss: 0.4819 - model_61_loss: 0.6928 - model_61_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4340 - model_60_loss: 0.4836 - model_61_loss: 0.6926 - model_61_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4378 - model_60_loss: 0.4813 - model_61_loss: 0.6928 - model_61_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4379 - model_60_loss: 0.4818 - model_61_loss: 0.6929 - model_61_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4359 - model_60_loss: 0.4813 - model_61_loss: 0.6927 - model_61_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9201 - model_61_loss: 0.6927 - model_61_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4347 - model_60_loss: 0.4825 - model_61_loss: 0.6925 - model_61_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4361 - model_60_loss: 0.4796 - model_61_loss: 0.6922 - model_61_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4378 - model_60_loss: 0.4806 - model_61_loss: 0.6926 - model_61_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4411 - model_60_loss: 0.4777 - model_61_loss: 0.6926 - model_61_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4373 - model_60_loss: 0.4780 - model_61_loss: 0.6924 - model_61_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9187 - model_61_loss: 0.6926 - model_61_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4379 - model_60_loss: 0.4779 - model_61_loss: 0.6924 - model_61_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4384 - model_60_loss: 0.4779 - model_61_loss: 0.6925 - model_61_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4395 - model_60_loss: 0.4782 - model_61_loss: 0.6925 - model_61_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4398 - model_60_loss: 0.4776 - model_61_loss: 0.6924 - model_61_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4414 - model_60_loss: 0.4755 - model_61_loss: 0.6922 - model_61_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9149 - model_61_loss: 0.6925 - model_61_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4405 - model_60_loss: 0.4759 - model_61_loss: 0.6919 - model_61_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4389 - model_60_loss: 0.4749 - model_61_loss: 0.6915 - model_61_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4429 - model_60_loss: 0.4749 - model_61_loss: 0.6922 - model_61_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4384 - model_60_loss: 0.4746 - model_61_loss: 0.6919 - model_61_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4424 - model_60_loss: 0.4741 - model_61_loss: 0.6920 - model_61_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9161 - model_61_loss: 0.6934 - model_61_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4417 - model_60_loss: 0.4741 - model_61_loss: 0.6920 - model_61_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4464 - model_60_loss: 0.4725 - model_61_loss: 0.6923 - model_61_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4450 - model_60_loss: 0.4725 - model_61_loss: 0.6922 - model_61_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4455 - model_60_loss: 0.4737 - model_61_loss: 0.6920 - model_61_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4412 - model_60_loss: 0.4740 - model_61_loss: 0.6919 - model_61_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9173 - model_61_loss: 0.6921 - model_61_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4466 - model_60_loss: 0.4724 - model_61_loss: 0.6920 - model_61_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4414 - model_60_loss: 0.4752 - model_61_loss: 0.6917 - model_61_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4452 - model_60_loss: 0.4735 - model_61_loss: 0.6921 - model_61_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4418 - model_60_loss: 0.4748 - model_61_loss: 0.6916 - model_61_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4446 - model_60_loss: 0.4752 - model_61_loss: 0.6921 - model_61_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9210 - model_61_loss: 0.6918 - model_61_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4418 - model_60_loss: 0.4765 - model_61_loss: 0.6919 - model_61_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4439 - model_60_loss: 0.4765 - model_61_loss: 0.6923 - model_61_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4428 - model_60_loss: 0.4763 - model_61_loss: 0.6921 - model_61_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4452 - model_60_loss: 0.4741 - model_61_loss: 0.6919 - model_61_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4433 - model_60_loss: 0.4765 - model_61_loss: 0.6918 - model_61_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9189 - model_61_loss: 0.6914 - model_61_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4486 - model_60_loss: 0.4750 - model_61_loss: 0.6927 - model_61_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4490 - model_60_loss: 0.4749 - model_61_loss: 0.6927 - model_61_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4448 - model_60_loss: 0.4763 - model_61_loss: 0.6924 - model_61_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4453 - model_60_loss: 0.4751 - model_61_loss: 0.6923 - model_61_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4460 - model_60_loss: 0.4763 - model_61_loss: 0.6923 - model_61_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9220 - model_61_loss: 0.6929 - model_61_1_loss: 0.69220s - loss: 6.9652 - model_61_loss: 0.7022 - model_61_1_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4436 - model_60_loss: 0.4773 - model_61_loss: 0.6918 - model_61_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4434 - model_60_loss: 0.4766 - model_61_loss: 0.6916 - model_61_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4472 - model_60_loss: 0.4759 - model_61_loss: 0.6922 - model_61_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4413 - model_60_loss: 0.4764 - model_61_loss: 0.6914 - model_61_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4458 - model_60_loss: 0.4766 - model_61_loss: 0.6921 - model_61_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9228 - model_61_loss: 0.6925 - model_61_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4448 - model_60_loss: 0.4749 - model_61_loss: 0.6919 - model_61_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4448 - model_60_loss: 0.4759 - model_61_loss: 0.6919 - model_61_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4438 - model_60_loss: 0.4767 - model_61_loss: 0.6919 - model_61_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4470 - model_60_loss: 0.4749 - model_61_loss: 0.6922 - model_61_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4443 - model_60_loss: 0.4759 - model_61_loss: 0.6919 - model_61_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9204 - model_61_loss: 0.6919 - model_61_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4442 - model_60_loss: 0.4750 - model_61_loss: 0.6918 - model_61_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4439 - model_60_loss: 0.4746 - model_61_loss: 0.6917 - model_61_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4434 - model_60_loss: 0.4728 - model_61_loss: 0.6917 - model_61_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4454 - model_60_loss: 0.4722 - model_61_loss: 0.6918 - model_61_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4450 - model_60_loss: 0.4720 - model_61_loss: 0.6916 - model_61_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9202 - model_61_loss: 0.6914 - model_61_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4465 - model_60_loss: 0.4705 - model_61_loss: 0.6916 - model_61_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4451 - model_60_loss: 0.4704 - model_61_loss: 0.6916 - model_61_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4437 - model_60_loss: 0.4733 - model_61_loss: 0.6916 - model_61_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4479 - model_60_loss: 0.4712 - model_61_loss: 0.6921 - model_61_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4443 - model_60_loss: 0.4718 - model_61_loss: 0.6918 - model_61_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9181 - model_61_loss: 0.6923 - model_61_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4483 - model_60_loss: 0.4718 - model_61_loss: 0.6924 - model_61_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4475 - model_60_loss: 0.4692 - model_61_loss: 0.6919 - model_61_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4476 - model_60_loss: 0.4704 - model_61_loss: 0.6921 - model_61_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4497 - model_60_loss: 0.4696 - model_61_loss: 0.6923 - model_61_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4518 - model_60_loss: 0.4694 - model_61_loss: 0.6927 - model_61_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9191 - model_61_loss: 0.6926 - model_61_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4493 - model_60_loss: 0.4705 - model_61_loss: 0.6923 - model_61_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4488 - model_60_loss: 0.4702 - model_61_loss: 0.6922 - model_61_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4508 - model_60_loss: 0.4688 - model_61_loss: 0.6922 - model_61_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4488 - model_60_loss: 0.4702 - model_61_loss: 0.6921 - model_61_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4508 - model_60_loss: 0.4694 - model_61_loss: 0.6923 - model_61_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9217 - model_61_loss: 0.6928 - model_61_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4512 - model_60_loss: 0.4693 - model_61_loss: 0.6924 - model_61_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4501 - model_60_loss: 0.4698 - model_61_loss: 0.6924 - model_61_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4495 - model_60_loss: 0.4714 - model_61_loss: 0.6924 - model_61_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4488 - model_60_loss: 0.4704 - model_61_loss: 0.6921 - model_61_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4500 - model_60_loss: 0.4720 - model_61_loss: 0.6926 - model_61_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9194 - model_61_loss: 0.6919 - model_61_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4512 - model_60_loss: 0.4682 - model_61_loss: 0.6923 - model_61_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4519 - model_60_loss: 0.4669 - model_61_loss: 0.6926 - model_61_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4505 - model_60_loss: 0.4675 - model_61_loss: 0.6923 - model_61_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4510 - model_60_loss: 0.4678 - model_61_loss: 0.6925 - model_61_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4500 - model_60_loss: 0.4691 - model_61_loss: 0.6925 - model_61_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9219 - model_61_loss: 0.6925 - model_61_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4525 - model_60_loss: 0.4671 - model_61_loss: 0.6924 - model_61_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4515 - model_60_loss: 0.4668 - model_61_loss: 0.6923 - model_61_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4525 - model_60_loss: 0.4670 - model_61_loss: 0.6925 - model_61_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4498 - model_60_loss: 0.4688 - model_61_loss: 0.6923 - model_61_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4530 - model_60_loss: 0.4657 - model_61_loss: 0.6923 - model_61_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9181 - model_61_loss: 0.6923 - model_61_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4532 - model_60_loss: 0.4653 - model_61_loss: 0.6924 - model_61_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4508 - model_60_loss: 0.4661 - model_61_loss: 0.6924 - model_61_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4514 - model_60_loss: 0.4659 - model_61_loss: 0.6924 - model_61_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4519 - model_60_loss: 0.4669 - model_61_loss: 0.6925 - model_61_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4495 - model_60_loss: 0.4686 - model_61_loss: 0.6925 - model_61_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9214 - model_61_loss: 0.6930 - model_61_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4495 - model_60_loss: 0.4672 - model_61_loss: 0.6922 - model_61_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4512 - model_60_loss: 0.4667 - model_61_loss: 0.6922 - model_61_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4504 - model_60_loss: 0.4687 - model_61_loss: 0.6924 - model_61_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4539 - model_60_loss: 0.4670 - model_61_loss: 0.6926 - model_61_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4520 - model_60_loss: 0.4681 - model_61_loss: 0.6923 - model_61_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9233 - model_61_loss: 0.6925 - model_61_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4543 - model_60_loss: 0.4675 - model_61_loss: 0.6924 - model_61_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4525 - model_60_loss: 0.4690 - model_61_loss: 0.6924 - model_61_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4537 - model_60_loss: 0.4697 - model_61_loss: 0.6928 - model_61_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4544 - model_60_loss: 0.4697 - model_61_loss: 0.6927 - model_61_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4515 - model_60_loss: 0.4712 - model_61_loss: 0.6925 - model_61_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9231 - model_61_loss: 0.6921 - model_61_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4532 - model_60_loss: 0.4712 - model_61_loss: 0.6926 - model_61_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4499 - model_60_loss: 0.4725 - model_61_loss: 0.6927 - model_61_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4536 - model_60_loss: 0.4711 - model_61_loss: 0.6927 - model_61_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4569 - model_60_loss: 0.4702 - model_61_loss: 0.6928 - model_61_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4558 - model_60_loss: 0.4720 - model_61_loss: 0.6928 - model_61_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9270 - model_61_loss: 0.6927 - model_61_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4525 - model_60_loss: 0.4723 - model_61_loss: 0.6927 - model_61_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4534 - model_60_loss: 0.4723 - model_61_loss: 0.6929 - model_61_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4509 - model_60_loss: 0.4715 - model_61_loss: 0.6925 - model_61_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4517 - model_60_loss: 0.4721 - model_61_loss: 0.6927 - model_61_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4542 - model_60_loss: 0.4699 - model_61_loss: 0.6926 - model_61_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9250 - model_61_loss: 0.6934 - model_61_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4529 - model_60_loss: 0.4693 - model_61_loss: 0.6926 - model_61_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4527 - model_60_loss: 0.4708 - model_61_loss: 0.6929 - model_61_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4522 - model_60_loss: 0.4699 - model_61_loss: 0.6925 - model_61_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4540 - model_60_loss: 0.4687 - model_61_loss: 0.6925 - model_61_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4527 - model_60_loss: 0.4685 - model_61_loss: 0.6925 - model_61_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9223 - model_61_loss: 0.6933 - model_61_1_loss: 0.69180s - loss: 6.8936 - model_61_loss: 0.6858 - model_61_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4540 - model_60_loss: 0.4671 - model_61_loss: 0.6925 - model_61_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4550 - model_60_loss: 0.4662 - model_61_loss: 0.6924 - model_61_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4545 - model_60_loss: 0.4663 - model_61_loss: 0.6925 - model_61_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4535 - model_60_loss: 0.4664 - model_61_loss: 0.6922 - model_61_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4545 - model_60_loss: 0.4668 - model_61_loss: 0.6925 - model_61_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9206 - model_61_loss: 0.6923 - model_61_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4513 - model_60_loss: 0.4677 - model_61_loss: 0.6924 - model_61_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4540 - model_60_loss: 0.4657 - model_61_loss: 0.6924 - model_61_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4511 - model_60_loss: 0.4654 - model_61_loss: 0.6922 - model_61_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4541 - model_60_loss: 0.4646 - model_61_loss: 0.6924 - model_61_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4535 - model_60_loss: 0.4645 - model_61_loss: 0.6923 - model_61_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9186 - model_61_loss: 0.6929 - model_61_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4560 - model_60_loss: 0.4642 - model_61_loss: 0.6928 - model_61_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4533 - model_60_loss: 0.4641 - model_61_loss: 0.6922 - model_61_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4528 - model_60_loss: 0.4666 - model_61_loss: 0.6925 - model_61_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4537 - model_60_loss: 0.4656 - model_61_loss: 0.6923 - model_61_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4547 - model_60_loss: 0.4644 - model_61_loss: 0.6925 - model_61_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9202 - model_61_loss: 0.6917 - model_61_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4536 - model_60_loss: 0.4641 - model_61_loss: 0.6923 - model_61_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4546 - model_60_loss: 0.4638 - model_61_loss: 0.6923 - model_61_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4554 - model_60_loss: 0.4640 - model_61_loss: 0.6924 - model_61_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4550 - model_60_loss: 0.4635 - model_61_loss: 0.6922 - model_61_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4537 - model_60_loss: 0.4653 - model_61_loss: 0.6924 - model_61_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9196 - model_61_loss: 0.6921 - model_61_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4523 - model_60_loss: 0.4642 - model_61_loss: 0.6921 - model_61_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4579 - model_60_loss: 0.4633 - model_61_loss: 0.6927 - model_61_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4540 - model_60_loss: 0.4655 - model_61_loss: 0.6922 - model_61_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4550 - model_60_loss: 0.4660 - model_61_loss: 0.6925 - model_61_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4530 - model_60_loss: 0.4653 - model_61_loss: 0.6922 - model_61_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9189 - model_61_loss: 0.6927 - model_61_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4541 - model_60_loss: 0.4661 - model_61_loss: 0.6924 - model_61_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4546 - model_60_loss: 0.4664 - model_61_loss: 0.6927 - model_61_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4578 - model_60_loss: 0.4655 - model_61_loss: 0.6927 - model_61_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4535 - model_60_loss: 0.4667 - model_61_loss: 0.6924 - model_61_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4539 - model_60_loss: 0.4683 - model_61_loss: 0.6927 - model_61_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9255 - model_61_loss: 0.6925 - model_61_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4551 - model_60_loss: 0.4656 - model_61_loss: 0.6923 - model_61_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4541 - model_60_loss: 0.4676 - model_61_loss: 0.6923 - model_61_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4567 - model_60_loss: 0.4679 - model_61_loss: 0.6928 - model_61_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4578 - model_60_loss: 0.4657 - model_61_loss: 0.6927 - model_61_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4540 - model_60_loss: 0.4689 - model_61_loss: 0.6923 - model_61_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9237 - model_61_loss: 0.6924 - model_61_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4548 - model_60_loss: 0.4687 - model_61_loss: 0.6926 - model_61_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4555 - model_60_loss: 0.4690 - model_61_loss: 0.6927 - model_61_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4535 - model_60_loss: 0.4695 - model_61_loss: 0.6924 - model_61_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4531 - model_60_loss: 0.4696 - model_61_loss: 0.6925 - model_61_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4543 - model_60_loss: 0.4693 - model_61_loss: 0.6925 - model_61_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9265 - model_61_loss: 0.6925 - model_61_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4544 - model_60_loss: 0.4698 - model_61_loss: 0.6925 - model_61_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4560 - model_60_loss: 0.4684 - model_61_loss: 0.6925 - model_61_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4563 - model_60_loss: 0.4685 - model_61_loss: 0.6927 - model_61_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4570 - model_60_loss: 0.4680 - model_61_loss: 0.6929 - model_61_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4571 - model_60_loss: 0.4673 - model_61_loss: 0.6927 - model_61_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9236 - model_61_loss: 0.6919 - model_61_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4581 - model_60_loss: 0.4668 - model_61_loss: 0.6928 - model_61_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4585 - model_60_loss: 0.4648 - model_61_loss: 0.6929 - model_61_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4556 - model_60_loss: 0.4659 - model_61_loss: 0.6927 - model_61_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4565 - model_60_loss: 0.4644 - model_61_loss: 0.6925 - model_61_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4552 - model_60_loss: 0.4652 - model_61_loss: 0.6926 - model_61_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9209 - model_61_loss: 0.6934 - model_61_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4563 - model_60_loss: 0.4637 - model_61_loss: 0.6925 - model_61_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4555 - model_60_loss: 0.4641 - model_61_loss: 0.6925 - model_61_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4551 - model_60_loss: 0.4634 - model_61_loss: 0.6923 - model_61_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4569 - model_60_loss: 0.4623 - model_61_loss: 0.6924 - model_61_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4563 - model_60_loss: 0.4621 - model_61_loss: 0.6922 - model_61_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9174 - model_61_loss: 0.6918 - model_61_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4585 - model_60_loss: 0.4612 - model_61_loss: 0.6925 - model_61_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4560 - model_60_loss: 0.4616 - model_61_loss: 0.6925 - model_61_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4566 - model_60_loss: 0.4631 - model_61_loss: 0.6923 - model_61_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4580 - model_60_loss: 0.4606 - model_61_loss: 0.6923 - model_61_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4545 - model_60_loss: 0.4638 - model_61_loss: 0.6922 - model_61_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9196 - model_61_loss: 0.6923 - model_61_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4577 - model_60_loss: 0.4643 - model_61_loss: 0.6924 - model_61_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4578 - model_60_loss: 0.4622 - model_61_loss: 0.6923 - model_61_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4580 - model_60_loss: 0.4624 - model_61_loss: 0.6924 - model_61_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4563 - model_60_loss: 0.4672 - model_61_loss: 0.6929 - model_61_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4549 - model_60_loss: 0.4684 - model_61_loss: 0.6926 - model_61_1_loss: 0.6921\n",
      "For Attention Module: 1.7000000000000002\n",
      "features X: 30940 samples, 71 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.3381 - model_65_loss: 0.6589 - model_65_1_loss: 0.6087\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -5.9588 - model_64_loss: 0.3728 - model_65_loss: 0.6575 - model_65_1_loss: 0.6088\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -5.9622 - model_64_loss: 0.3763 - model_65_loss: 0.6585 - model_65_1_loss: 0.6092\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -5.9930 - model_64_loss: 0.3742 - model_65_loss: 0.6605 - model_65_1_loss: 0.6130\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -5.9869 - model_64_loss: 0.3745 - model_65_loss: 0.6593 - model_65_1_loss: 0.6130\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0006 - model_64_loss: 0.3763 - model_65_loss: 0.6604 - model_65_1_loss: 0.6150\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.3927 - model_65_loss: 0.6608 - model_65_1_loss: 0.6175\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.0158 - model_64_loss: 0.3756 - model_65_loss: 0.6615 - model_65_1_loss: 0.6168\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0257 - model_64_loss: 0.3761 - model_65_loss: 0.6615 - model_65_1_loss: 0.6189\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0289 - model_64_loss: 0.3791 - model_65_loss: 0.6615 - model_65_1_loss: 0.6201\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0332 - model_64_loss: 0.3798 - model_65_loss: 0.6609 - model_65_1_loss: 0.6217\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0533 - model_64_loss: 0.3785 - model_65_loss: 0.6627 - model_65_1_loss: 0.6236\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.4570 - model_65_loss: 0.6644 - model_65_1_loss: 0.6270\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0665 - model_64_loss: 0.3805 - model_65_loss: 0.6633 - model_65_1_loss: 0.6261\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0721 - model_64_loss: 0.3824 - model_65_loss: 0.6637 - model_65_1_loss: 0.6272\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0906 - model_64_loss: 0.3828 - model_65_loss: 0.6640 - model_65_1_loss: 0.6306\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0956 - model_64_loss: 0.3829 - model_65_loss: 0.6642 - model_65_1_loss: 0.6315\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1132 - model_64_loss: 0.3845 - model_65_loss: 0.6654 - model_65_1_loss: 0.6342\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.5046 - model_65_loss: 0.6651 - model_65_1_loss: 0.6356\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.1178 - model_64_loss: 0.3858 - model_65_loss: 0.6649 - model_65_1_loss: 0.6358\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1304 - model_64_loss: 0.3864 - model_65_loss: 0.6661 - model_65_1_loss: 0.6373\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.1410 - model_64_loss: 0.3863 - model_65_loss: 0.6652 - model_65_1_loss: 0.6402\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1459 - model_64_loss: 0.3895 - model_65_loss: 0.6657 - model_65_1_loss: 0.6413\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1655 - model_64_loss: 0.3912 - model_65_loss: 0.6671 - model_65_1_loss: 0.6442\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.5751 - model_65_loss: 0.6682 - model_65_1_loss: 0.6470\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1722 - model_64_loss: 0.3902 - model_65_loss: 0.6674 - model_65_1_loss: 0.6451\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1934 - model_64_loss: 0.3928 - model_65_loss: 0.6694 - model_65_1_loss: 0.6478\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2131 - model_64_loss: 0.3934 - model_65_loss: 0.6703 - model_65_1_loss: 0.6510\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2169 - model_64_loss: 0.3947 - model_65_loss: 0.6706 - model_65_1_loss: 0.6517\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2262 - model_64_loss: 0.3974 - model_65_loss: 0.6712 - model_65_1_loss: 0.6536\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.6434 - model_65_loss: 0.6715 - model_65_1_loss: 0.6566\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2327 - model_64_loss: 0.4000 - model_65_loss: 0.6712 - model_65_1_loss: 0.6553\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2580 - model_64_loss: 0.3997 - model_65_loss: 0.6735 - model_65_1_loss: 0.6580\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2677 - model_64_loss: 0.4033 - model_65_loss: 0.6746 - model_65_1_loss: 0.6596\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2747 - model_64_loss: 0.4054 - model_65_loss: 0.6741 - model_65_1_loss: 0.6619\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2943 - model_64_loss: 0.4064 - model_65_loss: 0.6757 - model_65_1_loss: 0.6644\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.7040 - model_65_loss: 0.6756 - model_65_1_loss: 0.6645\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2855 - model_64_loss: 0.4085 - model_65_loss: 0.6761 - model_65_1_loss: 0.6627\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3002 - model_64_loss: 0.4130 - model_65_loss: 0.6773 - model_65_1_loss: 0.6653\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3041 - model_64_loss: 0.4147 - model_65_loss: 0.6773 - model_65_1_loss: 0.6665\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3205 - model_64_loss: 0.4169 - model_65_loss: 0.6790 - model_65_1_loss: 0.6684\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3241 - model_64_loss: 0.4189 - model_65_loss: 0.6784 - model_65_1_loss: 0.6702\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.7529 - model_65_loss: 0.6789 - model_65_1_loss: 0.6710\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3166 - model_64_loss: 0.4234 - model_65_loss: 0.6785 - model_65_1_loss: 0.6695\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3323 - model_64_loss: 0.4241 - model_65_loss: 0.6803 - model_65_1_loss: 0.6710\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3410 - model_64_loss: 0.4272 - model_65_loss: 0.6816 - model_65_1_loss: 0.6720\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3512 - model_64_loss: 0.4302 - model_65_loss: 0.6818 - model_65_1_loss: 0.6745\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3592 - model_64_loss: 0.4370 - model_65_loss: 0.6834 - model_65_1_loss: 0.6758\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.7924 - model_65_loss: 0.6828 - model_65_1_loss: 0.67550s - loss: 6.7874 - model_65_loss: 0.6825 - model_65_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3534 - model_64_loss: 0.4375 - model_65_loss: 0.6827 - model_65_1_loss: 0.6755\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3646 - model_64_loss: 0.4388 - model_65_loss: 0.6836 - model_65_1_loss: 0.6771\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3789 - model_64_loss: 0.4431 - model_65_loss: 0.6861 - model_65_1_loss: 0.6783\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3807 - model_64_loss: 0.4464 - model_65_loss: 0.6853 - model_65_1_loss: 0.6801\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3889 - model_64_loss: 0.4486 - model_65_loss: 0.6863 - model_65_1_loss: 0.6812\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.8426 - model_65_loss: 0.6874 - model_65_1_loss: 0.68130s - loss: 6.7975 - model_65_loss: 0.6781 - model_65_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3858 - model_64_loss: 0.4515 - model_65_loss: 0.6868 - model_65_1_loss: 0.6807\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.3948 - model_64_loss: 0.4532 - model_65_loss: 0.6878 - model_65_1_loss: 0.6818\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3955 - model_64_loss: 0.4594 - model_65_loss: 0.6878 - model_65_1_loss: 0.6832\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3974 - model_64_loss: 0.4593 - model_65_loss: 0.6880 - model_65_1_loss: 0.6833\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4004 - model_64_loss: 0.4634 - model_65_loss: 0.6888 - model_65_1_loss: 0.6840\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.8782 - model_65_loss: 0.6901 - model_65_1_loss: 0.6859\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4069 - model_64_loss: 0.4660 - model_65_loss: 0.6892 - model_65_1_loss: 0.6854\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4095 - model_64_loss: 0.4672 - model_65_loss: 0.6898 - model_65_1_loss: 0.6855\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4135 - model_64_loss: 0.4696 - model_65_loss: 0.6904 - model_65_1_loss: 0.6862\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4191 - model_64_loss: 0.4726 - model_65_loss: 0.6909 - model_65_1_loss: 0.6875\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4211 - model_64_loss: 0.4759 - model_65_loss: 0.6916 - model_65_1_loss: 0.6878\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.8956 - model_65_loss: 0.6915 - model_65_1_loss: 0.6881\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4171 - model_64_loss: 0.4764 - model_65_loss: 0.6909 - model_65_1_loss: 0.6878\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4176 - model_64_loss: 0.4787 - model_65_loss: 0.6908 - model_65_1_loss: 0.6884\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4194 - model_64_loss: 0.4812 - model_65_loss: 0.6913 - model_65_1_loss: 0.6888\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4220 - model_64_loss: 0.4824 - model_65_loss: 0.6918 - model_65_1_loss: 0.6891\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4250 - model_64_loss: 0.4847 - model_65_loss: 0.6922 - model_65_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9150 - model_65_loss: 0.6924 - model_65_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4275 - model_64_loss: 0.4847 - model_65_loss: 0.6924 - model_65_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4248 - model_64_loss: 0.4871 - model_65_loss: 0.6922 - model_65_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4295 - model_64_loss: 0.4866 - model_65_loss: 0.6924 - model_65_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4294 - model_64_loss: 0.4881 - model_65_loss: 0.6924 - model_65_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4300 - model_64_loss: 0.4909 - model_65_loss: 0.6928 - model_65_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9217 - model_65_loss: 0.6929 - model_65_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4305 - model_64_loss: 0.4892 - model_65_loss: 0.6922 - model_65_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4296 - model_64_loss: 0.4907 - model_65_loss: 0.6926 - model_65_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4316 - model_64_loss: 0.4905 - model_65_loss: 0.6925 - model_65_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4346 - model_64_loss: 0.4896 - model_65_loss: 0.6927 - model_65_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4404 - model_64_loss: 0.4882 - model_65_loss: 0.6933 - model_65_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9246 - model_65_loss: 0.6925 - model_65_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4320 - model_64_loss: 0.4897 - model_65_loss: 0.6923 - model_65_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4349 - model_64_loss: 0.4867 - model_65_loss: 0.6923 - model_65_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4347 - model_64_loss: 0.4868 - model_65_loss: 0.6923 - model_65_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4379 - model_64_loss: 0.4859 - model_65_loss: 0.6924 - model_65_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4391 - model_64_loss: 0.4851 - model_65_loss: 0.6926 - model_65_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9218 - model_65_loss: 0.6922 - model_65_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4319 - model_64_loss: 0.4835 - model_65_loss: 0.6913 - model_65_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4332 - model_64_loss: 0.4853 - model_65_loss: 0.6916 - model_65_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4309 - model_64_loss: 0.4836 - model_65_loss: 0.6912 - model_65_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4365 - model_64_loss: 0.4841 - model_65_loss: 0.6920 - model_65_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4329 - model_64_loss: 0.4817 - model_65_loss: 0.6913 - model_65_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9214 - model_65_loss: 0.6915 - model_65_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4295 - model_64_loss: 0.4806 - model_65_loss: 0.6908 - model_65_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4308 - model_64_loss: 0.4801 - model_65_loss: 0.6907 - model_65_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4360 - model_64_loss: 0.4781 - model_65_loss: 0.6913 - model_65_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4351 - model_64_loss: 0.4781 - model_65_loss: 0.6910 - model_65_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4355 - model_64_loss: 0.4776 - model_65_loss: 0.6913 - model_65_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9098 - model_65_loss: 0.6916 - model_65_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4257 - model_64_loss: 0.4775 - model_65_loss: 0.6901 - model_65_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4230 - model_64_loss: 0.4771 - model_65_loss: 0.6898 - model_65_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4298 - model_64_loss: 0.4759 - model_65_loss: 0.6907 - model_65_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4281 - model_64_loss: 0.4742 - model_65_loss: 0.6903 - model_65_1_loss: 0.6902\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4336 - model_64_loss: 0.4715 - model_65_loss: 0.6906 - model_65_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9045 - model_65_loss: 0.6903 - model_65_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4288 - model_64_loss: 0.4710 - model_65_loss: 0.6901 - model_65_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4270 - model_64_loss: 0.4716 - model_65_loss: 0.6903 - model_65_1_loss: 0.6894\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4324 - model_64_loss: 0.4696 - model_65_loss: 0.6906 - model_65_1_loss: 0.6898\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4309 - model_64_loss: 0.4700 - model_65_loss: 0.6904 - model_65_1_loss: 0.6898\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4366 - model_64_loss: 0.4692 - model_65_loss: 0.6906 - model_65_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9064 - model_65_loss: 0.6913 - model_65_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4344 - model_64_loss: 0.4689 - model_65_loss: 0.6907 - model_65_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4339 - model_64_loss: 0.4693 - model_65_loss: 0.6908 - model_65_1_loss: 0.6899\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4341 - model_64_loss: 0.4684 - model_65_loss: 0.6906 - model_65_1_loss: 0.6899\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4360 - model_64_loss: 0.4680 - model_65_loss: 0.6909 - model_65_1_loss: 0.6899\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4377 - model_64_loss: 0.4685 - model_65_loss: 0.6910 - model_65_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9081 - model_65_loss: 0.6911 - model_65_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4370 - model_64_loss: 0.4699 - model_65_loss: 0.6914 - model_65_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4385 - model_64_loss: 0.4702 - model_65_loss: 0.6919 - model_65_1_loss: 0.6899\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4405 - model_64_loss: 0.4699 - model_65_loss: 0.6920 - model_65_1_loss: 0.6901\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4422 - model_64_loss: 0.4665 - model_65_loss: 0.6917 - model_65_1_loss: 0.6900\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4411 - model_64_loss: 0.4694 - model_65_loss: 0.6918 - model_65_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9104 - model_65_loss: 0.6924 - model_65_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4448 - model_64_loss: 0.4680 - model_65_loss: 0.6914 - model_65_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4414 - model_64_loss: 0.4683 - model_65_loss: 0.6910 - model_65_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4431 - model_64_loss: 0.4681 - model_65_loss: 0.6912 - model_65_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4440 - model_64_loss: 0.4668 - model_65_loss: 0.6912 - model_65_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4450 - model_64_loss: 0.4677 - model_65_loss: 0.6909 - model_65_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9094 - model_65_loss: 0.6916 - model_65_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4405 - model_64_loss: 0.4690 - model_65_loss: 0.6918 - model_65_1_loss: 0.6901\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4390 - model_64_loss: 0.4694 - model_65_loss: 0.6914 - model_65_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4422 - model_64_loss: 0.4678 - model_65_loss: 0.6916 - model_65_1_loss: 0.6904\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4426 - model_64_loss: 0.4687 - model_65_loss: 0.6916 - model_65_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4427 - model_64_loss: 0.4694 - model_65_loss: 0.6914 - model_65_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9153 - model_65_loss: 0.6918 - model_65_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4460 - model_64_loss: 0.4694 - model_65_loss: 0.6915 - model_65_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4448 - model_64_loss: 0.4711 - model_65_loss: 0.6916 - model_65_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4468 - model_64_loss: 0.4715 - model_65_loss: 0.6919 - model_65_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4434 - model_64_loss: 0.4727 - model_65_loss: 0.6917 - model_65_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4506 - model_64_loss: 0.4709 - model_65_loss: 0.6921 - model_65_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9194 - model_65_loss: 0.6920 - model_65_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4484 - model_64_loss: 0.4723 - model_65_loss: 0.6922 - model_65_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4506 - model_64_loss: 0.4726 - model_65_loss: 0.6924 - model_65_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4542 - model_64_loss: 0.4717 - model_65_loss: 0.6927 - model_65_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4538 - model_64_loss: 0.4728 - model_65_loss: 0.6927 - model_65_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4504 - model_64_loss: 0.4755 - model_65_loss: 0.6923 - model_65_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9245 - model_65_loss: 0.6924 - model_65_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4381 - model_64_loss: 0.4773 - model_65_loss: 0.6924 - model_65_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4424 - model_64_loss: 0.4746 - model_65_loss: 0.6924 - model_65_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4388 - model_64_loss: 0.4763 - model_65_loss: 0.6923 - model_65_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4404 - model_64_loss: 0.4753 - model_65_loss: 0.6924 - model_65_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4449 - model_64_loss: 0.4733 - model_65_loss: 0.6925 - model_65_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9183 - model_65_loss: 0.6923 - model_65_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4425 - model_64_loss: 0.4732 - model_65_loss: 0.6922 - model_65_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4445 - model_64_loss: 0.4723 - model_65_loss: 0.6926 - model_65_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4479 - model_64_loss: 0.4711 - model_65_loss: 0.6924 - model_65_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4482 - model_64_loss: 0.4712 - model_65_loss: 0.6927 - model_65_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4503 - model_64_loss: 0.4701 - model_65_loss: 0.6926 - model_65_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9229 - model_65_loss: 0.6926 - model_65_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4492 - model_64_loss: 0.4696 - model_65_loss: 0.6921 - model_65_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4483 - model_64_loss: 0.4696 - model_65_loss: 0.6923 - model_65_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4488 - model_64_loss: 0.4675 - model_65_loss: 0.6919 - model_65_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4496 - model_64_loss: 0.4669 - model_65_loss: 0.6920 - model_65_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4497 - model_64_loss: 0.4682 - model_65_loss: 0.6920 - model_65_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9151 - model_65_loss: 0.6913 - model_65_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4507 - model_64_loss: 0.4670 - model_65_loss: 0.6923 - model_65_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4486 - model_64_loss: 0.4664 - model_65_loss: 0.6919 - model_65_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4496 - model_64_loss: 0.4662 - model_65_loss: 0.6922 - model_65_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4492 - model_64_loss: 0.4674 - model_65_loss: 0.6921 - model_65_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4502 - model_64_loss: 0.4664 - model_65_loss: 0.6921 - model_65_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9172 - model_65_loss: 0.6927 - model_65_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4502 - model_64_loss: 0.4668 - model_65_loss: 0.6920 - model_65_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4493 - model_64_loss: 0.4664 - model_65_loss: 0.6921 - model_65_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4477 - model_64_loss: 0.4689 - model_65_loss: 0.6920 - model_65_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4504 - model_64_loss: 0.4673 - model_65_loss: 0.6921 - model_65_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4505 - model_64_loss: 0.4676 - model_65_loss: 0.6924 - model_65_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9169 - model_65_loss: 0.6923 - model_65_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4456 - model_64_loss: 0.4687 - model_65_loss: 0.6920 - model_65_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4486 - model_64_loss: 0.4673 - model_65_loss: 0.6921 - model_65_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4492 - model_64_loss: 0.4688 - model_65_loss: 0.6922 - model_65_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4467 - model_64_loss: 0.4689 - model_65_loss: 0.6921 - model_65_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4514 - model_64_loss: 0.4661 - model_65_loss: 0.6922 - model_65_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9190 - model_65_loss: 0.6935 - model_65_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4465 - model_64_loss: 0.4673 - model_65_loss: 0.6918 - model_65_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4474 - model_64_loss: 0.4686 - model_65_loss: 0.6920 - model_65_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4490 - model_64_loss: 0.4680 - model_65_loss: 0.6922 - model_65_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4491 - model_64_loss: 0.4683 - model_65_loss: 0.6923 - model_65_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4523 - model_64_loss: 0.4674 - model_65_loss: 0.6924 - model_65_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9229 - model_65_loss: 0.6920 - model_65_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4520 - model_64_loss: 0.4662 - model_65_loss: 0.6922 - model_65_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4513 - model_64_loss: 0.4675 - model_65_loss: 0.6924 - model_65_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4524 - model_64_loss: 0.4670 - model_65_loss: 0.6926 - model_65_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4521 - model_64_loss: 0.4688 - model_65_loss: 0.6925 - model_65_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4529 - model_64_loss: 0.4682 - model_65_loss: 0.6927 - model_65_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9226 - model_65_loss: 0.6928 - model_65_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4514 - model_64_loss: 0.4680 - model_65_loss: 0.6926 - model_65_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4526 - model_64_loss: 0.4687 - model_65_loss: 0.6927 - model_65_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4537 - model_64_loss: 0.4674 - model_65_loss: 0.6926 - model_65_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4535 - model_64_loss: 0.4691 - model_65_loss: 0.6928 - model_65_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4534 - model_64_loss: 0.4669 - model_65_loss: 0.6925 - model_65_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9245 - model_65_loss: 0.6931 - model_65_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4483 - model_64_loss: 0.4702 - model_65_loss: 0.6924 - model_65_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4520 - model_64_loss: 0.4689 - model_65_loss: 0.6928 - model_65_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4488 - model_64_loss: 0.4698 - model_65_loss: 0.6925 - model_65_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4499 - model_64_loss: 0.4702 - model_65_loss: 0.6926 - model_65_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4496 - model_64_loss: 0.4701 - model_65_loss: 0.6925 - model_65_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9214 - model_65_loss: 0.6930 - model_65_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4512 - model_64_loss: 0.4685 - model_65_loss: 0.6925 - model_65_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4512 - model_64_loss: 0.4694 - model_65_loss: 0.6927 - model_65_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4526 - model_64_loss: 0.4685 - model_65_loss: 0.6925 - model_65_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4514 - model_64_loss: 0.4686 - model_65_loss: 0.6926 - model_65_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4536 - model_64_loss: 0.4673 - model_65_loss: 0.6926 - model_65_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9199 - model_65_loss: 0.6927 - model_65_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4493 - model_64_loss: 0.4688 - model_65_loss: 0.6922 - model_65_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4524 - model_64_loss: 0.4673 - model_65_loss: 0.6926 - model_65_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4502 - model_64_loss: 0.4700 - model_65_loss: 0.6925 - model_65_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4504 - model_64_loss: 0.4687 - model_65_loss: 0.6925 - model_65_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4503 - model_64_loss: 0.4709 - model_65_loss: 0.6926 - model_65_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9204 - model_65_loss: 0.6928 - model_65_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4502 - model_64_loss: 0.4685 - model_65_loss: 0.6924 - model_65_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4533 - model_64_loss: 0.4687 - model_65_loss: 0.6928 - model_65_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4529 - model_64_loss: 0.4682 - model_65_loss: 0.6925 - model_65_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4519 - model_64_loss: 0.4692 - model_65_loss: 0.6925 - model_65_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4568 - model_64_loss: 0.4674 - model_65_loss: 0.6927 - model_65_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9229 - model_65_loss: 0.6923 - model_65_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4553 - model_64_loss: 0.4701 - model_65_loss: 0.6927 - model_65_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4558 - model_64_loss: 0.4694 - model_65_loss: 0.6928 - model_65_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4558 - model_64_loss: 0.4695 - model_65_loss: 0.6927 - model_65_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4557 - model_64_loss: 0.4704 - model_65_loss: 0.6926 - model_65_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4560 - model_64_loss: 0.4704 - model_65_loss: 0.6930 - model_65_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9186 - model_65_loss: 0.6917 - model_65_1_loss: 0.692 - 0s 20us/sample - loss: 6.9240 - model_65_loss: 0.6922 - model_65_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4537 - model_64_loss: 0.4709 - model_65_loss: 0.6926 - model_65_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4546 - model_64_loss: 0.4703 - model_65_loss: 0.6928 - model_65_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4507 - model_64_loss: 0.4711 - model_65_loss: 0.6925 - model_65_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4531 - model_64_loss: 0.4703 - model_65_loss: 0.6926 - model_65_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4517 - model_64_loss: 0.4697 - model_65_loss: 0.6924 - model_65_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9236 - model_65_loss: 0.6927 - model_65_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4532 - model_64_loss: 0.4677 - model_65_loss: 0.6926 - model_65_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4519 - model_64_loss: 0.4679 - model_65_loss: 0.6924 - model_65_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4509 - model_64_loss: 0.4673 - model_65_loss: 0.6923 - model_65_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4505 - model_64_loss: 0.4681 - model_65_loss: 0.6923 - model_65_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4515 - model_64_loss: 0.4671 - model_65_loss: 0.6923 - model_65_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9204 - model_65_loss: 0.6929 - model_65_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4531 - model_64_loss: 0.4666 - model_65_loss: 0.6923 - model_65_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4521 - model_64_loss: 0.4671 - model_65_loss: 0.6925 - model_65_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4538 - model_64_loss: 0.4665 - model_65_loss: 0.6926 - model_65_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4563 - model_64_loss: 0.4634 - model_65_loss: 0.6924 - model_65_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4543 - model_64_loss: 0.4643 - model_65_loss: 0.6924 - model_65_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9218 - model_65_loss: 0.6932 - model_65_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4542 - model_64_loss: 0.4654 - model_65_loss: 0.6924 - model_65_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4546 - model_64_loss: 0.4655 - model_65_loss: 0.6925 - model_65_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4514 - model_64_loss: 0.4673 - model_65_loss: 0.6923 - model_65_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4538 - model_64_loss: 0.4647 - model_65_loss: 0.6923 - model_65_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4535 - model_64_loss: 0.4650 - model_65_loss: 0.6924 - model_65_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9198 - model_65_loss: 0.6929 - model_65_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4560 - model_64_loss: 0.4642 - model_65_loss: 0.6927 - model_65_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 6us/sample - loss: -6.4537 - model_64_loss: 0.4650 - model_65_loss: 0.6924 - model_65_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4570 - model_64_loss: 0.4627 - model_65_loss: 0.6924 - model_65_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4552 - model_64_loss: 0.4634 - model_65_loss: 0.6924 - model_65_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4535 - model_64_loss: 0.4651 - model_65_loss: 0.6922 - model_65_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9204 - model_65_loss: 0.6929 - model_65_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4552 - model_64_loss: 0.4631 - model_65_loss: 0.6923 - model_65_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4552 - model_64_loss: 0.4649 - model_65_loss: 0.6927 - model_65_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4574 - model_64_loss: 0.4643 - model_65_loss: 0.6926 - model_65_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4583 - model_64_loss: 0.4628 - model_65_loss: 0.6925 - model_65_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4584 - model_64_loss: 0.4634 - model_65_loss: 0.6927 - model_65_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9219 - model_65_loss: 0.6929 - model_65_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4547 - model_64_loss: 0.4642 - model_65_loss: 0.6923 - model_65_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4577 - model_64_loss: 0.4621 - model_65_loss: 0.6924 - model_65_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4531 - model_64_loss: 0.4645 - model_65_loss: 0.6921 - model_65_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4575 - model_64_loss: 0.4617 - model_65_loss: 0.6922 - model_65_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4544 - model_64_loss: 0.4637 - model_65_loss: 0.6921 - model_65_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9183 - model_65_loss: 0.6927 - model_65_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4525 - model_64_loss: 0.4641 - model_65_loss: 0.6922 - model_65_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4554 - model_64_loss: 0.4629 - model_65_loss: 0.6925 - model_65_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4539 - model_64_loss: 0.4633 - model_65_loss: 0.6923 - model_65_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4536 - model_64_loss: 0.4638 - model_65_loss: 0.6921 - model_65_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4567 - model_64_loss: 0.4642 - model_65_loss: 0.6929 - model_65_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9168 - model_65_loss: 0.6918 - model_65_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4527 - model_64_loss: 0.4631 - model_65_loss: 0.6921 - model_65_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4555 - model_64_loss: 0.4634 - model_65_loss: 0.6924 - model_65_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4566 - model_64_loss: 0.4636 - model_65_loss: 0.6925 - model_65_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4535 - model_64_loss: 0.4656 - model_65_loss: 0.6925 - model_65_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4552 - model_64_loss: 0.4645 - model_65_loss: 0.6925 - model_65_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9239 - model_65_loss: 0.6925 - model_65_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4541 - model_64_loss: 0.4661 - model_65_loss: 0.6922 - model_65_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4555 - model_64_loss: 0.4670 - model_65_loss: 0.6924 - model_65_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4564 - model_64_loss: 0.4672 - model_65_loss: 0.6926 - model_65_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4574 - model_64_loss: 0.4687 - model_65_loss: 0.6930 - model_65_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4592 - model_64_loss: 0.4666 - model_65_loss: 0.6929 - model_65_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9258 - model_65_loss: 0.6920 - model_65_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4553 - model_64_loss: 0.4693 - model_65_loss: 0.6926 - model_65_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4567 - model_64_loss: 0.4679 - model_65_loss: 0.6927 - model_65_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4595 - model_64_loss: 0.4688 - model_65_loss: 0.6933 - model_65_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4568 - model_64_loss: 0.4712 - model_65_loss: 0.6930 - model_65_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4570 - model_64_loss: 0.4689 - model_65_loss: 0.6928 - model_65_1_loss: 0.6924\n",
      "For Attention Module: 1.8000000000000003\n",
      "features X: 30940 samples, 71 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.3431 - model_69_loss: 0.6594 - model_69_1_loss: 0.6090\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -5.9701 - model_68_loss: 0.3772 - model_69_loss: 0.6598 - model_69_1_loss: 0.6096\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -5.9809 - model_68_loss: 0.3755 - model_69_loss: 0.6611 - model_69_1_loss: 0.6101\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -5.9916 - model_68_loss: 0.3740 - model_69_loss: 0.6613 - model_69_1_loss: 0.6118\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0021 - model_68_loss: 0.3765 - model_69_loss: 0.6615 - model_69_1_loss: 0.6142\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0114 - model_68_loss: 0.3760 - model_69_loss: 0.6624 - model_69_1_loss: 0.6151\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.4004 - model_69_loss: 0.6624 - model_69_1_loss: 0.6174\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0289 - model_68_loss: 0.3769 - model_69_loss: 0.6633 - model_69_1_loss: 0.6179\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0286 - model_68_loss: 0.3789 - model_69_loss: 0.6633 - model_69_1_loss: 0.6182\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0433 - model_68_loss: 0.3778 - model_69_loss: 0.6634 - model_69_1_loss: 0.6208\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0538 - model_68_loss: 0.3794 - model_69_loss: 0.6642 - model_69_1_loss: 0.6225\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0671 - model_68_loss: 0.3784 - model_69_loss: 0.6649 - model_69_1_loss: 0.6242\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.4550 - model_69_loss: 0.6644 - model_69_1_loss: 0.6262\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0720 - model_68_loss: 0.3814 - model_69_loss: 0.6650 - model_69_1_loss: 0.6257\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0934 - model_68_loss: 0.3811 - model_69_loss: 0.6656 - model_69_1_loss: 0.6293\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0959 - model_68_loss: 0.3813 - model_69_loss: 0.6662 - model_69_1_loss: 0.6293\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1106 - model_68_loss: 0.3834 - model_69_loss: 0.6666 - model_69_1_loss: 0.6322\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1279 - model_68_loss: 0.3839 - model_69_loss: 0.6674 - model_69_1_loss: 0.6350\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.5214 - model_69_loss: 0.6690 - model_69_1_loss: 0.6364\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1384 - model_68_loss: 0.3843 - model_69_loss: 0.6686 - model_69_1_loss: 0.6359\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1463 - model_68_loss: 0.3877 - model_69_loss: 0.6696 - model_69_1_loss: 0.6372\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1623 - model_68_loss: 0.3898 - model_69_loss: 0.6705 - model_69_1_loss: 0.6400\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1747 - model_68_loss: 0.3898 - model_69_loss: 0.6708 - model_69_1_loss: 0.6420\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.1859 - model_68_loss: 0.3904 - model_69_loss: 0.6720 - model_69_1_loss: 0.6433\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.5936 - model_69_loss: 0.6731 - model_69_1_loss: 0.6458\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1938 - model_68_loss: 0.3940 - model_69_loss: 0.6722 - model_69_1_loss: 0.6453\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2041 - model_68_loss: 0.3954 - model_69_loss: 0.6727 - model_69_1_loss: 0.6473\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2152 - model_68_loss: 0.3980 - model_69_loss: 0.6733 - model_69_1_loss: 0.6494\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2347 - model_68_loss: 0.3979 - model_69_loss: 0.6749 - model_69_1_loss: 0.6516\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2517 - model_68_loss: 0.4014 - model_69_loss: 0.6761 - model_69_1_loss: 0.6545\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.6635 - model_69_loss: 0.6766 - model_69_1_loss: 0.65620s - loss: 6.6557 - model_69_loss: 0.6747 - model_69_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2534 - model_68_loss: 0.4036 - model_69_loss: 0.6764 - model_69_1_loss: 0.6550\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2676 - model_68_loss: 0.4049 - model_69_loss: 0.6772 - model_69_1_loss: 0.6573\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2758 - model_68_loss: 0.4074 - model_69_loss: 0.6772 - model_69_1_loss: 0.6594\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2875 - model_68_loss: 0.4103 - model_69_loss: 0.6785 - model_69_1_loss: 0.6611\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.2878 - model_68_loss: 0.4110 - model_69_loss: 0.6773 - model_69_1_loss: 0.6625\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.7181 - model_69_loss: 0.6798 - model_69_1_loss: 0.6644\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2989 - model_68_loss: 0.4142 - model_69_loss: 0.6789 - model_69_1_loss: 0.6637\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3120 - model_68_loss: 0.4156 - model_69_loss: 0.6802 - model_69_1_loss: 0.6654\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3192 - model_68_loss: 0.4166 - model_69_loss: 0.6804 - model_69_1_loss: 0.6668\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3363 - model_68_loss: 0.4191 - model_69_loss: 0.6816 - model_69_1_loss: 0.6695\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3414 - model_68_loss: 0.4242 - model_69_loss: 0.6823 - model_69_1_loss: 0.6708\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.7646 - model_69_loss: 0.6812 - model_69_1_loss: 0.6712\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3307 - model_68_loss: 0.4252 - model_69_loss: 0.6816 - model_69_1_loss: 0.6696\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3449 - model_68_loss: 0.4289 - model_69_loss: 0.6827 - model_69_1_loss: 0.6721\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3586 - model_68_loss: 0.4273 - model_69_loss: 0.6830 - model_69_1_loss: 0.6742\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3662 - model_68_loss: 0.4319 - model_69_loss: 0.6840 - model_69_1_loss: 0.6757\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3660 - model_68_loss: 0.4352 - model_69_loss: 0.6838 - model_69_1_loss: 0.6764\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.8093 - model_69_loss: 0.6846 - model_69_1_loss: 0.6776\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3747 - model_68_loss: 0.4349 - model_69_loss: 0.6852 - model_69_1_loss: 0.6768\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3745 - model_68_loss: 0.4386 - model_69_loss: 0.6852 - model_69_1_loss: 0.6774\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3782 - model_68_loss: 0.4423 - model_69_loss: 0.6860 - model_69_1_loss: 0.6781\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3821 - model_68_loss: 0.4443 - model_69_loss: 0.6862 - model_69_1_loss: 0.6791\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3918 - model_68_loss: 0.4463 - model_69_loss: 0.6869 - model_69_1_loss: 0.6808\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.8456 - model_69_loss: 0.6869 - model_69_1_loss: 0.6815\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3975 - model_68_loss: 0.4490 - model_69_loss: 0.6878 - model_69_1_loss: 0.6815\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.3991 - model_68_loss: 0.4513 - model_69_loss: 0.6879 - model_69_1_loss: 0.6822\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4021 - model_68_loss: 0.4535 - model_69_loss: 0.6882 - model_69_1_loss: 0.6829\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4067 - model_68_loss: 0.4549 - model_69_loss: 0.6883 - model_69_1_loss: 0.6840\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4144 - model_68_loss: 0.4570 - model_69_loss: 0.6894 - model_69_1_loss: 0.6848\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.8794 - model_69_loss: 0.6895 - model_69_1_loss: 0.6857\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4128 - model_68_loss: 0.4610 - model_69_loss: 0.6895 - model_69_1_loss: 0.6853\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4199 - model_68_loss: 0.4617 - model_69_loss: 0.6905 - model_69_1_loss: 0.6858\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4186 - model_68_loss: 0.4632 - model_69_loss: 0.6896 - model_69_1_loss: 0.6868\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4177 - model_68_loss: 0.4666 - model_69_loss: 0.6902 - model_69_1_loss: 0.6866\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4217 - model_68_loss: 0.4690 - model_69_loss: 0.6904 - model_69_1_loss: 0.6878\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.8970 - model_69_loss: 0.6909 - model_69_1_loss: 0.6887\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4256 - model_68_loss: 0.4712 - model_69_loss: 0.6914 - model_69_1_loss: 0.6879\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4220 - model_68_loss: 0.4746 - model_69_loss: 0.6911 - model_69_1_loss: 0.6882\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4285 - model_68_loss: 0.4763 - model_69_loss: 0.6920 - model_69_1_loss: 0.6890\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4306 - model_68_loss: 0.4775 - model_69_loss: 0.6917 - model_69_1_loss: 0.6900\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4311 - model_68_loss: 0.4793 - model_69_loss: 0.6922 - model_69_1_loss: 0.6898\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9104 - model_69_loss: 0.6919 - model_69_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4270 - model_68_loss: 0.4805 - model_69_loss: 0.6913 - model_69_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4333 - model_68_loss: 0.4811 - model_69_loss: 0.6923 - model_69_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4301 - model_68_loss: 0.4844 - model_69_loss: 0.6921 - model_69_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4311 - model_68_loss: 0.4869 - model_69_loss: 0.6925 - model_69_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4304 - model_68_loss: 0.4852 - model_69_loss: 0.6921 - model_69_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9210 - model_69_loss: 0.6926 - model_69_1_loss: 0.69150s - loss: 6.9192 - model_69_loss: 0.6924 - model_69_1_loss: 0.691\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4305 - model_68_loss: 0.4886 - model_69_loss: 0.6924 - model_69_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4337 - model_68_loss: 0.4891 - model_69_loss: 0.6927 - model_69_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4336 - model_68_loss: 0.4883 - model_69_loss: 0.6926 - model_69_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4332 - model_68_loss: 0.4892 - model_69_loss: 0.6925 - model_69_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4352 - model_68_loss: 0.4880 - model_69_loss: 0.6925 - model_69_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9243 - model_69_loss: 0.6926 - model_69_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4360 - model_68_loss: 0.4874 - model_69_loss: 0.6925 - model_69_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4339 - model_68_loss: 0.4889 - model_69_loss: 0.6924 - model_69_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4337 - model_68_loss: 0.4885 - model_69_loss: 0.6924 - model_69_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4328 - model_68_loss: 0.4895 - model_69_loss: 0.6923 - model_69_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4362 - model_68_loss: 0.4884 - model_69_loss: 0.6925 - model_69_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9246 - model_69_loss: 0.6927 - model_69_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4335 - model_68_loss: 0.4876 - model_69_loss: 0.6921 - model_69_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4352 - model_68_loss: 0.4857 - model_69_loss: 0.6917 - model_69_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4381 - model_68_loss: 0.4846 - model_69_loss: 0.6922 - model_69_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4351 - model_68_loss: 0.4865 - model_69_loss: 0.6918 - model_69_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4386 - model_68_loss: 0.4839 - model_69_loss: 0.6922 - model_69_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9218 - model_69_loss: 0.6924 - model_69_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4350 - model_68_loss: 0.4838 - model_69_loss: 0.6915 - model_69_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4368 - model_68_loss: 0.4817 - model_69_loss: 0.6916 - model_69_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4381 - model_68_loss: 0.4820 - model_69_loss: 0.6917 - model_69_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4393 - model_68_loss: 0.4789 - model_69_loss: 0.6917 - model_69_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4413 - model_68_loss: 0.4783 - model_69_loss: 0.6918 - model_69_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9176 - model_69_loss: 0.6928 - model_69_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4364 - model_68_loss: 0.4786 - model_69_loss: 0.6911 - model_69_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4398 - model_68_loss: 0.4766 - model_69_loss: 0.6911 - model_69_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4416 - model_68_loss: 0.4754 - model_69_loss: 0.6917 - model_69_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4388 - model_68_loss: 0.4748 - model_69_loss: 0.6912 - model_69_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4423 - model_68_loss: 0.4744 - model_69_loss: 0.6915 - model_69_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9113 - model_69_loss: 0.6908 - model_69_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4373 - model_68_loss: 0.4738 - model_69_loss: 0.6914 - model_69_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4415 - model_68_loss: 0.4701 - model_69_loss: 0.6914 - model_69_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4395 - model_68_loss: 0.4711 - model_69_loss: 0.6915 - model_69_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4422 - model_68_loss: 0.4694 - model_69_loss: 0.6914 - model_69_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4422 - model_68_loss: 0.4692 - model_69_loss: 0.6915 - model_69_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9157 - model_69_loss: 0.6913 - model_69_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4393 - model_68_loss: 0.4694 - model_69_loss: 0.6911 - model_69_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4387 - model_68_loss: 0.4694 - model_69_loss: 0.6909 - model_69_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4410 - model_68_loss: 0.4702 - model_69_loss: 0.6914 - model_69_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4422 - model_68_loss: 0.4683 - model_69_loss: 0.6913 - model_69_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4440 - model_68_loss: 0.4693 - model_69_loss: 0.6918 - model_69_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9141 - model_69_loss: 0.6918 - model_69_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4423 - model_68_loss: 0.4693 - model_69_loss: 0.6917 - model_69_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4447 - model_68_loss: 0.4661 - model_69_loss: 0.6917 - model_69_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4431 - model_68_loss: 0.4678 - model_69_loss: 0.6913 - model_69_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4436 - model_68_loss: 0.4679 - model_69_loss: 0.6916 - model_69_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4433 - model_68_loss: 0.4688 - model_69_loss: 0.6918 - model_69_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9167 - model_69_loss: 0.6923 - model_69_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4440 - model_68_loss: 0.4692 - model_69_loss: 0.6920 - model_69_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4452 - model_68_loss: 0.4672 - model_69_loss: 0.6914 - model_69_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4469 - model_68_loss: 0.4684 - model_69_loss: 0.6915 - model_69_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4441 - model_68_loss: 0.4686 - model_69_loss: 0.6916 - model_69_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4451 - model_68_loss: 0.4694 - model_69_loss: 0.6918 - model_69_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: 6.9164 - model_69_loss: 0.6922 - model_69_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4439 - model_68_loss: 0.4717 - model_69_loss: 0.6921 - model_69_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4474 - model_68_loss: 0.4692 - model_69_loss: 0.6920 - model_69_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4450 - model_68_loss: 0.4720 - model_69_loss: 0.6920 - model_69_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4468 - model_68_loss: 0.4727 - model_69_loss: 0.6923 - model_69_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4483 - model_68_loss: 0.4718 - model_69_loss: 0.6922 - model_69_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9231 - model_69_loss: 0.6929 - model_69_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4450 - model_68_loss: 0.4737 - model_69_loss: 0.6920 - model_69_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4460 - model_68_loss: 0.4741 - model_69_loss: 0.6923 - model_69_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4460 - model_68_loss: 0.4749 - model_69_loss: 0.6920 - model_69_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4487 - model_68_loss: 0.4724 - model_69_loss: 0.6922 - model_69_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4489 - model_68_loss: 0.4752 - model_69_loss: 0.6925 - model_69_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9246 - model_69_loss: 0.6924 - model_69_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4482 - model_68_loss: 0.4749 - model_69_loss: 0.6925 - model_69_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4479 - model_68_loss: 0.4738 - model_69_loss: 0.6923 - model_69_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4471 - model_68_loss: 0.4753 - model_69_loss: 0.6922 - model_69_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4458 - model_68_loss: 0.4757 - model_69_loss: 0.6923 - model_69_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4489 - model_68_loss: 0.4734 - model_69_loss: 0.6925 - model_69_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9252 - model_69_loss: 0.6924 - model_69_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4495 - model_68_loss: 0.4735 - model_69_loss: 0.6924 - model_69_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4495 - model_68_loss: 0.4710 - model_69_loss: 0.6922 - model_69_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4493 - model_68_loss: 0.4720 - model_69_loss: 0.6923 - model_69_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4491 - model_68_loss: 0.4720 - model_69_loss: 0.6924 - model_69_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4503 - model_68_loss: 0.4713 - model_69_loss: 0.6924 - model_69_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9221 - model_69_loss: 0.6921 - model_69_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4478 - model_68_loss: 0.4700 - model_69_loss: 0.6922 - model_69_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4518 - model_68_loss: 0.4681 - model_69_loss: 0.6922 - model_69_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4514 - model_68_loss: 0.4689 - model_69_loss: 0.6922 - model_69_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4490 - model_68_loss: 0.4685 - model_69_loss: 0.6920 - model_69_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4511 - model_68_loss: 0.4680 - model_69_loss: 0.6925 - model_69_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9192 - model_69_loss: 0.6914 - model_69_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4490 - model_68_loss: 0.4670 - model_69_loss: 0.6922 - model_69_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4494 - model_68_loss: 0.4686 - model_69_loss: 0.6924 - model_69_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4493 - model_68_loss: 0.4670 - model_69_loss: 0.6921 - model_69_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4480 - model_68_loss: 0.4667 - model_69_loss: 0.6920 - model_69_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4497 - model_68_loss: 0.4671 - model_69_loss: 0.6921 - model_69_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9169 - model_69_loss: 0.6929 - model_69_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4511 - model_68_loss: 0.4664 - model_69_loss: 0.6923 - model_69_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4514 - model_68_loss: 0.4664 - model_69_loss: 0.6922 - model_69_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4518 - model_68_loss: 0.4667 - model_69_loss: 0.6926 - model_69_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4492 - model_68_loss: 0.4678 - model_69_loss: 0.6922 - model_69_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4512 - model_68_loss: 0.4666 - model_69_loss: 0.6922 - model_69_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9220 - model_69_loss: 0.6926 - model_69_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4517 - model_68_loss: 0.4679 - model_69_loss: 0.6925 - model_69_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4512 - model_68_loss: 0.4682 - model_69_loss: 0.6924 - model_69_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4500 - model_68_loss: 0.4695 - model_69_loss: 0.6923 - model_69_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4519 - model_68_loss: 0.4681 - model_69_loss: 0.6923 - model_69_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4488 - model_68_loss: 0.4697 - model_69_loss: 0.6921 - model_69_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9223 - model_69_loss: 0.6933 - model_69_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4511 - model_68_loss: 0.4696 - model_69_loss: 0.6923 - model_69_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4534 - model_68_loss: 0.4679 - model_69_loss: 0.6923 - model_69_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4498 - model_68_loss: 0.4698 - model_69_loss: 0.6922 - model_69_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4531 - model_68_loss: 0.4683 - model_69_loss: 0.6924 - model_69_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4510 - model_68_loss: 0.4693 - model_69_loss: 0.6922 - model_69_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.9202 - model_69_loss: 0.6922 - model_69_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4510 - model_68_loss: 0.4695 - model_69_loss: 0.6923 - model_69_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4496 - model_68_loss: 0.4690 - model_69_loss: 0.6922 - model_69_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4530 - model_68_loss: 0.4692 - model_69_loss: 0.6925 - model_69_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4526 - model_68_loss: 0.4692 - model_69_loss: 0.6925 - model_69_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4529 - model_68_loss: 0.4694 - model_69_loss: 0.6928 - model_69_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9231 - model_69_loss: 0.6937 - model_69_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4557 - model_68_loss: 0.4666 - model_69_loss: 0.6924 - model_69_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4509 - model_68_loss: 0.4693 - model_69_loss: 0.6922 - model_69_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4516 - model_68_loss: 0.4703 - model_69_loss: 0.6926 - model_69_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4526 - model_68_loss: 0.4675 - model_69_loss: 0.6922 - model_69_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4502 - model_68_loss: 0.4689 - model_69_loss: 0.6923 - model_69_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9238 - model_69_loss: 0.6932 - model_69_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4532 - model_68_loss: 0.4683 - model_69_loss: 0.6924 - model_69_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4523 - model_68_loss: 0.4674 - model_69_loss: 0.6923 - model_69_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4534 - model_68_loss: 0.4685 - model_69_loss: 0.6924 - model_69_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4558 - model_68_loss: 0.4657 - model_69_loss: 0.6926 - model_69_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4563 - model_68_loss: 0.4675 - model_69_loss: 0.6928 - model_69_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9233 - model_69_loss: 0.6926 - model_69_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4521 - model_68_loss: 0.4678 - model_69_loss: 0.6922 - model_69_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4535 - model_68_loss: 0.4686 - model_69_loss: 0.6925 - model_69_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4542 - model_68_loss: 0.4670 - model_69_loss: 0.6923 - model_69_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4541 - model_68_loss: 0.4671 - model_69_loss: 0.6925 - model_69_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4547 - model_68_loss: 0.4664 - model_69_loss: 0.6923 - model_69_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9221 - model_69_loss: 0.6927 - model_69_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4551 - model_68_loss: 0.4670 - model_69_loss: 0.6926 - model_69_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4515 - model_68_loss: 0.4673 - model_69_loss: 0.6921 - model_69_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4520 - model_68_loss: 0.4671 - model_69_loss: 0.6921 - model_69_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4532 - model_68_loss: 0.4665 - model_69_loss: 0.6922 - model_69_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4526 - model_68_loss: 0.4685 - model_69_loss: 0.6925 - model_69_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9213 - model_69_loss: 0.6925 - model_69_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4547 - model_68_loss: 0.4664 - model_69_loss: 0.6925 - model_69_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4528 - model_68_loss: 0.4676 - model_69_loss: 0.6924 - model_69_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4519 - model_68_loss: 0.4677 - model_69_loss: 0.6923 - model_69_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4538 - model_68_loss: 0.4651 - model_69_loss: 0.6922 - model_69_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4527 - model_68_loss: 0.4656 - model_69_loss: 0.6921 - model_69_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9195 - model_69_loss: 0.6924 - model_69_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4542 - model_68_loss: 0.4657 - model_69_loss: 0.6923 - model_69_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4529 - model_68_loss: 0.4673 - model_69_loss: 0.6924 - model_69_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4562 - model_68_loss: 0.4639 - model_69_loss: 0.6925 - model_69_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4550 - model_68_loss: 0.4638 - model_69_loss: 0.6921 - model_69_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4543 - model_68_loss: 0.4651 - model_69_loss: 0.6923 - model_69_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9204 - model_69_loss: 0.6918 - model_69_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4550 - model_68_loss: 0.4638 - model_69_loss: 0.6922 - model_69_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4537 - model_68_loss: 0.4635 - model_69_loss: 0.6920 - model_69_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4565 - model_68_loss: 0.4636 - model_69_loss: 0.6927 - model_69_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4529 - model_68_loss: 0.4633 - model_69_loss: 0.6920 - model_69_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4537 - model_68_loss: 0.4624 - model_69_loss: 0.6922 - model_69_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9206 - model_69_loss: 0.6934 - model_69_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4526 - model_68_loss: 0.4630 - model_69_loss: 0.6918 - model_69_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4541 - model_68_loss: 0.4629 - model_69_loss: 0.6922 - model_69_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4539 - model_68_loss: 0.4632 - model_69_loss: 0.6922 - model_69_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4552 - model_68_loss: 0.4625 - model_69_loss: 0.6923 - model_69_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4542 - model_68_loss: 0.4627 - model_69_loss: 0.6922 - model_69_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9170 - model_69_loss: 0.6929 - model_69_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4539 - model_68_loss: 0.4637 - model_69_loss: 0.6921 - model_69_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4533 - model_68_loss: 0.4629 - model_69_loss: 0.6920 - model_69_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4548 - model_68_loss: 0.4623 - model_69_loss: 0.6921 - model_69_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4543 - model_68_loss: 0.4626 - model_69_loss: 0.6921 - model_69_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4585 - model_68_loss: 0.4619 - model_69_loss: 0.6926 - model_69_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9206 - model_69_loss: 0.6925 - model_69_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4558 - model_68_loss: 0.4634 - model_69_loss: 0.6926 - model_69_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4582 - model_68_loss: 0.4629 - model_69_loss: 0.6926 - model_69_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4559 - model_68_loss: 0.4639 - model_69_loss: 0.6924 - model_69_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4589 - model_68_loss: 0.4639 - model_69_loss: 0.6928 - model_69_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4564 - model_68_loss: 0.4647 - model_69_loss: 0.6926 - model_69_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9226 - model_69_loss: 0.6924 - model_69_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4564 - model_68_loss: 0.4640 - model_69_loss: 0.6925 - model_69_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4536 - model_68_loss: 0.4667 - model_69_loss: 0.6925 - model_69_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4534 - model_68_loss: 0.4650 - model_69_loss: 0.6922 - model_69_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4540 - model_68_loss: 0.4660 - model_69_loss: 0.6923 - model_69_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4533 - model_68_loss: 0.4646 - model_69_loss: 0.6922 - model_69_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9240 - model_69_loss: 0.6939 - model_69_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4586 - model_68_loss: 0.4641 - model_69_loss: 0.6925 - model_69_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4583 - model_68_loss: 0.4641 - model_69_loss: 0.6925 - model_69_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4556 - model_68_loss: 0.4657 - model_69_loss: 0.6923 - model_69_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4594 - model_68_loss: 0.4636 - model_69_loss: 0.6925 - model_69_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4614 - model_68_loss: 0.4626 - model_69_loss: 0.6927 - model_69_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9233 - model_69_loss: 0.6924 - model_69_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4567 - model_68_loss: 0.4645 - model_69_loss: 0.6926 - model_69_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4564 - model_68_loss: 0.4639 - model_69_loss: 0.6921 - model_69_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4555 - model_68_loss: 0.4667 - model_69_loss: 0.6927 - model_69_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4574 - model_68_loss: 0.4658 - model_69_loss: 0.6929 - model_69_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4544 - model_68_loss: 0.4661 - model_69_loss: 0.6924 - model_69_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9236 - model_69_loss: 0.6922 - model_69_1_loss: 0.69190s - loss: 6.9343 - model_69_loss: 0.6949 - model_69_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4590 - model_68_loss: 0.4656 - model_69_loss: 0.6929 - model_69_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4589 - model_68_loss: 0.4646 - model_69_loss: 0.6926 - model_69_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4586 - model_68_loss: 0.4659 - model_69_loss: 0.6927 - model_69_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4553 - model_68_loss: 0.4664 - model_69_loss: 0.6927 - model_69_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.4577 - model_68_loss: 0.4655 - model_69_loss: 0.6926 - model_69_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9261 - model_69_loss: 0.6928 - model_69_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4562 - model_68_loss: 0.4669 - model_69_loss: 0.6926 - model_69_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4575 - model_68_loss: 0.4664 - model_69_loss: 0.6927 - model_69_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4552 - model_68_loss: 0.4672 - model_69_loss: 0.6926 - model_69_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4542 - model_68_loss: 0.4683 - model_69_loss: 0.6926 - model_69_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4561 - model_68_loss: 0.4666 - model_69_loss: 0.6925 - model_69_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9242 - model_69_loss: 0.6926 - model_69_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4607 - model_68_loss: 0.4651 - model_69_loss: 0.6931 - model_69_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4574 - model_68_loss: 0.4653 - model_69_loss: 0.6926 - model_69_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4582 - model_68_loss: 0.4645 - model_69_loss: 0.6926 - model_69_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4587 - model_68_loss: 0.4645 - model_69_loss: 0.6925 - model_69_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4561 - model_68_loss: 0.4659 - model_69_loss: 0.6927 - model_69_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9228 - model_69_loss: 0.6925 - model_69_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4551 - model_68_loss: 0.4639 - model_69_loss: 0.6923 - model_69_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4550 - model_68_loss: 0.4652 - model_69_loss: 0.6924 - model_69_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4532 - model_68_loss: 0.4663 - model_69_loss: 0.6923 - model_69_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4568 - model_68_loss: 0.4632 - model_69_loss: 0.6925 - model_69_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4571 - model_68_loss: 0.4620 - model_69_loss: 0.6922 - model_69_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9209 - model_69_loss: 0.6920 - model_69_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4606 - model_68_loss: 0.4609 - model_69_loss: 0.6925 - model_69_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4560 - model_68_loss: 0.4621 - model_69_loss: 0.6922 - model_69_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4560 - model_68_loss: 0.4611 - model_69_loss: 0.6924 - model_69_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4579 - model_68_loss: 0.4596 - model_69_loss: 0.6923 - model_69_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4581 - model_68_loss: 0.4601 - model_69_loss: 0.6925 - model_69_1_loss: 0.6912\n",
      "For Attention Module: 1.9000000000000001\n",
      "features X: 30940 samples, 76 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.3774 - model_73_loss: 0.6601 - model_73_1_loss: 0.6153\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.0040 - model_72_loss: 0.3710 - model_73_loss: 0.6598 - model_73_1_loss: 0.6152\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0202 - model_72_loss: 0.3713 - model_73_loss: 0.6606 - model_73_1_loss: 0.6177\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0375 - model_72_loss: 0.3697 - model_73_loss: 0.6612 - model_73_1_loss: 0.6202\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0489 - model_72_loss: 0.3708 - model_73_loss: 0.6612 - model_73_1_loss: 0.6227\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0625 - model_72_loss: 0.3711 - model_73_loss: 0.6612 - model_73_1_loss: 0.6255\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.4497 - model_73_loss: 0.6627 - model_73_1_loss: 0.6275\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.0662 - model_72_loss: 0.3706 - model_73_loss: 0.6617 - model_73_1_loss: 0.6257\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0905 - model_72_loss: 0.3700 - model_73_loss: 0.6627 - model_73_1_loss: 0.6294\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 7us/sample - loss: -6.0966 - model_72_loss: 0.3712 - model_73_loss: 0.6635 - model_73_1_loss: 0.6300\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1061 - model_72_loss: 0.3742 - model_73_loss: 0.6635 - model_73_1_loss: 0.6326\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1288 - model_72_loss: 0.3725 - model_73_loss: 0.6649 - model_73_1_loss: 0.6354\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: 6.5101 - model_73_loss: 0.6648 - model_73_1_loss: 0.6365\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1388 - model_72_loss: 0.3710 - model_73_loss: 0.6647 - model_73_1_loss: 0.6372\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1553 - model_72_loss: 0.3734 - model_73_loss: 0.6658 - model_73_1_loss: 0.6399\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1645 - model_72_loss: 0.3755 - model_73_loss: 0.6660 - model_73_1_loss: 0.6420\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1752 - model_72_loss: 0.3749 - model_73_loss: 0.6662 - model_73_1_loss: 0.6438\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1949 - model_72_loss: 0.3763 - model_73_loss: 0.6676 - model_73_1_loss: 0.6466\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.5934 - model_73_loss: 0.6681 - model_73_1_loss: 0.6503\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.1979 - model_72_loss: 0.3769 - model_73_loss: 0.6670 - model_73_1_loss: 0.6479\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2168 - model_72_loss: 0.3787 - model_73_loss: 0.6688 - model_73_1_loss: 0.6502\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2267 - model_72_loss: 0.3815 - model_73_loss: 0.6689 - model_73_1_loss: 0.6528\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2495 - model_72_loss: 0.3819 - model_73_loss: 0.6712 - model_73_1_loss: 0.6551\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2701 - model_72_loss: 0.3821 - model_73_loss: 0.6719 - model_73_1_loss: 0.6585\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.6590 - model_73_loss: 0.6723 - model_73_1_loss: 0.6599\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2739 - model_72_loss: 0.3855 - model_73_loss: 0.6729 - model_73_1_loss: 0.6590\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3018 - model_72_loss: 0.3857 - model_73_loss: 0.6750 - model_73_1_loss: 0.6625\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2995 - model_72_loss: 0.3883 - model_73_loss: 0.6746 - model_73_1_loss: 0.6629\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3186 - model_72_loss: 0.3921 - model_73_loss: 0.6760 - model_73_1_loss: 0.6662\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3411 - model_72_loss: 0.3928 - model_73_loss: 0.6773 - model_73_1_loss: 0.6695\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.7263 - model_73_loss: 0.6759 - model_73_1_loss: 0.6696\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3325 - model_72_loss: 0.3940 - model_73_loss: 0.6760 - model_73_1_loss: 0.6693\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3478 - model_72_loss: 0.3955 - model_73_loss: 0.6782 - model_73_1_loss: 0.6704\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3578 - model_72_loss: 0.3984 - model_73_loss: 0.6792 - model_73_1_loss: 0.6720\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3601 - model_72_loss: 0.3997 - model_73_loss: 0.6786 - model_73_1_loss: 0.6734\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3843 - model_72_loss: 0.4022 - model_73_loss: 0.6814 - model_73_1_loss: 0.6759\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.7857 - model_73_loss: 0.6801 - model_73_1_loss: 0.6765\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3773 - model_72_loss: 0.4024 - model_73_loss: 0.6805 - model_73_1_loss: 0.6754\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3849 - model_72_loss: 0.4079 - model_73_loss: 0.6811 - model_73_1_loss: 0.6775\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3948 - model_72_loss: 0.4096 - model_73_loss: 0.6824 - model_73_1_loss: 0.6785\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4085 - model_72_loss: 0.4113 - model_73_loss: 0.6838 - model_73_1_loss: 0.6801\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4206 - model_72_loss: 0.4151 - model_73_loss: 0.6862 - model_73_1_loss: 0.6809\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.8377 - model_73_loss: 0.6849 - model_73_1_loss: 0.6824\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4131 - model_72_loss: 0.4159 - model_73_loss: 0.6842 - model_73_1_loss: 0.6816\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4219 - model_72_loss: 0.4190 - model_73_loss: 0.6848 - model_73_1_loss: 0.6834\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4295 - model_72_loss: 0.4202 - model_73_loss: 0.6859 - model_73_1_loss: 0.6840\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4374 - model_72_loss: 0.4240 - model_73_loss: 0.6868 - model_73_1_loss: 0.6854\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4356 - model_72_loss: 0.4278 - model_73_loss: 0.6865 - model_73_1_loss: 0.6861\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.8762 - model_73_loss: 0.6877 - model_73_1_loss: 0.6872\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4382 - model_72_loss: 0.4294 - model_73_loss: 0.6877 - model_73_1_loss: 0.6858\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4381 - model_72_loss: 0.4326 - model_73_loss: 0.6877 - model_73_1_loss: 0.6864\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4435 - model_72_loss: 0.4330 - model_73_loss: 0.6885 - model_73_1_loss: 0.6868\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4525 - model_72_loss: 0.4355 - model_73_loss: 0.6892 - model_73_1_loss: 0.6884\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4568 - model_72_loss: 0.4357 - model_73_loss: 0.6901 - model_73_1_loss: 0.6884\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.8950 - model_73_loss: 0.6899 - model_73_1_loss: 0.6892\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4552 - model_72_loss: 0.4375 - model_73_loss: 0.6895 - model_73_1_loss: 0.6891\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4567 - model_72_loss: 0.4410 - model_73_loss: 0.6906 - model_73_1_loss: 0.6890\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4598 - model_72_loss: 0.4437 - model_73_loss: 0.6909 - model_73_1_loss: 0.6898\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4624 - model_72_loss: 0.4436 - model_73_loss: 0.6909 - model_73_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4652 - model_72_loss: 0.4479 - model_73_loss: 0.6915 - model_73_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9132 - model_73_loss: 0.6916 - model_73_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4649 - model_72_loss: 0.4461 - model_73_loss: 0.6915 - model_73_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4674 - model_72_loss: 0.4494 - model_73_loss: 0.6921 - model_73_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4697 - model_72_loss: 0.4492 - model_73_loss: 0.6922 - model_73_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4708 - model_72_loss: 0.4496 - model_73_loss: 0.6923 - model_73_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4680 - model_72_loss: 0.4503 - model_73_loss: 0.6921 - model_73_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9214 - model_73_loss: 0.6919 - model_73_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4718 - model_72_loss: 0.4505 - model_73_loss: 0.6927 - model_73_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4712 - model_72_loss: 0.4528 - model_73_loss: 0.6928 - model_73_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4711 - model_72_loss: 0.4522 - model_73_loss: 0.6930 - model_73_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4749 - model_72_loss: 0.4512 - model_73_loss: 0.6929 - model_73_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4720 - model_72_loss: 0.4536 - model_73_loss: 0.6932 - model_73_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9291 - model_73_loss: 0.6936 - model_73_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4688 - model_72_loss: 0.4532 - model_73_loss: 0.6924 - model_73_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4727 - model_72_loss: 0.4523 - model_73_loss: 0.6926 - model_73_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4711 - model_72_loss: 0.4524 - model_73_loss: 0.6926 - model_73_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4725 - model_72_loss: 0.4530 - model_73_loss: 0.6926 - model_73_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4733 - model_72_loss: 0.4514 - model_73_loss: 0.6927 - model_73_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9244 - model_73_loss: 0.6931 - model_73_1_loss: 0.69240s - loss: 6.9111 - model_73_loss: 0.6917 - model_73_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4720 - model_72_loss: 0.4508 - model_73_loss: 0.6925 - model_73_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4730 - model_72_loss: 0.4513 - model_73_loss: 0.6924 - model_73_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4774 - model_72_loss: 0.4501 - model_73_loss: 0.6928 - model_73_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4714 - model_72_loss: 0.4516 - model_73_loss: 0.6924 - model_73_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4756 - model_72_loss: 0.4497 - model_73_loss: 0.6925 - model_73_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9237 - model_73_loss: 0.6938 - model_73_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4764 - model_72_loss: 0.4478 - model_73_loss: 0.6925 - model_73_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4780 - model_72_loss: 0.4477 - model_73_loss: 0.6925 - model_73_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4818 - model_72_loss: 0.4457 - model_73_loss: 0.6925 - model_73_1_loss: 0.6930\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4800 - model_72_loss: 0.4465 - model_73_loss: 0.6929 - model_73_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4806 - model_72_loss: 0.4449 - model_73_loss: 0.6923 - model_73_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9283 - model_73_loss: 0.6935 - model_73_1_loss: 0.6931\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4805 - model_72_loss: 0.4448 - model_73_loss: 0.6922 - model_73_1_loss: 0.6929\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4812 - model_72_loss: 0.4430 - model_73_loss: 0.6923 - model_73_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4817 - model_72_loss: 0.4423 - model_73_loss: 0.6924 - model_73_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4805 - model_72_loss: 0.4436 - model_73_loss: 0.6921 - model_73_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4792 - model_72_loss: 0.4431 - model_73_loss: 0.6922 - model_73_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9234 - model_73_loss: 0.6920 - model_73_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4815 - model_72_loss: 0.4411 - model_73_loss: 0.6924 - model_73_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4807 - model_72_loss: 0.4416 - model_73_loss: 0.6922 - model_73_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4827 - model_72_loss: 0.4401 - model_73_loss: 0.6920 - model_73_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4860 - model_72_loss: 0.4385 - model_73_loss: 0.6925 - model_73_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4840 - model_72_loss: 0.4391 - model_73_loss: 0.6921 - model_73_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9221 - model_73_loss: 0.6922 - model_73_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4818 - model_72_loss: 0.4388 - model_73_loss: 0.6919 - model_73_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4833 - model_72_loss: 0.4376 - model_73_loss: 0.6921 - model_73_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4846 - model_72_loss: 0.4369 - model_73_loss: 0.6920 - model_73_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4843 - model_72_loss: 0.4365 - model_73_loss: 0.6920 - model_73_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4815 - model_72_loss: 0.4370 - model_73_loss: 0.6918 - model_73_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9194 - model_73_loss: 0.6926 - model_73_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4880 - model_72_loss: 0.4341 - model_73_loss: 0.6924 - model_73_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4830 - model_72_loss: 0.4358 - model_73_loss: 0.6921 - model_73_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4865 - model_72_loss: 0.4343 - model_73_loss: 0.6924 - model_73_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4875 - model_72_loss: 0.4343 - model_73_loss: 0.6923 - model_73_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4865 - model_72_loss: 0.4340 - model_73_loss: 0.6922 - model_73_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9194 - model_73_loss: 0.6922 - model_73_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4855 - model_72_loss: 0.4323 - model_73_loss: 0.6917 - model_73_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4855 - model_72_loss: 0.4325 - model_73_loss: 0.6917 - model_73_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4857 - model_72_loss: 0.4318 - model_73_loss: 0.6915 - model_73_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4846 - model_72_loss: 0.4324 - model_73_loss: 0.6914 - model_73_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4867 - model_72_loss: 0.4323 - model_73_loss: 0.6918 - model_73_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9187 - model_73_loss: 0.6921 - model_73_1_loss: 0.69190s - loss: 6.9231 - model_73_loss: 0.6935 - model_73_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4855 - model_72_loss: 0.4319 - model_73_loss: 0.6918 - model_73_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4867 - model_72_loss: 0.4315 - model_73_loss: 0.6917 - model_73_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4855 - model_72_loss: 0.4329 - model_73_loss: 0.6921 - model_73_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4871 - model_72_loss: 0.4319 - model_73_loss: 0.6918 - model_73_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4862 - model_72_loss: 0.4329 - model_73_loss: 0.6916 - model_73_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9233 - model_73_loss: 0.6927 - model_73_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4872 - model_72_loss: 0.4340 - model_73_loss: 0.6923 - model_73_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4874 - model_72_loss: 0.4328 - model_73_loss: 0.6923 - model_73_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4884 - model_72_loss: 0.4338 - model_73_loss: 0.6924 - model_73_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4886 - model_72_loss: 0.4338 - model_73_loss: 0.6927 - model_73_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4867 - model_72_loss: 0.4360 - model_73_loss: 0.6925 - model_73_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9257 - model_73_loss: 0.6931 - model_73_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4882 - model_72_loss: 0.4347 - model_73_loss: 0.6925 - model_73_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4865 - model_72_loss: 0.4338 - model_73_loss: 0.6923 - model_73_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4859 - model_72_loss: 0.4341 - model_73_loss: 0.6922 - model_73_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4868 - model_72_loss: 0.4349 - model_73_loss: 0.6923 - model_73_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4879 - model_72_loss: 0.4359 - model_73_loss: 0.6927 - model_73_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9231 - model_73_loss: 0.6936 - model_73_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4873 - model_72_loss: 0.4351 - model_73_loss: 0.6927 - model_73_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4856 - model_72_loss: 0.4364 - model_73_loss: 0.6926 - model_73_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4894 - model_72_loss: 0.4350 - model_73_loss: 0.6928 - model_73_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4860 - model_72_loss: 0.4360 - model_73_loss: 0.6925 - model_73_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4863 - model_72_loss: 0.4358 - model_73_loss: 0.6924 - model_73_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9232 - model_73_loss: 0.6919 - model_73_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4854 - model_72_loss: 0.4357 - model_73_loss: 0.6924 - model_73_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4880 - model_72_loss: 0.4351 - model_73_loss: 0.6927 - model_73_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4893 - model_72_loss: 0.4352 - model_73_loss: 0.6926 - model_73_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4851 - model_72_loss: 0.4372 - model_73_loss: 0.6925 - model_73_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4892 - model_72_loss: 0.4355 - model_73_loss: 0.6925 - model_73_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9239 - model_73_loss: 0.6917 - model_73_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4934 - model_72_loss: 0.4337 - model_73_loss: 0.6931 - model_73_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4925 - model_72_loss: 0.4353 - model_73_loss: 0.6933 - model_73_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4900 - model_72_loss: 0.4383 - model_73_loss: 0.6934 - model_73_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4918 - model_72_loss: 0.4367 - model_73_loss: 0.6933 - model_73_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4925 - model_72_loss: 0.4349 - model_73_loss: 0.6933 - model_73_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9261 - model_73_loss: 0.6932 - model_73_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4881 - model_72_loss: 0.4356 - model_73_loss: 0.6926 - model_73_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4879 - model_72_loss: 0.4363 - model_73_loss: 0.6927 - model_73_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4863 - model_72_loss: 0.4376 - model_73_loss: 0.6925 - model_73_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4902 - model_72_loss: 0.4346 - model_73_loss: 0.6926 - model_73_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4912 - model_72_loss: 0.4342 - model_73_loss: 0.6928 - model_73_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9258 - model_73_loss: 0.6920 - model_73_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4901 - model_72_loss: 0.4340 - model_73_loss: 0.6926 - model_73_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4905 - model_72_loss: 0.4336 - model_73_loss: 0.6928 - model_73_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4897 - model_72_loss: 0.4319 - model_73_loss: 0.6923 - model_73_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4896 - model_72_loss: 0.4338 - model_73_loss: 0.6926 - model_73_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4932 - model_72_loss: 0.4307 - model_73_loss: 0.6925 - model_73_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9233 - model_73_loss: 0.6927 - model_73_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4895 - model_72_loss: 0.4329 - model_73_loss: 0.6923 - model_73_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4882 - model_72_loss: 0.4325 - model_73_loss: 0.6925 - model_73_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4903 - model_72_loss: 0.4306 - model_73_loss: 0.6924 - model_73_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4917 - model_72_loss: 0.4304 - model_73_loss: 0.6924 - model_73_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4895 - model_72_loss: 0.4303 - model_73_loss: 0.6923 - model_73_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9228 - model_73_loss: 0.6930 - model_73_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4919 - model_72_loss: 0.4305 - model_73_loss: 0.6924 - model_73_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4929 - model_72_loss: 0.4296 - model_73_loss: 0.6925 - model_73_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4893 - model_72_loss: 0.4307 - model_73_loss: 0.6924 - model_73_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4909 - model_72_loss: 0.4293 - model_73_loss: 0.6923 - model_73_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4910 - model_72_loss: 0.4307 - model_73_loss: 0.6924 - model_73_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9214 - model_73_loss: 0.6929 - model_73_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4888 - model_72_loss: 0.4299 - model_73_loss: 0.6921 - model_73_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4888 - model_72_loss: 0.4295 - model_73_loss: 0.6920 - model_73_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4922 - model_72_loss: 0.4284 - model_73_loss: 0.6923 - model_73_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4925 - model_72_loss: 0.4279 - model_73_loss: 0.6922 - model_73_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4912 - model_72_loss: 0.4295 - model_73_loss: 0.6923 - model_73_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9222 - model_73_loss: 0.6924 - model_73_1_loss: 0.69200s - loss: 6.9217 - model_73_loss: 0.6929 - model_73_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4910 - model_72_loss: 0.4301 - model_73_loss: 0.6921 - model_73_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4901 - model_72_loss: 0.4309 - model_73_loss: 0.6921 - model_73_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4896 - model_72_loss: 0.4300 - model_73_loss: 0.6921 - model_73_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4930 - model_72_loss: 0.4281 - model_73_loss: 0.6921 - model_73_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4941 - model_72_loss: 0.4282 - model_73_loss: 0.6925 - model_73_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9223 - model_73_loss: 0.6922 - model_73_1_loss: 0.692 - 0s 20us/sample - loss: 6.9238 - model_73_loss: 0.6925 - model_73_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4916 - model_72_loss: 0.4292 - model_73_loss: 0.6924 - model_73_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4926 - model_72_loss: 0.4300 - model_73_loss: 0.6925 - model_73_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4912 - model_72_loss: 0.4299 - model_73_loss: 0.6924 - model_73_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4923 - model_72_loss: 0.4297 - model_73_loss: 0.6925 - model_73_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4917 - model_72_loss: 0.4297 - model_73_loss: 0.6923 - model_73_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9238 - model_73_loss: 0.6925 - model_73_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4918 - model_72_loss: 0.4302 - model_73_loss: 0.6923 - model_73_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4926 - model_72_loss: 0.4284 - model_73_loss: 0.6924 - model_73_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4933 - model_72_loss: 0.4269 - model_73_loss: 0.6923 - model_73_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4908 - model_72_loss: 0.4294 - model_73_loss: 0.6923 - model_73_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4929 - model_72_loss: 0.4290 - model_73_loss: 0.6923 - model_73_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9227 - model_73_loss: 0.6926 - model_73_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4914 - model_72_loss: 0.4300 - model_73_loss: 0.6926 - model_73_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4911 - model_72_loss: 0.4305 - model_73_loss: 0.6925 - model_73_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4921 - model_72_loss: 0.4303 - model_73_loss: 0.6927 - model_73_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4926 - model_72_loss: 0.4278 - model_73_loss: 0.6925 - model_73_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4933 - model_72_loss: 0.4303 - model_73_loss: 0.6927 - model_73_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9229 - model_73_loss: 0.6920 - model_73_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4929 - model_72_loss: 0.4298 - model_73_loss: 0.6927 - model_73_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4926 - model_72_loss: 0.4308 - model_73_loss: 0.6927 - model_73_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4941 - model_72_loss: 0.4293 - model_73_loss: 0.6927 - model_73_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4957 - model_72_loss: 0.4286 - model_73_loss: 0.6929 - model_73_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4906 - model_72_loss: 0.4308 - model_73_loss: 0.6926 - model_73_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9260 - model_73_loss: 0.6933 - model_73_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4923 - model_72_loss: 0.4307 - model_73_loss: 0.6926 - model_73_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4933 - model_72_loss: 0.4310 - model_73_loss: 0.6928 - model_73_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4936 - model_72_loss: 0.4310 - model_73_loss: 0.6927 - model_73_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4930 - model_72_loss: 0.4310 - model_73_loss: 0.6928 - model_73_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4971 - model_72_loss: 0.4282 - model_73_loss: 0.6928 - model_73_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9248 - model_73_loss: 0.6927 - model_73_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4945 - model_72_loss: 0.4298 - model_73_loss: 0.6929 - model_73_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4936 - model_72_loss: 0.4291 - model_73_loss: 0.6927 - model_73_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4908 - model_72_loss: 0.4318 - model_73_loss: 0.6927 - model_73_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4924 - model_72_loss: 0.4305 - model_73_loss: 0.6928 - model_73_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4917 - model_72_loss: 0.4307 - model_73_loss: 0.6928 - model_73_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: 6.9252 - model_73_loss: 0.6927 - model_73_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4954 - model_72_loss: 0.4295 - model_73_loss: 0.6929 - model_73_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4962 - model_72_loss: 0.4294 - model_73_loss: 0.6929 - model_73_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4931 - model_72_loss: 0.4309 - model_73_loss: 0.6927 - model_73_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4935 - model_72_loss: 0.4312 - model_73_loss: 0.6929 - model_73_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4940 - model_72_loss: 0.4298 - model_73_loss: 0.6928 - model_73_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9256 - model_73_loss: 0.6930 - model_73_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4938 - model_72_loss: 0.4294 - model_73_loss: 0.6928 - model_73_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4938 - model_72_loss: 0.4302 - model_73_loss: 0.6928 - model_73_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4944 - model_72_loss: 0.4298 - model_73_loss: 0.6928 - model_73_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4931 - model_72_loss: 0.4295 - model_73_loss: 0.6927 - model_73_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4936 - model_72_loss: 0.4300 - model_73_loss: 0.6926 - model_73_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9244 - model_73_loss: 0.6930 - model_73_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4919 - model_72_loss: 0.4303 - model_73_loss: 0.6927 - model_73_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4952 - model_72_loss: 0.4286 - model_73_loss: 0.6927 - model_73_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4944 - model_72_loss: 0.4287 - model_73_loss: 0.6926 - model_73_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4958 - model_72_loss: 0.4276 - model_73_loss: 0.6927 - model_73_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4943 - model_72_loss: 0.4294 - model_73_loss: 0.6926 - model_73_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9245 - model_73_loss: 0.6921 - model_73_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4937 - model_72_loss: 0.4287 - model_73_loss: 0.6928 - model_73_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4936 - model_72_loss: 0.4291 - model_73_loss: 0.6927 - model_73_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4939 - model_72_loss: 0.4284 - model_73_loss: 0.6926 - model_73_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4935 - model_72_loss: 0.4286 - model_73_loss: 0.6924 - model_73_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4957 - model_72_loss: 0.4271 - model_73_loss: 0.6927 - model_73_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9234 - model_73_loss: 0.6927 - model_73_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4948 - model_72_loss: 0.4281 - model_73_loss: 0.6923 - model_73_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4963 - model_72_loss: 0.4274 - model_73_loss: 0.6925 - model_73_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4930 - model_72_loss: 0.4282 - model_73_loss: 0.6923 - model_73_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4937 - model_72_loss: 0.4268 - model_73_loss: 0.6923 - model_73_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4971 - model_72_loss: 0.4259 - model_73_loss: 0.6923 - model_73_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9214 - model_73_loss: 0.6930 - model_73_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4990 - model_72_loss: 0.4265 - model_73_loss: 0.6928 - model_73_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4965 - model_72_loss: 0.4281 - model_73_loss: 0.6928 - model_73_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4987 - model_72_loss: 0.4276 - model_73_loss: 0.6927 - model_73_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4962 - model_72_loss: 0.4292 - model_73_loss: 0.6928 - model_73_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4973 - model_72_loss: 0.4287 - model_73_loss: 0.6928 - model_73_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9251 - model_73_loss: 0.6929 - model_73_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4944 - model_72_loss: 0.4273 - model_73_loss: 0.6924 - model_73_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4943 - model_72_loss: 0.4270 - model_73_loss: 0.6921 - model_73_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4936 - model_72_loss: 0.4282 - model_73_loss: 0.6922 - model_73_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4947 - model_72_loss: 0.4271 - model_73_loss: 0.6923 - model_73_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4922 - model_72_loss: 0.4285 - model_73_loss: 0.6921 - model_73_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9236 - model_73_loss: 0.6926 - model_73_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4939 - model_72_loss: 0.4291 - model_73_loss: 0.6923 - model_73_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4950 - model_72_loss: 0.4268 - model_73_loss: 0.6922 - model_73_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4928 - model_72_loss: 0.4289 - model_73_loss: 0.6921 - model_73_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4941 - model_72_loss: 0.4283 - model_73_loss: 0.6922 - model_73_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4942 - model_72_loss: 0.4287 - model_73_loss: 0.6923 - model_73_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.9220 - model_73_loss: 0.6929 - model_73_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4934 - model_72_loss: 0.4278 - model_73_loss: 0.6921 - model_73_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4961 - model_72_loss: 0.4263 - model_73_loss: 0.6923 - model_73_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4941 - model_72_loss: 0.4275 - model_73_loss: 0.6922 - model_73_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4962 - model_72_loss: 0.4257 - model_73_loss: 0.6924 - model_73_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4949 - model_72_loss: 0.4261 - model_73_loss: 0.6919 - model_73_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9227 - model_73_loss: 0.6926 - model_73_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4958 - model_72_loss: 0.4265 - model_73_loss: 0.6924 - model_73_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4947 - model_72_loss: 0.4264 - model_73_loss: 0.6924 - model_73_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4975 - model_72_loss: 0.4255 - model_73_loss: 0.6924 - model_73_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4972 - model_72_loss: 0.4261 - model_73_loss: 0.6927 - model_73_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4965 - model_72_loss: 0.4274 - model_73_loss: 0.6926 - model_73_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9221 - model_73_loss: 0.6929 - model_73_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4997 - model_72_loss: 0.4268 - model_73_loss: 0.6929 - model_73_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4995 - model_72_loss: 0.4258 - model_73_loss: 0.6929 - model_73_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4947 - model_72_loss: 0.4267 - model_73_loss: 0.6922 - model_73_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4976 - model_72_loss: 0.4268 - model_73_loss: 0.6927 - model_73_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4987 - model_72_loss: 0.4262 - model_73_loss: 0.6930 - model_73_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9255 - model_73_loss: 0.6932 - model_73_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4968 - model_72_loss: 0.4280 - model_73_loss: 0.6927 - model_73_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4940 - model_72_loss: 0.4302 - model_73_loss: 0.6927 - model_73_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4931 - model_72_loss: 0.4299 - model_73_loss: 0.6926 - model_73_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4951 - model_72_loss: 0.4288 - model_73_loss: 0.6928 - model_73_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4974 - model_72_loss: 0.4280 - model_73_loss: 0.6928 - model_73_1_loss: 0.6922\n",
      "For Attention Module: 2.0\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.3413 - model_77_loss: 0.6588 - model_77_1_loss: 0.6091\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -5.9594 - model_76_loss: 0.3776 - model_77_loss: 0.6584 - model_77_1_loss: 0.6090\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -5.9763 - model_76_loss: 0.3775 - model_77_loss: 0.6596 - model_77_1_loss: 0.6112\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -5.9819 - model_76_loss: 0.3754 - model_77_loss: 0.6590 - model_77_1_loss: 0.6125\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -5.9993 - model_76_loss: 0.3760 - model_77_loss: 0.6610 - model_77_1_loss: 0.6141\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.0071 - model_76_loss: 0.3760 - model_77_loss: 0.6605 - model_77_1_loss: 0.6161\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.3996 - model_77_loss: 0.6603 - model_77_1_loss: 0.6190\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0044 - model_76_loss: 0.3783 - model_77_loss: 0.6603 - model_77_1_loss: 0.6163\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0219 - model_76_loss: 0.3764 - model_77_loss: 0.6608 - model_77_1_loss: 0.6189\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0313 - model_76_loss: 0.3779 - model_77_loss: 0.6608 - model_77_1_loss: 0.6210\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0487 - model_76_loss: 0.3807 - model_77_loss: 0.6624 - model_77_1_loss: 0.6235\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.0691 - model_76_loss: 0.3781 - model_77_loss: 0.6634 - model_77_1_loss: 0.6261\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.4607 - model_77_loss: 0.6652 - model_77_1_loss: 0.6263\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0701 - model_76_loss: 0.3799 - model_77_loss: 0.6634 - model_77_1_loss: 0.6267\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.0717 - model_76_loss: 0.3831 - model_77_loss: 0.6637 - model_77_1_loss: 0.6272\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.0942 - model_76_loss: 0.3838 - model_77_loss: 0.6657 - model_77_1_loss: 0.6299\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0996 - model_76_loss: 0.3842 - model_77_loss: 0.6655 - model_77_1_loss: 0.6312\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.1195 - model_76_loss: 0.3863 - model_77_loss: 0.6667 - model_77_1_loss: 0.6344\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.5112 - model_77_loss: 0.6669 - model_77_1_loss: 0.6351\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1150 - model_76_loss: 0.3866 - model_77_loss: 0.6661 - model_77_1_loss: 0.6342\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1372 - model_76_loss: 0.3877 - model_77_loss: 0.6685 - model_77_1_loss: 0.6365\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1454 - model_76_loss: 0.3898 - model_77_loss: 0.6684 - model_77_1_loss: 0.6387\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1608 - model_76_loss: 0.3901 - model_77_loss: 0.6694 - model_77_1_loss: 0.6408\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.1712 - model_76_loss: 0.3922 - model_77_loss: 0.6704 - model_77_1_loss: 0.6423\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.5817 - model_77_loss: 0.6705 - model_77_1_loss: 0.6453\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1801 - model_76_loss: 0.3932 - model_77_loss: 0.6699 - model_77_1_loss: 0.6448\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2066 - model_76_loss: 0.3948 - model_77_loss: 0.6725 - model_77_1_loss: 0.6478\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2136 - model_76_loss: 0.3962 - model_77_loss: 0.6728 - model_77_1_loss: 0.6492\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2142 - model_76_loss: 0.3991 - model_77_loss: 0.6726 - model_77_1_loss: 0.6501\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2396 - model_76_loss: 0.4016 - model_77_loss: 0.6741 - model_77_1_loss: 0.6541\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.6378 - model_77_loss: 0.6736 - model_77_1_loss: 0.6537\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.2467 - model_76_loss: 0.4019 - model_77_loss: 0.6753 - model_77_1_loss: 0.6544\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2516 - model_76_loss: 0.4069 - model_77_loss: 0.6753 - model_77_1_loss: 0.6565\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2645 - model_76_loss: 0.4101 - model_77_loss: 0.6766 - model_77_1_loss: 0.6583\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2799 - model_76_loss: 0.4125 - model_77_loss: 0.6771 - model_77_1_loss: 0.6614\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.2956 - model_76_loss: 0.4139 - model_77_loss: 0.6773 - model_77_1_loss: 0.6646\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.7184 - model_77_loss: 0.6790 - model_77_1_loss: 0.66460s - loss: 6.7007 - model_77_loss: 0.6750 - model_77_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3003 - model_76_loss: 0.4185 - model_77_loss: 0.6783 - model_77_1_loss: 0.6655\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3090 - model_76_loss: 0.4192 - model_77_loss: 0.6791 - model_77_1_loss: 0.6665\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3204 - model_76_loss: 0.4229 - model_77_loss: 0.6802 - model_77_1_loss: 0.6685\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3344 - model_76_loss: 0.4278 - model_77_loss: 0.6808 - model_77_1_loss: 0.6717\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3505 - model_76_loss: 0.4290 - model_77_loss: 0.6824 - model_77_1_loss: 0.6735\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.7747 - model_77_loss: 0.6807 - model_77_1_loss: 0.6735\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3436 - model_76_loss: 0.4320 - model_77_loss: 0.6821 - model_77_1_loss: 0.6730\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3564 - model_76_loss: 0.4348 - model_77_loss: 0.6838 - model_77_1_loss: 0.6744\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3587 - model_76_loss: 0.4379 - model_77_loss: 0.6833 - model_77_1_loss: 0.6760\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3713 - model_76_loss: 0.4403 - model_77_loss: 0.6844 - model_77_1_loss: 0.6780\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3783 - model_76_loss: 0.4464 - model_77_loss: 0.6844 - model_77_1_loss: 0.6805\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.8297 - model_77_loss: 0.6853 - model_77_1_loss: 0.6808\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3794 - model_76_loss: 0.4482 - model_77_loss: 0.6853 - model_77_1_loss: 0.6802\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3892 - model_76_loss: 0.4511 - model_77_loss: 0.6862 - model_77_1_loss: 0.6819\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3898 - model_76_loss: 0.4542 - model_77_loss: 0.6870 - model_77_1_loss: 0.6818\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3969 - model_76_loss: 0.4586 - model_77_loss: 0.6879 - model_77_1_loss: 0.6832\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3993 - model_76_loss: 0.4615 - model_77_loss: 0.6877 - model_77_1_loss: 0.6844\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: 6.8689 - model_77_loss: 0.6890 - model_77_1_loss: 0.6847\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4001 - model_76_loss: 0.4640 - model_77_loss: 0.6881 - model_77_1_loss: 0.6848\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4041 - model_76_loss: 0.4687 - model_77_loss: 0.6890 - model_77_1_loss: 0.6856\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4047 - model_76_loss: 0.4697 - model_77_loss: 0.6886 - model_77_1_loss: 0.6863\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4163 - model_76_loss: 0.4728 - model_77_loss: 0.6900 - model_77_1_loss: 0.6878\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4147 - model_76_loss: 0.4766 - model_77_loss: 0.6902 - model_77_1_loss: 0.6881\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9004 - model_77_loss: 0.6915 - model_77_1_loss: 0.6890\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4211 - model_76_loss: 0.4803 - model_77_loss: 0.6915 - model_77_1_loss: 0.6888\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4200 - model_76_loss: 0.4811 - model_77_loss: 0.6911 - model_77_1_loss: 0.6891\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4170 - model_76_loss: 0.4824 - model_77_loss: 0.6906 - model_77_1_loss: 0.6892\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4245 - model_76_loss: 0.4836 - model_77_loss: 0.6920 - model_77_1_loss: 0.6896\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4241 - model_76_loss: 0.4854 - model_77_loss: 0.6919 - model_77_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9101 - model_77_loss: 0.6909 - model_77_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4213 - model_76_loss: 0.4884 - model_77_loss: 0.6920 - model_77_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4219 - model_76_loss: 0.4892 - model_77_loss: 0.6919 - model_77_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4203 - model_76_loss: 0.4923 - model_77_loss: 0.6921 - model_77_1_loss: 0.6904\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4265 - model_76_loss: 0.4911 - model_77_loss: 0.6921 - model_77_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4260 - model_76_loss: 0.4936 - model_77_loss: 0.6925 - model_77_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9216 - model_77_loss: 0.6921 - model_77_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4244 - model_76_loss: 0.4935 - model_77_loss: 0.6923 - model_77_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4264 - model_76_loss: 0.4962 - model_77_loss: 0.6928 - model_77_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4312 - model_76_loss: 0.4939 - model_77_loss: 0.6933 - model_77_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4263 - model_76_loss: 0.4944 - model_77_loss: 0.6926 - model_77_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4318 - model_76_loss: 0.4917 - model_77_loss: 0.6928 - model_77_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9237 - model_77_loss: 0.6925 - model_77_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4320 - model_76_loss: 0.4896 - model_77_loss: 0.6926 - model_77_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4320 - model_76_loss: 0.4912 - model_77_loss: 0.6926 - model_77_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4326 - model_76_loss: 0.4881 - model_77_loss: 0.6923 - model_77_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4305 - model_76_loss: 0.4916 - model_77_loss: 0.6924 - model_77_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4353 - model_76_loss: 0.4896 - model_77_loss: 0.6926 - model_77_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9255 - model_77_loss: 0.6925 - model_77_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4336 - model_76_loss: 0.4886 - model_77_loss: 0.6923 - model_77_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4363 - model_76_loss: 0.4860 - model_77_loss: 0.6921 - model_77_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4391 - model_76_loss: 0.4836 - model_77_loss: 0.6922 - model_77_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4391 - model_76_loss: 0.4837 - model_77_loss: 0.6921 - model_77_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4392 - model_76_loss: 0.4833 - model_77_loss: 0.6922 - model_77_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9204 - model_77_loss: 0.6916 - model_77_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4358 - model_76_loss: 0.4838 - model_77_loss: 0.6918 - model_77_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4352 - model_76_loss: 0.4820 - model_77_loss: 0.6915 - model_77_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4339 - model_76_loss: 0.4825 - model_77_loss: 0.6912 - model_77_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4372 - model_76_loss: 0.4792 - model_77_loss: 0.6914 - model_77_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4366 - model_76_loss: 0.4802 - model_77_loss: 0.6914 - model_77_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9158 - model_77_loss: 0.6915 - model_77_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4343 - model_76_loss: 0.4797 - model_77_loss: 0.6914 - model_77_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4349 - model_76_loss: 0.4777 - model_77_loss: 0.6912 - model_77_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4288 - model_76_loss: 0.4801 - model_77_loss: 0.6905 - model_77_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4383 - model_76_loss: 0.4773 - model_77_loss: 0.6921 - model_77_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4354 - model_76_loss: 0.4779 - model_77_loss: 0.6914 - model_77_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9139 - model_77_loss: 0.6912 - model_77_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4373 - model_76_loss: 0.4774 - model_77_loss: 0.6915 - model_77_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4367 - model_76_loss: 0.4774 - model_77_loss: 0.6913 - model_77_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4403 - model_76_loss: 0.4777 - model_77_loss: 0.6919 - model_77_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4408 - model_76_loss: 0.4752 - model_77_loss: 0.6917 - model_77_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4391 - model_76_loss: 0.4754 - model_77_loss: 0.6912 - model_77_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9135 - model_77_loss: 0.6914 - model_77_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4358 - model_76_loss: 0.4740 - model_77_loss: 0.6912 - model_77_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4362 - model_76_loss: 0.4745 - model_77_loss: 0.6912 - model_77_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4354 - model_76_loss: 0.4755 - model_77_loss: 0.6911 - model_77_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4351 - model_76_loss: 0.4760 - model_77_loss: 0.6912 - model_77_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4371 - model_76_loss: 0.4749 - model_77_loss: 0.6914 - model_77_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9124 - model_77_loss: 0.6916 - model_77_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4382 - model_76_loss: 0.4736 - model_77_loss: 0.6912 - model_77_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4412 - model_76_loss: 0.4740 - model_77_loss: 0.6918 - model_77_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4414 - model_76_loss: 0.4745 - model_77_loss: 0.6919 - model_77_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4398 - model_76_loss: 0.4739 - model_77_loss: 0.6914 - model_77_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4450 - model_76_loss: 0.4728 - model_77_loss: 0.6919 - model_77_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9166 - model_77_loss: 0.6917 - model_77_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4441 - model_76_loss: 0.4739 - model_77_loss: 0.6919 - model_77_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4446 - model_76_loss: 0.4739 - model_77_loss: 0.6920 - model_77_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4453 - model_76_loss: 0.4726 - model_77_loss: 0.6921 - model_77_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4421 - model_76_loss: 0.4725 - model_77_loss: 0.6917 - model_77_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4442 - model_76_loss: 0.4744 - model_77_loss: 0.6922 - model_77_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9181 - model_77_loss: 0.6930 - model_77_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4418 - model_76_loss: 0.4752 - model_77_loss: 0.6923 - model_77_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4423 - model_76_loss: 0.4733 - model_77_loss: 0.6919 - model_77_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4433 - model_76_loss: 0.4713 - model_77_loss: 0.6919 - model_77_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4440 - model_76_loss: 0.4712 - model_77_loss: 0.6920 - model_77_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4434 - model_76_loss: 0.4721 - model_77_loss: 0.6920 - model_77_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9146 - model_77_loss: 0.6919 - model_77_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4409 - model_76_loss: 0.4728 - model_77_loss: 0.6920 - model_77_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4392 - model_76_loss: 0.4720 - model_77_loss: 0.6915 - model_77_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4420 - model_76_loss: 0.4732 - model_77_loss: 0.6921 - model_77_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4449 - model_76_loss: 0.4703 - model_77_loss: 0.6922 - model_77_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4440 - model_76_loss: 0.4710 - model_77_loss: 0.6921 - model_77_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9154 - model_77_loss: 0.6918 - model_77_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4404 - model_76_loss: 0.4712 - model_77_loss: 0.6919 - model_77_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4408 - model_76_loss: 0.4705 - model_77_loss: 0.6917 - model_77_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4420 - model_76_loss: 0.4707 - model_77_loss: 0.6917 - model_77_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4440 - model_76_loss: 0.4696 - model_77_loss: 0.6920 - model_77_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4423 - model_76_loss: 0.4691 - model_77_loss: 0.6913 - model_77_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9168 - model_77_loss: 0.6919 - model_77_1_loss: 0.69100s - loss: 6.9241 - model_77_loss: 0.6930 - model_77_1_loss: 0.691\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4484 - model_76_loss: 0.4697 - model_77_loss: 0.6920 - model_77_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4485 - model_76_loss: 0.4701 - model_77_loss: 0.6923 - model_77_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4502 - model_76_loss: 0.4701 - model_77_loss: 0.6921 - model_77_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4478 - model_76_loss: 0.4715 - model_77_loss: 0.6920 - model_77_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4545 - model_76_loss: 0.4682 - model_77_loss: 0.6925 - model_77_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9206 - model_77_loss: 0.6925 - model_77_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4542 - model_76_loss: 0.4694 - model_77_loss: 0.6926 - model_77_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4516 - model_76_loss: 0.4707 - model_77_loss: 0.6921 - model_77_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4515 - model_76_loss: 0.4708 - model_77_loss: 0.6923 - model_77_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4472 - model_76_loss: 0.4733 - model_77_loss: 0.6920 - model_77_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4559 - model_76_loss: 0.4713 - model_77_loss: 0.6925 - model_77_1_loss: 0.6930\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9212 - model_77_loss: 0.6919 - model_77_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4471 - model_76_loss: 0.4746 - model_77_loss: 0.6924 - model_77_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4489 - model_76_loss: 0.4722 - model_77_loss: 0.6923 - model_77_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4491 - model_76_loss: 0.4727 - model_77_loss: 0.6925 - model_77_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4456 - model_76_loss: 0.4752 - model_77_loss: 0.6919 - model_77_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4465 - model_76_loss: 0.4760 - model_77_loss: 0.6923 - model_77_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9233 - model_77_loss: 0.6925 - model_77_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4440 - model_76_loss: 0.4738 - model_77_loss: 0.6919 - model_77_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4445 - model_76_loss: 0.4768 - model_77_loss: 0.6924 - model_77_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4448 - model_76_loss: 0.4770 - model_77_loss: 0.6925 - model_77_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4464 - model_76_loss: 0.4751 - model_77_loss: 0.6923 - model_77_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4450 - model_76_loss: 0.4770 - model_77_loss: 0.6926 - model_77_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9239 - model_77_loss: 0.6935 - model_77_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4509 - model_76_loss: 0.4751 - model_77_loss: 0.6928 - model_77_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4529 - model_76_loss: 0.4752 - model_77_loss: 0.6929 - model_77_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4503 - model_76_loss: 0.4766 - model_77_loss: 0.6928 - model_77_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4523 - model_76_loss: 0.4758 - model_77_loss: 0.6927 - model_77_1_loss: 0.6930\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4540 - model_76_loss: 0.4735 - model_77_loss: 0.6926 - model_77_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9250 - model_77_loss: 0.6924 - model_77_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4501 - model_76_loss: 0.4730 - model_77_loss: 0.6925 - model_77_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4504 - model_76_loss: 0.4723 - model_77_loss: 0.6924 - model_77_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4505 - model_76_loss: 0.4735 - model_77_loss: 0.6927 - model_77_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4491 - model_76_loss: 0.4729 - model_77_loss: 0.6926 - model_77_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4530 - model_76_loss: 0.4718 - model_77_loss: 0.6930 - model_77_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9236 - model_77_loss: 0.6926 - model_77_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4530 - model_76_loss: 0.4720 - model_77_loss: 0.6929 - model_77_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4516 - model_76_loss: 0.4710 - model_77_loss: 0.6923 - model_77_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4526 - model_76_loss: 0.4699 - model_77_loss: 0.6924 - model_77_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4526 - model_76_loss: 0.4700 - model_77_loss: 0.6925 - model_77_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4556 - model_76_loss: 0.4693 - model_77_loss: 0.6927 - model_77_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9232 - model_77_loss: 0.6932 - model_77_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4492 - model_76_loss: 0.4694 - model_77_loss: 0.6922 - model_77_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4497 - model_76_loss: 0.4689 - model_77_loss: 0.6924 - model_77_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4503 - model_76_loss: 0.4685 - model_77_loss: 0.6923 - model_77_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4492 - model_76_loss: 0.4690 - model_77_loss: 0.6922 - model_77_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4517 - model_76_loss: 0.4666 - model_77_loss: 0.6924 - model_77_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9165 - model_77_loss: 0.6923 - model_77_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4449 - model_76_loss: 0.4688 - model_77_loss: 0.6920 - model_77_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4473 - model_76_loss: 0.4671 - model_77_loss: 0.6920 - model_77_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4484 - model_76_loss: 0.4655 - model_77_loss: 0.6922 - model_77_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4486 - model_76_loss: 0.4653 - model_77_loss: 0.6921 - model_77_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4498 - model_76_loss: 0.4657 - model_77_loss: 0.6923 - model_77_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9155 - model_77_loss: 0.6921 - model_77_1_loss: 0.69100s - loss: 6.8823 - model_77_loss: 0.6853 - model_77_1_loss: \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4475 - model_76_loss: 0.4671 - model_77_loss: 0.6921 - model_77_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4507 - model_76_loss: 0.4642 - model_77_loss: 0.6922 - model_77_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4497 - model_76_loss: 0.4647 - model_77_loss: 0.6921 - model_77_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4516 - model_76_loss: 0.4642 - model_77_loss: 0.6925 - model_77_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4539 - model_76_loss: 0.4626 - model_77_loss: 0.6924 - model_77_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9154 - model_77_loss: 0.6925 - model_77_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4487 - model_76_loss: 0.4642 - model_77_loss: 0.6920 - model_77_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4509 - model_76_loss: 0.4648 - model_77_loss: 0.6921 - model_77_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4525 - model_76_loss: 0.4637 - model_77_loss: 0.6923 - model_77_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4494 - model_76_loss: 0.4647 - model_77_loss: 0.6923 - model_77_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4564 - model_76_loss: 0.4627 - model_77_loss: 0.6925 - model_77_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9196 - model_77_loss: 0.6930 - model_77_1_loss: 0.69140s - loss: 6.9629 - model_77_loss: 0.6993 - model_77_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4555 - model_76_loss: 0.4624 - model_77_loss: 0.6923 - model_77_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4548 - model_76_loss: 0.4641 - model_77_loss: 0.6925 - model_77_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4555 - model_76_loss: 0.4626 - model_77_loss: 0.6925 - model_77_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4571 - model_76_loss: 0.4639 - model_77_loss: 0.6925 - model_77_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4604 - model_76_loss: 0.4629 - model_77_loss: 0.6929 - model_77_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9190 - model_77_loss: 0.6922 - model_77_1_loss: 0.69090s - loss: 6.9435 - model_77_loss: 0.6944 - model_77_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4518 - model_76_loss: 0.4641 - model_77_loss: 0.6922 - model_77_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4540 - model_76_loss: 0.4630 - model_77_loss: 0.6925 - model_77_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4510 - model_76_loss: 0.4666 - model_77_loss: 0.6924 - model_77_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4537 - model_76_loss: 0.4651 - model_77_loss: 0.6924 - model_77_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4527 - model_76_loss: 0.4629 - model_77_loss: 0.6923 - model_77_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9115 - model_77_loss: 0.6919 - model_77_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4515 - model_76_loss: 0.4640 - model_77_loss: 0.6924 - model_77_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4488 - model_76_loss: 0.4662 - model_77_loss: 0.6925 - model_77_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4516 - model_76_loss: 0.4650 - model_77_loss: 0.6921 - model_77_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4505 - model_76_loss: 0.4650 - model_77_loss: 0.6922 - model_77_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4502 - model_76_loss: 0.4662 - model_77_loss: 0.6922 - model_77_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9149 - model_77_loss: 0.6921 - model_77_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4483 - model_76_loss: 0.4648 - model_77_loss: 0.6921 - model_77_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4482 - model_76_loss: 0.4662 - model_77_loss: 0.6924 - model_77_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4490 - model_76_loss: 0.4674 - model_77_loss: 0.6924 - model_77_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4566 - model_76_loss: 0.4654 - model_77_loss: 0.6929 - model_77_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4503 - model_76_loss: 0.4690 - model_77_loss: 0.6927 - model_77_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9211 - model_77_loss: 0.6926 - model_77_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4492 - model_76_loss: 0.4679 - model_77_loss: 0.6922 - model_77_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4509 - model_76_loss: 0.4681 - model_77_loss: 0.6922 - model_77_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4536 - model_76_loss: 0.4669 - model_77_loss: 0.6922 - model_77_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4542 - model_76_loss: 0.4676 - model_77_loss: 0.6923 - model_77_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4591 - model_76_loss: 0.4673 - model_77_loss: 0.6929 - model_77_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9274 - model_77_loss: 0.6923 - model_77_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4585 - model_76_loss: 0.4668 - model_77_loss: 0.6928 - model_77_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4560 - model_76_loss: 0.4682 - model_77_loss: 0.6926 - model_77_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4592 - model_76_loss: 0.4683 - model_77_loss: 0.6930 - model_77_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4605 - model_76_loss: 0.4672 - model_77_loss: 0.6930 - model_77_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4588 - model_76_loss: 0.4679 - model_77_loss: 0.6927 - model_77_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9228 - model_77_loss: 0.6930 - model_77_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4530 - model_76_loss: 0.4685 - model_77_loss: 0.6926 - model_77_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4525 - model_76_loss: 0.4681 - model_77_loss: 0.6925 - model_77_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4520 - model_76_loss: 0.4685 - model_77_loss: 0.6924 - model_77_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4523 - model_76_loss: 0.4672 - model_77_loss: 0.6926 - model_77_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4544 - model_76_loss: 0.4658 - model_77_loss: 0.6925 - model_77_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9169 - model_77_loss: 0.6920 - model_77_1_loss: 0.69110s - loss: 6.9443 - model_77_loss: 0.6972 - model_77_1_loss:\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4438 - model_76_loss: 0.4671 - model_77_loss: 0.6922 - model_77_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4434 - model_76_loss: 0.4660 - model_77_loss: 0.6919 - model_77_1_loss: 0.6900\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4428 - model_76_loss: 0.4651 - model_77_loss: 0.6919 - model_77_1_loss: 0.6897\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4484 - model_76_loss: 0.4653 - model_77_loss: 0.6921 - model_77_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4493 - model_76_loss: 0.4629 - model_77_loss: 0.6922 - model_77_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9130 - model_77_loss: 0.6921 - model_77_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4455 - model_76_loss: 0.4645 - model_77_loss: 0.6921 - model_77_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4467 - model_76_loss: 0.4660 - model_77_loss: 0.6922 - model_77_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4469 - model_76_loss: 0.4667 - model_77_loss: 0.6922 - model_77_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4555 - model_76_loss: 0.4630 - model_77_loss: 0.6928 - model_77_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4578 - model_76_loss: 0.4622 - model_77_loss: 0.6927 - model_77_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9223 - model_77_loss: 0.6924 - model_77_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4566 - model_76_loss: 0.4626 - model_77_loss: 0.6924 - model_77_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4561 - model_76_loss: 0.4639 - model_77_loss: 0.6925 - model_77_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4584 - model_76_loss: 0.4643 - model_77_loss: 0.6926 - model_77_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4576 - model_76_loss: 0.4639 - model_77_loss: 0.6926 - model_77_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4582 - model_76_loss: 0.4631 - model_77_loss: 0.6923 - model_77_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9202 - model_77_loss: 0.6927 - model_77_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4533 - model_76_loss: 0.4637 - model_77_loss: 0.6922 - model_77_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4536 - model_76_loss: 0.4637 - model_77_loss: 0.6921 - model_77_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4552 - model_76_loss: 0.4632 - model_77_loss: 0.6922 - model_77_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4539 - model_76_loss: 0.4643 - model_77_loss: 0.6923 - model_77_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4534 - model_76_loss: 0.4627 - model_77_loss: 0.6919 - model_77_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9162 - model_77_loss: 0.6922 - model_77_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4435 - model_76_loss: 0.4641 - model_77_loss: 0.6917 - model_77_1_loss: 0.6898\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4437 - model_76_loss: 0.4639 - model_77_loss: 0.6920 - model_77_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4436 - model_76_loss: 0.4647 - model_77_loss: 0.6920 - model_77_1_loss: 0.6896\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4401 - model_76_loss: 0.4645 - model_77_loss: 0.6916 - model_77_1_loss: 0.6893\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4460 - model_76_loss: 0.4635 - model_77_loss: 0.6920 - model_77_1_loss: 0.6899\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9094 - model_77_loss: 0.6928 - model_77_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4444 - model_76_loss: 0.4632 - model_77_loss: 0.6918 - model_77_1_loss: 0.6897\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4452 - model_76_loss: 0.4629 - model_77_loss: 0.6917 - model_77_1_loss: 0.6900\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4446 - model_76_loss: 0.4621 - model_77_loss: 0.6919 - model_77_1_loss: 0.6895\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4493 - model_76_loss: 0.4627 - model_77_loss: 0.6920 - model_77_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4476 - model_76_loss: 0.4623 - model_77_loss: 0.6918 - model_77_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9153 - model_77_loss: 0.6921 - model_77_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4487 - model_76_loss: 0.4634 - model_77_loss: 0.6920 - model_77_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4518 - model_76_loss: 0.4638 - model_77_loss: 0.6922 - model_77_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4528 - model_76_loss: 0.4652 - model_77_loss: 0.6925 - model_77_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4590 - model_76_loss: 0.4614 - model_77_loss: 0.6924 - model_77_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4612 - model_76_loss: 0.4623 - model_77_loss: 0.6929 - model_77_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9230 - model_77_loss: 0.6922 - model_77_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4551 - model_76_loss: 0.4634 - model_77_loss: 0.6921 - model_77_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4573 - model_76_loss: 0.4624 - model_77_loss: 0.6924 - model_77_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4573 - model_76_loss: 0.4636 - model_77_loss: 0.6926 - model_77_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4565 - model_76_loss: 0.4645 - model_77_loss: 0.6926 - model_77_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4594 - model_76_loss: 0.4633 - model_77_loss: 0.6926 - model_77_1_loss: 0.6919\n",
      "For Attention Module: 2.1\n",
      "features X: 30940 samples, 71 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.3412 - model_81_loss: 0.6589 - model_81_1_loss: 0.6096\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -5.9611 - model_80_loss: 0.3764 - model_81_loss: 0.6592 - model_81_1_loss: 0.6082\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -5.9691 - model_80_loss: 0.3754 - model_81_loss: 0.6593 - model_81_1_loss: 0.6096\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -5.9901 - model_80_loss: 0.3749 - model_81_loss: 0.6605 - model_81_1_loss: 0.6125\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0007 - model_80_loss: 0.3755 - model_81_loss: 0.6610 - model_81_1_loss: 0.6142\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.0100 - model_80_loss: 0.3770 - model_81_loss: 0.6614 - model_81_1_loss: 0.6160\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.4066 - model_81_loss: 0.6619 - model_81_1_loss: 0.6195\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0198 - model_80_loss: 0.3773 - model_81_loss: 0.6618 - model_81_1_loss: 0.6176\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0321 - model_80_loss: 0.3762 - model_81_loss: 0.6619 - model_81_1_loss: 0.6197\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.0422 - model_80_loss: 0.3775 - model_81_loss: 0.6626 - model_81_1_loss: 0.6213\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.0578 - model_80_loss: 0.3778 - model_81_loss: 0.6630 - model_81_1_loss: 0.6242\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.0754 - model_80_loss: 0.3779 - model_81_loss: 0.6640 - model_81_1_loss: 0.6266\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.4703 - model_81_loss: 0.6651 - model_81_1_loss: 0.62840s - loss: 6.4543 - model_81_loss: 0.6621 - model_81_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0809 - model_80_loss: 0.3800 - model_81_loss: 0.6643 - model_81_1_loss: 0.6278\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.0982 - model_80_loss: 0.3819 - model_81_loss: 0.6653 - model_81_1_loss: 0.6307\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.1067 - model_80_loss: 0.3831 - model_81_loss: 0.6665 - model_81_1_loss: 0.6314\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.1203 - model_80_loss: 0.3848 - model_81_loss: 0.6667 - model_81_1_loss: 0.6343\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.1374 - model_80_loss: 0.3849 - model_81_loss: 0.6681 - model_81_1_loss: 0.6363\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.5211 - model_81_loss: 0.6662 - model_81_1_loss: 0.6372\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1442 - model_80_loss: 0.3856 - model_81_loss: 0.6679 - model_81_1_loss: 0.6380\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.1460 - model_80_loss: 0.3870 - model_81_loss: 0.6677 - model_81_1_loss: 0.6389\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.1715 - model_80_loss: 0.3898 - model_81_loss: 0.6694 - model_81_1_loss: 0.6429\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.1789 - model_80_loss: 0.3937 - model_81_loss: 0.6700 - model_81_1_loss: 0.6445\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.1954 - model_80_loss: 0.3931 - model_81_loss: 0.6701 - model_81_1_loss: 0.6476\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.6145 - model_81_loss: 0.6732 - model_81_1_loss: 0.64930s - loss: 6.6259 - model_81_loss: 0.6756 - model_81_1_loss: 0.64\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1957 - model_80_loss: 0.3984 - model_81_loss: 0.6704 - model_81_1_loss: 0.6484\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2105 - model_80_loss: 0.3983 - model_81_loss: 0.6713 - model_81_1_loss: 0.6504\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2302 - model_80_loss: 0.3992 - model_81_loss: 0.6736 - model_81_1_loss: 0.6523\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2344 - model_80_loss: 0.4013 - model_81_loss: 0.6727 - model_81_1_loss: 0.6545\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2540 - model_80_loss: 0.4043 - model_81_loss: 0.6748 - model_81_1_loss: 0.6568\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.6667 - model_81_loss: 0.6758 - model_81_1_loss: 0.6567\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2512 - model_80_loss: 0.4050 - model_81_loss: 0.6746 - model_81_1_loss: 0.6566\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2755 - model_80_loss: 0.4090 - model_81_loss: 0.6769 - model_81_1_loss: 0.6600\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2699 - model_80_loss: 0.4111 - model_81_loss: 0.6764 - model_81_1_loss: 0.6598\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2913 - model_80_loss: 0.4131 - model_81_loss: 0.6774 - model_81_1_loss: 0.6634\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3082 - model_80_loss: 0.4158 - model_81_loss: 0.6794 - model_81_1_loss: 0.6654\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.7249 - model_81_loss: 0.6790 - model_81_1_loss: 0.6664\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3058 - model_80_loss: 0.4170 - model_81_loss: 0.6791 - model_81_1_loss: 0.6655\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3112 - model_80_loss: 0.4225 - model_81_loss: 0.6804 - model_81_1_loss: 0.6663\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3168 - model_80_loss: 0.4205 - model_81_loss: 0.6800 - model_81_1_loss: 0.6674\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3234 - model_80_loss: 0.4259 - model_81_loss: 0.6808 - model_81_1_loss: 0.6690\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3397 - model_80_loss: 0.4251 - model_81_loss: 0.6817 - model_81_1_loss: 0.6713\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.7722 - model_81_loss: 0.6820 - model_81_1_loss: 0.6718\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3297 - model_80_loss: 0.4311 - model_81_loss: 0.6811 - model_81_1_loss: 0.6710\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3425 - model_80_loss: 0.4313 - model_81_loss: 0.6822 - model_81_1_loss: 0.6726\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3527 - model_80_loss: 0.4367 - model_81_loss: 0.6833 - model_81_1_loss: 0.6746\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3640 - model_80_loss: 0.4390 - model_81_loss: 0.6841 - model_81_1_loss: 0.6765\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3694 - model_80_loss: 0.4394 - model_81_loss: 0.6847 - model_81_1_loss: 0.6771\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.8211 - model_81_loss: 0.6858 - model_81_1_loss: 0.6780\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3697 - model_80_loss: 0.4435 - model_81_loss: 0.6854 - model_81_1_loss: 0.6772\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3779 - model_80_loss: 0.4481 - model_81_loss: 0.6863 - model_81_1_loss: 0.6789\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3832 - model_80_loss: 0.4490 - model_81_loss: 0.6863 - model_81_1_loss: 0.6802\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.3871 - model_80_loss: 0.4521 - model_81_loss: 0.6871 - model_81_1_loss: 0.6808\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3917 - model_80_loss: 0.4553 - model_81_loss: 0.6880 - model_81_1_loss: 0.6814\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.8596 - model_81_loss: 0.6886 - model_81_1_loss: 0.6836\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3951 - model_80_loss: 0.4595 - model_81_loss: 0.6885 - model_81_1_loss: 0.6825\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3957 - model_80_loss: 0.4602 - model_81_loss: 0.6881 - model_81_1_loss: 0.6830\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4041 - model_80_loss: 0.4639 - model_81_loss: 0.6895 - model_81_1_loss: 0.6841\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4056 - model_80_loss: 0.4658 - model_81_loss: 0.6896 - model_81_1_loss: 0.6847\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4063 - model_80_loss: 0.4685 - model_81_loss: 0.6897 - model_81_1_loss: 0.6853\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.8862 - model_81_loss: 0.6908 - model_81_1_loss: 0.6864\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4127 - model_80_loss: 0.4707 - model_81_loss: 0.6909 - model_81_1_loss: 0.6857\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4145 - model_80_loss: 0.4757 - model_81_loss: 0.6909 - model_81_1_loss: 0.6872\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4225 - model_80_loss: 0.4764 - model_81_loss: 0.6920 - model_81_1_loss: 0.6878\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4200 - model_80_loss: 0.4803 - model_81_loss: 0.6918 - model_81_1_loss: 0.6883\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4221 - model_80_loss: 0.4829 - model_81_loss: 0.6919 - model_81_1_loss: 0.6891\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9061 - model_81_loss: 0.6916 - model_81_1_loss: 0.6891\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4239 - model_80_loss: 0.4854 - model_81_loss: 0.6919 - model_81_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4276 - model_80_loss: 0.4870 - model_81_loss: 0.6927 - model_81_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4287 - model_80_loss: 0.4903 - model_81_loss: 0.6928 - model_81_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4289 - model_80_loss: 0.4922 - model_81_loss: 0.6927 - model_81_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4370 - model_80_loss: 0.4924 - model_81_loss: 0.6934 - model_81_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9240 - model_81_loss: 0.6929 - model_81_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4249 - model_80_loss: 0.4947 - model_81_loss: 0.6928 - model_81_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4287 - model_80_loss: 0.4971 - model_81_loss: 0.6931 - model_81_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4296 - model_80_loss: 0.4996 - model_81_loss: 0.6934 - model_81_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4301 - model_80_loss: 0.5006 - model_81_loss: 0.6932 - model_81_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4317 - model_80_loss: 0.5013 - model_81_loss: 0.6934 - model_81_1_loss: 0.6932\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9325 - model_81_loss: 0.6928 - model_81_1_loss: 0.6934\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4276 - model_80_loss: 0.5011 - model_81_loss: 0.6930 - model_81_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4313 - model_80_loss: 0.4994 - model_81_loss: 0.6933 - model_81_1_loss: 0.6929\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4292 - model_80_loss: 0.5008 - model_81_loss: 0.6931 - model_81_1_loss: 0.6929\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4324 - model_80_loss: 0.4972 - model_81_loss: 0.6931 - model_81_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4335 - model_80_loss: 0.4974 - model_81_loss: 0.6929 - model_81_1_loss: 0.6933\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9298 - model_81_loss: 0.6929 - model_81_1_loss: 0.69290s - loss: 6.9039 - model_81_loss: 0.6875 - model_81_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4340 - model_80_loss: 0.4948 - model_81_loss: 0.6928 - model_81_1_loss: 0.6930\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4330 - model_80_loss: 0.4964 - model_81_loss: 0.6927 - model_81_1_loss: 0.6932\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4354 - model_80_loss: 0.4931 - model_81_loss: 0.6926 - model_81_1_loss: 0.6931\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4339 - model_80_loss: 0.4920 - model_81_loss: 0.6925 - model_81_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4386 - model_80_loss: 0.4903 - model_81_loss: 0.6927 - model_81_1_loss: 0.6931\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9256 - model_81_loss: 0.6931 - model_81_1_loss: 0.69260s - loss: 6.9219 - model_81_loss: 0.6920 - model_81_1_loss: 0.692\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4333 - model_80_loss: 0.4903 - model_81_loss: 0.6920 - model_81_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4300 - model_80_loss: 0.4903 - model_81_loss: 0.6916 - model_81_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4328 - model_80_loss: 0.4888 - model_81_loss: 0.6922 - model_81_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4310 - model_80_loss: 0.4862 - model_81_loss: 0.6915 - model_81_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4374 - model_80_loss: 0.4844 - model_81_loss: 0.6920 - model_81_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9185 - model_81_loss: 0.6915 - model_81_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4306 - model_80_loss: 0.4850 - model_81_loss: 0.6910 - model_81_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4292 - model_80_loss: 0.4834 - model_81_loss: 0.6910 - model_81_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4275 - model_80_loss: 0.4836 - model_81_loss: 0.6910 - model_81_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4323 - model_80_loss: 0.4830 - model_81_loss: 0.6912 - model_81_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4342 - model_80_loss: 0.4795 - model_81_loss: 0.6910 - model_81_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9142 - model_81_loss: 0.6914 - model_81_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4281 - model_80_loss: 0.4797 - model_81_loss: 0.6906 - model_81_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4326 - model_80_loss: 0.4784 - model_81_loss: 0.6908 - model_81_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4304 - model_80_loss: 0.4788 - model_81_loss: 0.6906 - model_81_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4328 - model_80_loss: 0.4777 - model_81_loss: 0.6910 - model_81_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4348 - model_80_loss: 0.4777 - model_81_loss: 0.6911 - model_81_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9082 - model_81_loss: 0.6902 - model_81_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4341 - model_80_loss: 0.4772 - model_81_loss: 0.6906 - model_81_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4319 - model_80_loss: 0.4764 - model_81_loss: 0.6906 - model_81_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4318 - model_80_loss: 0.4738 - model_81_loss: 0.6902 - model_81_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4348 - model_80_loss: 0.4752 - model_81_loss: 0.6907 - model_81_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4392 - model_80_loss: 0.4751 - model_81_loss: 0.6909 - model_81_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9156 - model_81_loss: 0.6919 - model_81_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4392 - model_80_loss: 0.4729 - model_81_loss: 0.6909 - model_81_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4382 - model_80_loss: 0.4738 - model_81_loss: 0.6911 - model_81_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4379 - model_80_loss: 0.4729 - model_81_loss: 0.6909 - model_81_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4368 - model_80_loss: 0.4733 - model_81_loss: 0.6906 - model_81_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4416 - model_80_loss: 0.4714 - model_81_loss: 0.6912 - model_81_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9125 - model_81_loss: 0.6907 - model_81_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4426 - model_80_loss: 0.4744 - model_81_loss: 0.6915 - model_81_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4398 - model_80_loss: 0.4737 - model_81_loss: 0.6911 - model_81_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4419 - model_80_loss: 0.4739 - model_81_loss: 0.6912 - model_81_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4415 - model_80_loss: 0.4739 - model_81_loss: 0.6916 - model_81_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4409 - model_80_loss: 0.4724 - model_81_loss: 0.6908 - model_81_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9166 - model_81_loss: 0.6917 - model_81_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4399 - model_80_loss: 0.4767 - model_81_loss: 0.6919 - model_81_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4409 - model_80_loss: 0.4767 - model_81_loss: 0.6920 - model_81_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4437 - model_80_loss: 0.4759 - model_81_loss: 0.6920 - model_81_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4412 - model_80_loss: 0.4760 - model_81_loss: 0.6916 - model_81_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4428 - model_80_loss: 0.4766 - model_81_loss: 0.6919 - model_81_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9226 - model_81_loss: 0.6923 - model_81_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4396 - model_80_loss: 0.4762 - model_81_loss: 0.6914 - model_81_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4419 - model_80_loss: 0.4756 - model_81_loss: 0.6918 - model_81_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4411 - model_80_loss: 0.4756 - model_81_loss: 0.6916 - model_81_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4412 - model_80_loss: 0.4763 - model_81_loss: 0.6918 - model_81_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4423 - model_80_loss: 0.4766 - model_81_loss: 0.6921 - model_81_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9174 - model_81_loss: 0.6915 - model_81_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4392 - model_80_loss: 0.4772 - model_81_loss: 0.6919 - model_81_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4420 - model_80_loss: 0.4754 - model_81_loss: 0.6921 - model_81_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4400 - model_80_loss: 0.4770 - model_81_loss: 0.6920 - model_81_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4417 - model_80_loss: 0.4748 - model_81_loss: 0.6919 - model_81_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4412 - model_80_loss: 0.4751 - model_81_loss: 0.6920 - model_81_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9170 - model_81_loss: 0.6921 - model_81_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4448 - model_80_loss: 0.4746 - model_81_loss: 0.6926 - model_81_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4437 - model_80_loss: 0.4749 - model_81_loss: 0.6924 - model_81_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4425 - model_80_loss: 0.4742 - model_81_loss: 0.6921 - model_81_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4461 - model_80_loss: 0.4732 - model_81_loss: 0.6923 - model_81_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4444 - model_80_loss: 0.4731 - model_81_loss: 0.6922 - model_81_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9195 - model_81_loss: 0.6930 - model_81_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4457 - model_80_loss: 0.4733 - model_81_loss: 0.6920 - model_81_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4494 - model_80_loss: 0.4718 - model_81_loss: 0.6924 - model_81_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4516 - model_80_loss: 0.4712 - model_81_loss: 0.6926 - model_81_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4473 - model_80_loss: 0.4728 - model_81_loss: 0.6921 - model_81_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4504 - model_80_loss: 0.4707 - model_81_loss: 0.6923 - model_81_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9209 - model_81_loss: 0.6918 - model_81_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4486 - model_80_loss: 0.4708 - model_81_loss: 0.6924 - model_81_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4479 - model_80_loss: 0.4707 - model_81_loss: 0.6923 - model_81_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4506 - model_80_loss: 0.4695 - model_81_loss: 0.6924 - model_81_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4489 - model_80_loss: 0.4697 - model_81_loss: 0.6923 - model_81_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4502 - model_80_loss: 0.4686 - model_81_loss: 0.6921 - model_81_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9223 - model_81_loss: 0.6926 - model_81_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4473 - model_80_loss: 0.4694 - model_81_loss: 0.6923 - model_81_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4489 - model_80_loss: 0.4679 - model_81_loss: 0.6921 - model_81_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4517 - model_80_loss: 0.4664 - model_81_loss: 0.6924 - model_81_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4513 - model_80_loss: 0.4685 - model_81_loss: 0.6925 - model_81_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4485 - model_80_loss: 0.4703 - model_81_loss: 0.6924 - model_81_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9224 - model_81_loss: 0.6921 - model_81_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4487 - model_80_loss: 0.4684 - model_81_loss: 0.6923 - model_81_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4487 - model_80_loss: 0.4681 - model_81_loss: 0.6922 - model_81_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4513 - model_80_loss: 0.4700 - model_81_loss: 0.6924 - model_81_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4485 - model_80_loss: 0.4714 - model_81_loss: 0.6924 - model_81_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4532 - model_80_loss: 0.4684 - model_81_loss: 0.6927 - model_81_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9194 - model_81_loss: 0.6923 - model_81_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4490 - model_80_loss: 0.4703 - model_81_loss: 0.6923 - model_81_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4479 - model_80_loss: 0.4708 - model_81_loss: 0.6922 - model_81_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4458 - model_80_loss: 0.4692 - model_81_loss: 0.6920 - model_81_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4492 - model_80_loss: 0.4702 - model_81_loss: 0.6926 - model_81_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4494 - model_80_loss: 0.4683 - model_81_loss: 0.6920 - model_81_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9208 - model_81_loss: 0.6928 - model_81_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4480 - model_80_loss: 0.4706 - model_81_loss: 0.6923 - model_81_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4519 - model_80_loss: 0.4704 - model_81_loss: 0.6924 - model_81_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4485 - model_80_loss: 0.4706 - model_81_loss: 0.6924 - model_81_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4522 - model_80_loss: 0.4700 - model_81_loss: 0.6926 - model_81_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4520 - model_80_loss: 0.4699 - model_81_loss: 0.6925 - model_81_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9236 - model_81_loss: 0.6926 - model_81_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4502 - model_80_loss: 0.4703 - model_81_loss: 0.6924 - model_81_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4535 - model_80_loss: 0.4670 - model_81_loss: 0.6924 - model_81_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4506 - model_80_loss: 0.4694 - model_81_loss: 0.6923 - model_81_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4514 - model_80_loss: 0.4685 - model_81_loss: 0.6923 - model_81_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4518 - model_80_loss: 0.4686 - model_81_loss: 0.6923 - model_81_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9216 - model_81_loss: 0.6918 - model_81_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4499 - model_80_loss: 0.4688 - model_81_loss: 0.6922 - model_81_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4515 - model_80_loss: 0.4663 - model_81_loss: 0.6921 - model_81_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4535 - model_80_loss: 0.4670 - model_81_loss: 0.6924 - model_81_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4512 - model_80_loss: 0.4670 - model_81_loss: 0.6922 - model_81_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4567 - model_80_loss: 0.4666 - model_81_loss: 0.6929 - model_81_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9197 - model_81_loss: 0.6923 - model_81_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4502 - model_80_loss: 0.4660 - model_81_loss: 0.6920 - model_81_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4507 - model_80_loss: 0.4669 - model_81_loss: 0.6922 - model_81_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4484 - model_80_loss: 0.4668 - model_81_loss: 0.6920 - model_81_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4517 - model_80_loss: 0.4647 - model_81_loss: 0.6922 - model_81_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4518 - model_80_loss: 0.4651 - model_81_loss: 0.6923 - model_81_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9138 - model_81_loss: 0.6921 - model_81_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4480 - model_80_loss: 0.4643 - model_81_loss: 0.6916 - model_81_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4510 - model_80_loss: 0.4630 - model_81_loss: 0.6919 - model_81_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4478 - model_80_loss: 0.4640 - model_81_loss: 0.6918 - model_81_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4496 - model_80_loss: 0.4634 - model_81_loss: 0.6918 - model_81_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4469 - model_80_loss: 0.4646 - model_81_loss: 0.6916 - model_81_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9124 - model_81_loss: 0.6911 - model_81_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4452 - model_80_loss: 0.4661 - model_81_loss: 0.6917 - model_81_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4482 - model_80_loss: 0.4646 - model_81_loss: 0.6919 - model_81_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4487 - model_80_loss: 0.4638 - model_81_loss: 0.6916 - model_81_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4485 - model_80_loss: 0.4653 - model_81_loss: 0.6918 - model_81_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4482 - model_80_loss: 0.4668 - model_81_loss: 0.6919 - model_81_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9173 - model_81_loss: 0.6918 - model_81_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4459 - model_80_loss: 0.4684 - model_81_loss: 0.6919 - model_81_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4514 - model_80_loss: 0.4661 - model_81_loss: 0.6923 - model_81_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4537 - model_80_loss: 0.4659 - model_81_loss: 0.6923 - model_81_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4516 - model_80_loss: 0.4675 - model_81_loss: 0.6921 - model_81_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4530 - model_80_loss: 0.4683 - model_81_loss: 0.6927 - model_81_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9232 - model_81_loss: 0.6936 - model_81_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4524 - model_80_loss: 0.4687 - model_81_loss: 0.6926 - model_81_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4530 - model_80_loss: 0.4683 - model_81_loss: 0.6925 - model_81_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4543 - model_80_loss: 0.4687 - model_81_loss: 0.6927 - model_81_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4543 - model_80_loss: 0.4695 - model_81_loss: 0.6925 - model_81_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4524 - model_80_loss: 0.4700 - model_81_loss: 0.6924 - model_81_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: 6.9265 - model_81_loss: 0.6924 - model_81_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4539 - model_80_loss: 0.4696 - model_81_loss: 0.6926 - model_81_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4502 - model_80_loss: 0.4717 - model_81_loss: 0.6925 - model_81_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4528 - model_80_loss: 0.4713 - model_81_loss: 0.6927 - model_81_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4517 - model_80_loss: 0.4714 - model_81_loss: 0.6925 - model_81_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4573 - model_80_loss: 0.4694 - model_81_loss: 0.6930 - model_81_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9248 - model_81_loss: 0.6923 - model_81_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4546 - model_80_loss: 0.4691 - model_81_loss: 0.6925 - model_81_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4519 - model_80_loss: 0.4706 - model_81_loss: 0.6924 - model_81_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4512 - model_80_loss: 0.4704 - model_81_loss: 0.6925 - model_81_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4502 - model_80_loss: 0.4703 - model_81_loss: 0.6923 - model_81_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4511 - model_80_loss: 0.4712 - model_81_loss: 0.6923 - model_81_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9201 - model_81_loss: 0.6924 - model_81_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4523 - model_80_loss: 0.4688 - model_81_loss: 0.6927 - model_81_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4494 - model_80_loss: 0.4698 - model_81_loss: 0.6925 - model_81_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4491 - model_80_loss: 0.4683 - model_81_loss: 0.6922 - model_81_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4525 - model_80_loss: 0.4669 - model_81_loss: 0.6925 - model_81_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4581 - model_80_loss: 0.4658 - model_81_loss: 0.6930 - model_81_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9194 - model_81_loss: 0.6922 - model_81_1_loss: 0.69100s - loss: 6.8987 - model_81_loss: 0.6899 - model_81_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4515 - model_80_loss: 0.4641 - model_81_loss: 0.6924 - model_81_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4526 - model_80_loss: 0.4646 - model_81_loss: 0.6923 - model_81_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4532 - model_80_loss: 0.4643 - model_81_loss: 0.6925 - model_81_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4558 - model_80_loss: 0.4624 - model_81_loss: 0.6925 - model_81_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4555 - model_80_loss: 0.4628 - model_81_loss: 0.6925 - model_81_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9193 - model_81_loss: 0.6921 - model_81_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4530 - model_80_loss: 0.4625 - model_81_loss: 0.6922 - model_81_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4533 - model_80_loss: 0.4615 - model_81_loss: 0.6919 - model_81_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4521 - model_80_loss: 0.4606 - model_81_loss: 0.6918 - model_81_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4559 - model_80_loss: 0.4604 - model_81_loss: 0.6922 - model_81_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4518 - model_80_loss: 0.4613 - model_81_loss: 0.6920 - model_81_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9171 - model_81_loss: 0.6921 - model_81_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4517 - model_80_loss: 0.4622 - model_81_loss: 0.6919 - model_81_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4509 - model_80_loss: 0.4625 - model_81_loss: 0.6919 - model_81_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4541 - model_80_loss: 0.4627 - model_81_loss: 0.6923 - model_81_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4505 - model_80_loss: 0.4644 - model_81_loss: 0.6921 - model_81_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4532 - model_80_loss: 0.4630 - model_81_loss: 0.6920 - model_81_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9167 - model_81_loss: 0.6922 - model_81_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4501 - model_80_loss: 0.4650 - model_81_loss: 0.6919 - model_81_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4501 - model_80_loss: 0.4647 - model_81_loss: 0.6919 - model_81_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4533 - model_80_loss: 0.4635 - model_81_loss: 0.6919 - model_81_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4522 - model_80_loss: 0.4640 - model_81_loss: 0.6921 - model_81_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4539 - model_80_loss: 0.4645 - model_81_loss: 0.6921 - model_81_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9199 - model_81_loss: 0.6922 - model_81_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4524 - model_80_loss: 0.4654 - model_81_loss: 0.6919 - model_81_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4533 - model_80_loss: 0.4661 - model_81_loss: 0.6923 - model_81_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4535 - model_80_loss: 0.4674 - model_81_loss: 0.6923 - model_81_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4525 - model_80_loss: 0.4674 - model_81_loss: 0.6921 - model_81_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4580 - model_80_loss: 0.4682 - model_81_loss: 0.6928 - model_81_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9257 - model_81_loss: 0.6921 - model_81_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4556 - model_80_loss: 0.4676 - model_81_loss: 0.6927 - model_81_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4579 - model_80_loss: 0.4682 - model_81_loss: 0.6928 - model_81_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4579 - model_80_loss: 0.4695 - model_81_loss: 0.6930 - model_81_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4569 - model_80_loss: 0.4690 - model_81_loss: 0.6926 - model_81_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4552 - model_80_loss: 0.4707 - model_81_loss: 0.6928 - model_81_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9265 - model_81_loss: 0.6928 - model_81_1_loss: 0.69250s - loss: 6.9310 - model_81_loss: 0.6937 - model_81_1_loss: 0.692\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4531 - model_80_loss: 0.4707 - model_81_loss: 0.6926 - model_81_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4555 - model_80_loss: 0.4683 - model_81_loss: 0.6924 - model_81_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4543 - model_80_loss: 0.4699 - model_81_loss: 0.6928 - model_81_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4568 - model_80_loss: 0.4683 - model_81_loss: 0.6928 - model_81_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4555 - model_80_loss: 0.4689 - model_81_loss: 0.6927 - model_81_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.9220 - model_81_loss: 0.6923 - model_81_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4537 - model_80_loss: 0.4678 - model_81_loss: 0.6926 - model_81_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 8us/sample - loss: -6.4526 - model_80_loss: 0.4675 - model_81_loss: 0.6924 - model_81_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4565 - model_80_loss: 0.4668 - model_81_loss: 0.6928 - model_81_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4529 - model_80_loss: 0.4687 - model_81_loss: 0.6924 - model_81_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4571 - model_80_loss: 0.4657 - model_81_loss: 0.6925 - model_81_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9209 - model_81_loss: 0.6924 - model_81_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4543 - model_80_loss: 0.4670 - model_81_loss: 0.6928 - model_81_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4526 - model_80_loss: 0.4660 - model_81_loss: 0.6926 - model_81_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4546 - model_80_loss: 0.4658 - model_81_loss: 0.6924 - model_81_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4532 - model_80_loss: 0.4650 - model_81_loss: 0.6924 - model_81_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4551 - model_80_loss: 0.4638 - model_81_loss: 0.6925 - model_81_1_loss: 0.6913\n",
      "For Attention Module: 2.2\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.3867 - model_85_loss: 0.6606 - model_85_1_loss: 0.6165\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.0015 - model_84_loss: 0.3852 - model_85_loss: 0.6594 - model_85_1_loss: 0.6180\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0067 - model_84_loss: 0.3844 - model_85_loss: 0.6591 - model_85_1_loss: 0.6191\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0272 - model_84_loss: 0.3828 - model_85_loss: 0.6614 - model_85_1_loss: 0.6206\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0408 - model_84_loss: 0.3824 - model_85_loss: 0.6604 - model_85_1_loss: 0.6242\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0422 - model_84_loss: 0.3846 - model_85_loss: 0.6611 - model_85_1_loss: 0.6243\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.4259 - model_85_loss: 0.6597 - model_85_1_loss: 0.6248\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.0389 - model_84_loss: 0.3858 - model_85_loss: 0.6612 - model_85_1_loss: 0.6237\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0554 - model_84_loss: 0.3865 - model_85_loss: 0.6616 - model_85_1_loss: 0.6268\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0712 - model_84_loss: 0.3876 - model_85_loss: 0.6622 - model_85_1_loss: 0.6296\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0817 - model_84_loss: 0.3874 - model_85_loss: 0.6625 - model_85_1_loss: 0.6313\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0810 - model_84_loss: 0.3889 - model_85_loss: 0.6624 - model_85_1_loss: 0.6316\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.4886 - model_85_loss: 0.6636 - model_85_1_loss: 0.6340\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.1012 - model_84_loss: 0.3889 - model_85_loss: 0.6642 - model_85_1_loss: 0.6339\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0967 - model_84_loss: 0.3906 - model_85_loss: 0.6633 - model_85_1_loss: 0.6342\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1161 - model_84_loss: 0.3897 - model_85_loss: 0.6642 - model_85_1_loss: 0.6370\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.1304 - model_84_loss: 0.3924 - model_85_loss: 0.6654 - model_85_1_loss: 0.6392\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1480 - model_84_loss: 0.3926 - model_85_loss: 0.6665 - model_85_1_loss: 0.6416\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.5417 - model_85_loss: 0.6655 - model_85_1_loss: 0.64150s - loss: 6.6172 - model_85_loss: 0.6734 - model_85_1_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1436 - model_84_loss: 0.3951 - model_85_loss: 0.6661 - model_85_1_loss: 0.6416\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.1648 - model_84_loss: 0.3938 - model_85_loss: 0.6668 - model_85_1_loss: 0.6449\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1719 - model_84_loss: 0.3974 - model_85_loss: 0.6677 - model_85_1_loss: 0.6461\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1812 - model_84_loss: 0.3979 - model_85_loss: 0.6681 - model_85_1_loss: 0.6477\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1993 - model_84_loss: 0.3989 - model_85_loss: 0.6693 - model_85_1_loss: 0.6504\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.6208 - model_85_loss: 0.6699 - model_85_1_loss: 0.6541\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2132 - model_84_loss: 0.3994 - model_85_loss: 0.6700 - model_85_1_loss: 0.6525\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2315 - model_84_loss: 0.3999 - model_85_loss: 0.6721 - model_85_1_loss: 0.6542\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2363 - model_84_loss: 0.4028 - model_85_loss: 0.6717 - model_85_1_loss: 0.6561\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2498 - model_84_loss: 0.4038 - model_85_loss: 0.6718 - model_85_1_loss: 0.6589\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2596 - model_84_loss: 0.4079 - model_85_loss: 0.6729 - model_85_1_loss: 0.6606\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.6745 - model_85_loss: 0.6739 - model_85_1_loss: 0.6620\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.2570 - model_84_loss: 0.4085 - model_85_loss: 0.6723 - model_85_1_loss: 0.6608\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2772 - model_84_loss: 0.4113 - model_85_loss: 0.6744 - model_85_1_loss: 0.6633\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2816 - model_84_loss: 0.4139 - model_85_loss: 0.6744 - model_85_1_loss: 0.6648\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2908 - model_84_loss: 0.4162 - model_85_loss: 0.6745 - model_85_1_loss: 0.6669\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3048 - model_84_loss: 0.4170 - model_85_loss: 0.6760 - model_85_1_loss: 0.6684\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.7297 - model_85_loss: 0.6767 - model_85_1_loss: 0.6690\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.2995 - model_84_loss: 0.4199 - model_85_loss: 0.6762 - model_85_1_loss: 0.6677\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3188 - model_84_loss: 0.4226 - model_85_loss: 0.6780 - model_85_1_loss: 0.6702\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3267 - model_84_loss: 0.4259 - model_85_loss: 0.6785 - model_85_1_loss: 0.6720\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3343 - model_84_loss: 0.4261 - model_85_loss: 0.6796 - model_85_1_loss: 0.6725\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3429 - model_84_loss: 0.4293 - model_85_loss: 0.6808 - model_85_1_loss: 0.6736\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: 6.7855 - model_85_loss: 0.6827 - model_85_1_loss: 0.6753\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3501 - model_84_loss: 0.4345 - model_85_loss: 0.6815 - model_85_1_loss: 0.6754\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3462 - model_84_loss: 0.4372 - model_85_loss: 0.6803 - model_85_1_loss: 0.6764\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3525 - model_84_loss: 0.4388 - model_85_loss: 0.6808 - model_85_1_loss: 0.6775\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3777 - model_84_loss: 0.4416 - model_85_loss: 0.6847 - model_85_1_loss: 0.6792\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3711 - model_84_loss: 0.4430 - model_85_loss: 0.6829 - model_85_1_loss: 0.6800\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.8233 - model_85_loss: 0.6849 - model_85_1_loss: 0.6801\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3879 - model_84_loss: 0.4469 - model_85_loss: 0.6860 - model_85_1_loss: 0.6810\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3920 - model_84_loss: 0.4484 - model_85_loss: 0.6856 - model_85_1_loss: 0.6824\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3989 - model_84_loss: 0.4516 - model_85_loss: 0.6868 - model_85_1_loss: 0.6833\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3963 - model_84_loss: 0.4560 - model_85_loss: 0.6866 - model_85_1_loss: 0.6838\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4041 - model_84_loss: 0.4607 - model_85_loss: 0.6880 - model_85_1_loss: 0.6849\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.8655 - model_85_loss: 0.6874 - model_85_1_loss: 0.6855\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4004 - model_84_loss: 0.4629 - model_85_loss: 0.6875 - model_85_1_loss: 0.6852\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4077 - model_84_loss: 0.4656 - model_85_loss: 0.6884 - model_85_1_loss: 0.6863\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4088 - model_84_loss: 0.4671 - model_85_loss: 0.6886 - model_85_1_loss: 0.6866\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4132 - model_84_loss: 0.4714 - model_85_loss: 0.6891 - model_85_1_loss: 0.6878\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4146 - model_84_loss: 0.4747 - model_85_loss: 0.6897 - model_85_1_loss: 0.6881\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.8937 - model_85_loss: 0.6900 - model_85_1_loss: 0.6889\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4155 - model_84_loss: 0.4781 - model_85_loss: 0.6899 - model_85_1_loss: 0.6888\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4211 - model_84_loss: 0.4783 - model_85_loss: 0.6904 - model_85_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4241 - model_84_loss: 0.4813 - model_85_loss: 0.6909 - model_85_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4237 - model_84_loss: 0.4818 - model_85_loss: 0.6909 - model_85_1_loss: 0.6902\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4257 - model_84_loss: 0.4842 - model_85_loss: 0.6910 - model_85_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9090 - model_85_loss: 0.6911 - model_85_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4240 - model_84_loss: 0.4842 - model_85_loss: 0.6907 - model_85_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4298 - model_84_loss: 0.4843 - model_85_loss: 0.6916 - model_85_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4282 - model_84_loss: 0.4864 - model_85_loss: 0.6917 - model_85_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4310 - model_84_loss: 0.4891 - model_85_loss: 0.6924 - model_85_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4350 - model_84_loss: 0.4867 - model_85_loss: 0.6922 - model_85_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9214 - model_85_loss: 0.6923 - model_85_1_loss: 0.69170s - loss: 6.9194 - model_85_loss: 0.6923 - model_85_1_loss: 0.691\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4302 - model_84_loss: 0.4895 - model_85_loss: 0.6923 - model_85_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4320 - model_84_loss: 0.4890 - model_85_loss: 0.6923 - model_85_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4347 - model_84_loss: 0.4893 - model_85_loss: 0.6924 - model_85_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4360 - model_84_loss: 0.4884 - model_85_loss: 0.6925 - model_85_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4367 - model_84_loss: 0.4907 - model_85_loss: 0.6929 - model_85_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9282 - model_85_loss: 0.6930 - model_85_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4321 - model_84_loss: 0.4889 - model_85_loss: 0.6922 - model_85_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4367 - model_84_loss: 0.4895 - model_85_loss: 0.6929 - model_85_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4335 - model_84_loss: 0.4897 - model_85_loss: 0.6923 - model_85_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4352 - model_84_loss: 0.4894 - model_85_loss: 0.6924 - model_85_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4368 - model_84_loss: 0.4900 - model_85_loss: 0.6928 - model_85_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9278 - model_85_loss: 0.6933 - model_85_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4371 - model_84_loss: 0.4892 - model_85_loss: 0.6924 - model_85_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4326 - model_84_loss: 0.4883 - model_85_loss: 0.6917 - model_85_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4347 - model_84_loss: 0.4884 - model_85_loss: 0.6923 - model_85_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4337 - model_84_loss: 0.4880 - model_85_loss: 0.6921 - model_85_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4373 - model_84_loss: 0.4863 - model_85_loss: 0.6927 - model_85_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9238 - model_85_loss: 0.6923 - model_85_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4351 - model_84_loss: 0.4852 - model_85_loss: 0.6919 - model_85_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4339 - model_84_loss: 0.4868 - model_85_loss: 0.6919 - model_85_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4323 - model_84_loss: 0.4871 - model_85_loss: 0.6919 - model_85_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4325 - model_84_loss: 0.4873 - model_85_loss: 0.6920 - model_85_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4350 - model_84_loss: 0.4842 - model_85_loss: 0.6920 - model_85_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9182 - model_85_loss: 0.6926 - model_85_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4323 - model_84_loss: 0.4840 - model_85_loss: 0.6915 - model_85_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4344 - model_84_loss: 0.4832 - model_85_loss: 0.6913 - model_85_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4328 - model_84_loss: 0.4831 - model_85_loss: 0.6911 - model_85_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4334 - model_84_loss: 0.4808 - model_85_loss: 0.6909 - model_85_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4313 - model_84_loss: 0.4829 - model_85_loss: 0.6910 - model_85_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9171 - model_85_loss: 0.6914 - model_85_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4270 - model_84_loss: 0.4807 - model_85_loss: 0.6902 - model_85_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4276 - model_84_loss: 0.4814 - model_85_loss: 0.6903 - model_85_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4284 - model_84_loss: 0.4820 - model_85_loss: 0.6905 - model_85_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4294 - model_84_loss: 0.4809 - model_85_loss: 0.6906 - model_85_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4313 - model_84_loss: 0.4809 - model_85_loss: 0.6908 - model_85_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9085 - model_85_loss: 0.6894 - model_85_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4223 - model_84_loss: 0.4799 - model_85_loss: 0.6891 - model_85_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4253 - model_84_loss: 0.4790 - model_85_loss: 0.6891 - model_85_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4289 - model_84_loss: 0.4804 - model_85_loss: 0.6902 - model_85_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4291 - model_84_loss: 0.4797 - model_85_loss: 0.6901 - model_85_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4305 - model_84_loss: 0.4796 - model_85_loss: 0.6904 - model_85_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9084 - model_85_loss: 0.6897 - model_85_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4271 - model_84_loss: 0.4790 - model_85_loss: 0.6900 - model_85_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4246 - model_84_loss: 0.4792 - model_85_loss: 0.6893 - model_85_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4256 - model_84_loss: 0.4798 - model_85_loss: 0.6899 - model_85_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4262 - model_84_loss: 0.4788 - model_85_loss: 0.6895 - model_85_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4334 - model_84_loss: 0.4789 - model_85_loss: 0.6907 - model_85_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9073 - model_85_loss: 0.6901 - model_85_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4197 - model_84_loss: 0.4794 - model_85_loss: 0.6890 - model_85_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4223 - model_84_loss: 0.4791 - model_85_loss: 0.6898 - model_85_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4221 - model_84_loss: 0.4785 - model_85_loss: 0.6896 - model_85_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4262 - model_84_loss: 0.4788 - model_85_loss: 0.6902 - model_85_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4291 - model_84_loss: 0.4757 - model_85_loss: 0.6902 - model_85_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9044 - model_85_loss: 0.6902 - model_85_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4247 - model_84_loss: 0.4778 - model_85_loss: 0.6901 - model_85_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4225 - model_84_loss: 0.4767 - model_85_loss: 0.6898 - model_85_1_loss: 0.6900\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4289 - model_84_loss: 0.4767 - model_85_loss: 0.6909 - model_85_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4307 - model_84_loss: 0.4762 - model_85_loss: 0.6909 - model_85_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4268 - model_84_loss: 0.4762 - model_85_loss: 0.6903 - model_85_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9045 - model_85_loss: 0.6911 - model_85_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4283 - model_84_loss: 0.4741 - model_85_loss: 0.6906 - model_85_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4333 - model_84_loss: 0.4720 - model_85_loss: 0.6909 - model_85_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4335 - model_84_loss: 0.4733 - model_85_loss: 0.6912 - model_85_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4361 - model_84_loss: 0.4720 - model_85_loss: 0.6914 - model_85_1_loss: 0.6902\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4397 - model_84_loss: 0.4713 - model_85_loss: 0.6921 - model_85_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9097 - model_85_loss: 0.6918 - model_85_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4364 - model_84_loss: 0.4707 - model_85_loss: 0.6909 - model_85_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4353 - model_84_loss: 0.4712 - model_85_loss: 0.6909 - model_85_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4357 - model_84_loss: 0.4722 - model_85_loss: 0.6909 - model_85_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4428 - model_84_loss: 0.4714 - model_85_loss: 0.6917 - model_85_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4441 - model_84_loss: 0.4714 - model_85_loss: 0.6917 - model_85_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9159 - model_85_loss: 0.6909 - model_85_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4456 - model_84_loss: 0.4706 - model_85_loss: 0.6918 - model_85_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4477 - model_84_loss: 0.4716 - model_85_loss: 0.6920 - model_85_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4455 - model_84_loss: 0.4724 - model_85_loss: 0.6918 - model_85_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4496 - model_84_loss: 0.4732 - model_85_loss: 0.6924 - model_85_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4495 - model_84_loss: 0.4737 - model_85_loss: 0.6922 - model_85_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9221 - model_85_loss: 0.6921 - model_85_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4427 - model_84_loss: 0.4730 - model_85_loss: 0.6918 - model_85_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4412 - model_84_loss: 0.4749 - model_85_loss: 0.6918 - model_85_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4432 - model_84_loss: 0.4738 - model_85_loss: 0.6920 - model_85_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4408 - model_84_loss: 0.4759 - model_85_loss: 0.6923 - model_85_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4414 - model_84_loss: 0.4753 - model_85_loss: 0.6920 - model_85_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9185 - model_85_loss: 0.6926 - model_85_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4375 - model_84_loss: 0.4763 - model_85_loss: 0.6920 - model_85_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4392 - model_84_loss: 0.4739 - model_85_loss: 0.6920 - model_85_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4385 - model_84_loss: 0.4742 - model_85_loss: 0.6920 - model_85_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4371 - model_84_loss: 0.4757 - model_85_loss: 0.6920 - model_85_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4413 - model_84_loss: 0.4731 - model_85_loss: 0.6924 - model_85_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9127 - model_85_loss: 0.6920 - model_85_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4372 - model_84_loss: 0.4750 - model_85_loss: 0.6924 - model_85_1_loss: 0.6901\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4341 - model_84_loss: 0.4730 - model_85_loss: 0.6916 - model_85_1_loss: 0.6899\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4375 - model_84_loss: 0.4734 - model_85_loss: 0.6919 - model_85_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4359 - model_84_loss: 0.4734 - model_85_loss: 0.6917 - model_85_1_loss: 0.6901\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4369 - model_84_loss: 0.4716 - model_85_loss: 0.6919 - model_85_1_loss: 0.6898\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9140 - model_85_loss: 0.6917 - model_85_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4394 - model_84_loss: 0.4725 - model_85_loss: 0.6920 - model_85_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4397 - model_84_loss: 0.4708 - model_85_loss: 0.6917 - model_85_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4432 - model_84_loss: 0.4702 - model_85_loss: 0.6924 - model_85_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4428 - model_84_loss: 0.4700 - model_85_loss: 0.6919 - model_85_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4428 - model_84_loss: 0.4687 - model_85_loss: 0.6919 - model_85_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9140 - model_85_loss: 0.6916 - model_85_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4461 - model_84_loss: 0.4690 - model_85_loss: 0.6925 - model_85_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4474 - model_84_loss: 0.4680 - model_85_loss: 0.6921 - model_85_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4460 - model_84_loss: 0.4687 - model_85_loss: 0.6923 - model_85_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4479 - model_84_loss: 0.4664 - model_85_loss: 0.6922 - model_85_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4494 - model_84_loss: 0.4674 - model_85_loss: 0.6925 - model_85_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9153 - model_85_loss: 0.6927 - model_85_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4464 - model_84_loss: 0.4656 - model_85_loss: 0.6920 - model_85_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4466 - model_84_loss: 0.4665 - model_85_loss: 0.6924 - model_85_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4474 - model_84_loss: 0.4655 - model_85_loss: 0.6922 - model_85_1_loss: 0.6904\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4465 - model_84_loss: 0.4652 - model_85_loss: 0.6917 - model_85_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4452 - model_84_loss: 0.4655 - model_85_loss: 0.6920 - model_85_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9169 - model_85_loss: 0.6922 - model_85_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4469 - model_84_loss: 0.4669 - model_85_loss: 0.6922 - model_85_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4477 - model_84_loss: 0.4657 - model_85_loss: 0.6919 - model_85_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4463 - model_84_loss: 0.4662 - model_85_loss: 0.6921 - model_85_1_loss: 0.6904\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4479 - model_84_loss: 0.4673 - model_85_loss: 0.6924 - model_85_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4482 - model_84_loss: 0.4660 - model_85_loss: 0.6922 - model_85_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9117 - model_85_loss: 0.6926 - model_85_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4475 - model_84_loss: 0.4640 - model_85_loss: 0.6922 - model_85_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4483 - model_84_loss: 0.4651 - model_85_loss: 0.6920 - model_85_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4503 - model_84_loss: 0.4652 - model_85_loss: 0.6925 - model_85_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4497 - model_84_loss: 0.4641 - model_85_loss: 0.6920 - model_85_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4470 - model_84_loss: 0.4665 - model_85_loss: 0.6919 - model_85_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9161 - model_85_loss: 0.6917 - model_85_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4445 - model_84_loss: 0.4657 - model_85_loss: 0.6916 - model_85_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4444 - model_84_loss: 0.4683 - model_85_loss: 0.6919 - model_85_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4466 - model_84_loss: 0.4689 - model_85_loss: 0.6922 - model_85_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4456 - model_84_loss: 0.4665 - model_85_loss: 0.6917 - model_85_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4516 - model_84_loss: 0.4681 - model_85_loss: 0.6925 - model_85_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9176 - model_85_loss: 0.6936 - model_85_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4486 - model_84_loss: 0.4672 - model_85_loss: 0.6920 - model_85_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4488 - model_84_loss: 0.4692 - model_85_loss: 0.6924 - model_85_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4479 - model_84_loss: 0.4684 - model_85_loss: 0.6920 - model_85_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4475 - model_84_loss: 0.4706 - model_85_loss: 0.6921 - model_85_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4499 - model_84_loss: 0.4704 - model_85_loss: 0.6923 - model_85_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.9234 - model_85_loss: 0.6933 - model_85_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4507 - model_84_loss: 0.4705 - model_85_loss: 0.6924 - model_85_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4507 - model_84_loss: 0.4701 - model_85_loss: 0.6924 - model_85_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4486 - model_84_loss: 0.4722 - model_85_loss: 0.6923 - model_85_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4494 - model_84_loss: 0.4733 - model_85_loss: 0.6926 - model_85_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4520 - model_84_loss: 0.4720 - model_85_loss: 0.6925 - model_85_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9264 - model_85_loss: 0.6928 - model_85_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4495 - model_84_loss: 0.4730 - model_85_loss: 0.6924 - model_85_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4506 - model_84_loss: 0.4730 - model_85_loss: 0.6924 - model_85_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4481 - model_84_loss: 0.4730 - model_85_loss: 0.6921 - model_85_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4492 - model_84_loss: 0.4730 - model_85_loss: 0.6924 - model_85_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4499 - model_84_loss: 0.4728 - model_85_loss: 0.6924 - model_85_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9234 - model_85_loss: 0.6920 - model_85_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4469 - model_84_loss: 0.4731 - model_85_loss: 0.6920 - model_85_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4511 - model_84_loss: 0.4724 - model_85_loss: 0.6925 - model_85_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4482 - model_84_loss: 0.4727 - model_85_loss: 0.6924 - model_85_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4497 - model_84_loss: 0.4738 - model_85_loss: 0.6927 - model_85_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4499 - model_84_loss: 0.4731 - model_85_loss: 0.6928 - model_85_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9201 - model_85_loss: 0.6921 - model_85_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4481 - model_84_loss: 0.4711 - model_85_loss: 0.6920 - model_85_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4502 - model_84_loss: 0.4697 - model_85_loss: 0.6924 - model_85_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4522 - model_84_loss: 0.4670 - model_85_loss: 0.6923 - model_85_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4512 - model_84_loss: 0.4705 - model_85_loss: 0.6925 - model_85_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4504 - model_84_loss: 0.4688 - model_85_loss: 0.6922 - model_85_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9211 - model_85_loss: 0.6925 - model_85_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4505 - model_84_loss: 0.4674 - model_85_loss: 0.6923 - model_85_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4481 - model_84_loss: 0.4668 - model_85_loss: 0.6921 - model_85_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4468 - model_84_loss: 0.4680 - model_85_loss: 0.6921 - model_85_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4501 - model_84_loss: 0.4661 - model_85_loss: 0.6922 - model_85_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4501 - model_84_loss: 0.4654 - model_85_loss: 0.6923 - model_85_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9148 - model_85_loss: 0.6927 - model_85_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4531 - model_84_loss: 0.4646 - model_85_loss: 0.6922 - model_85_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4520 - model_84_loss: 0.4635 - model_85_loss: 0.6920 - model_85_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4524 - model_84_loss: 0.4654 - model_85_loss: 0.6923 - model_85_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4547 - model_84_loss: 0.4639 - model_85_loss: 0.6926 - model_85_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4512 - model_84_loss: 0.4632 - model_85_loss: 0.6920 - model_85_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9155 - model_85_loss: 0.6932 - model_85_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4501 - model_84_loss: 0.4616 - model_85_loss: 0.6915 - model_85_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4499 - model_84_loss: 0.4622 - model_85_loss: 0.6916 - model_85_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4470 - model_84_loss: 0.4629 - model_85_loss: 0.6915 - model_85_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4510 - model_84_loss: 0.4627 - model_85_loss: 0.6921 - model_85_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4474 - model_84_loss: 0.4622 - model_85_loss: 0.6916 - model_85_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9127 - model_85_loss: 0.6917 - model_85_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4491 - model_84_loss: 0.4637 - model_85_loss: 0.6919 - model_85_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4525 - model_84_loss: 0.4622 - model_85_loss: 0.6919 - model_85_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4537 - model_84_loss: 0.4630 - model_85_loss: 0.6924 - model_85_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4531 - model_84_loss: 0.4621 - model_85_loss: 0.6917 - model_85_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4552 - model_84_loss: 0.4640 - model_85_loss: 0.6926 - model_85_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9206 - model_85_loss: 0.6921 - model_85_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4522 - model_84_loss: 0.4631 - model_85_loss: 0.6920 - model_85_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4503 - model_84_loss: 0.4640 - model_85_loss: 0.6917 - model_85_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4512 - model_84_loss: 0.4648 - model_85_loss: 0.6922 - model_85_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4534 - model_84_loss: 0.4639 - model_85_loss: 0.6919 - model_85_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4537 - model_84_loss: 0.4667 - model_85_loss: 0.6923 - model_85_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9158 - model_85_loss: 0.6925 - model_85_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4507 - model_84_loss: 0.4678 - model_85_loss: 0.6921 - model_85_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4526 - model_84_loss: 0.4660 - model_85_loss: 0.6921 - model_85_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4539 - model_84_loss: 0.4654 - model_85_loss: 0.6920 - model_85_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4525 - model_84_loss: 0.4667 - model_85_loss: 0.6922 - model_85_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4551 - model_84_loss: 0.4678 - model_85_loss: 0.6925 - model_85_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9228 - model_85_loss: 0.6921 - model_85_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4553 - model_84_loss: 0.4671 - model_85_loss: 0.6925 - model_85_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4521 - model_84_loss: 0.4671 - model_85_loss: 0.6922 - model_85_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4523 - model_84_loss: 0.4705 - model_85_loss: 0.6926 - model_85_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4544 - model_84_loss: 0.4685 - model_85_loss: 0.6926 - model_85_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4532 - model_84_loss: 0.4702 - model_85_loss: 0.6928 - model_85_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9238 - model_85_loss: 0.6929 - model_85_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4555 - model_84_loss: 0.4686 - model_85_loss: 0.6927 - model_85_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4566 - model_84_loss: 0.4686 - model_85_loss: 0.6928 - model_85_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4537 - model_84_loss: 0.4693 - model_85_loss: 0.6924 - model_85_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4538 - model_84_loss: 0.4691 - model_85_loss: 0.6926 - model_85_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4568 - model_84_loss: 0.4683 - model_85_loss: 0.6928 - model_85_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9249 - model_85_loss: 0.6926 - model_85_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4515 - model_84_loss: 0.4699 - model_85_loss: 0.6922 - model_85_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4553 - model_84_loss: 0.4674 - model_85_loss: 0.6924 - model_85_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4513 - model_84_loss: 0.4678 - model_85_loss: 0.6919 - model_85_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4518 - model_84_loss: 0.4675 - model_85_loss: 0.6921 - model_85_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4538 - model_84_loss: 0.4674 - model_85_loss: 0.6923 - model_85_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9201 - model_85_loss: 0.6922 - model_85_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4492 - model_84_loss: 0.4674 - model_85_loss: 0.6919 - model_85_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4505 - model_84_loss: 0.4667 - model_85_loss: 0.6922 - model_85_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4544 - model_84_loss: 0.4639 - model_85_loss: 0.6924 - model_85_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4509 - model_84_loss: 0.4656 - model_85_loss: 0.6922 - model_85_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4530 - model_84_loss: 0.4644 - model_85_loss: 0.6924 - model_85_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9179 - model_85_loss: 0.6928 - model_85_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4501 - model_84_loss: 0.4656 - model_85_loss: 0.6921 - model_85_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4534 - model_84_loss: 0.4625 - model_85_loss: 0.6923 - model_85_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4495 - model_84_loss: 0.4651 - model_85_loss: 0.6920 - model_85_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4537 - model_84_loss: 0.4627 - model_85_loss: 0.6923 - model_85_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4506 - model_84_loss: 0.4628 - model_85_loss: 0.6921 - model_85_1_loss: 0.6906\n",
      "For Attention Module: 2.3000000000000003\n",
      "features X: 30940 samples, 71 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.3597 - model_89_loss: 0.6595 - model_89_1_loss: 0.6120\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -5.9760 - model_88_loss: 0.3752 - model_89_loss: 0.6592 - model_89_1_loss: 0.6110\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -5.9848 - model_88_loss: 0.3737 - model_89_loss: 0.6600 - model_89_1_loss: 0.6117\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -5.9992 - model_88_loss: 0.3766 - model_89_loss: 0.6612 - model_89_1_loss: 0.6140\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.0026 - model_88_loss: 0.3757 - model_89_loss: 0.6594 - model_89_1_loss: 0.6163\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.0202 - model_88_loss: 0.3765 - model_89_loss: 0.6610 - model_89_1_loss: 0.6184\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.4201 - model_89_loss: 0.6612 - model_89_1_loss: 0.6221\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0334 - model_88_loss: 0.3777 - model_89_loss: 0.6615 - model_89_1_loss: 0.6207\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0444 - model_88_loss: 0.3768 - model_89_loss: 0.6619 - model_89_1_loss: 0.6224\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0485 - model_88_loss: 0.3785 - model_89_loss: 0.6610 - model_89_1_loss: 0.6244\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.0581 - model_88_loss: 0.3791 - model_89_loss: 0.6619 - model_89_1_loss: 0.6255\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0731 - model_88_loss: 0.3790 - model_89_loss: 0.6619 - model_89_1_loss: 0.6285\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.4673 - model_89_loss: 0.6617 - model_89_1_loss: 0.6310\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0818 - model_88_loss: 0.3799 - model_89_loss: 0.6621 - model_89_1_loss: 0.6303\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0896 - model_88_loss: 0.3835 - model_89_loss: 0.6626 - model_89_1_loss: 0.6321\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1076 - model_88_loss: 0.3830 - model_89_loss: 0.6641 - model_89_1_loss: 0.6340\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1221 - model_88_loss: 0.3845 - model_89_loss: 0.6643 - model_89_1_loss: 0.6370\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1286 - model_88_loss: 0.3855 - model_89_loss: 0.6645 - model_89_1_loss: 0.6383\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.5332 - model_89_loss: 0.6662 - model_89_1_loss: 0.6403\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.1464 - model_88_loss: 0.3868 - model_89_loss: 0.6662 - model_89_1_loss: 0.6404\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1497 - model_88_loss: 0.3895 - model_89_loss: 0.6654 - model_89_1_loss: 0.6425\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1652 - model_88_loss: 0.3889 - model_89_loss: 0.6672 - model_89_1_loss: 0.6437\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1747 - model_88_loss: 0.3898 - model_89_loss: 0.6672 - model_89_1_loss: 0.6457\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1850 - model_88_loss: 0.3928 - model_89_loss: 0.6676 - model_89_1_loss: 0.6480\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.5976 - model_89_loss: 0.6692 - model_89_1_loss: 0.6510\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1975 - model_88_loss: 0.3940 - model_89_loss: 0.6685 - model_89_1_loss: 0.6498\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2159 - model_88_loss: 0.3990 - model_89_loss: 0.6704 - model_89_1_loss: 0.6526\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2310 - model_88_loss: 0.3996 - model_89_loss: 0.6700 - model_89_1_loss: 0.6561\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2300 - model_88_loss: 0.4017 - model_89_loss: 0.6708 - model_89_1_loss: 0.6555\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2440 - model_88_loss: 0.4030 - model_89_loss: 0.6711 - model_89_1_loss: 0.6583\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.6629 - model_89_loss: 0.6725 - model_89_1_loss: 0.6595\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.2544 - model_88_loss: 0.4028 - model_89_loss: 0.6718 - model_89_1_loss: 0.6596\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2584 - model_88_loss: 0.4072 - model_89_loss: 0.6720 - model_89_1_loss: 0.6611\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2774 - model_88_loss: 0.4083 - model_89_loss: 0.6740 - model_89_1_loss: 0.6631\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.2820 - model_88_loss: 0.4108 - model_89_loss: 0.6746 - model_89_1_loss: 0.6640\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2964 - model_88_loss: 0.4123 - model_89_loss: 0.6761 - model_89_1_loss: 0.6656\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.7095 - model_89_loss: 0.6743 - model_89_1_loss: 0.6674\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3018 - model_88_loss: 0.4141 - model_89_loss: 0.6763 - model_89_1_loss: 0.6669\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3114 - model_88_loss: 0.4159 - model_89_loss: 0.6768 - model_89_1_loss: 0.6687\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3175 - model_88_loss: 0.4156 - model_89_loss: 0.6771 - model_89_1_loss: 0.6695\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3248 - model_88_loss: 0.4198 - model_89_loss: 0.6778 - model_89_1_loss: 0.6711\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3328 - model_88_loss: 0.4230 - model_89_loss: 0.6790 - model_89_1_loss: 0.6722\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: 6.7678 - model_89_loss: 0.6800 - model_89_1_loss: 0.6736\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3414 - model_88_loss: 0.4236 - model_89_loss: 0.6791 - model_89_1_loss: 0.6739\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3415 - model_88_loss: 0.4278 - model_89_loss: 0.6793 - model_89_1_loss: 0.6746\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.3540 - model_88_loss: 0.4286 - model_89_loss: 0.6805 - model_89_1_loss: 0.6760\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3583 - model_88_loss: 0.4328 - model_89_loss: 0.6814 - model_89_1_loss: 0.6769\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3661 - model_88_loss: 0.4343 - model_89_loss: 0.6819 - model_89_1_loss: 0.6782\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.8089 - model_89_loss: 0.6832 - model_89_1_loss: 0.6792\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3708 - model_88_loss: 0.4375 - model_89_loss: 0.6825 - model_89_1_loss: 0.6792\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3713 - model_88_loss: 0.4413 - model_89_loss: 0.6827 - model_89_1_loss: 0.6798\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3818 - model_88_loss: 0.4417 - model_89_loss: 0.6840 - model_89_1_loss: 0.6807\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3914 - model_88_loss: 0.4451 - model_89_loss: 0.6850 - model_89_1_loss: 0.6823\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3924 - model_88_loss: 0.4483 - model_89_loss: 0.6852 - model_89_1_loss: 0.6830\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.8420 - model_89_loss: 0.6851 - model_89_1_loss: 0.68330s - loss: 6.8150 - model_89_loss: 0.6810 - model_89_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3958 - model_88_loss: 0.4499 - model_89_loss: 0.6856 - model_89_1_loss: 0.6836\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3997 - model_88_loss: 0.4504 - model_89_loss: 0.6858 - model_89_1_loss: 0.6843\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4076 - model_88_loss: 0.4551 - model_89_loss: 0.6874 - model_89_1_loss: 0.6851\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4041 - model_88_loss: 0.4574 - model_89_loss: 0.6870 - model_89_1_loss: 0.6853\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4092 - model_88_loss: 0.4593 - model_89_loss: 0.6871 - model_89_1_loss: 0.6866\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.8803 - model_89_loss: 0.6892 - model_89_1_loss: 0.6873\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4091 - model_88_loss: 0.4630 - model_89_loss: 0.6877 - model_89_1_loss: 0.6867\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4158 - model_88_loss: 0.4644 - model_89_loss: 0.6886 - model_89_1_loss: 0.6874\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4152 - model_88_loss: 0.4690 - model_89_loss: 0.6886 - model_89_1_loss: 0.6882\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4244 - model_88_loss: 0.4669 - model_89_loss: 0.6896 - model_89_1_loss: 0.6887\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4248 - model_88_loss: 0.4698 - model_89_loss: 0.6895 - model_89_1_loss: 0.6894\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.8950 - model_89_loss: 0.6894 - model_89_1_loss: 0.6893\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4296 - model_88_loss: 0.4728 - model_89_loss: 0.6894 - model_89_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4372 - model_88_loss: 0.4743 - model_89_loss: 0.6910 - model_89_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4345 - model_88_loss: 0.4783 - model_89_loss: 0.6904 - model_89_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4379 - model_88_loss: 0.4792 - model_89_loss: 0.6909 - model_89_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4445 - model_88_loss: 0.4824 - model_89_loss: 0.6918 - model_89_1_loss: 0.6936\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9207 - model_89_loss: 0.6912 - model_89_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4333 - model_88_loss: 0.4857 - model_89_loss: 0.6913 - model_89_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4311 - model_88_loss: 0.4900 - model_89_loss: 0.6918 - model_89_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4341 - model_88_loss: 0.4922 - model_89_loss: 0.6923 - model_89_1_loss: 0.6930\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4352 - model_88_loss: 0.4919 - model_89_loss: 0.6924 - model_89_1_loss: 0.6930\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4377 - model_88_loss: 0.4946 - model_89_loss: 0.6932 - model_89_1_loss: 0.6932\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9308 - model_89_loss: 0.6923 - model_89_1_loss: 0.6934\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4334 - model_88_loss: 0.4938 - model_89_loss: 0.6924 - model_89_1_loss: 0.6930\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4304 - model_88_loss: 0.4959 - model_89_loss: 0.6920 - model_89_1_loss: 0.6932\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4308 - model_88_loss: 0.4976 - model_89_loss: 0.6925 - model_89_1_loss: 0.6932\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4325 - model_88_loss: 0.4983 - model_89_loss: 0.6929 - model_89_1_loss: 0.6932\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4368 - model_88_loss: 0.4956 - model_89_loss: 0.6932 - model_89_1_loss: 0.6933\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9298 - model_89_loss: 0.6923 - model_89_1_loss: 0.6933\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4300 - model_88_loss: 0.4972 - model_89_loss: 0.6923 - model_89_1_loss: 0.6931\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4296 - model_88_loss: 0.4965 - model_89_loss: 0.6922 - model_89_1_loss: 0.6930\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4330 - model_88_loss: 0.4952 - model_89_loss: 0.6926 - model_89_1_loss: 0.6931\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4326 - model_88_loss: 0.4943 - model_89_loss: 0.6924 - model_89_1_loss: 0.6930\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4311 - model_88_loss: 0.4941 - model_89_loss: 0.6923 - model_89_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9233 - model_89_loss: 0.6915 - model_89_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4301 - model_88_loss: 0.4901 - model_89_loss: 0.6917 - model_89_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4310 - model_88_loss: 0.4898 - model_89_loss: 0.6917 - model_89_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4308 - model_88_loss: 0.4893 - model_89_loss: 0.6916 - model_89_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4344 - model_88_loss: 0.4871 - model_89_loss: 0.6919 - model_89_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4376 - model_88_loss: 0.4831 - model_89_loss: 0.6920 - model_89_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9168 - model_89_loss: 0.6914 - model_89_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4338 - model_88_loss: 0.4828 - model_89_loss: 0.6910 - model_89_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4368 - model_88_loss: 0.4793 - model_89_loss: 0.6911 - model_89_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4358 - model_88_loss: 0.4793 - model_89_loss: 0.6909 - model_89_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4344 - model_88_loss: 0.4785 - model_89_loss: 0.6906 - model_89_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4349 - model_88_loss: 0.4788 - model_89_loss: 0.6909 - model_89_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9136 - model_89_loss: 0.6913 - model_89_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4347 - model_88_loss: 0.4763 - model_89_loss: 0.6908 - model_89_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4391 - model_88_loss: 0.4732 - model_89_loss: 0.6910 - model_89_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4343 - model_88_loss: 0.4741 - model_89_loss: 0.6906 - model_89_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4388 - model_88_loss: 0.4705 - model_89_loss: 0.6907 - model_89_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4359 - model_88_loss: 0.4743 - model_89_loss: 0.6907 - model_89_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9097 - model_89_loss: 0.6908 - model_89_1_loss: 0.69100s - loss: 6.9057 - model_89_loss: 0.6918 - model_89_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4390 - model_88_loss: 0.4703 - model_89_loss: 0.6906 - model_89_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4401 - model_88_loss: 0.4688 - model_89_loss: 0.6906 - model_89_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4390 - model_88_loss: 0.4700 - model_89_loss: 0.6906 - model_89_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4398 - model_88_loss: 0.4682 - model_89_loss: 0.6905 - model_89_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4423 - model_88_loss: 0.4685 - model_89_loss: 0.6908 - model_89_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9076 - model_89_loss: 0.6910 - model_89_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4380 - model_88_loss: 0.4685 - model_89_loss: 0.6906 - model_89_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4362 - model_88_loss: 0.4694 - model_89_loss: 0.6905 - model_89_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4390 - model_88_loss: 0.4671 - model_89_loss: 0.6906 - model_89_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4388 - model_88_loss: 0.4659 - model_89_loss: 0.6906 - model_89_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4436 - model_88_loss: 0.4673 - model_89_loss: 0.6910 - model_89_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9087 - model_89_loss: 0.6907 - model_89_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4400 - model_88_loss: 0.4674 - model_89_loss: 0.6908 - model_89_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4383 - model_88_loss: 0.4678 - model_89_loss: 0.6905 - model_89_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4398 - model_88_loss: 0.4681 - model_89_loss: 0.6910 - model_89_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4415 - model_88_loss: 0.4663 - model_89_loss: 0.6907 - model_89_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4399 - model_88_loss: 0.4691 - model_89_loss: 0.6908 - model_89_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9085 - model_89_loss: 0.6910 - model_89_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4426 - model_88_loss: 0.4673 - model_89_loss: 0.6910 - model_89_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4435 - model_88_loss: 0.4664 - model_89_loss: 0.6911 - model_89_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4430 - model_88_loss: 0.4681 - model_89_loss: 0.6911 - model_89_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4440 - model_88_loss: 0.4666 - model_89_loss: 0.6910 - model_89_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4443 - model_88_loss: 0.4663 - model_89_loss: 0.6910 - model_89_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9091 - model_89_loss: 0.6909 - model_89_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4394 - model_88_loss: 0.4675 - model_89_loss: 0.6912 - model_89_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4441 - model_88_loss: 0.4680 - model_89_loss: 0.6919 - model_89_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4421 - model_88_loss: 0.4661 - model_89_loss: 0.6913 - model_89_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4416 - model_88_loss: 0.4676 - model_89_loss: 0.6913 - model_89_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4433 - model_88_loss: 0.4677 - model_89_loss: 0.6918 - model_89_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9118 - model_89_loss: 0.6918 - model_89_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4398 - model_88_loss: 0.4673 - model_89_loss: 0.6914 - model_89_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4412 - model_88_loss: 0.4684 - model_89_loss: 0.6917 - model_89_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4439 - model_88_loss: 0.4677 - model_89_loss: 0.6918 - model_89_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4440 - model_88_loss: 0.4686 - model_89_loss: 0.6919 - model_89_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4440 - model_88_loss: 0.4681 - model_89_loss: 0.6919 - model_89_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9124 - model_89_loss: 0.6920 - model_89_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4406 - model_88_loss: 0.4697 - model_89_loss: 0.6920 - model_89_1_loss: 0.6901\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4431 - model_88_loss: 0.4696 - model_89_loss: 0.6920 - model_89_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4439 - model_88_loss: 0.4701 - model_89_loss: 0.6923 - model_89_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4443 - model_88_loss: 0.4696 - model_89_loss: 0.6922 - model_89_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4509 - model_88_loss: 0.4683 - model_89_loss: 0.6927 - model_89_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9190 - model_89_loss: 0.6921 - model_89_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4481 - model_88_loss: 0.4708 - model_89_loss: 0.6922 - model_89_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4498 - model_88_loss: 0.4699 - model_89_loss: 0.6923 - model_89_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4513 - model_88_loss: 0.4715 - model_89_loss: 0.6929 - model_89_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4535 - model_88_loss: 0.4710 - model_89_loss: 0.6930 - model_89_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4531 - model_88_loss: 0.4727 - model_89_loss: 0.6931 - model_89_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9230 - model_89_loss: 0.6920 - model_89_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4474 - model_88_loss: 0.4745 - model_89_loss: 0.6922 - model_89_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4496 - model_88_loss: 0.4733 - model_89_loss: 0.6924 - model_89_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4472 - model_88_loss: 0.4757 - model_89_loss: 0.6925 - model_89_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4498 - model_88_loss: 0.4755 - model_89_loss: 0.6928 - model_89_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4461 - model_88_loss: 0.4773 - model_89_loss: 0.6925 - model_89_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9247 - model_89_loss: 0.6929 - model_89_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4433 - model_88_loss: 0.4779 - model_89_loss: 0.6919 - model_89_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4473 - model_88_loss: 0.4754 - model_89_loss: 0.6926 - model_89_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4502 - model_88_loss: 0.4766 - model_89_loss: 0.6928 - model_89_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4437 - model_88_loss: 0.4768 - model_89_loss: 0.6919 - model_89_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4487 - model_88_loss: 0.4747 - model_89_loss: 0.6922 - model_89_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9253 - model_89_loss: 0.6923 - model_89_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4465 - model_88_loss: 0.4772 - model_89_loss: 0.6925 - model_89_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4471 - model_88_loss: 0.4751 - model_89_loss: 0.6923 - model_89_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4489 - model_88_loss: 0.4743 - model_89_loss: 0.6926 - model_89_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4486 - model_88_loss: 0.4753 - model_89_loss: 0.6926 - model_89_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4506 - model_88_loss: 0.4734 - model_89_loss: 0.6923 - model_89_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9232 - model_89_loss: 0.6924 - model_89_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4512 - model_88_loss: 0.4732 - model_89_loss: 0.6925 - model_89_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4488 - model_88_loss: 0.4726 - model_89_loss: 0.6925 - model_89_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4541 - model_88_loss: 0.4718 - model_89_loss: 0.6930 - model_89_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4484 - model_88_loss: 0.4717 - model_89_loss: 0.6923 - model_89_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4490 - model_88_loss: 0.4736 - model_89_loss: 0.6927 - model_89_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9236 - model_89_loss: 0.6923 - model_89_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4503 - model_88_loss: 0.4709 - model_89_loss: 0.6926 - model_89_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4496 - model_88_loss: 0.4697 - model_89_loss: 0.6920 - model_89_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4507 - model_88_loss: 0.4699 - model_89_loss: 0.6926 - model_89_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4496 - model_88_loss: 0.4714 - model_89_loss: 0.6924 - model_89_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4515 - model_88_loss: 0.4690 - model_89_loss: 0.6925 - model_89_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9200 - model_89_loss: 0.6918 - model_89_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4480 - model_88_loss: 0.4696 - model_89_loss: 0.6922 - model_89_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4481 - model_88_loss: 0.4695 - model_89_loss: 0.6923 - model_89_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4463 - model_88_loss: 0.4699 - model_89_loss: 0.6926 - model_89_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4477 - model_88_loss: 0.4695 - model_89_loss: 0.6922 - model_89_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4529 - model_88_loss: 0.4677 - model_89_loss: 0.6927 - model_89_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9187 - model_89_loss: 0.6924 - model_89_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4540 - model_88_loss: 0.4681 - model_89_loss: 0.6927 - model_89_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4557 - model_88_loss: 0.4697 - model_89_loss: 0.6932 - model_89_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4530 - model_88_loss: 0.4699 - model_89_loss: 0.6927 - model_89_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4532 - model_88_loss: 0.4702 - model_89_loss: 0.6926 - model_89_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4600 - model_88_loss: 0.4700 - model_89_loss: 0.6934 - model_89_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9249 - model_89_loss: 0.6927 - model_89_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4531 - model_88_loss: 0.4705 - model_89_loss: 0.6929 - model_89_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4528 - model_88_loss: 0.4714 - model_89_loss: 0.6928 - model_89_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4525 - model_88_loss: 0.4728 - model_89_loss: 0.6930 - model_89_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4508 - model_88_loss: 0.4745 - model_89_loss: 0.6931 - model_89_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4546 - model_88_loss: 0.4717 - model_89_loss: 0.6931 - model_89_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9277 - model_89_loss: 0.6929 - model_89_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4518 - model_88_loss: 0.4730 - model_89_loss: 0.6928 - model_89_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4515 - model_88_loss: 0.4729 - model_89_loss: 0.6926 - model_89_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4508 - model_88_loss: 0.4728 - model_89_loss: 0.6928 - model_89_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4516 - model_88_loss: 0.4731 - model_89_loss: 0.6927 - model_89_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4489 - model_88_loss: 0.4726 - model_89_loss: 0.6923 - model_89_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9248 - model_89_loss: 0.6924 - model_89_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4483 - model_88_loss: 0.4718 - model_89_loss: 0.6919 - model_89_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4492 - model_88_loss: 0.4713 - model_89_loss: 0.6921 - model_89_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4483 - model_88_loss: 0.4716 - model_89_loss: 0.6921 - model_89_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4518 - model_88_loss: 0.4689 - model_89_loss: 0.6922 - model_89_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4534 - model_88_loss: 0.4682 - model_89_loss: 0.6922 - model_89_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9196 - model_89_loss: 0.6921 - model_89_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4508 - model_88_loss: 0.4676 - model_89_loss: 0.6923 - model_89_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4554 - model_88_loss: 0.4638 - model_89_loss: 0.6925 - model_89_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4554 - model_88_loss: 0.4630 - model_89_loss: 0.6924 - model_89_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4532 - model_88_loss: 0.4638 - model_89_loss: 0.6925 - model_89_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4528 - model_88_loss: 0.4619 - model_89_loss: 0.6920 - model_89_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9195 - model_89_loss: 0.6926 - model_89_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4521 - model_88_loss: 0.4616 - model_89_loss: 0.6920 - model_89_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4541 - model_88_loss: 0.4610 - model_89_loss: 0.6921 - model_89_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4547 - model_88_loss: 0.4603 - model_89_loss: 0.6922 - model_89_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4550 - model_88_loss: 0.4604 - model_89_loss: 0.6921 - model_89_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4541 - model_88_loss: 0.4619 - model_89_loss: 0.6923 - model_89_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9144 - model_89_loss: 0.6929 - model_89_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4505 - model_88_loss: 0.4615 - model_89_loss: 0.6923 - model_89_1_loss: 0.6901\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4538 - model_88_loss: 0.4589 - model_89_loss: 0.6924 - model_89_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4518 - model_88_loss: 0.4588 - model_89_loss: 0.6921 - model_89_1_loss: 0.6900\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4512 - model_88_loss: 0.4605 - model_89_loss: 0.6922 - model_89_1_loss: 0.6901\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4528 - model_88_loss: 0.4600 - model_89_loss: 0.6923 - model_89_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9142 - model_89_loss: 0.6933 - model_89_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4523 - model_88_loss: 0.4613 - model_89_loss: 0.6925 - model_89_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4533 - model_88_loss: 0.4619 - model_89_loss: 0.6924 - model_89_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4476 - model_88_loss: 0.4628 - model_89_loss: 0.6920 - model_89_1_loss: 0.6900\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4507 - model_88_loss: 0.4626 - model_89_loss: 0.6924 - model_89_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4515 - model_88_loss: 0.4624 - model_89_loss: 0.6924 - model_89_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: 6.9169 - model_89_loss: 0.6933 - model_89_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4536 - model_88_loss: 0.4642 - model_89_loss: 0.6925 - model_89_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4536 - model_88_loss: 0.4649 - model_89_loss: 0.6927 - model_89_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4560 - model_88_loss: 0.4642 - model_89_loss: 0.6928 - model_89_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4562 - model_88_loss: 0.4656 - model_89_loss: 0.6928 - model_89_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4563 - model_88_loss: 0.4652 - model_89_loss: 0.6928 - model_89_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9213 - model_89_loss: 0.6924 - model_89_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4550 - model_88_loss: 0.4664 - model_89_loss: 0.6930 - model_89_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4542 - model_88_loss: 0.4665 - model_89_loss: 0.6927 - model_89_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 9us/sample - loss: -6.4574 - model_88_loss: 0.4664 - model_89_loss: 0.6928 - model_89_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4542 - model_88_loss: 0.4683 - model_89_loss: 0.6929 - model_89_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4573 - model_88_loss: 0.4680 - model_89_loss: 0.6929 - model_89_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9239 - model_89_loss: 0.6940 - model_89_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4547 - model_88_loss: 0.4685 - model_89_loss: 0.6926 - model_89_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4541 - model_88_loss: 0.4692 - model_89_loss: 0.6927 - model_89_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4522 - model_88_loss: 0.4701 - model_89_loss: 0.6926 - model_89_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4537 - model_88_loss: 0.4691 - model_89_loss: 0.6927 - model_89_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4507 - model_88_loss: 0.4703 - model_89_loss: 0.6926 - model_89_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9221 - model_89_loss: 0.6931 - model_89_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4544 - model_88_loss: 0.4675 - model_89_loss: 0.6925 - model_89_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4519 - model_88_loss: 0.4678 - model_89_loss: 0.6923 - model_89_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4544 - model_88_loss: 0.4667 - model_89_loss: 0.6923 - model_89_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4527 - model_88_loss: 0.4675 - model_89_loss: 0.6921 - model_89_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4536 - model_88_loss: 0.4651 - model_89_loss: 0.6921 - model_89_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9181 - model_89_loss: 0.6926 - model_89_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4477 - model_88_loss: 0.4656 - model_89_loss: 0.6916 - model_89_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4511 - model_88_loss: 0.4639 - model_89_loss: 0.6918 - model_89_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4524 - model_88_loss: 0.4626 - model_89_loss: 0.6920 - model_89_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4558 - model_88_loss: 0.4625 - model_89_loss: 0.6923 - model_89_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4532 - model_88_loss: 0.4611 - model_89_loss: 0.6920 - model_89_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9156 - model_89_loss: 0.6919 - model_89_1_loss: 0.69110s - loss: 6.9226 - model_89_loss: 0.6933 - model_89_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4549 - model_88_loss: 0.4602 - model_89_loss: 0.6920 - model_89_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4564 - model_88_loss: 0.4603 - model_89_loss: 0.6922 - model_89_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4591 - model_88_loss: 0.4580 - model_89_loss: 0.6925 - model_89_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4588 - model_88_loss: 0.4576 - model_89_loss: 0.6924 - model_89_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4606 - model_88_loss: 0.4568 - model_89_loss: 0.6928 - model_89_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9128 - model_89_loss: 0.6924 - model_89_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4507 - model_88_loss: 0.4563 - model_89_loss: 0.6919 - model_89_1_loss: 0.6895\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4518 - model_88_loss: 0.4573 - model_89_loss: 0.6923 - model_89_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4517 - model_88_loss: 0.4554 - model_89_loss: 0.6918 - model_89_1_loss: 0.6896\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4512 - model_88_loss: 0.4559 - model_89_loss: 0.6921 - model_89_1_loss: 0.6893\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4545 - model_88_loss: 0.4559 - model_89_loss: 0.6921 - model_89_1_loss: 0.6899\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9085 - model_89_loss: 0.6935 - model_89_1_loss: 0.6892\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4493 - model_88_loss: 0.4569 - model_89_loss: 0.6922 - model_89_1_loss: 0.6890\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4478 - model_88_loss: 0.4560 - model_89_loss: 0.6917 - model_89_1_loss: 0.6890\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4484 - model_88_loss: 0.4566 - model_89_loss: 0.6919 - model_89_1_loss: 0.6891\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4496 - model_88_loss: 0.4562 - model_89_loss: 0.6920 - model_89_1_loss: 0.6891\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4509 - model_88_loss: 0.4554 - model_89_loss: 0.6921 - model_89_1_loss: 0.6892\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9113 - model_89_loss: 0.6919 - model_89_1_loss: 0.6899\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4499 - model_88_loss: 0.4574 - model_89_loss: 0.6919 - model_89_1_loss: 0.6896\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4514 - model_88_loss: 0.4590 - model_89_loss: 0.6921 - model_89_1_loss: 0.6900\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4514 - model_88_loss: 0.4582 - model_89_loss: 0.6921 - model_89_1_loss: 0.6898\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4529 - model_88_loss: 0.4593 - model_89_loss: 0.6921 - model_89_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4523 - model_88_loss: 0.4610 - model_89_loss: 0.6924 - model_89_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9153 - model_89_loss: 0.6923 - model_89_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4534 - model_88_loss: 0.4616 - model_89_loss: 0.6921 - model_89_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4547 - model_88_loss: 0.4606 - model_89_loss: 0.6921 - model_89_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4579 - model_88_loss: 0.4598 - model_89_loss: 0.6922 - model_89_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4553 - model_88_loss: 0.4634 - model_89_loss: 0.6923 - model_89_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4592 - model_88_loss: 0.4641 - model_89_loss: 0.6924 - model_89_1_loss: 0.6922\n",
      "For Attention Module: 2.4000000000000004\n",
      "features X: 30940 samples, 71 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.3564 - model_93_loss: 0.6606 - model_93_1_loss: 0.6105\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -5.9725 - model_92_loss: 0.3757 - model_93_loss: 0.6613 - model_93_1_loss: 0.6083\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -5.9831 - model_92_loss: 0.3759 - model_93_loss: 0.6610 - model_93_1_loss: 0.6108\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -5.9918 - model_92_loss: 0.3751 - model_93_loss: 0.6610 - model_93_1_loss: 0.6124\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0021 - model_92_loss: 0.3745 - model_93_loss: 0.6607 - model_93_1_loss: 0.6146\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0155 - model_92_loss: 0.3776 - model_93_loss: 0.6622 - model_93_1_loss: 0.6164\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.3915 - model_93_loss: 0.6618 - model_93_1_loss: 0.6168\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0288 - model_92_loss: 0.3767 - model_93_loss: 0.6632 - model_93_1_loss: 0.6179\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0298 - model_92_loss: 0.3771 - model_93_loss: 0.6629 - model_93_1_loss: 0.6185\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0493 - model_92_loss: 0.3779 - model_93_loss: 0.6630 - model_93_1_loss: 0.6225\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0547 - model_92_loss: 0.3808 - model_93_loss: 0.6629 - model_93_1_loss: 0.6241\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0735 - model_92_loss: 0.3804 - model_93_loss: 0.6653 - model_93_1_loss: 0.6254\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.4574 - model_93_loss: 0.6642 - model_93_1_loss: 0.6269\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.0723 - model_92_loss: 0.3817 - model_93_loss: 0.6651 - model_93_1_loss: 0.6257\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0810 - model_92_loss: 0.3837 - model_93_loss: 0.6652 - model_93_1_loss: 0.6278\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.0832 - model_92_loss: 0.3838 - model_93_loss: 0.6648 - model_93_1_loss: 0.6285\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1060 - model_92_loss: 0.3836 - model_93_loss: 0.6657 - model_93_1_loss: 0.6322\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1223 - model_92_loss: 0.3849 - model_93_loss: 0.6676 - model_93_1_loss: 0.6339\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.5193 - model_93_loss: 0.6674 - model_93_1_loss: 0.63630s - loss: 6.4910 - model_93_loss: 0.6589 - model_93_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1299 - model_92_loss: 0.3865 - model_93_loss: 0.6676 - model_93_1_loss: 0.6357\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1306 - model_92_loss: 0.3873 - model_93_loss: 0.6674 - model_93_1_loss: 0.6362\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1411 - model_92_loss: 0.3895 - model_93_loss: 0.6680 - model_93_1_loss: 0.6381\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1564 - model_92_loss: 0.3906 - model_93_loss: 0.6678 - model_93_1_loss: 0.6415\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1838 - model_92_loss: 0.3915 - model_93_loss: 0.6703 - model_93_1_loss: 0.6447\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.5717 - model_93_loss: 0.6705 - model_93_1_loss: 0.6441\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1944 - model_92_loss: 0.3929 - model_93_loss: 0.6714 - model_93_1_loss: 0.6460\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.1887 - model_92_loss: 0.3944 - model_93_loss: 0.6702 - model_93_1_loss: 0.6464\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2046 - model_92_loss: 0.3953 - model_93_loss: 0.6722 - model_93_1_loss: 0.6478\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2121 - model_92_loss: 0.3976 - model_93_loss: 0.6727 - model_93_1_loss: 0.6493\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2264 - model_92_loss: 0.4004 - model_93_loss: 0.6734 - model_93_1_loss: 0.6520\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.6475 - model_93_loss: 0.6748 - model_93_1_loss: 0.6546\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2361 - model_92_loss: 0.4014 - model_93_loss: 0.6738 - model_93_1_loss: 0.6538\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2538 - model_92_loss: 0.4029 - model_93_loss: 0.6755 - model_93_1_loss: 0.6559\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2586 - model_92_loss: 0.4058 - model_93_loss: 0.6758 - model_93_1_loss: 0.6571\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2662 - model_92_loss: 0.4072 - model_93_loss: 0.6764 - model_93_1_loss: 0.6583\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2821 - model_92_loss: 0.4085 - model_93_loss: 0.6774 - model_93_1_loss: 0.6608\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.6970 - model_93_loss: 0.6777 - model_93_1_loss: 0.6616\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2842 - model_92_loss: 0.4119 - model_93_loss: 0.6777 - model_93_1_loss: 0.6615\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.2927 - model_92_loss: 0.4137 - model_93_loss: 0.6779 - model_93_1_loss: 0.6634\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3065 - model_92_loss: 0.4148 - model_93_loss: 0.6788 - model_93_1_loss: 0.6655\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3048 - model_92_loss: 0.4184 - model_93_loss: 0.6786 - model_93_1_loss: 0.6660\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3325 - model_92_loss: 0.4185 - model_93_loss: 0.6809 - model_93_1_loss: 0.6693\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.7566 - model_93_loss: 0.6817 - model_93_1_loss: 0.6699\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3313 - model_92_loss: 0.4213 - model_93_loss: 0.6808 - model_93_1_loss: 0.6697\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3363 - model_92_loss: 0.4265 - model_93_loss: 0.6818 - model_93_1_loss: 0.6708\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3364 - model_92_loss: 0.4280 - model_93_loss: 0.6812 - model_93_1_loss: 0.6717\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3496 - model_92_loss: 0.4284 - model_93_loss: 0.6825 - model_93_1_loss: 0.6731\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3636 - model_92_loss: 0.4344 - model_93_loss: 0.6841 - model_93_1_loss: 0.6755\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.7958 - model_93_loss: 0.6845 - model_93_1_loss: 0.6754\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3609 - model_92_loss: 0.4339 - model_93_loss: 0.6842 - model_93_1_loss: 0.6748\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3612 - model_92_loss: 0.4368 - model_93_loss: 0.6835 - model_93_1_loss: 0.6761\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3683 - model_92_loss: 0.4395 - model_93_loss: 0.6847 - model_93_1_loss: 0.6769\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3784 - model_92_loss: 0.4416 - model_93_loss: 0.6852 - model_93_1_loss: 0.6788\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3825 - model_92_loss: 0.4425 - model_93_loss: 0.6852 - model_93_1_loss: 0.6799\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.8326 - model_93_loss: 0.6860 - model_93_1_loss: 0.6804\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3775 - model_92_loss: 0.4508 - model_93_loss: 0.6856 - model_93_1_loss: 0.6800\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3964 - model_92_loss: 0.4516 - model_93_loss: 0.6872 - model_93_1_loss: 0.6824\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3978 - model_92_loss: 0.4536 - model_93_loss: 0.6870 - model_93_1_loss: 0.6832\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3974 - model_92_loss: 0.4552 - model_93_loss: 0.6875 - model_93_1_loss: 0.6830\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4050 - model_92_loss: 0.4573 - model_93_loss: 0.6877 - model_93_1_loss: 0.6847\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.8666 - model_93_loss: 0.6896 - model_93_1_loss: 0.6849\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4037 - model_92_loss: 0.4625 - model_93_loss: 0.6882 - model_93_1_loss: 0.6850\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4052 - model_92_loss: 0.4660 - model_93_loss: 0.6885 - model_93_1_loss: 0.6857\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4092 - model_92_loss: 0.4681 - model_93_loss: 0.6892 - model_93_1_loss: 0.6862\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4167 - model_92_loss: 0.4680 - model_93_loss: 0.6896 - model_93_1_loss: 0.6873\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4168 - model_92_loss: 0.4749 - model_93_loss: 0.6901 - model_93_1_loss: 0.6882\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.8957 - model_93_loss: 0.6909 - model_93_1_loss: 0.6886\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4153 - model_92_loss: 0.4780 - model_93_loss: 0.6907 - model_93_1_loss: 0.6879\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4183 - model_92_loss: 0.4791 - model_93_loss: 0.6908 - model_93_1_loss: 0.6886\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4144 - model_92_loss: 0.4838 - model_93_loss: 0.6905 - model_93_1_loss: 0.6892\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4248 - model_92_loss: 0.4848 - model_93_loss: 0.6917 - model_93_1_loss: 0.6902\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4246 - model_92_loss: 0.4843 - model_93_loss: 0.6914 - model_93_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9137 - model_93_loss: 0.6912 - model_93_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4207 - model_92_loss: 0.4891 - model_93_loss: 0.6914 - model_93_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4280 - model_92_loss: 0.4881 - model_93_loss: 0.6920 - model_93_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4227 - model_92_loss: 0.4937 - model_93_loss: 0.6923 - model_93_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4251 - model_92_loss: 0.4926 - model_93_loss: 0.6921 - model_93_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4289 - model_92_loss: 0.4950 - model_93_loss: 0.6931 - model_93_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9246 - model_93_loss: 0.6923 - model_93_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4235 - model_92_loss: 0.4958 - model_93_loss: 0.6921 - model_93_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4201 - model_92_loss: 0.4992 - model_93_loss: 0.6921 - model_93_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4270 - model_92_loss: 0.4976 - model_93_loss: 0.6928 - model_93_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4292 - model_92_loss: 0.4961 - model_93_loss: 0.6930 - model_93_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4272 - model_92_loss: 0.4974 - model_93_loss: 0.6929 - model_93_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9261 - model_93_loss: 0.6939 - model_93_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4286 - model_92_loss: 0.4953 - model_93_loss: 0.6925 - model_93_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4264 - model_92_loss: 0.4962 - model_93_loss: 0.6925 - model_93_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4259 - model_92_loss: 0.4967 - model_93_loss: 0.6926 - model_93_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4281 - model_92_loss: 0.4938 - model_93_loss: 0.6923 - model_93_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4280 - model_92_loss: 0.4923 - model_93_loss: 0.6922 - model_93_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9237 - model_93_loss: 0.6920 - model_93_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4321 - model_92_loss: 0.4885 - model_93_loss: 0.6921 - model_93_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4306 - model_92_loss: 0.4892 - model_93_loss: 0.6919 - model_93_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4299 - model_92_loss: 0.4893 - model_93_loss: 0.6920 - model_93_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4323 - model_92_loss: 0.4872 - model_93_loss: 0.6920 - model_93_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4325 - model_92_loss: 0.4856 - model_93_loss: 0.6919 - model_93_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9211 - model_93_loss: 0.6930 - model_93_1_loss: 0.69180s - loss: 6.9163 - model_93_loss: 0.6916 - model_93_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4346 - model_92_loss: 0.4833 - model_93_loss: 0.6921 - model_93_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4359 - model_92_loss: 0.4821 - model_93_loss: 0.6921 - model_93_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4335 - model_92_loss: 0.4822 - model_93_loss: 0.6920 - model_93_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4324 - model_92_loss: 0.4826 - model_93_loss: 0.6916 - model_93_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4369 - model_92_loss: 0.4785 - model_93_loss: 0.6918 - model_93_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9172 - model_93_loss: 0.6914 - model_93_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4334 - model_92_loss: 0.4805 - model_93_loss: 0.6915 - model_93_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4333 - model_92_loss: 0.4799 - model_93_loss: 0.6915 - model_93_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4322 - model_92_loss: 0.4792 - model_93_loss: 0.6911 - model_93_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4343 - model_92_loss: 0.4789 - model_93_loss: 0.6914 - model_93_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4361 - model_92_loss: 0.4777 - model_93_loss: 0.6914 - model_93_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9124 - model_93_loss: 0.6919 - model_93_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4388 - model_92_loss: 0.4762 - model_93_loss: 0.6917 - model_93_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4359 - model_92_loss: 0.4762 - model_93_loss: 0.6914 - model_93_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4356 - model_92_loss: 0.4766 - model_93_loss: 0.6914 - model_93_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4386 - model_92_loss: 0.4743 - model_93_loss: 0.6918 - model_93_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4347 - model_92_loss: 0.4754 - model_93_loss: 0.6913 - model_93_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9147 - model_93_loss: 0.6914 - model_93_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4345 - model_92_loss: 0.4754 - model_93_loss: 0.6908 - model_93_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4327 - model_92_loss: 0.4752 - model_93_loss: 0.6908 - model_93_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4342 - model_92_loss: 0.4753 - model_93_loss: 0.6911 - model_93_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4352 - model_92_loss: 0.4754 - model_93_loss: 0.6914 - model_93_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4339 - model_92_loss: 0.4742 - model_93_loss: 0.6909 - model_93_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9122 - model_93_loss: 0.6918 - model_93_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4372 - model_92_loss: 0.4752 - model_93_loss: 0.6914 - model_93_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4354 - model_92_loss: 0.4753 - model_93_loss: 0.6917 - model_93_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4394 - model_92_loss: 0.4740 - model_93_loss: 0.6917 - model_93_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4386 - model_92_loss: 0.4743 - model_93_loss: 0.6916 - model_93_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4387 - model_92_loss: 0.4733 - model_93_loss: 0.6917 - model_93_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9151 - model_93_loss: 0.6914 - model_93_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4392 - model_92_loss: 0.4738 - model_93_loss: 0.6914 - model_93_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4385 - model_92_loss: 0.4736 - model_93_loss: 0.6915 - model_93_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4399 - model_92_loss: 0.4736 - model_93_loss: 0.6917 - model_93_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4396 - model_92_loss: 0.4732 - model_93_loss: 0.6916 - model_93_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4397 - model_92_loss: 0.4722 - model_93_loss: 0.6914 - model_93_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: 6.9137 - model_93_loss: 0.6916 - model_93_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4404 - model_92_loss: 0.4729 - model_93_loss: 0.6917 - model_93_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4381 - model_92_loss: 0.4721 - model_93_loss: 0.6914 - model_93_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4387 - model_92_loss: 0.4710 - model_93_loss: 0.6913 - model_93_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4424 - model_92_loss: 0.4717 - model_93_loss: 0.6916 - model_93_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4411 - model_92_loss: 0.4724 - model_93_loss: 0.6918 - model_93_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9137 - model_93_loss: 0.6908 - model_93_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4463 - model_92_loss: 0.4723 - model_93_loss: 0.6920 - model_93_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4414 - model_92_loss: 0.4732 - model_93_loss: 0.6914 - model_93_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4427 - model_92_loss: 0.4735 - model_93_loss: 0.6920 - model_93_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4484 - model_92_loss: 0.4714 - model_93_loss: 0.6921 - model_93_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4489 - model_92_loss: 0.4710 - model_93_loss: 0.6922 - model_93_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9166 - model_93_loss: 0.6919 - model_93_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4412 - model_92_loss: 0.4723 - model_93_loss: 0.6917 - model_93_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4389 - model_92_loss: 0.4730 - model_93_loss: 0.6917 - model_93_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4409 - model_92_loss: 0.4725 - model_93_loss: 0.6919 - model_93_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4408 - model_92_loss: 0.4720 - model_93_loss: 0.6915 - model_93_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4432 - model_92_loss: 0.4699 - model_93_loss: 0.6916 - model_93_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.9150 - model_93_loss: 0.6932 - model_93_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4431 - model_92_loss: 0.4716 - model_93_loss: 0.6922 - model_93_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4438 - model_92_loss: 0.4715 - model_93_loss: 0.6921 - model_93_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4429 - model_92_loss: 0.4727 - model_93_loss: 0.6919 - model_93_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4440 - model_92_loss: 0.4723 - model_93_loss: 0.6922 - model_93_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4431 - model_92_loss: 0.4710 - model_93_loss: 0.6917 - model_93_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9157 - model_93_loss: 0.6927 - model_93_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4451 - model_92_loss: 0.4719 - model_93_loss: 0.6918 - model_93_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4434 - model_92_loss: 0.4718 - model_93_loss: 0.6917 - model_93_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4460 - model_92_loss: 0.4724 - model_93_loss: 0.6923 - model_93_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4441 - model_92_loss: 0.4725 - model_93_loss: 0.6920 - model_93_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4474 - model_92_loss: 0.4724 - model_93_loss: 0.6919 - model_93_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9189 - model_93_loss: 0.6919 - model_93_1_loss: 0.69150s - loss: 6.9105 - model_93_loss: 0.6889 - model_93_1_lo\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4461 - model_92_loss: 0.4710 - model_93_loss: 0.6918 - model_93_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4445 - model_92_loss: 0.4733 - model_93_loss: 0.6918 - model_93_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4436 - model_92_loss: 0.4739 - model_93_loss: 0.6919 - model_93_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4470 - model_92_loss: 0.4739 - model_93_loss: 0.6921 - model_93_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4473 - model_92_loss: 0.4733 - model_93_loss: 0.6920 - model_93_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.9227 - model_93_loss: 0.6921 - model_93_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4466 - model_92_loss: 0.4748 - model_93_loss: 0.6922 - model_93_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4474 - model_92_loss: 0.4740 - model_93_loss: 0.6922 - model_93_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4451 - model_92_loss: 0.4749 - model_93_loss: 0.6918 - model_93_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4467 - model_92_loss: 0.4762 - model_93_loss: 0.6921 - model_93_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4477 - model_92_loss: 0.4758 - model_93_loss: 0.6922 - model_93_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9241 - model_93_loss: 0.6927 - model_93_1_loss: 0.69220s - loss: 6.9527 - model_93_loss: 0.6963 - model_93_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4447 - model_92_loss: 0.4750 - model_93_loss: 0.6921 - model_93_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4456 - model_92_loss: 0.4760 - model_93_loss: 0.6924 - model_93_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4463 - model_92_loss: 0.4741 - model_93_loss: 0.6921 - model_93_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4464 - model_92_loss: 0.4743 - model_93_loss: 0.6919 - model_93_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4456 - model_92_loss: 0.4745 - model_93_loss: 0.6921 - model_93_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9208 - model_93_loss: 0.6920 - model_93_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4476 - model_92_loss: 0.4740 - model_93_loss: 0.6924 - model_93_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4479 - model_92_loss: 0.4740 - model_93_loss: 0.6924 - model_93_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4454 - model_92_loss: 0.4754 - model_93_loss: 0.6925 - model_93_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4453 - model_92_loss: 0.4746 - model_93_loss: 0.6923 - model_93_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4462 - model_92_loss: 0.4733 - model_93_loss: 0.6919 - model_93_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9230 - model_93_loss: 0.6926 - model_93_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4501 - model_92_loss: 0.4723 - model_93_loss: 0.6924 - model_93_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4476 - model_92_loss: 0.4730 - model_93_loss: 0.6922 - model_93_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4524 - model_92_loss: 0.4726 - model_93_loss: 0.6928 - model_93_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4511 - model_92_loss: 0.4707 - model_93_loss: 0.6925 - model_93_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4509 - model_92_loss: 0.4717 - model_93_loss: 0.6926 - model_93_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9221 - model_93_loss: 0.6934 - model_93_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4470 - model_92_loss: 0.4722 - model_93_loss: 0.6922 - model_93_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4484 - model_92_loss: 0.4711 - model_93_loss: 0.6922 - model_93_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4490 - model_92_loss: 0.4702 - model_93_loss: 0.6921 - model_93_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4491 - model_92_loss: 0.4699 - model_93_loss: 0.6922 - model_93_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4502 - model_92_loss: 0.4678 - model_93_loss: 0.6922 - model_93_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9194 - model_93_loss: 0.6922 - model_93_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4474 - model_92_loss: 0.4688 - model_93_loss: 0.6917 - model_93_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4481 - model_92_loss: 0.4680 - model_93_loss: 0.6917 - model_93_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4501 - model_92_loss: 0.4666 - model_93_loss: 0.6919 - model_93_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4494 - model_92_loss: 0.4677 - model_93_loss: 0.6917 - model_93_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4502 - model_92_loss: 0.4667 - model_93_loss: 0.6920 - model_93_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9175 - model_93_loss: 0.6920 - model_93_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4491 - model_92_loss: 0.4698 - model_93_loss: 0.6924 - model_93_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4479 - model_92_loss: 0.4674 - model_93_loss: 0.6919 - model_93_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4498 - model_92_loss: 0.4682 - model_93_loss: 0.6920 - model_93_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4513 - model_92_loss: 0.4649 - model_93_loss: 0.6920 - model_93_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4528 - model_92_loss: 0.4661 - model_93_loss: 0.6924 - model_93_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9159 - model_93_loss: 0.6917 - model_93_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4503 - model_92_loss: 0.4666 - model_93_loss: 0.6919 - model_93_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4527 - model_92_loss: 0.4662 - model_93_loss: 0.6920 - model_93_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4511 - model_92_loss: 0.4683 - model_93_loss: 0.6923 - model_93_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4538 - model_92_loss: 0.4674 - model_93_loss: 0.6924 - model_93_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4508 - model_92_loss: 0.4686 - model_93_loss: 0.6922 - model_93_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9229 - model_93_loss: 0.6936 - model_93_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4537 - model_92_loss: 0.4684 - model_93_loss: 0.6929 - model_93_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4544 - model_92_loss: 0.4672 - model_93_loss: 0.6925 - model_93_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4521 - model_92_loss: 0.4708 - model_93_loss: 0.6928 - model_93_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4536 - model_92_loss: 0.4682 - model_93_loss: 0.6927 - model_93_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4552 - model_92_loss: 0.4692 - model_93_loss: 0.6929 - model_93_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9236 - model_93_loss: 0.6926 - model_93_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4505 - model_92_loss: 0.4691 - model_93_loss: 0.6923 - model_93_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4535 - model_92_loss: 0.4686 - model_93_loss: 0.6926 - model_93_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4507 - model_92_loss: 0.4703 - model_93_loss: 0.6924 - model_93_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4514 - model_92_loss: 0.4693 - model_93_loss: 0.6926 - model_93_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4520 - model_92_loss: 0.4694 - model_93_loss: 0.6925 - model_93_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9221 - model_93_loss: 0.6927 - model_93_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4523 - model_92_loss: 0.4708 - model_93_loss: 0.6927 - model_93_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4531 - model_92_loss: 0.4695 - model_93_loss: 0.6924 - model_93_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4559 - model_92_loss: 0.4686 - model_93_loss: 0.6926 - model_93_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4521 - model_92_loss: 0.4689 - model_93_loss: 0.6924 - model_93_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4540 - model_92_loss: 0.4691 - model_93_loss: 0.6926 - model_93_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9239 - model_93_loss: 0.6930 - model_93_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4536 - model_92_loss: 0.4683 - model_93_loss: 0.6924 - model_93_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4523 - model_92_loss: 0.4680 - model_93_loss: 0.6922 - model_93_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4527 - model_92_loss: 0.4685 - model_93_loss: 0.6925 - model_93_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4525 - model_92_loss: 0.4696 - model_93_loss: 0.6927 - model_93_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4536 - model_92_loss: 0.4668 - model_93_loss: 0.6923 - model_93_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9210 - model_93_loss: 0.6924 - model_93_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4517 - model_92_loss: 0.4669 - model_93_loss: 0.6922 - model_93_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4534 - model_92_loss: 0.4666 - model_93_loss: 0.6922 - model_93_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4532 - model_92_loss: 0.4654 - model_93_loss: 0.6924 - model_93_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4512 - model_92_loss: 0.4654 - model_93_loss: 0.6921 - model_93_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4548 - model_92_loss: 0.4640 - model_93_loss: 0.6922 - model_93_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9194 - model_93_loss: 0.6923 - model_93_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4536 - model_92_loss: 0.4642 - model_93_loss: 0.6925 - model_93_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4521 - model_92_loss: 0.4641 - model_93_loss: 0.6922 - model_93_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4520 - model_92_loss: 0.4642 - model_93_loss: 0.6922 - model_93_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4528 - model_92_loss: 0.4641 - model_93_loss: 0.6923 - model_93_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4539 - model_92_loss: 0.4639 - model_93_loss: 0.6924 - model_93_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: 6.9191 - model_93_loss: 0.6928 - model_93_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4521 - model_92_loss: 0.4641 - model_93_loss: 0.6921 - model_93_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4539 - model_92_loss: 0.4631 - model_93_loss: 0.6923 - model_93_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4548 - model_92_loss: 0.4620 - model_93_loss: 0.6922 - model_93_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4545 - model_92_loss: 0.4628 - model_93_loss: 0.6923 - model_93_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4569 - model_92_loss: 0.4625 - model_93_loss: 0.6924 - model_93_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9206 - model_93_loss: 0.6925 - model_93_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4567 - model_92_loss: 0.4610 - model_93_loss: 0.6922 - model_93_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4532 - model_92_loss: 0.4633 - model_93_loss: 0.6922 - model_93_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4561 - model_92_loss: 0.4616 - model_93_loss: 0.6922 - model_93_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4539 - model_92_loss: 0.4628 - model_93_loss: 0.6921 - model_93_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4539 - model_92_loss: 0.4634 - model_93_loss: 0.6921 - model_93_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9173 - model_93_loss: 0.6921 - model_93_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4548 - model_92_loss: 0.4624 - model_93_loss: 0.6925 - model_93_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4562 - model_92_loss: 0.4608 - model_93_loss: 0.6924 - model_93_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4546 - model_92_loss: 0.4642 - model_93_loss: 0.6925 - model_93_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4544 - model_92_loss: 0.4630 - model_93_loss: 0.6924 - model_93_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4527 - model_92_loss: 0.4646 - model_93_loss: 0.6922 - model_93_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9190 - model_93_loss: 0.6924 - model_93_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4560 - model_92_loss: 0.4626 - model_93_loss: 0.6922 - model_93_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4545 - model_92_loss: 0.4632 - model_93_loss: 0.6921 - model_93_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4557 - model_92_loss: 0.4637 - model_93_loss: 0.6925 - model_93_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4558 - model_92_loss: 0.4641 - model_93_loss: 0.6925 - model_93_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4546 - model_92_loss: 0.4666 - model_93_loss: 0.6923 - model_93_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9214 - model_93_loss: 0.6922 - model_93_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4528 - model_92_loss: 0.4672 - model_93_loss: 0.6923 - model_93_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4545 - model_92_loss: 0.4670 - model_93_loss: 0.6925 - model_93_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4530 - model_92_loss: 0.4678 - model_93_loss: 0.6924 - model_93_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4547 - model_92_loss: 0.4667 - model_93_loss: 0.6924 - model_93_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4519 - model_92_loss: 0.4684 - model_93_loss: 0.6922 - model_93_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9238 - model_93_loss: 0.6921 - model_93_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4556 - model_92_loss: 0.4678 - model_93_loss: 0.6927 - model_93_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4543 - model_92_loss: 0.4675 - model_93_loss: 0.6926 - model_93_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4558 - model_92_loss: 0.4683 - model_93_loss: 0.6926 - model_93_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4548 - model_92_loss: 0.4680 - model_93_loss: 0.6925 - model_93_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4538 - model_92_loss: 0.4683 - model_93_loss: 0.6925 - model_93_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9270 - model_93_loss: 0.6941 - model_93_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4563 - model_92_loss: 0.4673 - model_93_loss: 0.6928 - model_93_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4573 - model_92_loss: 0.4673 - model_93_loss: 0.6928 - model_93_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4556 - model_92_loss: 0.4677 - model_93_loss: 0.6925 - model_93_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4552 - model_92_loss: 0.4683 - model_93_loss: 0.6927 - model_93_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4550 - model_92_loss: 0.4675 - model_93_loss: 0.6926 - model_93_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9232 - model_93_loss: 0.6924 - model_93_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4572 - model_92_loss: 0.4664 - model_93_loss: 0.6927 - model_93_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4581 - model_92_loss: 0.4662 - model_93_loss: 0.6928 - model_93_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4572 - model_92_loss: 0.4653 - model_93_loss: 0.6927 - model_93_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4578 - model_92_loss: 0.4656 - model_93_loss: 0.6928 - model_93_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4581 - model_92_loss: 0.4642 - model_93_loss: 0.6927 - model_93_1_loss: 0.6918\n",
      "For Attention Module: 2.5000000000000004\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.3401 - model_97_loss: 0.6605 - model_97_1_loss: 0.6081\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -5.9729 - model_96_loss: 0.3769 - model_97_loss: 0.6610 - model_97_1_loss: 0.6090\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -5.9916 - model_96_loss: 0.3766 - model_97_loss: 0.6617 - model_97_1_loss: 0.6120\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.0147 - model_96_loss: 0.3761 - model_97_loss: 0.6621 - model_97_1_loss: 0.6161\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0165 - model_96_loss: 0.3794 - model_97_loss: 0.6619 - model_97_1_loss: 0.6172\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0393 - model_96_loss: 0.3782 - model_97_loss: 0.6636 - model_97_1_loss: 0.6199\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.4132 - model_97_loss: 0.6632 - model_97_1_loss: 0.6187\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0157 - model_96_loss: 0.3789 - model_97_loss: 0.6621 - model_97_1_loss: 0.6168\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0395 - model_96_loss: 0.3787 - model_97_loss: 0.6645 - model_97_1_loss: 0.6191\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0474 - model_96_loss: 0.3803 - model_97_loss: 0.6645 - model_97_1_loss: 0.6211\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0537 - model_96_loss: 0.3824 - model_97_loss: 0.6642 - model_97_1_loss: 0.6230\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0704 - model_96_loss: 0.3858 - model_97_loss: 0.6654 - model_97_1_loss: 0.6259\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.4614 - model_97_loss: 0.6666 - model_97_1_loss: 0.6266\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0769 - model_96_loss: 0.3842 - model_97_loss: 0.6658 - model_97_1_loss: 0.6264\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0877 - model_96_loss: 0.3885 - model_97_loss: 0.6661 - model_97_1_loss: 0.6291\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.0890 - model_96_loss: 0.3888 - model_97_loss: 0.6659 - model_97_1_loss: 0.6296\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.1135 - model_96_loss: 0.3907 - model_97_loss: 0.6683 - model_97_1_loss: 0.6326\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.1335 - model_96_loss: 0.3944 - model_97_loss: 0.6708 - model_97_1_loss: 0.6347\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.5300 - model_97_loss: 0.6690 - model_97_1_loss: 0.6372\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1342 - model_96_loss: 0.3955 - model_97_loss: 0.6693 - model_97_1_loss: 0.6367\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.1455 - model_96_loss: 0.3967 - model_97_loss: 0.6698 - model_97_1_loss: 0.6386\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.1617 - model_96_loss: 0.3979 - model_97_loss: 0.6702 - model_97_1_loss: 0.6417\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1636 - model_96_loss: 0.4015 - model_97_loss: 0.6707 - model_97_1_loss: 0.6423\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.1809 - model_96_loss: 0.4038 - model_97_loss: 0.6716 - model_97_1_loss: 0.6453\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.6005 - model_97_loss: 0.6724 - model_97_1_loss: 0.6478\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.1883 - model_96_loss: 0.4067 - model_97_loss: 0.6718 - model_97_1_loss: 0.6472\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.2040 - model_96_loss: 0.4068 - model_97_loss: 0.6724 - model_97_1_loss: 0.6497\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2239 - model_96_loss: 0.4088 - model_97_loss: 0.6744 - model_97_1_loss: 0.6521\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2302 - model_96_loss: 0.4128 - model_97_loss: 0.6752 - model_97_1_loss: 0.6534\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.2457 - model_96_loss: 0.4137 - model_97_loss: 0.6755 - model_97_1_loss: 0.6563\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.6748 - model_97_loss: 0.6763 - model_97_1_loss: 0.6588\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2561 - model_96_loss: 0.4162 - model_97_loss: 0.6759 - model_97_1_loss: 0.6586\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.2660 - model_96_loss: 0.4182 - model_97_loss: 0.6765 - model_97_1_loss: 0.6604\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.2730 - model_96_loss: 0.4236 - model_97_loss: 0.6771 - model_97_1_loss: 0.6622\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3014 - model_96_loss: 0.4226 - model_97_loss: 0.6795 - model_97_1_loss: 0.6653\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3135 - model_96_loss: 0.4276 - model_97_loss: 0.6803 - model_97_1_loss: 0.6679\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.7421 - model_97_loss: 0.6804 - model_97_1_loss: 0.6686\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3100 - model_96_loss: 0.4306 - model_97_loss: 0.6804 - model_97_1_loss: 0.6677\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3204 - model_96_loss: 0.4310 - model_97_loss: 0.6807 - model_97_1_loss: 0.6696\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3202 - model_96_loss: 0.4361 - model_97_loss: 0.6810 - model_97_1_loss: 0.6703\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3347 - model_96_loss: 0.4420 - model_97_loss: 0.6821 - model_97_1_loss: 0.6732\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3442 - model_96_loss: 0.4430 - model_97_loss: 0.6824 - model_97_1_loss: 0.6751\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.8022 - model_97_loss: 0.6833 - model_97_1_loss: 0.67620s - loss: 6.8321 - model_97_loss: 0.6880 - model_97_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3490 - model_96_loss: 0.4459 - model_97_loss: 0.6828 - model_97_1_loss: 0.6762\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3569 - model_96_loss: 0.4496 - model_97_loss: 0.6842 - model_97_1_loss: 0.6771\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3635 - model_96_loss: 0.4554 - model_97_loss: 0.6849 - model_97_1_loss: 0.6789\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.3792 - model_96_loss: 0.4553 - model_97_loss: 0.6858 - model_97_1_loss: 0.6811\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3754 - model_96_loss: 0.4611 - model_97_loss: 0.6854 - model_97_1_loss: 0.6819\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.8449 - model_97_loss: 0.6857 - model_97_1_loss: 0.6825\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3747 - model_96_loss: 0.4643 - model_97_loss: 0.6858 - model_97_1_loss: 0.6820\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3875 - model_96_loss: 0.4686 - model_97_loss: 0.6876 - model_97_1_loss: 0.6836\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3901 - model_96_loss: 0.4722 - model_97_loss: 0.6874 - model_97_1_loss: 0.6851\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3905 - model_96_loss: 0.4781 - model_97_loss: 0.6875 - model_97_1_loss: 0.6862\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4022 - model_96_loss: 0.4806 - model_97_loss: 0.6892 - model_97_1_loss: 0.6873\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.8785 - model_97_loss: 0.6891 - model_97_1_loss: 0.6869\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3919 - model_96_loss: 0.4852 - model_97_loss: 0.6895 - model_97_1_loss: 0.6859\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.3899 - model_96_loss: 0.4915 - model_97_loss: 0.6891 - model_97_1_loss: 0.6872\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3933 - model_96_loss: 0.4940 - model_97_loss: 0.6900 - model_97_1_loss: 0.6875\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4001 - model_96_loss: 0.4937 - model_97_loss: 0.6905 - model_97_1_loss: 0.6882\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4069 - model_96_loss: 0.4960 - model_97_loss: 0.6911 - model_97_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9005 - model_97_loss: 0.6910 - model_97_1_loss: 0.6886\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3993 - model_96_loss: 0.4998 - model_97_loss: 0.6907 - model_97_1_loss: 0.6891\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4006 - model_96_loss: 0.5028 - model_97_loss: 0.6916 - model_97_1_loss: 0.6891\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4036 - model_96_loss: 0.5026 - model_97_loss: 0.6919 - model_97_1_loss: 0.6894\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4063 - model_96_loss: 0.5026 - model_97_loss: 0.6921 - model_97_1_loss: 0.6897\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4063 - model_96_loss: 0.5028 - model_97_loss: 0.6919 - model_97_1_loss: 0.6899\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9116 - model_97_loss: 0.6923 - model_97_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4058 - model_96_loss: 0.5042 - model_97_loss: 0.6922 - model_97_1_loss: 0.6898\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4033 - model_96_loss: 0.5044 - model_97_loss: 0.6918 - model_97_1_loss: 0.6897\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4107 - model_96_loss: 0.5022 - model_97_loss: 0.6923 - model_97_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4119 - model_96_loss: 0.5013 - model_97_loss: 0.6924 - model_97_1_loss: 0.6902\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4155 - model_96_loss: 0.5012 - model_97_loss: 0.6924 - model_97_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9164 - model_97_loss: 0.6925 - model_97_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4162 - model_96_loss: 0.5023 - model_97_loss: 0.6926 - model_97_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4177 - model_96_loss: 0.4983 - model_97_loss: 0.6923 - model_97_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4193 - model_96_loss: 0.4967 - model_97_loss: 0.6924 - model_97_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4248 - model_96_loss: 0.4944 - model_97_loss: 0.6926 - model_97_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4290 - model_96_loss: 0.4906 - model_97_loss: 0.6927 - model_97_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9203 - model_97_loss: 0.6925 - model_97_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4275 - model_96_loss: 0.4886 - model_97_loss: 0.6923 - model_97_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4310 - model_96_loss: 0.4881 - model_97_loss: 0.6926 - model_97_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4328 - model_96_loss: 0.4862 - model_97_loss: 0.6927 - model_97_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4372 - model_96_loss: 0.4821 - model_97_loss: 0.6925 - model_97_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4367 - model_96_loss: 0.4829 - model_97_loss: 0.6925 - model_97_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9116 - model_97_loss: 0.6919 - model_97_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4284 - model_96_loss: 0.4783 - model_97_loss: 0.6913 - model_97_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4254 - model_96_loss: 0.4788 - model_97_loss: 0.6912 - model_97_1_loss: 0.6896\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4262 - model_96_loss: 0.4790 - model_97_loss: 0.6913 - model_97_1_loss: 0.6898\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4288 - model_96_loss: 0.4761 - model_97_loss: 0.6914 - model_97_1_loss: 0.6895\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4211 - model_96_loss: 0.4767 - model_97_loss: 0.6911 - model_97_1_loss: 0.6884\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9005 - model_97_loss: 0.6911 - model_97_1_loss: 0.6891\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4207 - model_96_loss: 0.4728 - model_97_loss: 0.6900 - model_97_1_loss: 0.6887\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4190 - model_96_loss: 0.4737 - model_97_loss: 0.6903 - model_97_1_loss: 0.6883\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4206 - model_96_loss: 0.4739 - model_97_loss: 0.6906 - model_97_1_loss: 0.6883\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4258 - model_96_loss: 0.4733 - model_97_loss: 0.6911 - model_97_1_loss: 0.6887\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4248 - model_96_loss: 0.4722 - model_97_loss: 0.6904 - model_97_1_loss: 0.6890\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.8975 - model_97_loss: 0.6902 - model_97_1_loss: 0.6890\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4273 - model_96_loss: 0.4717 - model_97_loss: 0.6906 - model_97_1_loss: 0.6892\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4207 - model_96_loss: 0.4756 - model_97_loss: 0.6902 - model_97_1_loss: 0.6891\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4251 - model_96_loss: 0.4722 - model_97_loss: 0.6906 - model_97_1_loss: 0.6888\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4222 - model_96_loss: 0.4749 - model_97_loss: 0.6903 - model_97_1_loss: 0.6892\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4261 - model_96_loss: 0.4739 - model_97_loss: 0.6904 - model_97_1_loss: 0.6896\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.8964 - model_97_loss: 0.6891 - model_97_1_loss: 0.6893\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4207 - model_96_loss: 0.4721 - model_97_loss: 0.6899 - model_97_1_loss: 0.6886\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4186 - model_96_loss: 0.4739 - model_97_loss: 0.6898 - model_97_1_loss: 0.6886\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4231 - model_96_loss: 0.4738 - model_97_loss: 0.6904 - model_97_1_loss: 0.6890\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4250 - model_96_loss: 0.4750 - model_97_loss: 0.6902 - model_97_1_loss: 0.6898\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4210 - model_96_loss: 0.4745 - model_97_loss: 0.6896 - model_97_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9030 - model_97_loss: 0.6905 - model_97_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4267 - model_96_loss: 0.4727 - model_97_loss: 0.6903 - model_97_1_loss: 0.6895\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4278 - model_96_loss: 0.4752 - model_97_loss: 0.6903 - model_97_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4305 - model_96_loss: 0.4756 - model_97_loss: 0.6908 - model_97_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4333 - model_96_loss: 0.4762 - model_97_loss: 0.6912 - model_97_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4326 - model_96_loss: 0.4790 - model_97_loss: 0.6911 - model_97_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9147 - model_97_loss: 0.6916 - model_97_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4337 - model_96_loss: 0.4772 - model_97_loss: 0.6914 - model_97_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4348 - model_96_loss: 0.4783 - model_97_loss: 0.6914 - model_97_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4333 - model_96_loss: 0.4804 - model_97_loss: 0.6914 - model_97_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4336 - model_96_loss: 0.4809 - model_97_loss: 0.6915 - model_97_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4379 - model_96_loss: 0.4796 - model_97_loss: 0.6921 - model_97_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9195 - model_97_loss: 0.6925 - model_97_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4341 - model_96_loss: 0.4830 - model_97_loss: 0.6922 - model_97_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4369 - model_96_loss: 0.4801 - model_97_loss: 0.6921 - model_97_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4364 - model_96_loss: 0.4831 - model_97_loss: 0.6923 - model_97_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4345 - model_96_loss: 0.4836 - model_97_loss: 0.6922 - model_97_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4369 - model_96_loss: 0.4812 - model_97_loss: 0.6922 - model_97_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9192 - model_97_loss: 0.6927 - model_97_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4360 - model_96_loss: 0.4826 - model_97_loss: 0.6926 - model_97_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4364 - model_96_loss: 0.4809 - model_97_loss: 0.6924 - model_97_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4352 - model_96_loss: 0.4797 - model_97_loss: 0.6921 - model_97_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4351 - model_96_loss: 0.4802 - model_97_loss: 0.6923 - model_97_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4344 - model_96_loss: 0.4797 - model_97_loss: 0.6920 - model_97_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9139 - model_97_loss: 0.6920 - model_97_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4344 - model_96_loss: 0.4786 - model_97_loss: 0.6921 - model_97_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4340 - model_96_loss: 0.4785 - model_97_loss: 0.6917 - model_97_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4366 - model_96_loss: 0.4772 - model_97_loss: 0.6921 - model_97_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4394 - model_96_loss: 0.4767 - model_97_loss: 0.6923 - model_97_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4427 - model_96_loss: 0.4748 - model_97_loss: 0.6924 - model_97_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9155 - model_97_loss: 0.6919 - model_97_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4383 - model_96_loss: 0.4762 - model_97_loss: 0.6920 - model_97_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4404 - model_96_loss: 0.4749 - model_97_loss: 0.6921 - model_97_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4404 - model_96_loss: 0.4754 - model_97_loss: 0.6923 - model_97_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4424 - model_96_loss: 0.4745 - model_97_loss: 0.6921 - model_97_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4474 - model_96_loss: 0.4735 - model_97_loss: 0.6927 - model_97_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9193 - model_97_loss: 0.6927 - model_97_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4412 - model_96_loss: 0.4743 - model_97_loss: 0.6920 - model_97_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4434 - model_96_loss: 0.4736 - model_97_loss: 0.6924 - model_97_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4476 - model_96_loss: 0.4724 - model_97_loss: 0.6927 - model_97_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4460 - model_96_loss: 0.4727 - model_97_loss: 0.6925 - model_97_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4483 - model_96_loss: 0.4724 - model_97_loss: 0.6926 - model_97_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9201 - model_97_loss: 0.6929 - model_97_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4436 - model_96_loss: 0.4718 - model_97_loss: 0.6926 - model_97_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4445 - model_96_loss: 0.4730 - model_97_loss: 0.6927 - model_97_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4414 - model_96_loss: 0.4732 - model_97_loss: 0.6924 - model_97_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4418 - model_96_loss: 0.4740 - model_97_loss: 0.6927 - model_97_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4417 - model_96_loss: 0.4733 - model_97_loss: 0.6925 - model_97_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9132 - model_97_loss: 0.6925 - model_97_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4449 - model_96_loss: 0.4717 - model_97_loss: 0.6928 - model_97_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4425 - model_96_loss: 0.4710 - model_97_loss: 0.6922 - model_97_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4445 - model_96_loss: 0.4696 - model_97_loss: 0.6920 - model_97_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4433 - model_96_loss: 0.4717 - model_97_loss: 0.6924 - model_97_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4413 - model_96_loss: 0.4728 - model_97_loss: 0.6919 - model_97_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9170 - model_97_loss: 0.6920 - model_97_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4388 - model_96_loss: 0.4719 - model_97_loss: 0.6923 - model_97_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4408 - model_96_loss: 0.4720 - model_97_loss: 0.6924 - model_97_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4369 - model_96_loss: 0.4727 - model_97_loss: 0.6918 - model_97_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4414 - model_96_loss: 0.4716 - model_97_loss: 0.6922 - model_97_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4406 - model_96_loss: 0.4729 - model_97_loss: 0.6926 - model_97_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9185 - model_97_loss: 0.6925 - model_97_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4470 - model_96_loss: 0.4717 - model_97_loss: 0.6925 - model_97_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4460 - model_96_loss: 0.4719 - model_97_loss: 0.6925 - model_97_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4456 - model_96_loss: 0.4712 - model_97_loss: 0.6925 - model_97_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4424 - model_96_loss: 0.4742 - model_97_loss: 0.6925 - model_97_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4459 - model_96_loss: 0.4731 - model_97_loss: 0.6925 - model_97_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9225 - model_97_loss: 0.6919 - model_97_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4474 - model_96_loss: 0.4727 - model_97_loss: 0.6926 - model_97_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4473 - model_96_loss: 0.4729 - model_97_loss: 0.6926 - model_97_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4504 - model_96_loss: 0.4722 - model_97_loss: 0.6926 - model_97_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4492 - model_96_loss: 0.4702 - model_97_loss: 0.6924 - model_97_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4508 - model_96_loss: 0.4705 - model_97_loss: 0.6925 - model_97_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: 6.9203 - model_97_loss: 0.6929 - model_97_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4495 - model_96_loss: 0.4711 - model_97_loss: 0.6926 - model_97_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4473 - model_96_loss: 0.4710 - model_97_loss: 0.6922 - model_97_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4453 - model_96_loss: 0.4725 - model_97_loss: 0.6925 - model_97_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4460 - model_96_loss: 0.4712 - model_97_loss: 0.6924 - model_97_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4501 - model_96_loss: 0.4706 - model_97_loss: 0.6929 - model_97_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9184 - model_97_loss: 0.6926 - model_97_1_loss: 0.69120s - loss: 6.8887 - model_97_loss: 0.6875 - model_97_1_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4494 - model_96_loss: 0.4682 - model_97_loss: 0.6926 - model_97_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4464 - model_96_loss: 0.4695 - model_97_loss: 0.6923 - model_97_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4476 - model_96_loss: 0.4703 - model_97_loss: 0.6926 - model_97_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4469 - model_96_loss: 0.4705 - model_97_loss: 0.6927 - model_97_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4451 - model_96_loss: 0.4723 - model_97_loss: 0.6924 - model_97_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9180 - model_97_loss: 0.6930 - model_97_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4462 - model_96_loss: 0.4697 - model_97_loss: 0.6922 - model_97_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4452 - model_96_loss: 0.4705 - model_97_loss: 0.6926 - model_97_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4471 - model_96_loss: 0.4706 - model_97_loss: 0.6925 - model_97_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4460 - model_96_loss: 0.4709 - model_97_loss: 0.6924 - model_97_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4503 - model_96_loss: 0.4702 - model_97_loss: 0.6926 - model_97_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9208 - model_97_loss: 0.6927 - model_97_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4484 - model_96_loss: 0.4715 - model_97_loss: 0.6925 - model_97_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4507 - model_96_loss: 0.4711 - model_97_loss: 0.6928 - model_97_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4501 - model_96_loss: 0.4708 - model_97_loss: 0.6926 - model_97_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4487 - model_96_loss: 0.4721 - model_97_loss: 0.6926 - model_97_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4498 - model_96_loss: 0.4718 - model_97_loss: 0.6929 - model_97_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9224 - model_97_loss: 0.6929 - model_97_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4502 - model_96_loss: 0.4720 - model_97_loss: 0.6928 - model_97_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4504 - model_96_loss: 0.4734 - model_97_loss: 0.6930 - model_97_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4495 - model_96_loss: 0.4735 - model_97_loss: 0.6929 - model_97_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4511 - model_96_loss: 0.4716 - model_97_loss: 0.6928 - model_97_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4498 - model_96_loss: 0.4727 - model_97_loss: 0.6928 - model_97_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9222 - model_97_loss: 0.6933 - model_97_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4483 - model_96_loss: 0.4721 - model_97_loss: 0.6926 - model_97_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4479 - model_96_loss: 0.4716 - model_97_loss: 0.6925 - model_97_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4491 - model_96_loss: 0.4729 - model_97_loss: 0.6927 - model_97_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4507 - model_96_loss: 0.4710 - model_97_loss: 0.6929 - model_97_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4522 - model_96_loss: 0.4700 - model_97_loss: 0.6929 - model_97_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9231 - model_97_loss: 0.6927 - model_97_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4485 - model_96_loss: 0.4713 - model_97_loss: 0.6925 - model_97_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4488 - model_96_loss: 0.4707 - model_97_loss: 0.6925 - model_97_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4480 - model_96_loss: 0.4709 - model_97_loss: 0.6925 - model_97_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4501 - model_96_loss: 0.4693 - model_97_loss: 0.6926 - model_97_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4477 - model_96_loss: 0.4690 - model_97_loss: 0.6924 - model_97_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9187 - model_97_loss: 0.6922 - model_97_1_loss: 0.69130s - loss: 6.9257 - model_97_loss: 0.6943 - model_97_1_loss: 0.69 - ETA: 0s - loss: 6.9417 - model_97_loss: 0.6961 - model_97_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4473 - model_96_loss: 0.4699 - model_97_loss: 0.6923 - model_97_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4492 - model_96_loss: 0.4689 - model_97_loss: 0.6925 - model_97_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4504 - model_96_loss: 0.4685 - model_97_loss: 0.6926 - model_97_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4462 - model_96_loss: 0.4688 - model_97_loss: 0.6922 - model_97_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4481 - model_96_loss: 0.4683 - model_97_loss: 0.6925 - model_97_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9213 - model_97_loss: 0.6931 - model_97_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4502 - model_96_loss: 0.4683 - model_97_loss: 0.6925 - model_97_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4521 - model_96_loss: 0.4670 - model_97_loss: 0.6928 - model_97_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4518 - model_96_loss: 0.4678 - model_97_loss: 0.6928 - model_97_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4529 - model_96_loss: 0.4656 - model_97_loss: 0.6926 - model_97_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4509 - model_96_loss: 0.4673 - model_97_loss: 0.6925 - model_97_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9222 - model_97_loss: 0.6933 - model_97_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4526 - model_96_loss: 0.4661 - model_97_loss: 0.6926 - model_97_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4487 - model_96_loss: 0.4662 - model_97_loss: 0.6922 - model_97_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4534 - model_96_loss: 0.4638 - model_97_loss: 0.6926 - model_97_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4538 - model_96_loss: 0.4659 - model_97_loss: 0.6927 - model_97_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4512 - model_96_loss: 0.4661 - model_97_loss: 0.6924 - model_97_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9190 - model_97_loss: 0.6919 - model_97_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4540 - model_96_loss: 0.4643 - model_97_loss: 0.6925 - model_97_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4530 - model_96_loss: 0.4656 - model_97_loss: 0.6925 - model_97_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4531 - model_96_loss: 0.4652 - model_97_loss: 0.6926 - model_97_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4499 - model_96_loss: 0.4668 - model_97_loss: 0.6924 - model_97_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4507 - model_96_loss: 0.4667 - model_97_loss: 0.6923 - model_97_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9188 - model_97_loss: 0.6925 - model_97_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4534 - model_96_loss: 0.4643 - model_97_loss: 0.6925 - model_97_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4518 - model_96_loss: 0.4649 - model_97_loss: 0.6923 - model_97_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4521 - model_96_loss: 0.4643 - model_97_loss: 0.6927 - model_97_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4537 - model_96_loss: 0.4635 - model_97_loss: 0.6926 - model_97_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4545 - model_96_loss: 0.4631 - model_97_loss: 0.6925 - model_97_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9053 - model_97_loss: 0.6897 - model_97_1_loss: 0.691 - 1s 29us/sample - loss: 6.9177 - model_97_loss: 0.6924 - model_97_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4506 - model_96_loss: 0.4641 - model_97_loss: 0.6923 - model_97_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4518 - model_96_loss: 0.4648 - model_97_loss: 0.6926 - model_97_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4539 - model_96_loss: 0.4643 - model_97_loss: 0.6926 - model_97_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4509 - model_96_loss: 0.4654 - model_97_loss: 0.6925 - model_97_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4527 - model_96_loss: 0.4656 - model_97_loss: 0.6927 - model_97_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9203 - model_97_loss: 0.6935 - model_97_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4522 - model_96_loss: 0.4652 - model_97_loss: 0.6925 - model_97_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4516 - model_96_loss: 0.4657 - model_97_loss: 0.6925 - model_97_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4508 - model_96_loss: 0.4656 - model_97_loss: 0.6924 - model_97_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4519 - model_96_loss: 0.4666 - model_97_loss: 0.6927 - model_97_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4510 - model_96_loss: 0.4667 - model_97_loss: 0.6925 - model_97_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9209 - model_97_loss: 0.6932 - model_97_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4517 - model_96_loss: 0.4667 - model_97_loss: 0.6925 - model_97_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4534 - model_96_loss: 0.4663 - model_97_loss: 0.6926 - model_97_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4519 - model_96_loss: 0.4666 - model_97_loss: 0.6924 - model_97_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4538 - model_96_loss: 0.4647 - model_97_loss: 0.6925 - model_97_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4553 - model_96_loss: 0.4655 - model_97_loss: 0.6927 - model_97_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9184 - model_97_loss: 0.6925 - model_97_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4531 - model_96_loss: 0.4659 - model_97_loss: 0.6925 - model_97_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4531 - model_96_loss: 0.4650 - model_97_loss: 0.6925 - model_97_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4529 - model_96_loss: 0.4650 - model_97_loss: 0.6925 - model_97_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4511 - model_96_loss: 0.4659 - model_97_loss: 0.6925 - model_97_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4534 - model_96_loss: 0.4648 - model_97_loss: 0.6922 - model_97_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9198 - model_97_loss: 0.6923 - model_97_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4522 - model_96_loss: 0.4666 - model_97_loss: 0.6926 - model_97_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4559 - model_96_loss: 0.4641 - model_97_loss: 0.6928 - model_97_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4518 - model_96_loss: 0.4664 - model_97_loss: 0.6924 - model_97_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4543 - model_96_loss: 0.4663 - model_97_loss: 0.6927 - model_97_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4517 - model_96_loss: 0.4666 - model_97_loss: 0.6925 - model_97_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9202 - model_97_loss: 0.6925 - model_97_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4540 - model_96_loss: 0.4669 - model_97_loss: 0.6925 - model_97_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4553 - model_96_loss: 0.4666 - model_97_loss: 0.6926 - model_97_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4537 - model_96_loss: 0.4666 - model_97_loss: 0.6925 - model_97_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4551 - model_96_loss: 0.4659 - model_97_loss: 0.6925 - model_97_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4549 - model_96_loss: 0.4674 - model_97_loss: 0.6926 - model_97_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9202 - model_97_loss: 0.6923 - model_97_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4572 - model_96_loss: 0.4654 - model_97_loss: 0.6928 - model_97_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4511 - model_96_loss: 0.4678 - model_97_loss: 0.6923 - model_97_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4557 - model_96_loss: 0.4669 - model_97_loss: 0.6928 - model_97_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4549 - model_96_loss: 0.4680 - model_97_loss: 0.6928 - model_97_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4514 - model_96_loss: 0.4678 - model_97_loss: 0.6923 - model_97_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9210 - model_97_loss: 0.6925 - model_97_1_loss: 0.69150s - loss: 6.9028 - model_97_loss: 0.6900 - model_97_1_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4541 - model_96_loss: 0.4661 - model_97_loss: 0.6925 - model_97_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4533 - model_96_loss: 0.4663 - model_97_loss: 0.6925 - model_97_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4553 - model_96_loss: 0.4657 - model_97_loss: 0.6924 - model_97_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4541 - model_96_loss: 0.4662 - model_97_loss: 0.6924 - model_97_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4514 - model_96_loss: 0.4669 - model_97_loss: 0.6923 - model_97_1_loss: 0.6914\n",
      "For Attention Module: 2.6\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.3296 - model_101_loss: 0.6612 - model_101_1_loss: 0.60450s - loss: 6.3442 - model_101_loss: 0.6621 - model_101_1_loss: 0.60\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -5.9590 - model_100_loss: 0.3772 - model_101_loss: 0.6611 - model_101_1_loss: 0.6061\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -5.9719 - model_100_loss: 0.3766 - model_101_loss: 0.6618 - model_101_1_loss: 0.6079\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -5.9821 - model_100_loss: 0.3774 - model_101_loss: 0.6618 - model_101_1_loss: 0.6101\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -5.9936 - model_100_loss: 0.3784 - model_101_loss: 0.6629 - model_101_1_loss: 0.6115\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0039 - model_100_loss: 0.3794 - model_101_loss: 0.6627 - model_101_1_loss: 0.6140\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.3888 - model_101_loss: 0.6636 - model_101_1_loss: 0.6142\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -5.9953 - model_100_loss: 0.3808 - model_101_loss: 0.6635 - model_101_1_loss: 0.6118\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.0069 - model_100_loss: 0.3820 - model_101_loss: 0.6628 - model_101_1_loss: 0.6150\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.0175 - model_100_loss: 0.3827 - model_101_loss: 0.6635 - model_101_1_loss: 0.6166\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0343 - model_100_loss: 0.3840 - model_101_loss: 0.6642 - model_101_1_loss: 0.6194\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.0392 - model_100_loss: 0.3846 - model_101_loss: 0.6646 - model_101_1_loss: 0.6201\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.4430 - model_101_loss: 0.6668 - model_101_1_loss: 0.6224\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.0531 - model_100_loss: 0.3862 - model_101_loss: 0.6653 - model_101_1_loss: 0.6226\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.0658 - model_100_loss: 0.3880 - model_101_loss: 0.6663 - model_101_1_loss: 0.6245\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.0735 - model_100_loss: 0.3900 - model_101_loss: 0.6667 - model_101_1_loss: 0.6260\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.0876 - model_100_loss: 0.3889 - model_101_loss: 0.6678 - model_101_1_loss: 0.6275\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1123 - model_100_loss: 0.3907 - model_101_loss: 0.6690 - model_101_1_loss: 0.6316\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.4949 - model_101_loss: 0.6673 - model_101_1_loss: 0.6316\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.1088 - model_100_loss: 0.3912 - model_101_loss: 0.6686 - model_101_1_loss: 0.6314\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.1223 - model_100_loss: 0.3939 - model_101_loss: 0.6690 - model_101_1_loss: 0.6342\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1355 - model_100_loss: 0.3948 - model_101_loss: 0.6705 - model_101_1_loss: 0.6356\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.1481 - model_100_loss: 0.3956 - model_101_loss: 0.6709 - model_101_1_loss: 0.6378\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1665 - model_100_loss: 0.3957 - model_101_loss: 0.6714 - model_101_1_loss: 0.6411\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.5757 - model_101_loss: 0.6738 - model_101_1_loss: 0.6413\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.1628 - model_100_loss: 0.3970 - model_101_loss: 0.6713 - model_101_1_loss: 0.6407\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1819 - model_100_loss: 0.4008 - model_101_loss: 0.6730 - model_101_1_loss: 0.6435\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2043 - model_100_loss: 0.3995 - model_101_loss: 0.6745 - model_101_1_loss: 0.6463\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2146 - model_100_loss: 0.3998 - model_101_loss: 0.6744 - model_101_1_loss: 0.6484\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2269 - model_100_loss: 0.4021 - model_101_loss: 0.6751 - model_101_1_loss: 0.6507\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.6401 - model_101_loss: 0.6761 - model_101_1_loss: 0.6514\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2322 - model_100_loss: 0.4032 - model_101_loss: 0.6755 - model_101_1_loss: 0.6515\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2485 - model_100_loss: 0.4049 - model_101_loss: 0.6766 - model_101_1_loss: 0.6541\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2611 - model_100_loss: 0.4070 - model_101_loss: 0.6771 - model_101_1_loss: 0.6565\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2814 - model_100_loss: 0.4079 - model_101_loss: 0.6786 - model_101_1_loss: 0.6592\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2863 - model_100_loss: 0.4096 - model_101_loss: 0.6772 - model_101_1_loss: 0.6620\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.7062 - model_101_loss: 0.6784 - model_101_1_loss: 0.66280s - loss: 6.7132 - model_101_loss: 0.6788 - model_101_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.2887 - model_100_loss: 0.4142 - model_101_loss: 0.6783 - model_101_1_loss: 0.6623\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3037 - model_100_loss: 0.4138 - model_101_loss: 0.6790 - model_101_1_loss: 0.6645\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3207 - model_100_loss: 0.4166 - model_101_loss: 0.6804 - model_101_1_loss: 0.6670\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3286 - model_100_loss: 0.4181 - model_101_loss: 0.6801 - model_101_1_loss: 0.6693\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3440 - model_100_loss: 0.4239 - model_101_loss: 0.6822 - model_101_1_loss: 0.6714\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.7653 - model_101_loss: 0.6821 - model_101_1_loss: 0.6705\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3330 - model_100_loss: 0.4293 - model_101_loss: 0.6818 - model_101_1_loss: 0.6706\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3489 - model_100_loss: 0.4281 - model_101_loss: 0.6836 - model_101_1_loss: 0.6718\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3540 - model_100_loss: 0.4331 - model_101_loss: 0.6837 - model_101_1_loss: 0.6737\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3708 - model_100_loss: 0.4353 - model_101_loss: 0.6850 - model_101_1_loss: 0.6762\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3730 - model_100_loss: 0.4402 - model_101_loss: 0.6851 - model_101_1_loss: 0.6775\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.8214 - model_101_loss: 0.6861 - model_101_1_loss: 0.6779\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3688 - model_100_loss: 0.4419 - model_101_loss: 0.6852 - model_101_1_loss: 0.6770\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3782 - model_100_loss: 0.4468 - model_101_loss: 0.6865 - model_101_1_loss: 0.6786\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3782 - model_100_loss: 0.4488 - model_101_loss: 0.6859 - model_101_1_loss: 0.6795\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3887 - model_100_loss: 0.4523 - model_101_loss: 0.6874 - model_101_1_loss: 0.6808\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3922 - model_100_loss: 0.4563 - model_101_loss: 0.6872 - model_101_1_loss: 0.6825\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.8564 - model_101_loss: 0.6875 - model_101_1_loss: 0.68340s - loss: 6.8470 - model_101_loss: 0.6873 - model_101_1_loss:\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4033 - model_100_loss: 0.4564 - model_101_loss: 0.6887 - model_101_1_loss: 0.6832\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3989 - model_100_loss: 0.4599 - model_101_loss: 0.6885 - model_101_1_loss: 0.6833\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4063 - model_100_loss: 0.4625 - model_101_loss: 0.6893 - model_101_1_loss: 0.6845\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4081 - model_100_loss: 0.4666 - model_101_loss: 0.6896 - model_101_1_loss: 0.6853\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4139 - model_100_loss: 0.4671 - model_101_loss: 0.6903 - model_101_1_loss: 0.6859\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.8827 - model_101_loss: 0.6904 - model_101_1_loss: 0.6865\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4142 - model_100_loss: 0.4692 - model_101_loss: 0.6897 - model_101_1_loss: 0.6869\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4195 - model_100_loss: 0.4691 - model_101_loss: 0.6905 - model_101_1_loss: 0.6872\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4226 - model_100_loss: 0.4723 - model_101_loss: 0.6908 - model_101_1_loss: 0.6882\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4305 - model_100_loss: 0.4740 - model_101_loss: 0.6916 - model_101_1_loss: 0.6893\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4278 - model_100_loss: 0.4783 - model_101_loss: 0.6917 - model_101_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9078 - model_101_loss: 0.6927 - model_101_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4307 - model_100_loss: 0.4755 - model_101_loss: 0.6917 - model_101_1_loss: 0.6895\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4249 - model_100_loss: 0.4829 - model_101_loss: 0.6921 - model_101_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4317 - model_100_loss: 0.4825 - model_101_loss: 0.6923 - model_101_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4346 - model_100_loss: 0.4845 - model_101_loss: 0.6927 - model_101_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4373 - model_100_loss: 0.4837 - model_101_loss: 0.6929 - model_101_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9200 - model_101_loss: 0.6923 - model_101_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4326 - model_100_loss: 0.4852 - model_101_loss: 0.6927 - model_101_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4325 - model_100_loss: 0.4863 - model_101_loss: 0.6925 - model_101_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4362 - model_100_loss: 0.4877 - model_101_loss: 0.6926 - model_101_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4359 - model_100_loss: 0.4897 - model_101_loss: 0.6928 - model_101_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4363 - model_100_loss: 0.4900 - model_101_loss: 0.6929 - model_101_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9263 - model_101_loss: 0.6923 - model_101_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4375 - model_100_loss: 0.4889 - model_101_loss: 0.6929 - model_101_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4378 - model_100_loss: 0.4874 - model_101_loss: 0.6928 - model_101_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4333 - model_100_loss: 0.4914 - model_101_loss: 0.6926 - model_101_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4384 - model_100_loss: 0.4891 - model_101_loss: 0.6930 - model_101_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4384 - model_100_loss: 0.4890 - model_101_loss: 0.6928 - model_101_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9248 - model_101_loss: 0.6936 - model_101_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4342 - model_100_loss: 0.4888 - model_101_loss: 0.6922 - model_101_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4322 - model_100_loss: 0.4890 - model_101_loss: 0.6917 - model_101_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4382 - model_100_loss: 0.4887 - model_101_loss: 0.6927 - model_101_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4382 - model_100_loss: 0.4883 - model_101_loss: 0.6924 - model_101_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4356 - model_100_loss: 0.4884 - model_101_loss: 0.6923 - model_101_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9251 - model_101_loss: 0.6925 - model_101_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4331 - model_100_loss: 0.4864 - model_101_loss: 0.6917 - model_101_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4309 - model_100_loss: 0.4875 - model_101_loss: 0.6916 - model_101_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4318 - model_100_loss: 0.4838 - model_101_loss: 0.6911 - model_101_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4327 - model_100_loss: 0.4850 - model_101_loss: 0.6917 - model_101_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4334 - model_100_loss: 0.4843 - model_101_loss: 0.6915 - model_101_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9176 - model_101_loss: 0.6919 - model_101_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4354 - model_100_loss: 0.4820 - model_101_loss: 0.6916 - model_101_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4310 - model_100_loss: 0.4800 - model_101_loss: 0.6907 - model_101_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4285 - model_100_loss: 0.4825 - model_101_loss: 0.6909 - model_101_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4342 - model_100_loss: 0.4782 - model_101_loss: 0.6911 - model_101_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4339 - model_100_loss: 0.4796 - model_101_loss: 0.6911 - model_101_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9168 - model_101_loss: 0.6908 - model_101_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4319 - model_100_loss: 0.4792 - model_101_loss: 0.6909 - model_101_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4341 - model_100_loss: 0.4791 - model_101_loss: 0.6909 - model_101_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4342 - model_100_loss: 0.4791 - model_101_loss: 0.6912 - model_101_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4343 - model_100_loss: 0.4798 - model_101_loss: 0.6910 - model_101_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4357 - model_100_loss: 0.4775 - model_101_loss: 0.6910 - model_101_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9151 - model_101_loss: 0.6915 - model_101_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4330 - model_100_loss: 0.4779 - model_101_loss: 0.6908 - model_101_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4361 - model_100_loss: 0.4771 - model_101_loss: 0.6911 - model_101_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4403 - model_100_loss: 0.4774 - model_101_loss: 0.6918 - model_101_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4368 - model_100_loss: 0.4784 - model_101_loss: 0.6912 - model_101_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4384 - model_100_loss: 0.4768 - model_101_loss: 0.6914 - model_101_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9174 - model_101_loss: 0.6916 - model_101_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4390 - model_100_loss: 0.4767 - model_101_loss: 0.6912 - model_101_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4410 - model_100_loss: 0.4756 - model_101_loss: 0.6914 - model_101_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4371 - model_100_loss: 0.4767 - model_101_loss: 0.6910 - model_101_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4396 - model_100_loss: 0.4755 - model_101_loss: 0.6912 - model_101_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4394 - model_100_loss: 0.4761 - model_101_loss: 0.6913 - model_101_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9158 - model_101_loss: 0.6915 - model_101_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4375 - model_100_loss: 0.4764 - model_101_loss: 0.6914 - model_101_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4368 - model_100_loss: 0.4757 - model_101_loss: 0.6912 - model_101_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4398 - model_100_loss: 0.4729 - model_101_loss: 0.6913 - model_101_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4400 - model_100_loss: 0.4729 - model_101_loss: 0.6915 - model_101_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4431 - model_100_loss: 0.4712 - model_101_loss: 0.6914 - model_101_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9138 - model_101_loss: 0.6921 - model_101_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4408 - model_100_loss: 0.4730 - model_101_loss: 0.6914 - model_101_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4415 - model_100_loss: 0.4711 - model_101_loss: 0.6913 - model_101_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4406 - model_100_loss: 0.4706 - model_101_loss: 0.6913 - model_101_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4395 - model_100_loss: 0.4717 - model_101_loss: 0.6913 - model_101_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4370 - model_100_loss: 0.4725 - model_101_loss: 0.6913 - model_101_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: 6.9129 - model_101_loss: 0.6914 - model_101_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4384 - model_100_loss: 0.4703 - model_101_loss: 0.6911 - model_101_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4414 - model_100_loss: 0.4699 - model_101_loss: 0.6916 - model_101_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4405 - model_100_loss: 0.4689 - model_101_loss: 0.6913 - model_101_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4422 - model_100_loss: 0.4688 - model_101_loss: 0.6915 - model_101_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4425 - model_100_loss: 0.4664 - model_101_loss: 0.6913 - model_101_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9113 - model_101_loss: 0.6923 - model_101_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4416 - model_100_loss: 0.4686 - model_101_loss: 0.6915 - model_101_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4455 - model_100_loss: 0.4649 - model_101_loss: 0.6915 - model_101_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4452 - model_100_loss: 0.4675 - model_101_loss: 0.6918 - model_101_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4455 - model_100_loss: 0.4660 - model_101_loss: 0.6919 - model_101_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4478 - model_100_loss: 0.4661 - model_101_loss: 0.6918 - model_101_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9150 - model_101_loss: 0.6926 - model_101_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4454 - model_100_loss: 0.4651 - model_101_loss: 0.6917 - model_101_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4458 - model_100_loss: 0.4671 - model_101_loss: 0.6920 - model_101_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4461 - model_100_loss: 0.4647 - model_101_loss: 0.6916 - model_101_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4442 - model_100_loss: 0.4651 - model_101_loss: 0.6916 - model_101_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4466 - model_100_loss: 0.4651 - model_101_loss: 0.6919 - model_101_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9126 - model_101_loss: 0.6918 - model_101_1_loss: 0.690 - 1s 29us/sample - loss: 6.9120 - model_101_loss: 0.6916 - model_101_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4455 - model_100_loss: 0.4656 - model_101_loss: 0.6915 - model_101_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4476 - model_100_loss: 0.4661 - model_101_loss: 0.6919 - model_101_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4489 - model_100_loss: 0.4648 - model_101_loss: 0.6922 - model_101_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4456 - model_100_loss: 0.4645 - model_101_loss: 0.6914 - model_101_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4434 - model_100_loss: 0.4657 - model_101_loss: 0.6914 - model_101_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9147 - model_101_loss: 0.6920 - model_101_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4482 - model_100_loss: 0.4661 - model_101_loss: 0.6919 - model_101_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4520 - model_100_loss: 0.4661 - model_101_loss: 0.6922 - model_101_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4500 - model_100_loss: 0.4669 - model_101_loss: 0.6922 - model_101_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4546 - model_100_loss: 0.4660 - model_101_loss: 0.6926 - model_101_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4549 - model_100_loss: 0.4678 - model_101_loss: 0.6928 - model_101_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9143 - model_101_loss: 0.6919 - model_101_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4424 - model_100_loss: 0.4689 - model_101_loss: 0.6920 - model_101_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4438 - model_100_loss: 0.4685 - model_101_loss: 0.6919 - model_101_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4419 - model_100_loss: 0.4700 - model_101_loss: 0.6917 - model_101_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4456 - model_100_loss: 0.4705 - model_101_loss: 0.6922 - model_101_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4465 - model_100_loss: 0.4698 - model_101_loss: 0.6920 - model_101_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9173 - model_101_loss: 0.6927 - model_101_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4475 - model_100_loss: 0.4701 - model_101_loss: 0.6927 - model_101_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4468 - model_100_loss: 0.4713 - model_101_loss: 0.6925 - model_101_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4449 - model_100_loss: 0.4732 - model_101_loss: 0.6923 - model_101_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4496 - model_100_loss: 0.4714 - model_101_loss: 0.6925 - model_101_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4521 - model_100_loss: 0.4711 - model_101_loss: 0.6927 - model_101_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9229 - model_101_loss: 0.6928 - model_101_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4476 - model_100_loss: 0.4714 - model_101_loss: 0.6924 - model_101_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 10us/sample - loss: -6.4467 - model_100_loss: 0.4724 - model_101_loss: 0.6925 - model_101_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4493 - model_100_loss: 0.4711 - model_101_loss: 0.6927 - model_101_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4480 - model_100_loss: 0.4713 - model_101_loss: 0.6927 - model_101_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4469 - model_100_loss: 0.4720 - model_101_loss: 0.6924 - model_101_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9224 - model_101_loss: 0.6928 - model_101_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4505 - model_100_loss: 0.4720 - model_101_loss: 0.6929 - model_101_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4509 - model_100_loss: 0.4722 - model_101_loss: 0.6930 - model_101_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4502 - model_100_loss: 0.4716 - model_101_loss: 0.6928 - model_101_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4492 - model_100_loss: 0.4733 - model_101_loss: 0.6927 - model_101_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4490 - model_100_loss: 0.4722 - model_101_loss: 0.6928 - model_101_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9239 - model_101_loss: 0.6932 - model_101_1_loss: 0.69170s - loss: 6.9425 - model_101_loss: 0.6961 - model_101_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4497 - model_100_loss: 0.4706 - model_101_loss: 0.6924 - model_101_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4499 - model_100_loss: 0.4718 - model_101_loss: 0.6926 - model_101_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4486 - model_100_loss: 0.4726 - model_101_loss: 0.6925 - model_101_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4503 - model_100_loss: 0.4701 - model_101_loss: 0.6925 - model_101_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4489 - model_100_loss: 0.4703 - model_101_loss: 0.6924 - model_101_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: 6.9196 - model_101_loss: 0.6926 - model_101_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4513 - model_100_loss: 0.4677 - model_101_loss: 0.6923 - model_101_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4530 - model_100_loss: 0.4656 - model_101_loss: 0.6925 - model_101_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4467 - model_100_loss: 0.4693 - model_101_loss: 0.6922 - model_101_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4512 - model_100_loss: 0.4666 - model_101_loss: 0.6925 - model_101_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4532 - model_100_loss: 0.4665 - model_101_loss: 0.6925 - model_101_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9154 - model_101_loss: 0.6923 - model_101_1_loss: 0.69070s - loss: 6.9125 - model_101_loss: 0.6908 - model_101_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4532 - model_100_loss: 0.4634 - model_101_loss: 0.6922 - model_101_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4508 - model_100_loss: 0.4661 - model_101_loss: 0.6921 - model_101_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4505 - model_100_loss: 0.4649 - model_101_loss: 0.6923 - model_101_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4518 - model_100_loss: 0.4651 - model_101_loss: 0.6922 - model_101_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4490 - model_100_loss: 0.4672 - model_101_loss: 0.6921 - model_101_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9167 - model_101_loss: 0.6925 - model_101_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4512 - model_100_loss: 0.4654 - model_101_loss: 0.6922 - model_101_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4506 - model_100_loss: 0.4655 - model_101_loss: 0.6922 - model_101_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4500 - model_100_loss: 0.4679 - model_101_loss: 0.6924 - model_101_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4519 - model_100_loss: 0.4648 - model_101_loss: 0.6924 - model_101_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4508 - model_100_loss: 0.4663 - model_101_loss: 0.6921 - model_101_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9204 - model_101_loss: 0.6929 - model_101_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4529 - model_100_loss: 0.4657 - model_101_loss: 0.6924 - model_101_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4526 - model_100_loss: 0.4651 - model_101_loss: 0.6923 - model_101_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4548 - model_100_loss: 0.4651 - model_101_loss: 0.6925 - model_101_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4540 - model_100_loss: 0.4655 - model_101_loss: 0.6925 - model_101_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4554 - model_100_loss: 0.4659 - model_101_loss: 0.6926 - model_101_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9217 - model_101_loss: 0.6926 - model_101_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4512 - model_100_loss: 0.4675 - model_101_loss: 0.6923 - model_101_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4531 - model_100_loss: 0.4656 - model_101_loss: 0.6924 - model_101_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4510 - model_100_loss: 0.4678 - model_101_loss: 0.6924 - model_101_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4544 - model_100_loss: 0.4652 - model_101_loss: 0.6924 - model_101_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4504 - model_100_loss: 0.4695 - model_101_loss: 0.6924 - model_101_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9230 - model_101_loss: 0.6923 - model_101_1_loss: 0.69180s - loss: 6.9466 - model_101_loss: 0.6968 - model_101_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4537 - model_100_loss: 0.4681 - model_101_loss: 0.6926 - model_101_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4525 - model_100_loss: 0.4672 - model_101_loss: 0.6922 - model_101_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4548 - model_100_loss: 0.4673 - model_101_loss: 0.6923 - model_101_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4511 - model_100_loss: 0.4691 - model_101_loss: 0.6923 - model_101_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4544 - model_100_loss: 0.4679 - model_101_loss: 0.6924 - model_101_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9234 - model_101_loss: 0.6924 - model_101_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4526 - model_100_loss: 0.4685 - model_101_loss: 0.6924 - model_101_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4509 - model_100_loss: 0.4696 - model_101_loss: 0.6924 - model_101_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4529 - model_100_loss: 0.4684 - model_101_loss: 0.6922 - model_101_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4542 - model_100_loss: 0.4669 - model_101_loss: 0.6924 - model_101_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4524 - model_100_loss: 0.4678 - model_101_loss: 0.6922 - model_101_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9213 - model_101_loss: 0.6922 - model_101_1_loss: 0.692 - 1s 29us/sample - loss: 6.9224 - model_101_loss: 0.6921 - model_101_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4565 - model_100_loss: 0.4651 - model_101_loss: 0.6924 - model_101_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4550 - model_100_loss: 0.4666 - model_101_loss: 0.6925 - model_101_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4544 - model_100_loss: 0.4671 - model_101_loss: 0.6924 - model_101_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4537 - model_100_loss: 0.4672 - model_101_loss: 0.6925 - model_101_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4560 - model_100_loss: 0.4663 - model_101_loss: 0.6925 - model_101_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9208 - model_101_loss: 0.6918 - model_101_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4568 - model_100_loss: 0.4646 - model_101_loss: 0.6925 - model_101_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4534 - model_100_loss: 0.4657 - model_101_loss: 0.6922 - model_101_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4514 - model_100_loss: 0.4655 - model_101_loss: 0.6921 - model_101_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4540 - model_100_loss: 0.4642 - model_101_loss: 0.6923 - model_101_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4509 - model_100_loss: 0.4653 - model_101_loss: 0.6921 - model_101_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9184 - model_101_loss: 0.6927 - model_101_1_loss: 0.69130s - loss: 6.9035 - model_101_loss: 0.6899 - model_101_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4524 - model_100_loss: 0.4665 - model_101_loss: 0.6922 - model_101_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4535 - model_100_loss: 0.4648 - model_101_loss: 0.6921 - model_101_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4540 - model_100_loss: 0.4646 - model_101_loss: 0.6922 - model_101_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4565 - model_100_loss: 0.4645 - model_101_loss: 0.6928 - model_101_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4543 - model_100_loss: 0.4650 - model_101_loss: 0.6921 - model_101_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9198 - model_101_loss: 0.6924 - model_101_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4512 - model_100_loss: 0.4648 - model_101_loss: 0.6922 - model_101_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4502 - model_100_loss: 0.4644 - model_101_loss: 0.6918 - model_101_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4535 - model_100_loss: 0.4639 - model_101_loss: 0.6921 - model_101_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4525 - model_100_loss: 0.4653 - model_101_loss: 0.6922 - model_101_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4544 - model_100_loss: 0.4663 - model_101_loss: 0.6923 - model_101_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9201 - model_101_loss: 0.6924 - model_101_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4555 - model_100_loss: 0.4654 - model_101_loss: 0.6926 - model_101_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4543 - model_100_loss: 0.4659 - model_101_loss: 0.6924 - model_101_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4525 - model_100_loss: 0.4668 - model_101_loss: 0.6922 - model_101_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4550 - model_100_loss: 0.4660 - model_101_loss: 0.6923 - model_101_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4568 - model_100_loss: 0.4668 - model_101_loss: 0.6926 - model_101_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9249 - model_101_loss: 0.6926 - model_101_1_loss: 0.69210s - loss: 6.9541 - model_101_loss: 0.6982 - model_101_1_loss: \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4580 - model_100_loss: 0.4658 - model_101_loss: 0.6927 - model_101_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4528 - model_100_loss: 0.4684 - model_101_loss: 0.6923 - model_101_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4535 - model_100_loss: 0.4684 - model_101_loss: 0.6923 - model_101_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4556 - model_100_loss: 0.4676 - model_101_loss: 0.6925 - model_101_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4513 - model_100_loss: 0.4697 - model_101_loss: 0.6923 - model_101_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9234 - model_101_loss: 0.6934 - model_101_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4567 - model_100_loss: 0.4679 - model_101_loss: 0.6925 - model_101_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4601 - model_100_loss: 0.4679 - model_101_loss: 0.6931 - model_101_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4549 - model_100_loss: 0.4694 - model_101_loss: 0.6926 - model_101_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4563 - model_100_loss: 0.4682 - model_101_loss: 0.6926 - model_101_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4584 - model_100_loss: 0.4667 - model_101_loss: 0.6926 - model_101_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9224 - model_101_loss: 0.6920 - model_101_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4564 - model_100_loss: 0.4680 - model_101_loss: 0.6925 - model_101_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4545 - model_100_loss: 0.4681 - model_101_loss: 0.6924 - model_101_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4551 - model_100_loss: 0.4682 - model_101_loss: 0.6926 - model_101_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4539 - model_100_loss: 0.4671 - model_101_loss: 0.6923 - model_101_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4582 - model_100_loss: 0.4666 - model_101_loss: 0.6928 - model_101_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9244 - model_101_loss: 0.6926 - model_101_1_loss: 0.69200s - loss: 6.8953 - model_101_loss: 0.6879 - model_101_1_loss: \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4567 - model_100_loss: 0.4664 - model_101_loss: 0.6926 - model_101_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4577 - model_100_loss: 0.4662 - model_101_loss: 0.6928 - model_101_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4554 - model_100_loss: 0.4659 - model_101_loss: 0.6925 - model_101_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4545 - model_100_loss: 0.4661 - model_101_loss: 0.6925 - model_101_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4579 - model_100_loss: 0.4642 - model_101_loss: 0.6926 - model_101_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: 6.9206 - model_101_loss: 0.6928 - model_101_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4525 - model_100_loss: 0.4661 - model_101_loss: 0.6924 - model_101_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4549 - model_100_loss: 0.4642 - model_101_loss: 0.6923 - model_101_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4573 - model_100_loss: 0.4615 - model_101_loss: 0.6922 - model_101_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4551 - model_100_loss: 0.4644 - model_101_loss: 0.6924 - model_101_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4550 - model_100_loss: 0.4641 - model_101_loss: 0.6925 - model_101_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9199 - model_101_loss: 0.6919 - model_101_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4572 - model_100_loss: 0.4622 - model_101_loss: 0.6924 - model_101_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4534 - model_100_loss: 0.4632 - model_101_loss: 0.6921 - model_101_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4553 - model_100_loss: 0.4642 - model_101_loss: 0.6925 - model_101_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4560 - model_100_loss: 0.4631 - model_101_loss: 0.6925 - model_101_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4569 - model_100_loss: 0.4621 - model_101_loss: 0.6924 - model_101_1_loss: 0.6914\n",
      "For Attention Module: 2.7\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: 6.3204 - model_105_loss: 0.6569 - model_105_1_loss: 0.6065\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: -5.9449 - model_104_loss: 0.3760 - model_105_loss: 0.6580 - model_105_1_loss: 0.6062\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -5.9458 - model_104_loss: 0.3774 - model_105_loss: 0.6565 - model_105_1_loss: 0.6081\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -5.9633 - model_104_loss: 0.3766 - model_105_loss: 0.6592 - model_105_1_loss: 0.6088\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -5.9696 - model_104_loss: 0.3773 - model_105_loss: 0.6583 - model_105_1_loss: 0.6111\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -5.9832 - model_104_loss: 0.3768 - model_105_loss: 0.6592 - model_105_1_loss: 0.6128\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.3752 - model_105_loss: 0.6597 - model_105_1_loss: 0.6147\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -5.9965 - model_104_loss: 0.3763 - model_105_loss: 0.6598 - model_105_1_loss: 0.6147\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -5.9991 - model_104_loss: 0.3788 - model_105_loss: 0.6601 - model_105_1_loss: 0.6155\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0111 - model_104_loss: 0.3758 - model_105_loss: 0.6598 - model_105_1_loss: 0.6176\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0180 - model_104_loss: 0.3788 - model_105_loss: 0.6603 - model_105_1_loss: 0.6191\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0419 - model_104_loss: 0.3781 - model_105_loss: 0.6621 - model_105_1_loss: 0.6219\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.4247 - model_105_loss: 0.6623 - model_105_1_loss: 0.6227\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.0327 - model_104_loss: 0.3810 - model_105_loss: 0.6616 - model_105_1_loss: 0.6211\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0526 - model_104_loss: 0.3802 - model_105_loss: 0.6619 - model_105_1_loss: 0.6246\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.0578 - model_104_loss: 0.3826 - model_105_loss: 0.6620 - model_105_1_loss: 0.6261\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0819 - model_104_loss: 0.3820 - model_105_loss: 0.6639 - model_105_1_loss: 0.6288\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0894 - model_104_loss: 0.3830 - model_105_loss: 0.6639 - model_105_1_loss: 0.6306\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.4787 - model_105_loss: 0.6635 - model_105_1_loss: 0.6318\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.0874 - model_104_loss: 0.3822 - model_105_loss: 0.6628 - model_105_1_loss: 0.6311\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.1116 - model_104_loss: 0.3845 - model_105_loss: 0.6654 - model_105_1_loss: 0.6338\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.1141 - model_104_loss: 0.3862 - model_105_loss: 0.6651 - model_105_1_loss: 0.6350\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1286 - model_104_loss: 0.3878 - model_105_loss: 0.6654 - model_105_1_loss: 0.6378\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1470 - model_104_loss: 0.3881 - model_105_loss: 0.6675 - model_105_1_loss: 0.6395\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.5426 - model_105_loss: 0.6658 - model_105_1_loss: 0.6422\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.1511 - model_104_loss: 0.3893 - model_105_loss: 0.6669 - model_105_1_loss: 0.6411\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.1637 - model_104_loss: 0.3912 - model_105_loss: 0.6665 - model_105_1_loss: 0.6445\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1735 - model_104_loss: 0.3934 - model_105_loss: 0.6685 - model_105_1_loss: 0.6449\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.1961 - model_104_loss: 0.3971 - model_105_loss: 0.6697 - model_105_1_loss: 0.6489\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2066 - model_104_loss: 0.3985 - model_105_loss: 0.6703 - model_105_1_loss: 0.6507\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.6217 - model_105_loss: 0.6718 - model_105_1_loss: 0.6530\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.2252 - model_104_loss: 0.3996 - model_105_loss: 0.6719 - model_105_1_loss: 0.6531\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2350 - model_104_loss: 0.4028 - model_105_loss: 0.6723 - model_105_1_loss: 0.6553\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2489 - model_104_loss: 0.4023 - model_105_loss: 0.6736 - model_105_1_loss: 0.6567\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2571 - model_104_loss: 0.4081 - model_105_loss: 0.6747 - model_105_1_loss: 0.6584\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2680 - model_104_loss: 0.4094 - model_105_loss: 0.6749 - model_105_1_loss: 0.6606\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.6864 - model_105_loss: 0.6769 - model_105_1_loss: 0.6606\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.2675 - model_104_loss: 0.4120 - model_105_loss: 0.6749 - model_105_1_loss: 0.6610\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.2809 - model_104_loss: 0.4127 - model_105_loss: 0.6758 - model_105_1_loss: 0.6629\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2826 - model_104_loss: 0.4165 - model_105_loss: 0.6760 - model_105_1_loss: 0.6638\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2943 - model_104_loss: 0.4191 - model_105_loss: 0.6765 - model_105_1_loss: 0.6661\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3052 - model_104_loss: 0.4203 - model_105_loss: 0.6781 - model_105_1_loss: 0.6670\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.7367 - model_105_loss: 0.6788 - model_105_1_loss: 0.6689\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3043 - model_104_loss: 0.4262 - model_105_loss: 0.6785 - model_105_1_loss: 0.6676\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3220 - model_104_loss: 0.4289 - model_105_loss: 0.6802 - model_105_1_loss: 0.6700\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3346 - model_104_loss: 0.4292 - model_105_loss: 0.6811 - model_105_1_loss: 0.6717\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3395 - model_104_loss: 0.4341 - model_105_loss: 0.6823 - model_105_1_loss: 0.6724\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3452 - model_104_loss: 0.4373 - model_105_loss: 0.6823 - model_105_1_loss: 0.6742\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.8026 - model_105_loss: 0.6846 - model_105_1_loss: 0.6760\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3458 - model_104_loss: 0.4391 - model_105_loss: 0.6827 - model_105_1_loss: 0.6743\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3550 - model_104_loss: 0.4414 - model_105_loss: 0.6834 - model_105_1_loss: 0.6759\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3663 - model_104_loss: 0.4456 - model_105_loss: 0.6853 - model_105_1_loss: 0.6771\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3696 - model_104_loss: 0.4498 - model_105_loss: 0.6850 - model_105_1_loss: 0.6789\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3722 - model_104_loss: 0.4530 - model_105_loss: 0.6851 - model_105_1_loss: 0.6800\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.8372 - model_105_loss: 0.6866 - model_105_1_loss: 0.6810\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3810 - model_104_loss: 0.4547 - model_105_loss: 0.6865 - model_105_1_loss: 0.6806\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3776 - model_104_loss: 0.4580 - model_105_loss: 0.6860 - model_105_1_loss: 0.6811\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3839 - model_104_loss: 0.4621 - model_105_loss: 0.6870 - model_105_1_loss: 0.6822\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3956 - model_104_loss: 0.4657 - model_105_loss: 0.6883 - model_105_1_loss: 0.6839\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.3982 - model_104_loss: 0.4704 - model_105_loss: 0.6885 - model_105_1_loss: 0.6852\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.8774 - model_105_loss: 0.6895 - model_105_1_loss: 0.68600s - loss: 6.8682 - model_105_loss: 0.6870 - model_105_1_loss: \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4034 - model_104_loss: 0.4720 - model_105_loss: 0.6893 - model_105_1_loss: 0.6858\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4028 - model_104_loss: 0.4728 - model_105_loss: 0.6891 - model_105_1_loss: 0.6860\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4069 - model_104_loss: 0.4786 - model_105_loss: 0.6897 - model_105_1_loss: 0.6874\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4106 - model_104_loss: 0.4801 - model_105_loss: 0.6902 - model_105_1_loss: 0.6880\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4159 - model_104_loss: 0.4822 - model_105_loss: 0.6908 - model_105_1_loss: 0.6888\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.8968 - model_105_loss: 0.6905 - model_105_1_loss: 0.68930s - loss: 6.9248 - model_105_loss: 0.6951 - model_105_1_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4180 - model_104_loss: 0.4851 - model_105_loss: 0.6912 - model_105_1_loss: 0.6895\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4200 - model_104_loss: 0.4875 - model_105_loss: 0.6911 - model_105_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4215 - model_104_loss: 0.4894 - model_105_loss: 0.6917 - model_105_1_loss: 0.6904\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4142 - model_104_loss: 0.4918 - model_105_loss: 0.6908 - model_105_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4238 - model_104_loss: 0.4929 - model_105_loss: 0.6921 - model_105_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9199 - model_105_loss: 0.6932 - model_105_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4246 - model_104_loss: 0.4950 - model_105_loss: 0.6920 - model_105_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4251 - model_104_loss: 0.4933 - model_105_loss: 0.6919 - model_105_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4263 - model_104_loss: 0.4947 - model_105_loss: 0.6921 - model_105_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4270 - model_104_loss: 0.4938 - model_105_loss: 0.6920 - model_105_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4304 - model_104_loss: 0.4932 - model_105_loss: 0.6923 - model_105_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9212 - model_105_loss: 0.6924 - model_105_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4261 - model_104_loss: 0.4911 - model_105_loss: 0.6918 - model_105_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4293 - model_104_loss: 0.4916 - model_105_loss: 0.6921 - model_105_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4283 - model_104_loss: 0.4902 - model_105_loss: 0.6917 - model_105_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4315 - model_104_loss: 0.4899 - model_105_loss: 0.6923 - model_105_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4300 - model_104_loss: 0.4914 - model_105_loss: 0.6923 - model_105_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9183 - model_105_loss: 0.6915 - model_105_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4276 - model_104_loss: 0.4903 - model_105_loss: 0.6919 - model_105_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4301 - model_104_loss: 0.4894 - model_105_loss: 0.6922 - model_105_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4314 - model_104_loss: 0.4871 - model_105_loss: 0.6919 - model_105_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4327 - model_104_loss: 0.4870 - model_105_loss: 0.6923 - model_105_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4316 - model_104_loss: 0.4878 - model_105_loss: 0.6921 - model_105_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9180 - model_105_loss: 0.6920 - model_105_1_loss: 0.691 - 1s 32us/sample - loss: 6.9222 - model_105_loss: 0.6923 - model_105_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4306 - model_104_loss: 0.4855 - model_105_loss: 0.6917 - model_105_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4336 - model_104_loss: 0.4859 - model_105_loss: 0.6921 - model_105_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4366 - model_104_loss: 0.4828 - model_105_loss: 0.6922 - model_105_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4351 - model_104_loss: 0.4859 - model_105_loss: 0.6923 - model_105_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4373 - model_104_loss: 0.4821 - model_105_loss: 0.6920 - model_105_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.9202 - model_105_loss: 0.6916 - model_105_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4320 - model_104_loss: 0.4858 - model_105_loss: 0.6919 - model_105_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4369 - model_104_loss: 0.4822 - model_105_loss: 0.6923 - model_105_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4355 - model_104_loss: 0.4828 - model_105_loss: 0.6919 - model_105_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4357 - model_104_loss: 0.4819 - model_105_loss: 0.6918 - model_105_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 11us/sample - loss: -6.4363 - model_104_loss: 0.4830 - model_105_loss: 0.6919 - model_105_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9187 - model_105_loss: 0.6918 - model_105_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4396 - model_104_loss: 0.4811 - model_105_loss: 0.6925 - model_105_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4365 - model_104_loss: 0.4801 - model_105_loss: 0.6920 - model_105_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4351 - model_104_loss: 0.4805 - model_105_loss: 0.6919 - model_105_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4385 - model_104_loss: 0.4796 - model_105_loss: 0.6921 - model_105_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4433 - model_104_loss: 0.4766 - model_105_loss: 0.6925 - model_105_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9188 - model_105_loss: 0.6927 - model_105_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4421 - model_104_loss: 0.4774 - model_105_loss: 0.6924 - model_105_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4385 - model_104_loss: 0.4781 - model_105_loss: 0.6913 - model_105_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4414 - model_104_loss: 0.4772 - model_105_loss: 0.6923 - model_105_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4415 - model_104_loss: 0.4794 - model_105_loss: 0.6923 - model_105_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4450 - model_104_loss: 0.4767 - model_105_loss: 0.6926 - model_105_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9192 - model_105_loss: 0.6932 - model_105_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4402 - model_104_loss: 0.4786 - model_105_loss: 0.6922 - model_105_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4395 - model_104_loss: 0.4785 - model_105_loss: 0.6920 - model_105_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4384 - model_104_loss: 0.4783 - model_105_loss: 0.6919 - model_105_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4408 - model_104_loss: 0.4787 - model_105_loss: 0.6921 - model_105_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4410 - model_104_loss: 0.4794 - model_105_loss: 0.6923 - model_105_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9194 - model_105_loss: 0.6924 - model_105_1_loss: 0.69170s - loss: 6.9259 - model_105_loss: 0.6906 - model_105_1_loss: \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4413 - model_104_loss: 0.4795 - model_105_loss: 0.6921 - model_105_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4441 - model_104_loss: 0.4779 - model_105_loss: 0.6921 - model_105_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4429 - model_104_loss: 0.4781 - model_105_loss: 0.6920 - model_105_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4412 - model_104_loss: 0.4790 - model_105_loss: 0.6922 - model_105_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4417 - model_104_loss: 0.4812 - model_105_loss: 0.6924 - model_105_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9249 - model_105_loss: 0.6924 - model_105_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4435 - model_104_loss: 0.4785 - model_105_loss: 0.6920 - model_105_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4430 - model_104_loss: 0.4791 - model_105_loss: 0.6920 - model_105_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4463 - model_104_loss: 0.4785 - model_105_loss: 0.6924 - model_105_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4447 - model_104_loss: 0.4792 - model_105_loss: 0.6921 - model_105_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4450 - model_104_loss: 0.4792 - model_105_loss: 0.6925 - model_105_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9245 - model_105_loss: 0.6930 - model_105_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4454 - model_104_loss: 0.4774 - model_105_loss: 0.6923 - model_105_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4442 - model_104_loss: 0.4777 - model_105_loss: 0.6920 - model_105_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4441 - model_104_loss: 0.4765 - model_105_loss: 0.6920 - model_105_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4464 - model_104_loss: 0.4747 - model_105_loss: 0.6920 - model_105_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4465 - model_104_loss: 0.4746 - model_105_loss: 0.6922 - model_105_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9229 - model_105_loss: 0.6926 - model_105_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4477 - model_104_loss: 0.4733 - model_105_loss: 0.6920 - model_105_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4449 - model_104_loss: 0.4728 - model_105_loss: 0.6919 - model_105_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4459 - model_104_loss: 0.4722 - model_105_loss: 0.6920 - model_105_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4484 - model_104_loss: 0.4712 - model_105_loss: 0.6923 - model_105_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4449 - model_104_loss: 0.4716 - model_105_loss: 0.6920 - model_105_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: 6.9164 - model_105_loss: 0.6919 - model_105_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4523 - model_104_loss: 0.4697 - model_105_loss: 0.6927 - model_105_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4481 - model_104_loss: 0.4690 - model_105_loss: 0.6922 - model_105_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4466 - model_104_loss: 0.4702 - model_105_loss: 0.6919 - model_105_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4498 - model_104_loss: 0.4707 - model_105_loss: 0.6925 - model_105_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4486 - model_104_loss: 0.4704 - model_105_loss: 0.6924 - model_105_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9176 - model_105_loss: 0.6926 - model_105_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4470 - model_104_loss: 0.4706 - model_105_loss: 0.6921 - model_105_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4474 - model_104_loss: 0.4698 - model_105_loss: 0.6918 - model_105_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4481 - model_104_loss: 0.4721 - model_105_loss: 0.6923 - model_105_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4473 - model_104_loss: 0.4724 - model_105_loss: 0.6923 - model_105_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4504 - model_104_loss: 0.4719 - model_105_loss: 0.6923 - model_105_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9229 - model_105_loss: 0.6925 - model_105_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4497 - model_104_loss: 0.4737 - model_105_loss: 0.6925 - model_105_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4513 - model_104_loss: 0.4726 - model_105_loss: 0.6926 - model_105_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4488 - model_104_loss: 0.4737 - model_105_loss: 0.6921 - model_105_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4491 - model_104_loss: 0.4754 - model_105_loss: 0.6923 - model_105_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4498 - model_104_loss: 0.4749 - model_105_loss: 0.6927 - model_105_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9269 - model_105_loss: 0.6929 - model_105_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4495 - model_104_loss: 0.4754 - model_105_loss: 0.6928 - model_105_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4483 - model_104_loss: 0.4746 - model_105_loss: 0.6922 - model_105_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4479 - model_104_loss: 0.4752 - model_105_loss: 0.6924 - model_105_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4467 - model_104_loss: 0.4753 - model_105_loss: 0.6920 - model_105_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4491 - model_104_loss: 0.4755 - model_105_loss: 0.6926 - model_105_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9253 - model_105_loss: 0.6922 - model_105_1_loss: 0.69250s - loss: 6.9017 - model_105_loss: 0.6882 - model_105_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4509 - model_104_loss: 0.4754 - model_105_loss: 0.6928 - model_105_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4511 - model_104_loss: 0.4727 - model_105_loss: 0.6924 - model_105_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4516 - model_104_loss: 0.4718 - model_105_loss: 0.6924 - model_105_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4513 - model_104_loss: 0.4733 - model_105_loss: 0.6926 - model_105_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4527 - model_104_loss: 0.4707 - model_105_loss: 0.6926 - model_105_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9226 - model_105_loss: 0.6924 - model_105_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4511 - model_104_loss: 0.4700 - model_105_loss: 0.6924 - model_105_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4486 - model_104_loss: 0.4695 - model_105_loss: 0.6921 - model_105_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4524 - model_104_loss: 0.4685 - model_105_loss: 0.6925 - model_105_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4535 - model_104_loss: 0.4661 - model_105_loss: 0.6922 - model_105_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4507 - model_104_loss: 0.4666 - model_105_loss: 0.6920 - model_105_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: 6.9210 - model_105_loss: 0.6924 - model_105_1_loss: 0.69140s - loss: 6.9380 - model_105_loss: 0.6965 - model_105_1_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4544 - model_104_loss: 0.4656 - model_105_loss: 0.6926 - model_105_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4483 - model_104_loss: 0.4669 - model_105_loss: 0.6921 - model_105_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4550 - model_104_loss: 0.4632 - model_105_loss: 0.6923 - model_105_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4522 - model_104_loss: 0.4672 - model_105_loss: 0.6923 - model_105_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4527 - model_104_loss: 0.4661 - model_105_loss: 0.6923 - model_105_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9193 - model_105_loss: 0.6924 - model_105_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4507 - model_104_loss: 0.4672 - model_105_loss: 0.6924 - model_105_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4548 - model_104_loss: 0.4647 - model_105_loss: 0.6924 - model_105_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4537 - model_104_loss: 0.4670 - model_105_loss: 0.6925 - model_105_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4550 - model_104_loss: 0.4653 - model_105_loss: 0.6923 - model_105_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4537 - model_104_loss: 0.4670 - model_105_loss: 0.6926 - model_105_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9200 - model_105_loss: 0.6924 - model_105_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4480 - model_104_loss: 0.4683 - model_105_loss: 0.6922 - model_105_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4507 - model_104_loss: 0.4691 - model_105_loss: 0.6925 - model_105_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4529 - model_104_loss: 0.4665 - model_105_loss: 0.6925 - model_105_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4517 - model_104_loss: 0.4686 - model_105_loss: 0.6926 - model_105_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4521 - model_104_loss: 0.4680 - model_105_loss: 0.6927 - model_105_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.9196 - model_105_loss: 0.6929 - model_105_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4502 - model_104_loss: 0.4689 - model_105_loss: 0.6922 - model_105_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4486 - model_104_loss: 0.4682 - model_105_loss: 0.6920 - model_105_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4501 - model_104_loss: 0.4657 - model_105_loss: 0.6920 - model_105_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4488 - model_104_loss: 0.4670 - model_105_loss: 0.6921 - model_105_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4499 - model_104_loss: 0.4678 - model_105_loss: 0.6922 - model_105_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9186 - model_105_loss: 0.6924 - model_105_1_loss: 0.69140s - loss: 6.9011 - model_105_loss: 0.6896 - model_105_1_loss: \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4493 - model_104_loss: 0.4668 - model_105_loss: 0.6920 - model_105_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4534 - model_104_loss: 0.4635 - model_105_loss: 0.6920 - model_105_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4514 - model_104_loss: 0.4651 - model_105_loss: 0.6921 - model_105_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4526 - model_104_loss: 0.4652 - model_105_loss: 0.6923 - model_105_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4536 - model_104_loss: 0.4644 - model_105_loss: 0.6923 - model_105_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9163 - model_105_loss: 0.6924 - model_105_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4533 - model_104_loss: 0.4642 - model_105_loss: 0.6923 - model_105_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4553 - model_104_loss: 0.4631 - model_105_loss: 0.6924 - model_105_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4578 - model_104_loss: 0.4629 - model_105_loss: 0.6924 - model_105_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4554 - model_104_loss: 0.4617 - model_105_loss: 0.6920 - model_105_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4574 - model_104_loss: 0.4616 - model_105_loss: 0.6922 - model_105_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9177 - model_105_loss: 0.6928 - model_105_1_loss: 0.690 - 1s 31us/sample - loss: 6.9152 - model_105_loss: 0.6916 - model_105_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4554 - model_104_loss: 0.4606 - model_105_loss: 0.6920 - model_105_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4548 - model_104_loss: 0.4609 - model_105_loss: 0.6922 - model_105_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4508 - model_104_loss: 0.4618 - model_105_loss: 0.6918 - model_105_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4536 - model_104_loss: 0.4609 - model_105_loss: 0.6920 - model_105_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4499 - model_104_loss: 0.4618 - model_105_loss: 0.6916 - model_105_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9163 - model_105_loss: 0.6928 - model_105_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4515 - model_104_loss: 0.4618 - model_105_loss: 0.6922 - model_105_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4463 - model_104_loss: 0.4649 - model_105_loss: 0.6918 - model_105_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4519 - model_104_loss: 0.4628 - model_105_loss: 0.6921 - model_105_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4529 - model_104_loss: 0.4632 - model_105_loss: 0.6921 - model_105_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4514 - model_104_loss: 0.4655 - model_105_loss: 0.6922 - model_105_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9185 - model_105_loss: 0.6927 - model_105_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4536 - model_104_loss: 0.4646 - model_105_loss: 0.6923 - model_105_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4535 - model_104_loss: 0.4666 - model_105_loss: 0.6927 - model_105_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4507 - model_104_loss: 0.4704 - model_105_loss: 0.6923 - model_105_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4540 - model_104_loss: 0.4705 - model_105_loss: 0.6926 - model_105_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4562 - model_104_loss: 0.4704 - model_105_loss: 0.6927 - model_105_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9269 - model_105_loss: 0.6924 - model_105_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4526 - model_104_loss: 0.4729 - model_105_loss: 0.6927 - model_105_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4548 - model_104_loss: 0.4733 - model_105_loss: 0.6927 - model_105_1_loss: 0.6930\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4535 - model_104_loss: 0.4760 - model_105_loss: 0.6930 - model_105_1_loss: 0.6929\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4557 - model_104_loss: 0.4755 - model_105_loss: 0.6928 - model_105_1_loss: 0.6934\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4552 - model_104_loss: 0.4765 - model_105_loss: 0.6929 - model_105_1_loss: 0.6934\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9317 - model_105_loss: 0.6931 - model_105_1_loss: 0.6934\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4473 - model_104_loss: 0.4784 - model_105_loss: 0.6922 - model_105_1_loss: 0.6929\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4489 - model_104_loss: 0.4778 - model_105_loss: 0.6924 - model_105_1_loss: 0.6929\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4493 - model_104_loss: 0.4778 - model_105_loss: 0.6926 - model_105_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4508 - model_104_loss: 0.4754 - model_105_loss: 0.6925 - model_105_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4484 - model_104_loss: 0.4748 - model_105_loss: 0.6920 - model_105_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9259 - model_105_loss: 0.6924 - model_105_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4528 - model_104_loss: 0.4727 - model_105_loss: 0.6927 - model_105_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4561 - model_104_loss: 0.4680 - model_105_loss: 0.6926 - model_105_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4588 - model_104_loss: 0.4669 - model_105_loss: 0.6933 - model_105_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4592 - model_104_loss: 0.4654 - model_105_loss: 0.6929 - model_105_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4611 - model_104_loss: 0.4627 - model_105_loss: 0.6927 - model_105_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.9207 - model_105_loss: 0.6935 - model_105_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4541 - model_104_loss: 0.4617 - model_105_loss: 0.6921 - model_105_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4578 - model_104_loss: 0.4575 - model_105_loss: 0.6921 - model_105_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4567 - model_104_loss: 0.4587 - model_105_loss: 0.6923 - model_105_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4551 - model_104_loss: 0.4583 - model_105_loss: 0.6922 - model_105_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4535 - model_104_loss: 0.4582 - model_105_loss: 0.6920 - model_105_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9112 - model_105_loss: 0.6922 - model_105_1_loss: 0.6899\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4517 - model_104_loss: 0.4551 - model_105_loss: 0.6917 - model_105_1_loss: 0.6896\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4506 - model_104_loss: 0.4547 - model_105_loss: 0.6916 - model_105_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4541 - model_104_loss: 0.4533 - model_105_loss: 0.6918 - model_105_1_loss: 0.6897\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4519 - model_104_loss: 0.4550 - model_105_loss: 0.6915 - model_105_1_loss: 0.6899\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4526 - model_104_loss: 0.4541 - model_105_loss: 0.6917 - model_105_1_loss: 0.6896\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9099 - model_105_loss: 0.6924 - model_105_1_loss: 0.6899\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4551 - model_104_loss: 0.4553 - model_105_loss: 0.6920 - model_105_1_loss: 0.6901\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4523 - model_104_loss: 0.4563 - model_105_loss: 0.6916 - model_105_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4551 - model_104_loss: 0.4584 - model_105_loss: 0.6920 - model_105_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4548 - model_104_loss: 0.4575 - model_105_loss: 0.6918 - model_105_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4534 - model_104_loss: 0.4609 - model_105_loss: 0.6920 - model_105_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9154 - model_105_loss: 0.6922 - model_105_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4534 - model_104_loss: 0.4609 - model_105_loss: 0.6921 - model_105_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4609 - model_104_loss: 0.4634 - model_105_loss: 0.6930 - model_105_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4565 - model_104_loss: 0.4655 - model_105_loss: 0.6923 - model_105_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4570 - model_104_loss: 0.4664 - model_105_loss: 0.6927 - model_105_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4602 - model_104_loss: 0.4691 - model_105_loss: 0.6932 - model_105_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9303 - model_105_loss: 0.6932 - model_105_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4560 - model_104_loss: 0.4711 - model_105_loss: 0.6928 - model_105_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4563 - model_104_loss: 0.4731 - model_105_loss: 0.6931 - model_105_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4535 - model_104_loss: 0.4755 - model_105_loss: 0.6930 - model_105_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4526 - model_104_loss: 0.4763 - model_105_loss: 0.6930 - model_105_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4553 - model_104_loss: 0.4746 - model_105_loss: 0.6931 - model_105_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9294 - model_105_loss: 0.6923 - model_105_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4518 - model_104_loss: 0.4757 - model_105_loss: 0.6927 - model_105_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4518 - model_104_loss: 0.4754 - model_105_loss: 0.6927 - model_105_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4505 - model_104_loss: 0.4762 - model_105_loss: 0.6926 - model_105_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4534 - model_104_loss: 0.4738 - model_105_loss: 0.6929 - model_105_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4531 - model_104_loss: 0.4745 - model_105_loss: 0.6929 - model_105_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9273 - model_105_loss: 0.6928 - model_105_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4517 - model_104_loss: 0.4726 - model_105_loss: 0.6925 - model_105_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4533 - model_104_loss: 0.4703 - model_105_loss: 0.6923 - model_105_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4546 - model_104_loss: 0.4702 - model_105_loss: 0.6926 - model_105_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4579 - model_104_loss: 0.4666 - model_105_loss: 0.6925 - model_105_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4620 - model_104_loss: 0.4641 - model_105_loss: 0.6930 - model_105_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.9220 - model_105_loss: 0.6926 - model_105_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4556 - model_104_loss: 0.4633 - model_105_loss: 0.6924 - model_105_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4591 - model_104_loss: 0.4578 - model_105_loss: 0.6924 - model_105_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4598 - model_104_loss: 0.4573 - model_105_loss: 0.6924 - model_105_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4599 - model_104_loss: 0.4560 - model_105_loss: 0.6923 - model_105_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4586 - model_104_loss: 0.4539 - model_105_loss: 0.6921 - model_105_1_loss: 0.6904\n",
      "For Attention Module: 2.8000000000000003\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.3232 - model_109_loss: 0.6581 - model_109_1_loss: 0.6061\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -5.9491 - model_108_loss: 0.3747 - model_109_loss: 0.6585 - model_109_1_loss: 0.6062\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -5.9621 - model_108_loss: 0.3758 - model_109_loss: 0.6595 - model_109_1_loss: 0.6080\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -5.9711 - model_108_loss: 0.3770 - model_109_loss: 0.6601 - model_109_1_loss: 0.6095\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -5.9715 - model_108_loss: 0.3772 - model_109_loss: 0.6600 - model_109_1_loss: 0.6098\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -5.9789 - model_108_loss: 0.3794 - model_109_loss: 0.6600 - model_109_1_loss: 0.6117\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.3765 - model_109_loss: 0.6604 - model_109_1_loss: 0.6139\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -5.9875 - model_108_loss: 0.3796 - model_109_loss: 0.6603 - model_109_1_loss: 0.6131\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -5.9964 - model_108_loss: 0.3781 - model_109_loss: 0.6611 - model_109_1_loss: 0.6138\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0205 - model_108_loss: 0.3793 - model_109_loss: 0.6631 - model_109_1_loss: 0.6169\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.0307 - model_108_loss: 0.3792 - model_109_loss: 0.6641 - model_109_1_loss: 0.6178\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0336 - model_108_loss: 0.3785 - model_109_loss: 0.6630 - model_109_1_loss: 0.6194\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.4332 - model_109_loss: 0.6649 - model_109_1_loss: 0.6216\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.0418 - model_108_loss: 0.3798 - model_109_loss: 0.6642 - model_109_1_loss: 0.6201\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0516 - model_108_loss: 0.3822 - model_109_loss: 0.6632 - model_109_1_loss: 0.6236\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0630 - model_108_loss: 0.3813 - model_109_loss: 0.6645 - model_109_1_loss: 0.6244\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0785 - model_108_loss: 0.3809 - model_109_loss: 0.6657 - model_109_1_loss: 0.6262\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.0788 - model_108_loss: 0.3833 - model_109_loss: 0.6645 - model_109_1_loss: 0.6280\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.4804 - model_109_loss: 0.6661 - model_109_1_loss: 0.6293\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.0954 - model_108_loss: 0.3856 - model_109_loss: 0.6669 - model_109_1_loss: 0.6293\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.1096 - model_108_loss: 0.3848 - model_109_loss: 0.6673 - model_109_1_loss: 0.6316\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.1256 - model_108_loss: 0.3848 - model_109_loss: 0.6682 - model_109_1_loss: 0.6338\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.1294 - model_108_loss: 0.3892 - model_109_loss: 0.6681 - model_109_1_loss: 0.6356\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.1460 - model_108_loss: 0.3872 - model_109_loss: 0.6692 - model_109_1_loss: 0.6375\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.5458 - model_109_loss: 0.6712 - model_109_1_loss: 0.6379\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.1383 - model_108_loss: 0.3909 - model_109_loss: 0.6692 - model_109_1_loss: 0.6367\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.1565 - model_108_loss: 0.3903 - model_109_loss: 0.6701 - model_109_1_loss: 0.6392\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.1711 - model_108_loss: 0.3915 - model_109_loss: 0.6712 - model_109_1_loss: 0.6414\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.1855 - model_108_loss: 0.3937 - model_109_loss: 0.6722 - model_109_1_loss: 0.6436\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2067 - model_108_loss: 0.3965 - model_109_loss: 0.6738 - model_109_1_loss: 0.6468\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.5960 - model_109_loss: 0.6734 - model_109_1_loss: 0.64540s - loss: 6.5946 - model_109_loss: 0.6720 - model_109_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.1960 - model_108_loss: 0.3975 - model_109_loss: 0.6728 - model_109_1_loss: 0.6459\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2062 - model_108_loss: 0.3999 - model_109_loss: 0.6738 - model_109_1_loss: 0.6474\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2219 - model_108_loss: 0.4015 - model_109_loss: 0.6749 - model_109_1_loss: 0.6498\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.2404 - model_108_loss: 0.4024 - model_109_loss: 0.6762 - model_109_1_loss: 0.6524\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2408 - model_108_loss: 0.4039 - model_109_loss: 0.6745 - model_109_1_loss: 0.6544\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.6652 - model_109_loss: 0.6782 - model_109_1_loss: 0.6552\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.2626 - model_108_loss: 0.4052 - model_109_loss: 0.6778 - model_109_1_loss: 0.6557\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2690 - model_108_loss: 0.4077 - model_109_loss: 0.6778 - model_109_1_loss: 0.6575\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2825 - model_108_loss: 0.4094 - model_109_loss: 0.6788 - model_109_1_loss: 0.6596\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2907 - model_108_loss: 0.4104 - model_109_loss: 0.6792 - model_109_1_loss: 0.6610\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.2960 - model_108_loss: 0.4126 - model_109_loss: 0.6800 - model_109_1_loss: 0.6618\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.7232 - model_109_loss: 0.6800 - model_109_1_loss: 0.6642\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3031 - model_108_loss: 0.4154 - model_109_loss: 0.6800 - model_109_1_loss: 0.6637\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3148 - model_108_loss: 0.4163 - model_109_loss: 0.6808 - model_109_1_loss: 0.6654\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3264 - model_108_loss: 0.4192 - model_109_loss: 0.6818 - model_109_1_loss: 0.6673\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3313 - model_108_loss: 0.4209 - model_109_loss: 0.6823 - model_109_1_loss: 0.6681\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3408 - model_108_loss: 0.4234 - model_109_loss: 0.6831 - model_109_1_loss: 0.6697\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.7697 - model_109_loss: 0.6821 - model_109_1_loss: 0.6707\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3356 - model_108_loss: 0.4278 - model_109_loss: 0.6827 - model_109_1_loss: 0.6700\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3476 - model_108_loss: 0.4279 - model_109_loss: 0.6836 - model_109_1_loss: 0.6715\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3503 - model_108_loss: 0.4329 - model_109_loss: 0.6838 - model_109_1_loss: 0.6728\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3608 - model_108_loss: 0.4340 - model_109_loss: 0.6844 - model_109_1_loss: 0.6746\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3685 - model_108_loss: 0.4351 - model_109_loss: 0.6844 - model_109_1_loss: 0.6763\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.8067 - model_109_loss: 0.6846 - model_109_1_loss: 0.6763\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3681 - model_108_loss: 0.4372 - model_109_loss: 0.6852 - model_109_1_loss: 0.6758\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3781 - model_108_loss: 0.4404 - model_109_loss: 0.6867 - model_109_1_loss: 0.6770\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.3807 - model_108_loss: 0.4429 - model_109_loss: 0.6862 - model_109_1_loss: 0.6785\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3924 - model_108_loss: 0.4450 - model_109_loss: 0.6879 - model_109_1_loss: 0.6796\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3934 - model_108_loss: 0.4467 - model_109_loss: 0.6875 - model_109_1_loss: 0.6805\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.8473 - model_109_loss: 0.6876 - model_109_1_loss: 0.6817\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3959 - model_108_loss: 0.4494 - model_109_loss: 0.6878 - model_109_1_loss: 0.6813\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3961 - model_108_loss: 0.4524 - model_109_loss: 0.6878 - model_109_1_loss: 0.6819\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4027 - model_108_loss: 0.4535 - model_109_loss: 0.6886 - model_109_1_loss: 0.6827\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4084 - model_108_loss: 0.4569 - model_109_loss: 0.6890 - model_109_1_loss: 0.6841\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4131 - model_108_loss: 0.4581 - model_109_loss: 0.6895 - model_109_1_loss: 0.6848\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.8745 - model_109_loss: 0.6893 - model_109_1_loss: 0.6854\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4163 - model_108_loss: 0.4580 - model_109_loss: 0.6891 - model_109_1_loss: 0.6857\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4121 - model_108_loss: 0.4634 - model_109_loss: 0.6891 - model_109_1_loss: 0.6860\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4209 - model_108_loss: 0.4642 - model_109_loss: 0.6901 - model_109_1_loss: 0.6869\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4233 - model_108_loss: 0.4665 - model_109_loss: 0.6906 - model_109_1_loss: 0.6873\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4253 - model_108_loss: 0.4683 - model_109_loss: 0.6903 - model_109_1_loss: 0.6884\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.8986 - model_109_loss: 0.6910 - model_109_1_loss: 0.6888\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4276 - model_108_loss: 0.4686 - model_109_loss: 0.6908 - model_109_1_loss: 0.6884\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4327 - model_108_loss: 0.4697 - model_109_loss: 0.6909 - model_109_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4299 - model_108_loss: 0.4737 - model_109_loss: 0.6909 - model_109_1_loss: 0.6899\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4311 - model_108_loss: 0.4770 - model_109_loss: 0.6913 - model_109_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4338 - model_108_loss: 0.4797 - model_109_loss: 0.6913 - model_109_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9121 - model_109_loss: 0.6910 - model_109_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4277 - model_108_loss: 0.4798 - model_109_loss: 0.6911 - model_109_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4298 - model_108_loss: 0.4821 - model_109_loss: 0.6914 - model_109_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4324 - model_108_loss: 0.4832 - model_109_loss: 0.6916 - model_109_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4346 - model_108_loss: 0.4857 - model_109_loss: 0.6923 - model_109_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4366 - model_108_loss: 0.4869 - model_109_loss: 0.6923 - model_109_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9243 - model_109_loss: 0.6930 - model_109_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4310 - model_108_loss: 0.4883 - model_109_loss: 0.6916 - model_109_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4298 - model_108_loss: 0.4890 - model_109_loss: 0.6913 - model_109_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4314 - model_108_loss: 0.4891 - model_109_loss: 0.6917 - model_109_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4355 - model_108_loss: 0.4896 - model_109_loss: 0.6920 - model_109_1_loss: 0.6930\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4325 - model_108_loss: 0.4899 - model_109_loss: 0.6916 - model_109_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9223 - model_109_loss: 0.6914 - model_109_1_loss: 0.6930\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4302 - model_108_loss: 0.4898 - model_109_loss: 0.6911 - model_109_1_loss: 0.6929\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4335 - model_108_loss: 0.4892 - model_109_loss: 0.6917 - model_109_1_loss: 0.6929\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4299 - model_108_loss: 0.4915 - model_109_loss: 0.6914 - model_109_1_loss: 0.6929\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4338 - model_108_loss: 0.4880 - model_109_loss: 0.6914 - model_109_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4326 - model_108_loss: 0.4886 - model_109_loss: 0.6914 - model_109_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9219 - model_109_loss: 0.6910 - model_109_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4320 - model_108_loss: 0.4858 - model_109_loss: 0.6910 - model_109_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4298 - model_108_loss: 0.4875 - model_109_loss: 0.6906 - model_109_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4325 - model_108_loss: 0.4877 - model_109_loss: 0.6911 - model_109_1_loss: 0.6929\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4339 - model_108_loss: 0.4850 - model_109_loss: 0.6909 - model_109_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4351 - model_108_loss: 0.4853 - model_109_loss: 0.6913 - model_109_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9165 - model_109_loss: 0.6910 - model_109_1_loss: 0.69240s - loss: 6.9299 - model_109_loss: 0.6939 - model_109_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4302 - model_108_loss: 0.4827 - model_109_loss: 0.6903 - model_109_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4289 - model_108_loss: 0.4820 - model_109_loss: 0.6900 - model_109_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4329 - model_108_loss: 0.4813 - model_109_loss: 0.6903 - model_109_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4332 - model_108_loss: 0.4783 - model_109_loss: 0.6900 - model_109_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4326 - model_108_loss: 0.4805 - model_109_loss: 0.6902 - model_109_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9117 - model_109_loss: 0.6904 - model_109_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4320 - model_108_loss: 0.4787 - model_109_loss: 0.6903 - model_109_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4304 - model_108_loss: 0.4796 - model_109_loss: 0.6900 - model_109_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4360 - model_108_loss: 0.4757 - model_109_loss: 0.6908 - model_109_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4303 - model_108_loss: 0.4772 - model_109_loss: 0.6900 - model_109_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4330 - model_108_loss: 0.4775 - model_109_loss: 0.6905 - model_109_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9134 - model_109_loss: 0.6908 - model_109_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4348 - model_108_loss: 0.4753 - model_109_loss: 0.6903 - model_109_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4347 - model_108_loss: 0.4749 - model_109_loss: 0.6906 - model_109_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4378 - model_108_loss: 0.4739 - model_109_loss: 0.6908 - model_109_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4365 - model_108_loss: 0.4747 - model_109_loss: 0.6909 - model_109_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4371 - model_108_loss: 0.4734 - model_109_loss: 0.6904 - model_109_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9170 - model_109_loss: 0.6925 - model_109_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4380 - model_108_loss: 0.4748 - model_109_loss: 0.6910 - model_109_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4394 - model_108_loss: 0.4744 - model_109_loss: 0.6913 - model_109_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4398 - model_108_loss: 0.4733 - model_109_loss: 0.6908 - model_109_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4382 - model_108_loss: 0.4746 - model_109_loss: 0.6912 - model_109_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4398 - model_108_loss: 0.4733 - model_109_loss: 0.6911 - model_109_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9183 - model_109_loss: 0.6922 - model_109_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4440 - model_108_loss: 0.4724 - model_109_loss: 0.6917 - model_109_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4438 - model_108_loss: 0.4724 - model_109_loss: 0.6917 - model_109_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4451 - model_108_loss: 0.4720 - model_109_loss: 0.6920 - model_109_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4442 - model_108_loss: 0.4721 - model_109_loss: 0.6919 - model_109_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4452 - model_108_loss: 0.4706 - model_109_loss: 0.6918 - model_109_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9125 - model_109_loss: 0.6925 - model_109_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4445 - model_108_loss: 0.4682 - model_109_loss: 0.6913 - model_109_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4436 - model_108_loss: 0.4694 - model_109_loss: 0.6915 - model_109_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4467 - model_108_loss: 0.4682 - model_109_loss: 0.6919 - model_109_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4458 - model_108_loss: 0.4679 - model_109_loss: 0.6918 - model_109_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4405 - model_108_loss: 0.4688 - model_109_loss: 0.6908 - model_109_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9130 - model_109_loss: 0.6918 - model_109_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4404 - model_108_loss: 0.4671 - model_109_loss: 0.6912 - model_109_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4425 - model_108_loss: 0.4644 - model_109_loss: 0.6910 - model_109_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4439 - model_108_loss: 0.4662 - model_109_loss: 0.6915 - model_109_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4474 - model_108_loss: 0.4647 - model_109_loss: 0.6918 - model_109_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4478 - model_108_loss: 0.4649 - model_109_loss: 0.6918 - model_109_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9121 - model_109_loss: 0.6914 - model_109_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4423 - model_108_loss: 0.4678 - model_109_loss: 0.6915 - model_109_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4466 - model_108_loss: 0.4655 - model_109_loss: 0.6917 - model_109_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4451 - model_108_loss: 0.4662 - model_109_loss: 0.6916 - model_109_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4442 - model_108_loss: 0.4690 - model_109_loss: 0.6919 - model_109_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4478 - model_108_loss: 0.4667 - model_109_loss: 0.6920 - model_109_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9144 - model_109_loss: 0.6923 - model_109_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4423 - model_108_loss: 0.4674 - model_109_loss: 0.6915 - model_109_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4427 - model_108_loss: 0.4688 - model_109_loss: 0.6916 - model_109_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4444 - model_108_loss: 0.4680 - model_109_loss: 0.6915 - model_109_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4468 - model_108_loss: 0.4696 - model_109_loss: 0.6920 - model_109_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4448 - model_108_loss: 0.4704 - model_109_loss: 0.6921 - model_109_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9138 - model_109_loss: 0.6919 - model_109_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4465 - model_108_loss: 0.4699 - model_109_loss: 0.6921 - model_109_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4469 - model_108_loss: 0.4717 - model_109_loss: 0.6922 - model_109_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4480 - model_108_loss: 0.4723 - model_109_loss: 0.6927 - model_109_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4486 - model_108_loss: 0.4722 - model_109_loss: 0.6927 - model_109_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4469 - model_108_loss: 0.4748 - model_109_loss: 0.6920 - model_109_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9222 - model_109_loss: 0.6922 - model_109_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4482 - model_108_loss: 0.4745 - model_109_loss: 0.6924 - model_109_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4475 - model_108_loss: 0.4738 - model_109_loss: 0.6921 - model_109_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4491 - model_108_loss: 0.4743 - model_109_loss: 0.6924 - model_109_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4486 - model_108_loss: 0.4749 - model_109_loss: 0.6924 - model_109_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4482 - model_108_loss: 0.4759 - model_109_loss: 0.6927 - model_109_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9251 - model_109_loss: 0.6931 - model_109_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4445 - model_108_loss: 0.4786 - model_109_loss: 0.6924 - model_109_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4484 - model_108_loss: 0.4774 - model_109_loss: 0.6928 - model_109_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4458 - model_108_loss: 0.4785 - model_109_loss: 0.6926 - model_109_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4450 - model_108_loss: 0.4782 - model_109_loss: 0.6923 - model_109_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4468 - model_108_loss: 0.4765 - model_109_loss: 0.6925 - model_109_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9274 - model_109_loss: 0.6929 - model_109_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4497 - model_108_loss: 0.4779 - model_109_loss: 0.6927 - model_109_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4493 - model_108_loss: 0.4788 - model_109_loss: 0.6929 - model_109_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4482 - model_108_loss: 0.4781 - model_109_loss: 0.6928 - model_109_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4474 - model_108_loss: 0.4778 - model_109_loss: 0.6927 - model_109_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4463 - model_108_loss: 0.4784 - model_109_loss: 0.6926 - model_109_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9261 - model_109_loss: 0.6936 - model_109_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4461 - model_108_loss: 0.4763 - model_109_loss: 0.6924 - model_109_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4475 - model_108_loss: 0.4750 - model_109_loss: 0.6925 - model_109_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4502 - model_108_loss: 0.4734 - model_109_loss: 0.6926 - model_109_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4473 - model_108_loss: 0.4734 - model_109_loss: 0.6923 - model_109_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4524 - model_108_loss: 0.4710 - model_109_loss: 0.6926 - model_109_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9221 - model_109_loss: 0.6928 - model_109_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4468 - model_108_loss: 0.4713 - model_109_loss: 0.6920 - model_109_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4496 - model_108_loss: 0.4700 - model_109_loss: 0.6923 - model_109_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4492 - model_108_loss: 0.4713 - model_109_loss: 0.6926 - model_109_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4516 - model_108_loss: 0.4684 - model_109_loss: 0.6924 - model_109_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4485 - model_108_loss: 0.4693 - model_109_loss: 0.6922 - model_109_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9184 - model_109_loss: 0.6925 - model_109_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4534 - model_108_loss: 0.4669 - model_109_loss: 0.6925 - model_109_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4526 - model_108_loss: 0.4679 - model_109_loss: 0.6920 - model_109_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4522 - model_108_loss: 0.4665 - model_109_loss: 0.6920 - model_109_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4547 - model_108_loss: 0.4681 - model_109_loss: 0.6928 - model_109_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4523 - model_108_loss: 0.4683 - model_109_loss: 0.6924 - model_109_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.9204 - model_109_loss: 0.6932 - model_109_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4508 - model_108_loss: 0.4691 - model_109_loss: 0.6924 - model_109_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4498 - model_108_loss: 0.4694 - model_109_loss: 0.6923 - model_109_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4511 - model_108_loss: 0.4707 - model_109_loss: 0.6924 - model_109_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4506 - model_108_loss: 0.4718 - model_109_loss: 0.6927 - model_109_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4503 - model_108_loss: 0.4718 - model_109_loss: 0.6924 - model_109_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9239 - model_109_loss: 0.6928 - model_109_1_loss: 0.69220s - loss: 6.9331 - model_109_loss: 0.6950 - model_109_1_l\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4499 - model_108_loss: 0.4746 - model_109_loss: 0.6928 - model_109_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4514 - model_108_loss: 0.4727 - model_109_loss: 0.6927 - model_109_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4517 - model_108_loss: 0.4729 - model_109_loss: 0.6928 - model_109_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4515 - model_108_loss: 0.4722 - model_109_loss: 0.6927 - model_109_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4518 - model_108_loss: 0.4730 - model_109_loss: 0.6928 - model_109_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9246 - model_109_loss: 0.6922 - model_109_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4519 - model_108_loss: 0.4715 - model_109_loss: 0.6926 - model_109_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4508 - model_108_loss: 0.4720 - model_109_loss: 0.6926 - model_109_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4518 - model_108_loss: 0.4740 - model_109_loss: 0.6928 - model_109_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4524 - model_108_loss: 0.4727 - model_109_loss: 0.6928 - model_109_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4490 - model_108_loss: 0.4729 - model_109_loss: 0.6923 - model_109_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.9258 - model_109_loss: 0.6933 - model_109_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4495 - model_108_loss: 0.4721 - model_109_loss: 0.6924 - model_109_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4493 - model_108_loss: 0.4722 - model_109_loss: 0.6925 - model_109_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4514 - model_108_loss: 0.4696 - model_109_loss: 0.6925 - model_109_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4520 - model_108_loss: 0.4699 - model_109_loss: 0.6924 - model_109_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4504 - model_108_loss: 0.4714 - model_109_loss: 0.6925 - model_109_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9232 - model_109_loss: 0.6927 - model_109_1_loss: 0.69150s - loss: 6.9095 - model_109_loss: 0.6886 - model_109_1_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4485 - model_108_loss: 0.4702 - model_109_loss: 0.6922 - model_109_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4505 - model_108_loss: 0.4702 - model_109_loss: 0.6925 - model_109_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4501 - model_108_loss: 0.4681 - model_109_loss: 0.6923 - model_109_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4512 - model_108_loss: 0.4662 - model_109_loss: 0.6923 - model_109_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4509 - model_108_loss: 0.4683 - model_109_loss: 0.6924 - model_109_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9189 - model_109_loss: 0.6927 - model_109_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4492 - model_108_loss: 0.4663 - model_109_loss: 0.6920 - model_109_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4500 - model_108_loss: 0.4676 - model_109_loss: 0.6925 - model_109_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4512 - model_108_loss: 0.4651 - model_109_loss: 0.6923 - model_109_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4542 - model_108_loss: 0.4661 - model_109_loss: 0.6925 - model_109_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4501 - model_108_loss: 0.4660 - model_109_loss: 0.6918 - model_109_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9223 - model_109_loss: 0.6929 - model_109_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4522 - model_108_loss: 0.4664 - model_109_loss: 0.6925 - model_109_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4501 - model_108_loss: 0.4674 - model_109_loss: 0.6922 - model_109_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4494 - model_108_loss: 0.4682 - model_109_loss: 0.6924 - model_109_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4499 - model_108_loss: 0.4680 - model_109_loss: 0.6924 - model_109_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4528 - model_108_loss: 0.4667 - model_109_loss: 0.6923 - model_109_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9209 - model_109_loss: 0.6927 - model_109_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4519 - model_108_loss: 0.4672 - model_109_loss: 0.6925 - model_109_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4534 - model_108_loss: 0.4672 - model_109_loss: 0.6925 - model_109_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4567 - model_108_loss: 0.4657 - model_109_loss: 0.6927 - model_109_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4536 - model_108_loss: 0.4667 - model_109_loss: 0.6925 - model_109_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4508 - model_108_loss: 0.4677 - model_109_loss: 0.6922 - model_109_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9210 - model_109_loss: 0.6924 - model_109_1_loss: 0.69130s - loss: 6.8971 - model_109_loss: 0.6917 - model_109_1_l\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4563 - model_108_loss: 0.4659 - model_109_loss: 0.6926 - model_109_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4543 - model_108_loss: 0.4657 - model_109_loss: 0.6923 - model_109_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4571 - model_108_loss: 0.4647 - model_109_loss: 0.6926 - model_109_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4555 - model_108_loss: 0.4651 - model_109_loss: 0.6925 - model_109_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4557 - model_108_loss: 0.4636 - model_109_loss: 0.6920 - model_109_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9190 - model_109_loss: 0.6926 - model_109_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4535 - model_108_loss: 0.4642 - model_109_loss: 0.6924 - model_109_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4543 - model_108_loss: 0.4645 - model_109_loss: 0.6926 - model_109_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4550 - model_108_loss: 0.4635 - model_109_loss: 0.6924 - model_109_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4550 - model_108_loss: 0.4628 - model_109_loss: 0.6925 - model_109_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4565 - model_108_loss: 0.4611 - model_109_loss: 0.6923 - model_109_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9180 - model_109_loss: 0.6924 - model_109_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4507 - model_108_loss: 0.4636 - model_109_loss: 0.6920 - model_109_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4526 - model_108_loss: 0.4623 - model_109_loss: 0.6921 - model_109_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4539 - model_108_loss: 0.4619 - model_109_loss: 0.6923 - model_109_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4501 - model_108_loss: 0.4639 - model_109_loss: 0.6921 - model_109_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4535 - model_108_loss: 0.4615 - model_109_loss: 0.6923 - model_109_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9147 - model_109_loss: 0.6920 - model_109_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4513 - model_108_loss: 0.4629 - model_109_loss: 0.6920 - model_109_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4520 - model_108_loss: 0.4628 - model_109_loss: 0.6924 - model_109_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4538 - model_108_loss: 0.4609 - model_109_loss: 0.6922 - model_109_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4487 - model_108_loss: 0.4631 - model_109_loss: 0.6916 - model_109_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4511 - model_108_loss: 0.4636 - model_109_loss: 0.6921 - model_109_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9208 - model_109_loss: 0.6925 - model_109_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4511 - model_108_loss: 0.4628 - model_109_loss: 0.6920 - model_109_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4540 - model_108_loss: 0.4631 - model_109_loss: 0.6922 - model_109_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4531 - model_108_loss: 0.4645 - model_109_loss: 0.6923 - model_109_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4565 - model_108_loss: 0.4638 - model_109_loss: 0.6927 - model_109_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4542 - model_108_loss: 0.4648 - model_109_loss: 0.6924 - model_109_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9233 - model_109_loss: 0.6927 - model_109_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4555 - model_108_loss: 0.4645 - model_109_loss: 0.6924 - model_109_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4534 - model_108_loss: 0.4663 - model_109_loss: 0.6923 - model_109_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4567 - model_108_loss: 0.4667 - model_109_loss: 0.6926 - model_109_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4526 - model_108_loss: 0.4673 - model_109_loss: 0.6922 - model_109_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4541 - model_108_loss: 0.4683 - model_109_loss: 0.6926 - model_109_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9250 - model_109_loss: 0.6926 - model_109_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4525 - model_108_loss: 0.4678 - model_109_loss: 0.6921 - model_109_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4541 - model_108_loss: 0.4690 - model_109_loss: 0.6925 - model_109_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4555 - model_108_loss: 0.4683 - model_109_loss: 0.6928 - model_109_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4555 - model_108_loss: 0.4683 - model_109_loss: 0.6925 - model_109_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4550 - model_108_loss: 0.4700 - model_109_loss: 0.6928 - model_109_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9248 - model_109_loss: 0.6921 - model_109_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4548 - model_108_loss: 0.4697 - model_109_loss: 0.6925 - model_109_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4529 - model_108_loss: 0.4695 - model_109_loss: 0.6924 - model_109_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4539 - model_108_loss: 0.4698 - model_109_loss: 0.6925 - model_109_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4544 - model_108_loss: 0.4692 - model_109_loss: 0.6924 - model_109_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4543 - model_108_loss: 0.4693 - model_109_loss: 0.6926 - model_109_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9243 - model_109_loss: 0.6931 - model_109_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4530 - model_108_loss: 0.4680 - model_109_loss: 0.6923 - model_109_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4573 - model_108_loss: 0.4675 - model_109_loss: 0.6929 - model_109_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4545 - model_108_loss: 0.4671 - model_109_loss: 0.6924 - model_109_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4556 - model_108_loss: 0.4677 - model_109_loss: 0.6926 - model_109_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4566 - model_108_loss: 0.4656 - model_109_loss: 0.6925 - model_109_1_loss: 0.6919\n",
      "For Attention Module: 2.9000000000000004\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: 6.3611 - model_113_loss: 0.6609 - model_113_1_loss: 0.6110\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -5.9786 - model_112_loss: 0.3748 - model_113_loss: 0.6604 - model_113_1_loss: 0.6102\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -5.9818 - model_112_loss: 0.3770 - model_113_loss: 0.6615 - model_113_1_loss: 0.6103\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -5.9899 - model_112_loss: 0.3770 - model_113_loss: 0.6604 - model_113_1_loss: 0.6130\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.0116 - model_112_loss: 0.3768 - model_113_loss: 0.6631 - model_113_1_loss: 0.6146\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.0128 - model_112_loss: 0.3766 - model_113_loss: 0.6625 - model_113_1_loss: 0.6154\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.3977 - model_113_loss: 0.6618 - model_113_1_loss: 0.6167\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.0247 - model_112_loss: 0.3778 - model_113_loss: 0.6628 - model_113_1_loss: 0.6177\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.0289 - model_112_loss: 0.3807 - model_113_loss: 0.6631 - model_113_1_loss: 0.6188\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.0461 - model_112_loss: 0.3819 - model_113_loss: 0.6639 - model_113_1_loss: 0.6217\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.0609 - model_112_loss: 0.3805 - model_113_loss: 0.6652 - model_113_1_loss: 0.6231\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.0597 - model_112_loss: 0.3834 - model_113_loss: 0.6638 - model_113_1_loss: 0.6249\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.4659 - model_113_loss: 0.6654 - model_113_1_loss: 0.6274\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.0719 - model_112_loss: 0.3833 - model_113_loss: 0.6652 - model_113_1_loss: 0.6259\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.1021 - model_112_loss: 0.3841 - model_113_loss: 0.6663 - model_113_1_loss: 0.6310\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.1189 - model_112_loss: 0.3839 - model_113_loss: 0.6678 - model_113_1_loss: 0.6327\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.1197 - model_112_loss: 0.3863 - model_113_loss: 0.6674 - model_113_1_loss: 0.6338\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.1339 - model_112_loss: 0.3879 - model_113_loss: 0.6684 - model_113_1_loss: 0.6360\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.5377 - model_113_loss: 0.6687 - model_113_1_loss: 0.6389\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.1450 - model_112_loss: 0.3890 - model_113_loss: 0.6685 - model_113_1_loss: 0.6383\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.1542 - model_112_loss: 0.3914 - model_113_loss: 0.6688 - model_113_1_loss: 0.6403\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.1763 - model_112_loss: 0.3909 - model_113_loss: 0.6708 - model_113_1_loss: 0.6427\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.1955 - model_112_loss: 0.3945 - model_113_loss: 0.6715 - model_113_1_loss: 0.6465\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.2118 - model_112_loss: 0.3957 - model_113_loss: 0.6728 - model_113_1_loss: 0.6487\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.6066 - model_113_loss: 0.6729 - model_113_1_loss: 0.6478\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.2079 - model_112_loss: 0.3962 - model_113_loss: 0.6734 - model_113_1_loss: 0.6475\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2136 - model_112_loss: 0.4015 - model_113_loss: 0.6735 - model_113_1_loss: 0.6495\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.2262 - model_112_loss: 0.4015 - model_113_loss: 0.6735 - model_113_1_loss: 0.6520\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.2365 - model_112_loss: 0.4034 - model_113_loss: 0.6742 - model_113_1_loss: 0.6538\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.2604 - model_112_loss: 0.4035 - model_113_loss: 0.6761 - model_113_1_loss: 0.6566\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.6735 - model_113_loss: 0.6772 - model_113_1_loss: 0.6578\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.2498 - model_112_loss: 0.4082 - model_113_loss: 0.6752 - model_113_1_loss: 0.6564\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2694 - model_112_loss: 0.4100 - model_113_loss: 0.6775 - model_113_1_loss: 0.6583\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.2745 - model_112_loss: 0.4134 - model_113_loss: 0.6772 - model_113_1_loss: 0.6603\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.2842 - model_112_loss: 0.4130 - model_113_loss: 0.6775 - model_113_1_loss: 0.6619\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3011 - model_112_loss: 0.4184 - model_113_loss: 0.6795 - model_113_1_loss: 0.6644\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.7213 - model_113_loss: 0.6790 - model_113_1_loss: 0.6650\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3083 - model_112_loss: 0.4190 - model_113_loss: 0.6803 - model_113_1_loss: 0.6652\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3083 - model_112_loss: 0.4234 - model_113_loss: 0.6794 - model_113_1_loss: 0.6669\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3266 - model_112_loss: 0.4239 - model_113_loss: 0.6820 - model_113_1_loss: 0.6681\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3321 - model_112_loss: 0.4289 - model_113_loss: 0.6820 - model_113_1_loss: 0.6702\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3367 - model_112_loss: 0.4294 - model_113_loss: 0.6816 - model_113_1_loss: 0.6716\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.7752 - model_113_loss: 0.6835 - model_113_1_loss: 0.6721\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3498 - model_112_loss: 0.4324 - model_113_loss: 0.6846 - model_113_1_loss: 0.6718\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3528 - model_112_loss: 0.4324 - model_113_loss: 0.6840 - model_113_1_loss: 0.6730\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3639 - model_112_loss: 0.4364 - model_113_loss: 0.6850 - model_113_1_loss: 0.6751\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3661 - model_112_loss: 0.4384 - model_113_loss: 0.6853 - model_113_1_loss: 0.6756\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3818 - model_112_loss: 0.4413 - model_113_loss: 0.6868 - model_113_1_loss: 0.6778\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.8192 - model_113_loss: 0.6859 - model_113_1_loss: 0.6782\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3737 - model_112_loss: 0.4448 - model_113_loss: 0.6860 - model_113_1_loss: 0.6777\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3877 - model_112_loss: 0.4452 - model_113_loss: 0.6872 - model_113_1_loss: 0.6793\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3802 - model_112_loss: 0.4509 - model_113_loss: 0.6863 - model_113_1_loss: 0.6799\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3909 - model_112_loss: 0.4511 - model_113_loss: 0.6869 - model_113_1_loss: 0.6815\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3962 - model_112_loss: 0.4524 - model_113_loss: 0.6875 - model_113_1_loss: 0.6822\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: 6.8578 - model_113_loss: 0.6876 - model_113_1_loss: 0.6833\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3962 - model_112_loss: 0.4584 - model_113_loss: 0.6878 - model_113_1_loss: 0.6831\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 12us/sample - loss: -6.4015 - model_112_loss: 0.4591 - model_113_loss: 0.6882 - model_113_1_loss: 0.6839\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4040 - model_112_loss: 0.4616 - model_113_loss: 0.6891 - model_113_1_loss: 0.6840\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4082 - model_112_loss: 0.4644 - model_113_loss: 0.6892 - model_113_1_loss: 0.6853\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4125 - model_112_loss: 0.4655 - model_113_loss: 0.6895 - model_113_1_loss: 0.6861\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.8841 - model_113_loss: 0.6893 - model_113_1_loss: 0.68690s - loss: 6.8554 - model_113_loss: 0.6829 - model_113_1_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4118 - model_112_loss: 0.4686 - model_113_loss: 0.6894 - model_113_1_loss: 0.6867\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4093 - model_112_loss: 0.4734 - model_113_loss: 0.6897 - model_113_1_loss: 0.6868\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4197 - model_112_loss: 0.4715 - model_113_loss: 0.6906 - model_113_1_loss: 0.6877\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4200 - model_112_loss: 0.4727 - model_113_loss: 0.6903 - model_113_1_loss: 0.6882\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4210 - model_112_loss: 0.4764 - model_113_loss: 0.6907 - model_113_1_loss: 0.6887\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.8988 - model_113_loss: 0.6907 - model_113_1_loss: 0.6888\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4223 - model_112_loss: 0.4796 - model_113_loss: 0.6912 - model_113_1_loss: 0.6892\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4223 - model_112_loss: 0.4809 - model_113_loss: 0.6911 - model_113_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4263 - model_112_loss: 0.4810 - model_113_loss: 0.6918 - model_113_1_loss: 0.6896\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4253 - model_112_loss: 0.4853 - model_113_loss: 0.6918 - model_113_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4271 - model_112_loss: 0.4851 - model_113_loss: 0.6919 - model_113_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9165 - model_113_loss: 0.6926 - model_113_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4245 - model_112_loss: 0.4884 - model_113_loss: 0.6919 - model_113_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4247 - model_112_loss: 0.4908 - model_113_loss: 0.6924 - model_113_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4266 - model_112_loss: 0.4889 - model_113_loss: 0.6922 - model_113_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4256 - model_112_loss: 0.4901 - model_113_loss: 0.6923 - model_113_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4281 - model_112_loss: 0.4930 - model_113_loss: 0.6929 - model_113_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9209 - model_113_loss: 0.6922 - model_113_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4242 - model_112_loss: 0.4951 - model_113_loss: 0.6927 - model_113_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4296 - model_112_loss: 0.4930 - model_113_loss: 0.6927 - model_113_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4294 - model_112_loss: 0.4939 - model_113_loss: 0.6931 - model_113_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4306 - model_112_loss: 0.4947 - model_113_loss: 0.6935 - model_113_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4307 - model_112_loss: 0.4948 - model_113_loss: 0.6931 - model_113_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9285 - model_113_loss: 0.6928 - model_113_1_loss: 0.69210s - loss: 6.9213 - model_113_loss: 0.6914 - model_113_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4297 - model_112_loss: 0.4941 - model_113_loss: 0.6931 - model_113_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4329 - model_112_loss: 0.4932 - model_113_loss: 0.6933 - model_113_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4307 - model_112_loss: 0.4937 - model_113_loss: 0.6929 - model_113_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4363 - model_112_loss: 0.4909 - model_113_loss: 0.6934 - model_113_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4361 - model_112_loss: 0.4927 - model_113_loss: 0.6933 - model_113_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9300 - model_113_loss: 0.6937 - model_113_1_loss: 0.69270s - loss: 7.0184 - model_113_loss: 0.7091 - model_11\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4383 - model_112_loss: 0.4904 - model_113_loss: 0.6933 - model_113_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4406 - model_112_loss: 0.4895 - model_113_loss: 0.6934 - model_113_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4373 - model_112_loss: 0.4905 - model_113_loss: 0.6931 - model_113_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4386 - model_112_loss: 0.4902 - model_113_loss: 0.6933 - model_113_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4429 - model_112_loss: 0.4854 - model_113_loss: 0.6932 - model_113_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.9262 - model_113_loss: 0.6937 - model_113_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4375 - model_112_loss: 0.4859 - model_113_loss: 0.6930 - model_113_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4403 - model_112_loss: 0.4838 - model_113_loss: 0.6931 - model_113_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4413 - model_112_loss: 0.4835 - model_113_loss: 0.6930 - model_113_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4387 - model_112_loss: 0.4852 - model_113_loss: 0.6929 - model_113_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4414 - model_112_loss: 0.4822 - model_113_loss: 0.6932 - model_113_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9217 - model_113_loss: 0.6938 - model_113_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4376 - model_112_loss: 0.4830 - model_113_loss: 0.6927 - model_113_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4392 - model_112_loss: 0.4807 - model_113_loss: 0.6927 - model_113_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4425 - model_112_loss: 0.4767 - model_113_loss: 0.6928 - model_113_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4364 - model_112_loss: 0.4809 - model_113_loss: 0.6926 - model_113_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4389 - model_112_loss: 0.4788 - model_113_loss: 0.6926 - model_113_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: 6.9168 - model_113_loss: 0.6925 - model_113_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4354 - model_112_loss: 0.4785 - model_113_loss: 0.6925 - model_113_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4368 - model_112_loss: 0.4754 - model_113_loss: 0.6922 - model_113_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4402 - model_112_loss: 0.4754 - model_113_loss: 0.6926 - model_113_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4385 - model_112_loss: 0.4743 - model_113_loss: 0.6922 - model_113_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4382 - model_112_loss: 0.4729 - model_113_loss: 0.6922 - model_113_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9100 - model_113_loss: 0.6918 - model_113_1_loss: 0.68970s - loss: 6.9073 - model_113_loss: 0.6913 - model_113_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4375 - model_112_loss: 0.4729 - model_113_loss: 0.6920 - model_113_1_loss: 0.6901\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4372 - model_112_loss: 0.4716 - model_113_loss: 0.6919 - model_113_1_loss: 0.6899\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4352 - model_112_loss: 0.4714 - model_113_loss: 0.6918 - model_113_1_loss: 0.6895\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4373 - model_112_loss: 0.4704 - model_113_loss: 0.6917 - model_113_1_loss: 0.6898\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4347 - model_112_loss: 0.4731 - model_113_loss: 0.6920 - model_113_1_loss: 0.6896\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9052 - model_113_loss: 0.6912 - model_113_1_loss: 0.6891\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4345 - model_112_loss: 0.4691 - model_113_loss: 0.6914 - model_113_1_loss: 0.6893\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4337 - model_112_loss: 0.4699 - model_113_loss: 0.6914 - model_113_1_loss: 0.6893\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4366 - model_112_loss: 0.4700 - model_113_loss: 0.6917 - model_113_1_loss: 0.6897\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4358 - model_112_loss: 0.4700 - model_113_loss: 0.6916 - model_113_1_loss: 0.6895\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4359 - model_112_loss: 0.4693 - model_113_loss: 0.6916 - model_113_1_loss: 0.6894\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.9074 - model_113_loss: 0.6916 - model_113_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4348 - model_112_loss: 0.4690 - model_113_loss: 0.6912 - model_113_1_loss: 0.6896\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4322 - model_112_loss: 0.4697 - model_113_loss: 0.6917 - model_113_1_loss: 0.6887\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4373 - model_112_loss: 0.4688 - model_113_loss: 0.6916 - model_113_1_loss: 0.6896\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4315 - model_112_loss: 0.4700 - model_113_loss: 0.6914 - model_113_1_loss: 0.6889\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4363 - model_112_loss: 0.4701 - model_113_loss: 0.6916 - model_113_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9077 - model_113_loss: 0.6913 - model_113_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4359 - model_112_loss: 0.4697 - model_113_loss: 0.6914 - model_113_1_loss: 0.6897\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4367 - model_112_loss: 0.4694 - model_113_loss: 0.6914 - model_113_1_loss: 0.6899\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4401 - model_112_loss: 0.4686 - model_113_loss: 0.6915 - model_113_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4369 - model_112_loss: 0.4697 - model_113_loss: 0.6913 - model_113_1_loss: 0.6900\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4413 - model_112_loss: 0.4690 - model_113_loss: 0.6918 - model_113_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9120 - model_113_loss: 0.6920 - model_113_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4383 - model_112_loss: 0.4694 - model_113_loss: 0.6918 - model_113_1_loss: 0.6897\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4373 - model_112_loss: 0.4706 - model_113_loss: 0.6913 - model_113_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4402 - model_112_loss: 0.4707 - model_113_loss: 0.6918 - model_113_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4406 - model_112_loss: 0.4710 - model_113_loss: 0.6920 - model_113_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4396 - model_112_loss: 0.4715 - model_113_loss: 0.6919 - model_113_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9157 - model_113_loss: 0.6926 - model_113_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4388 - model_112_loss: 0.4733 - model_113_loss: 0.6917 - model_113_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4409 - model_112_loss: 0.4716 - model_113_loss: 0.6919 - model_113_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4408 - model_112_loss: 0.4730 - model_113_loss: 0.6918 - model_113_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4443 - model_112_loss: 0.4721 - model_113_loss: 0.6921 - model_113_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4418 - model_112_loss: 0.4727 - model_113_loss: 0.6919 - model_113_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.9135 - model_113_loss: 0.6920 - model_113_1_loss: 0.69090s - loss: 6.9487 - model_113_loss: 0.6961 - model_113_1_lo\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4415 - model_112_loss: 0.4734 - model_113_loss: 0.6921 - model_113_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4415 - model_112_loss: 0.4751 - model_113_loss: 0.6919 - model_113_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4439 - model_112_loss: 0.4739 - model_113_loss: 0.6926 - model_113_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4431 - model_112_loss: 0.4733 - model_113_loss: 0.6922 - model_113_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4476 - model_112_loss: 0.4716 - model_113_loss: 0.6924 - model_113_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9170 - model_113_loss: 0.6923 - model_113_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4411 - model_112_loss: 0.4746 - model_113_loss: 0.6919 - model_113_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4436 - model_112_loss: 0.4737 - model_113_loss: 0.6920 - model_113_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4410 - model_112_loss: 0.4760 - model_113_loss: 0.6922 - model_113_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4442 - model_112_loss: 0.4744 - model_113_loss: 0.6923 - model_113_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4463 - model_112_loss: 0.4736 - model_113_loss: 0.6924 - model_113_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9165 - model_113_loss: 0.6916 - model_113_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4409 - model_112_loss: 0.4742 - model_113_loss: 0.6921 - model_113_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4412 - model_112_loss: 0.4754 - model_113_loss: 0.6923 - model_113_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4434 - model_112_loss: 0.4742 - model_113_loss: 0.6924 - model_113_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4426 - model_112_loss: 0.4735 - model_113_loss: 0.6923 - model_113_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4420 - model_112_loss: 0.4734 - model_113_loss: 0.6921 - model_113_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9171 - model_113_loss: 0.6921 - model_113_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4419 - model_112_loss: 0.4722 - model_113_loss: 0.6921 - model_113_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4435 - model_112_loss: 0.4722 - model_113_loss: 0.6922 - model_113_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4420 - model_112_loss: 0.4726 - model_113_loss: 0.6922 - model_113_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4440 - model_112_loss: 0.4720 - model_113_loss: 0.6925 - model_113_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4431 - model_112_loss: 0.4705 - model_113_loss: 0.6920 - model_113_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9163 - model_113_loss: 0.6928 - model_113_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4439 - model_112_loss: 0.4711 - model_113_loss: 0.6922 - model_113_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4418 - model_112_loss: 0.4706 - model_113_loss: 0.6920 - model_113_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4438 - model_112_loss: 0.4722 - model_113_loss: 0.6924 - model_113_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4464 - model_112_loss: 0.4698 - model_113_loss: 0.6922 - model_113_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4428 - model_112_loss: 0.4717 - model_113_loss: 0.6922 - model_113_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.9159 - model_113_loss: 0.6922 - model_113_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4461 - model_112_loss: 0.4714 - model_113_loss: 0.6924 - model_113_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4472 - model_112_loss: 0.4709 - model_113_loss: 0.6924 - model_113_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4484 - model_112_loss: 0.4708 - model_113_loss: 0.6927 - model_113_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4466 - model_112_loss: 0.4696 - model_113_loss: 0.6922 - model_113_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4504 - model_112_loss: 0.4698 - model_113_loss: 0.6926 - model_113_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9175 - model_113_loss: 0.6915 - model_113_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4491 - model_112_loss: 0.4677 - model_113_loss: 0.6920 - model_113_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4474 - model_112_loss: 0.4688 - model_113_loss: 0.6921 - model_113_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4501 - model_112_loss: 0.4695 - model_113_loss: 0.6925 - model_113_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4494 - model_112_loss: 0.4680 - model_113_loss: 0.6923 - model_113_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4464 - model_112_loss: 0.4702 - model_113_loss: 0.6921 - model_113_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: 6.9223 - model_113_loss: 0.6937 - model_113_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4492 - model_112_loss: 0.4690 - model_113_loss: 0.6923 - model_113_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4511 - model_112_loss: 0.4703 - model_113_loss: 0.6924 - model_113_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4503 - model_112_loss: 0.4688 - model_113_loss: 0.6926 - model_113_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4518 - model_112_loss: 0.4692 - model_113_loss: 0.6927 - model_113_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4495 - model_112_loss: 0.4693 - model_113_loss: 0.6923 - model_113_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9216 - model_113_loss: 0.6917 - model_113_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4455 - model_112_loss: 0.4720 - model_113_loss: 0.6922 - model_113_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4469 - model_112_loss: 0.4708 - model_113_loss: 0.6923 - model_113_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4484 - model_112_loss: 0.4707 - model_113_loss: 0.6924 - model_113_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4534 - model_112_loss: 0.4703 - model_113_loss: 0.6927 - model_113_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4463 - model_112_loss: 0.4728 - model_113_loss: 0.6923 - model_113_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9188 - model_113_loss: 0.6918 - model_113_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4490 - model_112_loss: 0.4695 - model_113_loss: 0.6923 - model_113_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4487 - model_112_loss: 0.4704 - model_113_loss: 0.6925 - model_113_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4479 - model_112_loss: 0.4720 - model_113_loss: 0.6924 - model_113_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4494 - model_112_loss: 0.4699 - model_113_loss: 0.6923 - model_113_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4493 - model_112_loss: 0.4712 - model_113_loss: 0.6923 - model_113_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.9202 - model_113_loss: 0.6928 - model_113_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4471 - model_112_loss: 0.4733 - model_113_loss: 0.6924 - model_113_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4467 - model_112_loss: 0.4728 - model_113_loss: 0.6924 - model_113_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4489 - model_112_loss: 0.4707 - model_113_loss: 0.6923 - model_113_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4497 - model_112_loss: 0.4709 - model_113_loss: 0.6927 - model_113_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4517 - model_112_loss: 0.4699 - model_113_loss: 0.6927 - model_113_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9234 - model_113_loss: 0.6933 - model_113_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4490 - model_112_loss: 0.4712 - model_113_loss: 0.6924 - model_113_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4502 - model_112_loss: 0.4695 - model_113_loss: 0.6923 - model_113_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4531 - model_112_loss: 0.4701 - model_113_loss: 0.6929 - model_113_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4521 - model_112_loss: 0.4703 - model_113_loss: 0.6926 - model_113_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4509 - model_112_loss: 0.4703 - model_113_loss: 0.6925 - model_113_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9208 - model_113_loss: 0.6917 - model_113_1_loss: 0.69140s - loss: 6.9478 - model_113_loss: 0.6979 - model_113_1_los\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4496 - model_112_loss: 0.4706 - model_113_loss: 0.6924 - model_113_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4502 - model_112_loss: 0.4702 - model_113_loss: 0.6924 - model_113_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4502 - model_112_loss: 0.4693 - model_113_loss: 0.6924 - model_113_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4515 - model_112_loss: 0.4694 - model_113_loss: 0.6924 - model_113_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4520 - model_112_loss: 0.4700 - model_113_loss: 0.6923 - model_113_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.9231 - model_113_loss: 0.6923 - model_113_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4504 - model_112_loss: 0.4707 - model_113_loss: 0.6925 - model_113_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4516 - model_112_loss: 0.4695 - model_113_loss: 0.6926 - model_113_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4499 - model_112_loss: 0.4714 - model_113_loss: 0.6925 - model_113_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4508 - model_112_loss: 0.4702 - model_113_loss: 0.6925 - model_113_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4523 - model_112_loss: 0.4690 - model_113_loss: 0.6926 - model_113_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9215 - model_113_loss: 0.6929 - model_113_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4520 - model_112_loss: 0.4691 - model_113_loss: 0.6925 - model_113_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4540 - model_112_loss: 0.4681 - model_113_loss: 0.6926 - model_113_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4514 - model_112_loss: 0.4694 - model_113_loss: 0.6925 - model_113_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4502 - model_112_loss: 0.4689 - model_113_loss: 0.6923 - model_113_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4543 - model_112_loss: 0.4679 - model_113_loss: 0.6926 - model_113_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.9217 - model_113_loss: 0.6922 - model_113_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4541 - model_112_loss: 0.4668 - model_113_loss: 0.6925 - model_113_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4539 - model_112_loss: 0.4653 - model_113_loss: 0.6924 - model_113_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4528 - model_112_loss: 0.4670 - model_113_loss: 0.6925 - model_113_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4505 - model_112_loss: 0.4670 - model_113_loss: 0.6923 - model_113_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4515 - model_112_loss: 0.4676 - model_113_loss: 0.6924 - model_113_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.9219 - model_113_loss: 0.6925 - model_113_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4529 - model_112_loss: 0.4667 - model_113_loss: 0.6925 - model_113_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4518 - model_112_loss: 0.4652 - model_113_loss: 0.6923 - model_113_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4532 - model_112_loss: 0.4652 - model_113_loss: 0.6925 - model_113_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4519 - model_112_loss: 0.4659 - model_113_loss: 0.6924 - model_113_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4532 - model_112_loss: 0.4650 - model_113_loss: 0.6924 - model_113_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: 6.9164 - model_113_loss: 0.6923 - model_113_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4521 - model_112_loss: 0.4653 - model_113_loss: 0.6924 - model_113_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4540 - model_112_loss: 0.4639 - model_113_loss: 0.6924 - model_113_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4533 - model_112_loss: 0.4653 - model_113_loss: 0.6927 - model_113_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4547 - model_112_loss: 0.4632 - model_113_loss: 0.6926 - model_113_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4537 - model_112_loss: 0.4641 - model_113_loss: 0.6924 - model_113_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.9186 - model_113_loss: 0.6921 - model_113_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4541 - model_112_loss: 0.4633 - model_113_loss: 0.6923 - model_113_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4538 - model_112_loss: 0.4640 - model_113_loss: 0.6923 - model_113_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4552 - model_112_loss: 0.4636 - model_113_loss: 0.6925 - model_113_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4543 - model_112_loss: 0.4643 - model_113_loss: 0.6925 - model_113_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4552 - model_112_loss: 0.4630 - model_113_loss: 0.6925 - model_113_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.9237 - model_113_loss: 0.6932 - model_113_1_loss: 0.69180s - loss: 6.9052 - model_113_loss: 0.6916 - model_113_1_lo\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4570 - model_112_loss: 0.4635 - model_113_loss: 0.6926 - model_113_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4526 - model_112_loss: 0.4672 - model_113_loss: 0.6924 - model_113_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4582 - model_112_loss: 0.4643 - model_113_loss: 0.6928 - model_113_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4552 - model_112_loss: 0.4667 - model_113_loss: 0.6926 - model_113_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4540 - model_112_loss: 0.4667 - model_113_loss: 0.6925 - model_113_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: 6.9213 - model_113_loss: 0.6933 - model_113_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4514 - model_112_loss: 0.4671 - model_113_loss: 0.6923 - model_113_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4531 - model_112_loss: 0.4665 - model_113_loss: 0.6924 - model_113_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4559 - model_112_loss: 0.4665 - model_113_loss: 0.6926 - model_113_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4519 - model_112_loss: 0.4673 - model_113_loss: 0.6924 - model_113_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4531 - model_112_loss: 0.4670 - model_113_loss: 0.6925 - model_113_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9219 - model_113_loss: 0.6926 - model_113_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4551 - model_112_loss: 0.4638 - model_113_loss: 0.6922 - model_113_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4552 - model_112_loss: 0.4650 - model_113_loss: 0.6924 - model_113_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4556 - model_112_loss: 0.4658 - model_113_loss: 0.6927 - model_113_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4523 - model_112_loss: 0.4658 - model_113_loss: 0.6921 - model_113_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4537 - model_112_loss: 0.4649 - model_113_loss: 0.6923 - model_113_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.9190 - model_113_loss: 0.6927 - model_113_1_loss: 0.69130s - loss: 6.9151 - model_113_loss: 0.6908 - model_113_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4537 - model_112_loss: 0.4664 - model_113_loss: 0.6923 - model_113_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4558 - model_112_loss: 0.4654 - model_113_loss: 0.6925 - model_113_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4542 - model_112_loss: 0.4675 - model_113_loss: 0.6926 - model_113_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4541 - model_112_loss: 0.4677 - model_113_loss: 0.6925 - model_113_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4567 - model_112_loss: 0.4652 - model_113_loss: 0.6925 - model_113_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9203 - model_113_loss: 0.6922 - model_113_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4558 - model_112_loss: 0.4656 - model_113_loss: 0.6927 - model_113_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4569 - model_112_loss: 0.4645 - model_113_loss: 0.6924 - model_113_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4547 - model_112_loss: 0.4656 - model_113_loss: 0.6923 - model_113_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4556 - model_112_loss: 0.4648 - model_113_loss: 0.6925 - model_113_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4554 - model_112_loss: 0.4650 - model_113_loss: 0.6924 - model_113_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9232 - model_113_loss: 0.6926 - model_113_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4558 - model_112_loss: 0.4647 - model_113_loss: 0.6924 - model_113_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4551 - model_112_loss: 0.4648 - model_113_loss: 0.6925 - model_113_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4565 - model_112_loss: 0.4640 - model_113_loss: 0.6925 - model_113_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4545 - model_112_loss: 0.4647 - model_113_loss: 0.6923 - model_113_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4580 - model_112_loss: 0.4645 - model_113_loss: 0.6926 - model_113_1_loss: 0.6919\n",
      "For Attention Module: 3.0000000000000004\n",
      "features X: 30940 samples, 63 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.8149 - model_117_loss: 0.6735 - model_117_1_loss: 0.6891\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: -6.3295 - model_116_loss: 0.4855 - model_117_loss: 0.6741 - model_117_1_loss: 0.6889\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3300 - model_116_loss: 0.4866 - model_117_loss: 0.6743 - model_117_1_loss: 0.6890\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3405 - model_116_loss: 0.4849 - model_117_loss: 0.6760 - model_117_1_loss: 0.6890\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3360 - model_116_loss: 0.4858 - model_117_loss: 0.6753 - model_117_1_loss: 0.6891\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3382 - model_116_loss: 0.4858 - model_117_loss: 0.6754 - model_117_1_loss: 0.6894\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.8295 - model_117_loss: 0.6775 - model_117_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3371 - model_116_loss: 0.4860 - model_117_loss: 0.6755 - model_117_1_loss: 0.6891\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3375 - model_116_loss: 0.4856 - model_117_loss: 0.6755 - model_117_1_loss: 0.6891\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3424 - model_116_loss: 0.4847 - model_117_loss: 0.6761 - model_117_1_loss: 0.6894\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3447 - model_116_loss: 0.4857 - model_117_loss: 0.6766 - model_117_1_loss: 0.6895\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3458 - model_116_loss: 0.4872 - model_117_loss: 0.6770 - model_117_1_loss: 0.6896\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.8358 - model_117_loss: 0.6780 - model_117_1_loss: 0.68970s - loss: 6.8404 - model_117_loss: 0.6774 - model_117_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3490 - model_116_loss: 0.4853 - model_117_loss: 0.6771 - model_117_1_loss: 0.6898\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3514 - model_116_loss: 0.4863 - model_117_loss: 0.6781 - model_117_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3506 - model_116_loss: 0.4869 - model_117_loss: 0.6777 - model_117_1_loss: 0.6897\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3589 - model_116_loss: 0.4850 - model_117_loss: 0.6790 - model_117_1_loss: 0.6898\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3583 - model_116_loss: 0.4875 - model_117_loss: 0.6792 - model_117_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.8482 - model_117_loss: 0.6794 - model_117_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3610 - model_116_loss: 0.4870 - model_117_loss: 0.6793 - model_117_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3660 - model_116_loss: 0.4855 - model_117_loss: 0.6803 - model_117_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3674 - model_116_loss: 0.4861 - model_117_loss: 0.6807 - model_117_1_loss: 0.6899\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3675 - model_116_loss: 0.4879 - model_117_loss: 0.6811 - model_117_1_loss: 0.6900\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3728 - model_116_loss: 0.4875 - model_117_loss: 0.6820 - model_117_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.8611 - model_117_loss: 0.6831 - model_117_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3710 - model_116_loss: 0.4889 - model_117_loss: 0.6819 - model_117_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3658 - model_116_loss: 0.4901 - model_117_loss: 0.6810 - model_117_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3714 - model_116_loss: 0.4903 - model_117_loss: 0.6820 - model_117_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.3807 - model_116_loss: 0.4902 - model_117_loss: 0.6838 - model_117_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3808 - model_116_loss: 0.4913 - model_117_loss: 0.6837 - model_117_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.8736 - model_117_loss: 0.6844 - model_117_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3869 - model_116_loss: 0.4917 - model_117_loss: 0.6851 - model_117_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3814 - model_116_loss: 0.4927 - model_117_loss: 0.6844 - model_117_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.3936 - model_116_loss: 0.4923 - model_117_loss: 0.6869 - model_117_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3992 - model_116_loss: 0.4920 - model_117_loss: 0.6874 - model_117_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4060 - model_116_loss: 0.4942 - model_117_loss: 0.6889 - model_117_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.8980 - model_117_loss: 0.6877 - model_117_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3957 - model_116_loss: 0.4952 - model_117_loss: 0.6870 - model_117_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4014 - model_116_loss: 0.4955 - model_117_loss: 0.6883 - model_117_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4067 - model_116_loss: 0.4966 - model_117_loss: 0.6895 - model_117_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4086 - model_116_loss: 0.4972 - model_117_loss: 0.6898 - model_117_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4069 - model_116_loss: 0.4976 - model_117_loss: 0.6896 - model_117_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9120 - model_117_loss: 0.6913 - model_117_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4115 - model_116_loss: 0.4987 - model_117_loss: 0.6907 - model_117_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4142 - model_116_loss: 0.4989 - model_117_loss: 0.6909 - model_117_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4138 - model_116_loss: 0.4994 - model_117_loss: 0.6908 - model_117_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4177 - model_116_loss: 0.5020 - model_117_loss: 0.6922 - model_117_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4192 - model_116_loss: 0.5004 - model_117_loss: 0.6920 - model_117_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9239 - model_117_loss: 0.6929 - model_117_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4165 - model_116_loss: 0.5022 - model_117_loss: 0.6919 - model_117_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4175 - model_116_loss: 0.5036 - model_117_loss: 0.6924 - model_117_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4187 - model_116_loss: 0.5037 - model_117_loss: 0.6923 - model_117_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4199 - model_116_loss: 0.5030 - model_117_loss: 0.6924 - model_117_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4212 - model_116_loss: 0.5053 - model_117_loss: 0.6931 - model_117_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9293 - model_117_loss: 0.6928 - model_117_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4190 - model_116_loss: 0.5048 - model_117_loss: 0.6927 - model_117_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4246 - model_116_loss: 0.5030 - model_117_loss: 0.6930 - model_117_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4199 - model_116_loss: 0.5052 - model_117_loss: 0.6928 - model_117_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4183 - model_116_loss: 0.5064 - model_117_loss: 0.6926 - model_117_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4218 - model_116_loss: 0.5060 - model_117_loss: 0.6932 - model_117_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9268 - model_117_loss: 0.6921 - model_117_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4181 - model_116_loss: 0.5091 - model_117_loss: 0.6931 - model_117_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4210 - model_116_loss: 0.5072 - model_117_loss: 0.6931 - model_117_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4210 - model_116_loss: 0.5067 - model_117_loss: 0.6930 - model_117_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4171 - model_116_loss: 0.5086 - model_117_loss: 0.6927 - model_117_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4192 - model_116_loss: 0.5084 - model_117_loss: 0.6930 - model_117_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9275 - model_117_loss: 0.6929 - model_117_1_loss: 0.69250s - loss: 6.8866 - model_117_loss: 0.6850 - model_117_1_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4182 - model_116_loss: 0.5082 - model_117_loss: 0.6927 - model_117_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4193 - model_116_loss: 0.5070 - model_117_loss: 0.6929 - model_117_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4168 - model_116_loss: 0.5085 - model_117_loss: 0.6927 - model_117_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4185 - model_116_loss: 0.5074 - model_117_loss: 0.6928 - model_117_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4187 - model_116_loss: 0.5077 - model_117_loss: 0.6928 - model_117_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9279 - model_117_loss: 0.6927 - model_117_1_loss: 0.69280s - loss: 6.9274 - model_117_loss: 0.6925 - model_117_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4198 - model_116_loss: 0.5055 - model_117_loss: 0.6927 - model_117_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4167 - model_116_loss: 0.5072 - model_117_loss: 0.6924 - model_117_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4193 - model_116_loss: 0.5062 - model_117_loss: 0.6926 - model_117_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4175 - model_116_loss: 0.5069 - model_117_loss: 0.6925 - model_117_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4200 - model_116_loss: 0.5041 - model_117_loss: 0.6924 - model_117_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9259 - model_117_loss: 0.6932 - model_117_1_loss: 0.69260s - loss: 6.9225 - model_117_loss: 0.6923 - model_117_1_loss: 0.692\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4170 - model_116_loss: 0.5050 - model_117_loss: 0.6921 - model_117_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4186 - model_116_loss: 0.5040 - model_117_loss: 0.6922 - model_117_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4185 - model_116_loss: 0.5046 - model_117_loss: 0.6923 - model_117_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4199 - model_116_loss: 0.5039 - model_117_loss: 0.6924 - model_117_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4176 - model_116_loss: 0.5053 - model_117_loss: 0.6924 - model_117_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9244 - model_117_loss: 0.6925 - model_117_1_loss: 0.69230s - loss: 6.9102 - model_117_loss: 0.6890 - model_117_1_loss:\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4191 - model_116_loss: 0.5036 - model_117_loss: 0.6924 - model_117_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4217 - model_116_loss: 0.5022 - model_117_loss: 0.6924 - model_117_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4216 - model_116_loss: 0.5017 - model_117_loss: 0.6924 - model_117_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4222 - model_116_loss: 0.5007 - model_117_loss: 0.6925 - model_117_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4226 - model_116_loss: 0.5013 - model_117_loss: 0.6925 - model_117_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9239 - model_117_loss: 0.6926 - model_117_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4213 - model_116_loss: 0.5008 - model_117_loss: 0.6923 - model_117_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4211 - model_116_loss: 0.5004 - model_117_loss: 0.6921 - model_117_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4201 - model_116_loss: 0.5006 - model_117_loss: 0.6920 - model_117_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4224 - model_116_loss: 0.4998 - model_117_loss: 0.6924 - model_117_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4236 - model_116_loss: 0.4993 - model_117_loss: 0.6924 - model_117_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9231 - model_117_loss: 0.6923 - model_117_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4238 - model_116_loss: 0.4987 - model_117_loss: 0.6923 - model_117_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4216 - model_116_loss: 0.4998 - model_117_loss: 0.6923 - model_117_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4229 - model_116_loss: 0.4994 - model_117_loss: 0.6924 - model_117_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4229 - model_116_loss: 0.4977 - model_117_loss: 0.6922 - model_117_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4239 - model_116_loss: 0.4980 - model_117_loss: 0.6924 - model_117_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9235 - model_117_loss: 0.6929 - model_117_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4265 - model_116_loss: 0.4965 - model_117_loss: 0.6923 - model_117_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4270 - model_116_loss: 0.4960 - model_117_loss: 0.6924 - model_117_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4243 - model_116_loss: 0.4968 - model_117_loss: 0.6921 - model_117_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4249 - model_116_loss: 0.4973 - model_117_loss: 0.6923 - model_117_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4267 - model_116_loss: 0.4954 - model_117_loss: 0.6921 - model_117_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9231 - model_117_loss: 0.6927 - model_117_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4239 - model_116_loss: 0.4975 - model_117_loss: 0.6923 - model_117_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4244 - model_116_loss: 0.4950 - model_117_loss: 0.6920 - model_117_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4245 - model_116_loss: 0.4973 - model_117_loss: 0.6922 - model_117_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4212 - model_116_loss: 0.4974 - model_117_loss: 0.6920 - model_117_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4240 - model_116_loss: 0.4972 - model_117_loss: 0.6922 - model_117_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9234 - model_117_loss: 0.6931 - model_117_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4244 - model_116_loss: 0.4964 - model_117_loss: 0.6923 - model_117_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4250 - model_116_loss: 0.4980 - model_117_loss: 0.6925 - model_117_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4262 - model_116_loss: 0.4964 - model_117_loss: 0.6925 - model_117_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4262 - model_116_loss: 0.4973 - model_117_loss: 0.6926 - model_117_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4251 - model_116_loss: 0.4977 - model_117_loss: 0.6925 - model_117_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9241 - model_117_loss: 0.6925 - model_117_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4243 - model_116_loss: 0.4975 - model_117_loss: 0.6923 - model_117_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4273 - model_116_loss: 0.4981 - model_117_loss: 0.6928 - model_117_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4221 - model_116_loss: 0.4981 - model_117_loss: 0.6922 - model_117_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4253 - model_116_loss: 0.4973 - model_117_loss: 0.6924 - model_117_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4236 - model_116_loss: 0.4988 - model_117_loss: 0.6926 - model_117_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9249 - model_117_loss: 0.6935 - model_117_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4249 - model_116_loss: 0.4985 - model_117_loss: 0.6926 - model_117_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4263 - model_116_loss: 0.4990 - model_117_loss: 0.6927 - model_117_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4269 - model_116_loss: 0.4984 - model_117_loss: 0.6928 - model_117_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4260 - model_116_loss: 0.4999 - model_117_loss: 0.6930 - model_117_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4243 - model_116_loss: 0.4992 - model_117_loss: 0.6926 - model_117_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9255 - model_117_loss: 0.6928 - model_117_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4238 - model_116_loss: 0.4994 - model_117_loss: 0.6924 - model_117_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4239 - model_116_loss: 0.5001 - model_117_loss: 0.6927 - model_117_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4254 - model_116_loss: 0.4998 - model_117_loss: 0.6927 - model_117_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4252 - model_116_loss: 0.4992 - model_117_loss: 0.6926 - model_117_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4269 - model_116_loss: 0.4989 - model_117_loss: 0.6927 - model_117_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9264 - model_117_loss: 0.6931 - model_117_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4250 - model_116_loss: 0.5000 - model_117_loss: 0.6928 - model_117_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4248 - model_116_loss: 0.5015 - model_117_loss: 0.6927 - model_117_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4266 - model_116_loss: 0.4994 - model_117_loss: 0.6928 - model_117_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4252 - model_116_loss: 0.4999 - model_117_loss: 0.6927 - model_117_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4272 - model_116_loss: 0.4982 - model_117_loss: 0.6928 - model_117_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9273 - model_117_loss: 0.6931 - model_117_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4242 - model_116_loss: 0.5008 - model_117_loss: 0.6927 - model_117_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4254 - model_116_loss: 0.5006 - model_117_loss: 0.6927 - model_117_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4256 - model_116_loss: 0.4996 - model_117_loss: 0.6927 - model_117_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4256 - model_116_loss: 0.5001 - model_117_loss: 0.6928 - model_117_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4272 - model_116_loss: 0.4995 - model_117_loss: 0.6929 - model_117_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9270 - model_117_loss: 0.6930 - model_117_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4244 - model_116_loss: 0.5010 - model_117_loss: 0.6928 - model_117_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4254 - model_116_loss: 0.4987 - model_117_loss: 0.6925 - model_117_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4258 - model_116_loss: 0.4989 - model_117_loss: 0.6926 - model_117_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4256 - model_116_loss: 0.4990 - model_117_loss: 0.6925 - model_117_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4268 - model_116_loss: 0.4984 - model_117_loss: 0.6927 - model_117_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9275 - model_117_loss: 0.6927 - model_117_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4270 - model_116_loss: 0.4997 - model_117_loss: 0.6929 - model_117_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4274 - model_116_loss: 0.4991 - model_117_loss: 0.6929 - model_117_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4264 - model_116_loss: 0.4988 - model_117_loss: 0.6925 - model_117_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4255 - model_116_loss: 0.5000 - model_117_loss: 0.6926 - model_117_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4304 - model_116_loss: 0.4973 - model_117_loss: 0.6929 - model_117_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9265 - model_117_loss: 0.6926 - model_117_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4249 - model_116_loss: 0.4997 - model_117_loss: 0.6925 - model_117_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4268 - model_116_loss: 0.4980 - model_117_loss: 0.6926 - model_117_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4263 - model_116_loss: 0.4992 - model_117_loss: 0.6926 - model_117_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4257 - model_116_loss: 0.4985 - model_117_loss: 0.6923 - model_117_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4267 - model_116_loss: 0.4989 - model_117_loss: 0.6927 - model_117_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9249 - model_117_loss: 0.6925 - model_117_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4274 - model_116_loss: 0.4988 - model_117_loss: 0.6928 - model_117_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4277 - model_116_loss: 0.4989 - model_117_loss: 0.6929 - model_117_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4268 - model_116_loss: 0.4984 - model_117_loss: 0.6928 - model_117_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4282 - model_116_loss: 0.4986 - model_117_loss: 0.6930 - model_117_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4294 - model_116_loss: 0.4969 - model_117_loss: 0.6929 - model_117_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9256 - model_117_loss: 0.6931 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4264 - model_116_loss: 0.4981 - model_117_loss: 0.6925 - model_117_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4264 - model_116_loss: 0.4967 - model_117_loss: 0.6924 - model_117_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4265 - model_116_loss: 0.4973 - model_117_loss: 0.6925 - model_117_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4259 - model_116_loss: 0.4973 - model_117_loss: 0.6923 - model_117_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4272 - model_116_loss: 0.4975 - model_117_loss: 0.6926 - model_117_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9248 - model_117_loss: 0.6924 - model_117_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4278 - model_116_loss: 0.4976 - model_117_loss: 0.6927 - model_117_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4264 - model_116_loss: 0.4972 - model_117_loss: 0.6924 - model_117_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4274 - model_116_loss: 0.4969 - model_117_loss: 0.6925 - model_117_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4259 - model_116_loss: 0.4974 - model_117_loss: 0.6923 - model_117_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4256 - model_116_loss: 0.4974 - model_117_loss: 0.6923 - model_117_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9238 - model_117_loss: 0.6924 - model_117_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4272 - model_116_loss: 0.4970 - model_117_loss: 0.6924 - model_117_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4286 - model_116_loss: 0.4963 - model_117_loss: 0.6925 - model_117_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4299 - model_116_loss: 0.4968 - model_117_loss: 0.6928 - model_117_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4294 - model_116_loss: 0.4969 - model_117_loss: 0.6927 - model_117_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4284 - model_116_loss: 0.4978 - model_117_loss: 0.6928 - model_117_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9254 - model_117_loss: 0.6930 - model_117_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4282 - model_116_loss: 0.4969 - model_117_loss: 0.6924 - model_117_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4286 - model_116_loss: 0.4972 - model_117_loss: 0.6926 - model_117_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4280 - model_116_loss: 0.4981 - model_117_loss: 0.6927 - model_117_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4298 - model_116_loss: 0.4969 - model_117_loss: 0.6927 - model_117_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4292 - model_116_loss: 0.4974 - model_117_loss: 0.6927 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9268 - model_117_loss: 0.6927 - model_117_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4262 - model_116_loss: 0.4976 - model_117_loss: 0.6923 - model_117_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4260 - model_116_loss: 0.4981 - model_117_loss: 0.6925 - model_117_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4289 - model_116_loss: 0.4976 - model_117_loss: 0.6928 - model_117_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4275 - model_116_loss: 0.4981 - model_117_loss: 0.6927 - model_117_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4290 - model_116_loss: 0.4967 - model_117_loss: 0.6927 - model_117_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9276 - model_117_loss: 0.6932 - model_117_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4299 - model_116_loss: 0.4953 - model_117_loss: 0.6926 - model_117_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4273 - model_116_loss: 0.4965 - model_117_loss: 0.6925 - model_117_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4301 - model_116_loss: 0.4959 - model_117_loss: 0.6928 - model_117_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4295 - model_116_loss: 0.4959 - model_117_loss: 0.6927 - model_117_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4272 - model_116_loss: 0.4966 - model_117_loss: 0.6924 - model_117_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9252 - model_117_loss: 0.6923 - model_117_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4295 - model_116_loss: 0.4970 - model_117_loss: 0.6928 - model_117_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4304 - model_116_loss: 0.4959 - model_117_loss: 0.6928 - model_117_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4307 - model_116_loss: 0.4974 - model_117_loss: 0.6928 - model_117_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4321 - model_116_loss: 0.4959 - model_117_loss: 0.6929 - model_117_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4297 - model_116_loss: 0.4979 - model_117_loss: 0.6928 - model_117_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 66us/sample - loss: 6.9263 - model_117_loss: 0.6922 - model_117_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4291 - model_116_loss: 0.4981 - model_117_loss: 0.6927 - model_117_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4277 - model_116_loss: 0.4978 - model_117_loss: 0.6927 - model_117_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4292 - model_116_loss: 0.4985 - model_117_loss: 0.6929 - model_117_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4265 - model_116_loss: 0.4987 - model_117_loss: 0.6926 - model_117_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4283 - model_116_loss: 0.4981 - model_117_loss: 0.6927 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9280 - model_117_loss: 0.6932 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4286 - model_116_loss: 0.4982 - model_117_loss: 0.6928 - model_117_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4278 - model_116_loss: 0.4983 - model_117_loss: 0.6927 - model_117_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4276 - model_116_loss: 0.4988 - model_117_loss: 0.6928 - model_117_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4287 - model_116_loss: 0.4976 - model_117_loss: 0.6928 - model_117_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4301 - model_116_loss: 0.4959 - model_117_loss: 0.6927 - model_117_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 81us/sample - loss: 6.9264 - model_117_loss: 0.6921 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4285 - model_116_loss: 0.4972 - model_117_loss: 0.6926 - model_117_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4291 - model_116_loss: 0.4967 - model_117_loss: 0.6925 - model_117_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4268 - model_116_loss: 0.4986 - model_117_loss: 0.6926 - model_117_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4295 - model_116_loss: 0.4978 - model_117_loss: 0.6929 - model_117_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4280 - model_116_loss: 0.4982 - model_117_loss: 0.6926 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 70us/sample - loss: 6.9275 - model_117_loss: 0.6928 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4295 - model_116_loss: 0.4976 - model_117_loss: 0.6928 - model_117_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4278 - model_116_loss: 0.4979 - model_117_loss: 0.6926 - model_117_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4292 - model_116_loss: 0.4970 - model_117_loss: 0.6927 - model_117_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4280 - model_116_loss: 0.4975 - model_117_loss: 0.6926 - model_117_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4288 - model_116_loss: 0.4977 - model_117_loss: 0.6926 - model_117_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 66us/sample - loss: 6.9260 - model_117_loss: 0.6926 - model_117_1_loss: 0.69270s - loss: 6.9494 - model_117_loss: 0.6981 - model_117_1_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4285 - model_116_loss: 0.4980 - model_117_loss: 0.6926 - model_117_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4309 - model_116_loss: 0.4968 - model_117_loss: 0.6929 - model_117_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4300 - model_116_loss: 0.4969 - model_117_loss: 0.6929 - model_117_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4294 - model_116_loss: 0.4974 - model_117_loss: 0.6927 - model_117_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4296 - model_116_loss: 0.4975 - model_117_loss: 0.6929 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 65us/sample - loss: 6.9262 - model_117_loss: 0.6930 - model_117_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4285 - model_116_loss: 0.4982 - model_117_loss: 0.6928 - model_117_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4292 - model_116_loss: 0.4972 - model_117_loss: 0.6928 - model_117_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4290 - model_116_loss: 0.4976 - model_117_loss: 0.6927 - model_117_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4286 - model_116_loss: 0.4990 - model_117_loss: 0.6931 - model_117_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4278 - model_116_loss: 0.4974 - model_117_loss: 0.6925 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9268 - model_117_loss: 0.6932 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4284 - model_116_loss: 0.4981 - model_117_loss: 0.6927 - model_117_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4294 - model_116_loss: 0.4972 - model_117_loss: 0.6927 - model_117_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4286 - model_116_loss: 0.4979 - model_117_loss: 0.6926 - model_117_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4295 - model_116_loss: 0.4973 - model_117_loss: 0.6927 - model_117_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4281 - model_116_loss: 0.4979 - model_117_loss: 0.6926 - model_117_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 101us/sample - loss: 6.9275 - model_117_loss: 0.6935 - model_117_1_loss: 0.6926s - loss: 6.9581 - model_117_los\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4288 - model_116_loss: 0.4982 - model_117_loss: 0.6928 - model_117_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4303 - model_116_loss: 0.4971 - model_117_loss: 0.6928 - model_117_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4300 - model_116_loss: 0.4963 - model_117_loss: 0.6926 - model_117_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4301 - model_116_loss: 0.4965 - model_117_loss: 0.6927 - model_117_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4294 - model_116_loss: 0.4973 - model_117_loss: 0.6926 - model_117_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9281 - model_117_loss: 0.6926 - model_117_1_loss: 0.69281s - loss: 6.9191 - model_117_loss: 0.6916 - model\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4293 - model_116_loss: 0.4978 - model_117_loss: 0.6928 - model_117_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4297 - model_116_loss: 0.4970 - model_117_loss: 0.6929 - model_117_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4301 - model_116_loss: 0.4967 - model_117_loss: 0.6928 - model_117_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4297 - model_116_loss: 0.4965 - model_117_loss: 0.6927 - model_117_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4281 - model_116_loss: 0.4980 - model_117_loss: 0.6926 - model_117_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9277 - model_117_loss: 0.6923 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4293 - model_116_loss: 0.4969 - model_117_loss: 0.6926 - model_117_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4298 - model_116_loss: 0.4967 - model_117_loss: 0.6926 - model_117_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4288 - model_116_loss: 0.4972 - model_117_loss: 0.6925 - model_117_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4302 - model_116_loss: 0.4966 - model_117_loss: 0.6926 - model_117_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4294 - model_116_loss: 0.4969 - model_117_loss: 0.6927 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.9291 - model_117_loss: 0.6927 - model_117_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4297 - model_116_loss: 0.4963 - model_117_loss: 0.6927 - model_117_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4307 - model_116_loss: 0.4963 - model_117_loss: 0.6928 - model_117_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4295 - model_116_loss: 0.4967 - model_117_loss: 0.6927 - model_117_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4291 - model_116_loss: 0.4958 - model_117_loss: 0.6926 - model_117_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4316 - model_116_loss: 0.4948 - model_117_loss: 0.6927 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.9278 - model_117_loss: 0.6927 - model_117_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4307 - model_116_loss: 0.4963 - model_117_loss: 0.6927 - model_117_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4294 - model_116_loss: 0.4966 - model_117_loss: 0.6927 - model_117_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4303 - model_116_loss: 0.4951 - model_117_loss: 0.6926 - model_117_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4297 - model_116_loss: 0.4966 - model_117_loss: 0.6928 - model_117_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4293 - model_116_loss: 0.4959 - model_117_loss: 0.6926 - model_117_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9278 - model_117_loss: 0.6931 - model_117_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4302 - model_116_loss: 0.4955 - model_117_loss: 0.6927 - model_117_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4305 - model_116_loss: 0.4946 - model_117_loss: 0.6926 - model_117_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4292 - model_116_loss: 0.4956 - model_117_loss: 0.6924 - model_117_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4294 - model_116_loss: 0.4959 - model_117_loss: 0.6925 - model_117_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4310 - model_116_loss: 0.4950 - model_117_loss: 0.6926 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9257 - model_117_loss: 0.6922 - model_117_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4314 - model_116_loss: 0.4946 - model_117_loss: 0.6927 - model_117_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4289 - model_116_loss: 0.4967 - model_117_loss: 0.6926 - model_117_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4308 - model_116_loss: 0.4959 - model_117_loss: 0.6927 - model_117_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4305 - model_116_loss: 0.4961 - model_117_loss: 0.6927 - model_117_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4291 - model_116_loss: 0.4962 - model_117_loss: 0.6924 - model_117_1_loss: 0.6926\n",
      "For Attention Module: 3.1\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.3158 - model_121_loss: 0.6586 - model_121_1_loss: 0.6037\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: -5.9471 - model_120_loss: 0.3759 - model_121_loss: 0.6607 - model_121_1_loss: 0.6039\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -5.9602 - model_120_loss: 0.3754 - model_121_loss: 0.6610 - model_121_1_loss: 0.6061\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -5.9629 - model_120_loss: 0.3759 - model_121_loss: 0.6613 - model_121_1_loss: 0.6065\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -5.9705 - model_120_loss: 0.3760 - model_121_loss: 0.6618 - model_121_1_loss: 0.6075\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -5.9788 - model_120_loss: 0.3774 - model_121_loss: 0.6617 - model_121_1_loss: 0.6096\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: 6.3807 - model_121_loss: 0.6635 - model_121_1_loss: 0.6129\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -5.9878 - model_120_loss: 0.3775 - model_121_loss: 0.6607 - model_121_1_loss: 0.6123\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.0013 - model_120_loss: 0.3789 - model_121_loss: 0.6628 - model_121_1_loss: 0.6132\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.0098 - model_120_loss: 0.3806 - model_121_loss: 0.6634 - model_121_1_loss: 0.6147\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.0194 - model_120_loss: 0.3791 - model_121_loss: 0.6640 - model_121_1_loss: 0.6157\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.0211 - model_120_loss: 0.3810 - model_121_loss: 0.6632 - model_121_1_loss: 0.6172\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.4258 - model_121_loss: 0.6654 - model_121_1_loss: 0.62050s - loss: 6.4830 - model_121_loss: 0.6716 - model_121_1_los\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.0454 - model_120_loss: 0.3793 - model_121_loss: 0.6643 - model_121_1_loss: 0.6206\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.0477 - model_120_loss: 0.3810 - model_121_loss: 0.6649 - model_121_1_loss: 0.6208\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.0606 - model_120_loss: 0.3831 - model_121_loss: 0.6647 - model_121_1_loss: 0.6240\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.0721 - model_120_loss: 0.3816 - model_121_loss: 0.6656 - model_121_1_loss: 0.6251\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.0832 - model_120_loss: 0.3827 - model_121_loss: 0.6668 - model_121_1_loss: 0.6264\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: 6.4837 - model_121_loss: 0.6667 - model_121_1_loss: 0.6299\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.1000 - model_120_loss: 0.3848 - model_121_loss: 0.6668 - model_121_1_loss: 0.6302\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.1123 - model_120_loss: 0.3865 - model_121_loss: 0.6676 - model_121_1_loss: 0.6322\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.1258 - model_120_loss: 0.3846 - model_121_loss: 0.6685 - model_121_1_loss: 0.6336\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.1348 - model_120_loss: 0.3869 - model_121_loss: 0.6687 - model_121_1_loss: 0.6356\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.1440 - model_120_loss: 0.3881 - model_121_loss: 0.6691 - model_121_1_loss: 0.6373\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.5524 - model_121_loss: 0.6706 - model_121_1_loss: 0.6394\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.1654 - model_120_loss: 0.3888 - model_121_loss: 0.6709 - model_121_1_loss: 0.6400\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.1729 - model_120_loss: 0.3906 - model_121_loss: 0.6712 - model_121_1_loss: 0.6415\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.1941 - model_120_loss: 0.3924 - model_121_loss: 0.6728 - model_121_1_loss: 0.6445\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.2010 - model_120_loss: 0.3946 - model_121_loss: 0.6732 - model_121_1_loss: 0.6459\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.2189 - model_120_loss: 0.3952 - model_121_loss: 0.6732 - model_121_1_loss: 0.6496\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.6230 - model_121_loss: 0.6736 - model_121_1_loss: 0.6505\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.2262 - model_120_loss: 0.3962 - model_121_loss: 0.6745 - model_121_1_loss: 0.6500\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.2340 - model_120_loss: 0.3992 - model_121_loss: 0.6751 - model_121_1_loss: 0.6515\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.2500 - model_120_loss: 0.4004 - model_121_loss: 0.6766 - model_121_1_loss: 0.6535\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.2662 - model_120_loss: 0.4025 - model_121_loss: 0.6774 - model_121_1_loss: 0.6564\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.2759 - model_120_loss: 0.4037 - model_121_loss: 0.6777 - model_121_1_loss: 0.6583\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.6927 - model_121_loss: 0.6786 - model_121_1_loss: 0.6598\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.2699 - model_120_loss: 0.4054 - model_121_loss: 0.6777 - model_121_1_loss: 0.6573\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.2879 - model_120_loss: 0.4087 - model_121_loss: 0.6793 - model_121_1_loss: 0.6600\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.2938 - model_120_loss: 0.4118 - model_121_loss: 0.6797 - model_121_1_loss: 0.6614\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3103 - model_120_loss: 0.4123 - model_121_loss: 0.6807 - model_121_1_loss: 0.6638\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3141 - model_120_loss: 0.4145 - model_121_loss: 0.6812 - model_121_1_loss: 0.6645\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.7404 - model_121_loss: 0.6824 - model_121_1_loss: 0.6658\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3224 - model_120_loss: 0.4169 - model_121_loss: 0.6824 - model_121_1_loss: 0.6655\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3265 - model_120_loss: 0.4216 - model_121_loss: 0.6829 - model_121_1_loss: 0.6667\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3433 - model_120_loss: 0.4213 - model_121_loss: 0.6839 - model_121_1_loss: 0.6691\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3410 - model_120_loss: 0.4236 - model_121_loss: 0.6830 - model_121_1_loss: 0.6699\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3516 - model_120_loss: 0.4248 - model_121_loss: 0.6836 - model_121_1_loss: 0.6717\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: 6.7897 - model_121_loss: 0.6854 - model_121_1_loss: 0.67280s - loss: 6.7598 - model_121_loss: 0.6816 - model_121_1_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3491 - model_120_loss: 0.4309 - model_121_loss: 0.6844 - model_121_1_loss: 0.6716\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3643 - model_120_loss: 0.4311 - model_121_loss: 0.6854 - model_121_1_loss: 0.6737\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3733 - model_120_loss: 0.4340 - model_121_loss: 0.6861 - model_121_1_loss: 0.6754\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3749 - model_120_loss: 0.4352 - model_121_loss: 0.6863 - model_121_1_loss: 0.6758\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3847 - model_120_loss: 0.4380 - model_121_loss: 0.6871 - model_121_1_loss: 0.6774\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.8292 - model_121_loss: 0.6876 - model_121_1_loss: 0.6782\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3883 - model_120_loss: 0.4426 - model_121_loss: 0.6880 - model_121_1_loss: 0.6782\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3972 - model_120_loss: 0.4447 - model_121_loss: 0.6886 - model_121_1_loss: 0.6798\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3979 - model_120_loss: 0.4500 - model_121_loss: 0.6889 - model_121_1_loss: 0.6806\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4061 - model_120_loss: 0.4496 - model_121_loss: 0.6889 - model_121_1_loss: 0.6822\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4053 - model_120_loss: 0.4555 - model_121_loss: 0.6898 - model_121_1_loss: 0.6824\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.8686 - model_121_loss: 0.6898 - model_121_1_loss: 0.6841\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4088 - model_120_loss: 0.4560 - model_121_loss: 0.6896 - model_121_1_loss: 0.6833\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4148 - model_120_loss: 0.4579 - model_121_loss: 0.6905 - model_121_1_loss: 0.6840\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4121 - model_120_loss: 0.4621 - model_121_loss: 0.6904 - model_121_1_loss: 0.6844\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4172 - model_120_loss: 0.4659 - model_121_loss: 0.6903 - model_121_1_loss: 0.6863\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4180 - model_120_loss: 0.4698 - model_121_loss: 0.6912 - model_121_1_loss: 0.6863\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.8896 - model_121_loss: 0.6909 - model_121_1_loss: 0.6867\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4220 - model_120_loss: 0.4716 - model_121_loss: 0.6911 - model_121_1_loss: 0.6876\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4273 - model_120_loss: 0.4718 - model_121_loss: 0.6917 - model_121_1_loss: 0.6882\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4307 - model_120_loss: 0.4758 - model_121_loss: 0.6921 - model_121_1_loss: 0.6892\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4294 - model_120_loss: 0.4768 - model_121_loss: 0.6917 - model_121_1_loss: 0.6895\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4360 - model_120_loss: 0.4774 - model_121_loss: 0.6924 - model_121_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9035 - model_121_loss: 0.6915 - model_121_1_loss: 0.6889\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4214 - model_120_loss: 0.4819 - model_121_loss: 0.6922 - model_121_1_loss: 0.6885\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4212 - model_120_loss: 0.4843 - model_121_loss: 0.6925 - model_121_1_loss: 0.6886\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4184 - model_120_loss: 0.4862 - model_121_loss: 0.6921 - model_121_1_loss: 0.6888\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4252 - model_120_loss: 0.4848 - model_121_loss: 0.6927 - model_121_1_loss: 0.6893\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4268 - model_120_loss: 0.4874 - model_121_loss: 0.6928 - model_121_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9125 - model_121_loss: 0.6933 - model_121_1_loss: 0.6896\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4211 - model_120_loss: 0.4893 - model_121_loss: 0.6919 - model_121_1_loss: 0.6901\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4264 - model_120_loss: 0.4875 - model_121_loss: 0.6924 - model_121_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4250 - model_120_loss: 0.4895 - model_121_loss: 0.6925 - model_121_1_loss: 0.6904\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4281 - model_120_loss: 0.4885 - model_121_loss: 0.6925 - model_121_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4278 - model_120_loss: 0.4894 - model_121_loss: 0.6925 - model_121_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9165 - model_121_loss: 0.6924 - model_121_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4293 - model_120_loss: 0.4899 - model_121_loss: 0.6923 - model_121_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4293 - model_120_loss: 0.4903 - model_121_loss: 0.6921 - model_121_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4350 - model_120_loss: 0.4886 - model_121_loss: 0.6922 - model_121_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4341 - model_120_loss: 0.4867 - model_121_loss: 0.6921 - model_121_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4342 - model_120_loss: 0.4866 - model_121_loss: 0.6920 - model_121_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9179 - model_121_loss: 0.6916 - model_121_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4270 - model_120_loss: 0.4879 - model_121_loss: 0.6918 - model_121_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4328 - model_120_loss: 0.4848 - model_121_loss: 0.6920 - model_121_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4346 - model_120_loss: 0.4823 - model_121_loss: 0.6921 - model_121_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4291 - model_120_loss: 0.4840 - model_121_loss: 0.6913 - model_121_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4322 - model_120_loss: 0.4841 - model_121_loss: 0.6914 - model_121_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9152 - model_121_loss: 0.6920 - model_121_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4306 - model_120_loss: 0.4812 - model_121_loss: 0.6908 - model_121_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4342 - model_120_loss: 0.4806 - model_121_loss: 0.6911 - model_121_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4333 - model_120_loss: 0.4782 - model_121_loss: 0.6909 - model_121_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4333 - model_120_loss: 0.4781 - model_121_loss: 0.6905 - model_121_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4324 - model_120_loss: 0.4773 - model_121_loss: 0.6903 - model_121_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9100 - model_121_loss: 0.6906 - model_121_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4292 - model_120_loss: 0.4782 - model_121_loss: 0.6901 - model_121_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4285 - model_120_loss: 0.4773 - model_121_loss: 0.6897 - model_121_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4294 - model_120_loss: 0.4757 - model_121_loss: 0.6900 - model_121_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4311 - model_120_loss: 0.4754 - model_121_loss: 0.6899 - model_121_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4364 - model_120_loss: 0.4756 - model_121_loss: 0.6908 - model_121_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9059 - model_121_loss: 0.6897 - model_121_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4360 - model_120_loss: 0.4745 - model_121_loss: 0.6908 - model_121_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4372 - model_120_loss: 0.4728 - model_121_loss: 0.6907 - model_121_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4349 - model_120_loss: 0.4719 - model_121_loss: 0.6903 - model_121_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4360 - model_120_loss: 0.4719 - model_121_loss: 0.6904 - model_121_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4366 - model_120_loss: 0.4729 - model_121_loss: 0.6909 - model_121_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9091 - model_121_loss: 0.6895 - model_121_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4380 - model_120_loss: 0.4719 - model_121_loss: 0.6904 - model_121_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4376 - model_120_loss: 0.4718 - model_121_loss: 0.6905 - model_121_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4371 - model_120_loss: 0.4744 - model_121_loss: 0.6905 - model_121_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4365 - model_120_loss: 0.4740 - model_121_loss: 0.6907 - model_121_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4426 - model_120_loss: 0.4724 - model_121_loss: 0.6912 - model_121_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9144 - model_121_loss: 0.6915 - model_121_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4371 - model_120_loss: 0.4736 - model_121_loss: 0.6909 - model_121_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4366 - model_120_loss: 0.4741 - model_121_loss: 0.6910 - model_121_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4390 - model_120_loss: 0.4751 - model_121_loss: 0.6914 - model_121_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4392 - model_120_loss: 0.4752 - model_121_loss: 0.6912 - model_121_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4424 - model_120_loss: 0.4732 - model_121_loss: 0.6919 - model_121_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9217 - model_121_loss: 0.6930 - model_121_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4441 - model_120_loss: 0.4719 - model_121_loss: 0.6918 - model_121_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4402 - model_120_loss: 0.4759 - model_121_loss: 0.6919 - model_121_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4466 - model_120_loss: 0.4727 - model_121_loss: 0.6923 - model_121_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4443 - model_120_loss: 0.4730 - model_121_loss: 0.6923 - model_121_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4442 - model_120_loss: 0.4743 - model_121_loss: 0.6923 - model_121_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9196 - model_121_loss: 0.6930 - model_121_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4432 - model_120_loss: 0.4743 - model_121_loss: 0.6920 - model_121_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4445 - model_120_loss: 0.4747 - model_121_loss: 0.6924 - model_121_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4444 - model_120_loss: 0.4751 - model_121_loss: 0.6924 - model_121_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4450 - model_120_loss: 0.4755 - model_121_loss: 0.6925 - model_121_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4489 - model_120_loss: 0.4744 - model_121_loss: 0.6931 - model_121_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9222 - model_121_loss: 0.6925 - model_121_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4451 - model_120_loss: 0.4729 - model_121_loss: 0.6921 - model_121_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4456 - model_120_loss: 0.4758 - model_121_loss: 0.6927 - model_121_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4442 - model_120_loss: 0.4757 - model_121_loss: 0.6925 - model_121_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4442 - model_120_loss: 0.4752 - model_121_loss: 0.6925 - model_121_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4455 - model_120_loss: 0.4744 - model_121_loss: 0.6927 - model_121_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9210 - model_121_loss: 0.6931 - model_121_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4456 - model_120_loss: 0.4745 - model_121_loss: 0.6926 - model_121_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4442 - model_120_loss: 0.4742 - model_121_loss: 0.6925 - model_121_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4468 - model_120_loss: 0.4735 - model_121_loss: 0.6926 - model_121_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4471 - model_120_loss: 0.4724 - model_121_loss: 0.6927 - model_121_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4467 - model_120_loss: 0.4742 - model_121_loss: 0.6927 - model_121_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9209 - model_121_loss: 0.6932 - model_121_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4462 - model_120_loss: 0.4718 - model_121_loss: 0.6922 - model_121_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4476 - model_120_loss: 0.4706 - model_121_loss: 0.6924 - model_121_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4456 - model_120_loss: 0.4718 - model_121_loss: 0.6923 - model_121_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4485 - model_120_loss: 0.4695 - model_121_loss: 0.6923 - model_121_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4486 - model_120_loss: 0.4692 - model_121_loss: 0.6926 - model_121_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9181 - model_121_loss: 0.6931 - model_121_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4469 - model_120_loss: 0.4702 - model_121_loss: 0.6924 - model_121_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4465 - model_120_loss: 0.4700 - model_121_loss: 0.6922 - model_121_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4492 - model_120_loss: 0.4683 - model_121_loss: 0.6926 - model_121_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4466 - model_120_loss: 0.4699 - model_121_loss: 0.6922 - model_121_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4495 - model_120_loss: 0.4676 - model_121_loss: 0.6926 - model_121_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9169 - model_121_loss: 0.6930 - model_121_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4492 - model_120_loss: 0.4668 - model_121_loss: 0.6921 - model_121_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4481 - model_120_loss: 0.4671 - model_121_loss: 0.6921 - model_121_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4470 - model_120_loss: 0.4671 - model_121_loss: 0.6920 - model_121_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4511 - model_120_loss: 0.4669 - model_121_loss: 0.6925 - model_121_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4493 - model_120_loss: 0.4680 - model_121_loss: 0.6924 - model_121_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9176 - model_121_loss: 0.6924 - model_121_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4489 - model_120_loss: 0.4682 - model_121_loss: 0.6926 - model_121_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4476 - model_120_loss: 0.4676 - model_121_loss: 0.6923 - model_121_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4509 - model_120_loss: 0.4664 - model_121_loss: 0.6924 - model_121_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4481 - model_120_loss: 0.4672 - model_121_loss: 0.6922 - model_121_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4522 - model_120_loss: 0.4666 - model_121_loss: 0.6925 - model_121_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9177 - model_121_loss: 0.6921 - model_121_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4504 - model_120_loss: 0.4671 - model_121_loss: 0.6924 - model_121_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4487 - model_120_loss: 0.4682 - model_121_loss: 0.6922 - model_121_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4482 - model_120_loss: 0.4679 - model_121_loss: 0.6923 - model_121_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4477 - model_120_loss: 0.4691 - model_121_loss: 0.6924 - model_121_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4486 - model_120_loss: 0.4684 - model_121_loss: 0.6924 - model_121_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9163 - model_121_loss: 0.6925 - model_121_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4487 - model_120_loss: 0.4705 - model_121_loss: 0.6926 - model_121_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4502 - model_120_loss: 0.4686 - model_121_loss: 0.6923 - model_121_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4498 - model_120_loss: 0.4693 - model_121_loss: 0.6924 - model_121_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4514 - model_120_loss: 0.4701 - model_121_loss: 0.6927 - model_121_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4531 - model_120_loss: 0.4694 - model_121_loss: 0.6925 - model_121_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9233 - model_121_loss: 0.6938 - model_121_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4500 - model_120_loss: 0.4711 - model_121_loss: 0.6923 - model_121_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4522 - model_120_loss: 0.4714 - model_121_loss: 0.6929 - model_121_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4509 - model_120_loss: 0.4710 - model_121_loss: 0.6925 - model_121_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4520 - model_120_loss: 0.4722 - model_121_loss: 0.6929 - model_121_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4482 - model_120_loss: 0.4732 - model_121_loss: 0.6924 - model_121_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9243 - model_121_loss: 0.6929 - model_121_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4493 - model_120_loss: 0.4733 - model_121_loss: 0.6926 - model_121_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4493 - model_120_loss: 0.4739 - model_121_loss: 0.6927 - model_121_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4513 - model_120_loss: 0.4727 - model_121_loss: 0.6928 - model_121_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4504 - model_120_loss: 0.4726 - model_121_loss: 0.6928 - model_121_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4508 - model_120_loss: 0.4725 - model_121_loss: 0.6928 - model_121_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9246 - model_121_loss: 0.6939 - model_121_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4510 - model_120_loss: 0.4734 - model_121_loss: 0.6930 - model_121_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4522 - model_120_loss: 0.4714 - model_121_loss: 0.6928 - model_121_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4504 - model_120_loss: 0.4720 - model_121_loss: 0.6926 - model_121_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4510 - model_120_loss: 0.4700 - model_121_loss: 0.6925 - model_121_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4547 - model_120_loss: 0.4692 - model_121_loss: 0.6928 - model_121_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9217 - model_121_loss: 0.6932 - model_121_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4497 - model_120_loss: 0.4696 - model_121_loss: 0.6926 - model_121_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4492 - model_120_loss: 0.4693 - model_121_loss: 0.6925 - model_121_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4497 - model_120_loss: 0.4692 - model_121_loss: 0.6925 - model_121_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4546 - model_120_loss: 0.4654 - model_121_loss: 0.6927 - model_121_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4496 - model_120_loss: 0.4681 - model_121_loss: 0.6925 - model_121_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9209 - model_121_loss: 0.6924 - model_121_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4497 - model_120_loss: 0.4652 - model_121_loss: 0.6921 - model_121_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4517 - model_120_loss: 0.4665 - model_121_loss: 0.6926 - model_121_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4521 - model_120_loss: 0.4641 - model_121_loss: 0.6924 - model_121_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4505 - model_120_loss: 0.4674 - model_121_loss: 0.6923 - model_121_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4555 - model_120_loss: 0.4635 - model_121_loss: 0.6928 - model_121_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9150 - model_121_loss: 0.6918 - model_121_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4507 - model_120_loss: 0.4640 - model_121_loss: 0.6922 - model_121_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4502 - model_120_loss: 0.4660 - model_121_loss: 0.6922 - model_121_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4534 - model_120_loss: 0.4640 - model_121_loss: 0.6925 - model_121_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4526 - model_120_loss: 0.4636 - model_121_loss: 0.6923 - model_121_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4533 - model_120_loss: 0.4633 - model_121_loss: 0.6927 - model_121_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9172 - model_121_loss: 0.6932 - model_121_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4522 - model_120_loss: 0.4645 - model_121_loss: 0.6922 - model_121_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4524 - model_120_loss: 0.4647 - model_121_loss: 0.6925 - model_121_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4567 - model_120_loss: 0.4629 - model_121_loss: 0.6926 - model_121_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4540 - model_120_loss: 0.4651 - model_121_loss: 0.6926 - model_121_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4528 - model_120_loss: 0.4638 - model_121_loss: 0.6923 - model_121_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9188 - model_121_loss: 0.6933 - model_121_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4516 - model_120_loss: 0.4647 - model_121_loss: 0.6924 - model_121_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4508 - model_120_loss: 0.4644 - model_121_loss: 0.6923 - model_121_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4538 - model_120_loss: 0.4640 - model_121_loss: 0.6927 - model_121_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4528 - model_120_loss: 0.4639 - model_121_loss: 0.6922 - model_121_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4550 - model_120_loss: 0.4637 - model_121_loss: 0.6925 - model_121_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9182 - model_121_loss: 0.6923 - model_121_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4570 - model_120_loss: 0.4648 - model_121_loss: 0.6928 - model_121_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4576 - model_120_loss: 0.4657 - model_121_loss: 0.6929 - model_121_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4561 - model_120_loss: 0.4664 - model_121_loss: 0.6929 - model_121_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4556 - model_120_loss: 0.4661 - model_121_loss: 0.6925 - model_121_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4553 - model_120_loss: 0.4680 - model_121_loss: 0.6929 - model_121_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9210 - model_121_loss: 0.6918 - model_121_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4552 - model_120_loss: 0.4675 - model_121_loss: 0.6928 - model_121_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4514 - model_120_loss: 0.4689 - model_121_loss: 0.6925 - model_121_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4554 - model_120_loss: 0.4677 - model_121_loss: 0.6931 - model_121_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4519 - model_120_loss: 0.4699 - model_121_loss: 0.6930 - model_121_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4523 - model_120_loss: 0.4693 - model_121_loss: 0.6928 - model_121_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9238 - model_121_loss: 0.6930 - model_121_1_loss: 0.69180s - loss: 6.9586 - model_121_loss: 0.6995 - model_121_1_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4515 - model_120_loss: 0.4695 - model_121_loss: 0.6926 - model_121_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4508 - model_120_loss: 0.4716 - model_121_loss: 0.6927 - model_121_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4528 - model_120_loss: 0.4710 - model_121_loss: 0.6928 - model_121_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4510 - model_120_loss: 0.4709 - model_121_loss: 0.6926 - model_121_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4520 - model_120_loss: 0.4697 - model_121_loss: 0.6926 - model_121_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9245 - model_121_loss: 0.6924 - model_121_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4536 - model_120_loss: 0.4685 - model_121_loss: 0.6927 - model_121_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4536 - model_120_loss: 0.4683 - model_121_loss: 0.6929 - model_121_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4534 - model_120_loss: 0.4687 - model_121_loss: 0.6928 - model_121_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4539 - model_120_loss: 0.4678 - model_121_loss: 0.6928 - model_121_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4553 - model_120_loss: 0.4672 - model_121_loss: 0.6929 - model_121_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9216 - model_121_loss: 0.6921 - model_121_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4545 - model_120_loss: 0.4671 - model_121_loss: 0.6927 - model_121_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4564 - model_120_loss: 0.4651 - model_121_loss: 0.6927 - model_121_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4525 - model_120_loss: 0.4675 - model_121_loss: 0.6927 - model_121_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4538 - model_120_loss: 0.4660 - model_121_loss: 0.6925 - model_121_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4540 - model_120_loss: 0.4666 - model_121_loss: 0.6927 - model_121_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.9210 - model_121_loss: 0.6920 - model_121_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4571 - model_120_loss: 0.4648 - model_121_loss: 0.6927 - model_121_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4587 - model_120_loss: 0.4637 - model_121_loss: 0.6930 - model_121_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4545 - model_120_loss: 0.4648 - model_121_loss: 0.6925 - model_121_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4556 - model_120_loss: 0.4631 - model_121_loss: 0.6924 - model_121_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4587 - model_120_loss: 0.4631 - model_121_loss: 0.6926 - model_121_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9190 - model_121_loss: 0.6928 - model_121_1_loss: 0.69110s - loss: 6.9054 - model_121_loss: 0.6902 - model_121_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4504 - model_120_loss: 0.4642 - model_121_loss: 0.6921 - model_121_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4546 - model_120_loss: 0.4620 - model_121_loss: 0.6924 - model_121_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4532 - model_120_loss: 0.4622 - model_121_loss: 0.6925 - model_121_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4549 - model_120_loss: 0.4613 - model_121_loss: 0.6924 - model_121_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4546 - model_120_loss: 0.4630 - model_121_loss: 0.6926 - model_121_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9196 - model_121_loss: 0.6932 - model_121_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4542 - model_120_loss: 0.4635 - model_121_loss: 0.6926 - model_121_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4532 - model_120_loss: 0.4636 - model_121_loss: 0.6925 - model_121_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4563 - model_120_loss: 0.4630 - model_121_loss: 0.6925 - model_121_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4545 - model_120_loss: 0.4631 - model_121_loss: 0.6924 - model_121_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4564 - model_120_loss: 0.4645 - model_121_loss: 0.6926 - model_121_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9207 - model_121_loss: 0.6921 - model_121_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4533 - model_120_loss: 0.4656 - model_121_loss: 0.6924 - model_121_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4512 - model_120_loss: 0.4671 - model_121_loss: 0.6925 - model_121_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4550 - model_120_loss: 0.4661 - model_121_loss: 0.6928 - model_121_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4540 - model_120_loss: 0.4669 - model_121_loss: 0.6926 - model_121_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4564 - model_120_loss: 0.4658 - model_121_loss: 0.6927 - model_121_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9230 - model_121_loss: 0.6923 - model_121_1_loss: 0.69180s - loss: 6.9559 - model_121_loss: 0.6982 - model_121_1_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4546 - model_120_loss: 0.4656 - model_121_loss: 0.6926 - model_121_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4534 - model_120_loss: 0.4661 - model_121_loss: 0.6925 - model_121_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4559 - model_120_loss: 0.4651 - model_121_loss: 0.6926 - model_121_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4563 - model_120_loss: 0.4664 - model_121_loss: 0.6927 - model_121_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4521 - model_120_loss: 0.4675 - model_121_loss: 0.6925 - model_121_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9226 - model_121_loss: 0.6927 - model_121_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4523 - model_120_loss: 0.4676 - model_121_loss: 0.6924 - model_121_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4539 - model_120_loss: 0.4662 - model_121_loss: 0.6924 - model_121_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4555 - model_120_loss: 0.4667 - model_121_loss: 0.6926 - model_121_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4540 - model_120_loss: 0.4658 - model_121_loss: 0.6923 - model_121_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4546 - model_120_loss: 0.4662 - model_121_loss: 0.6926 - model_121_1_loss: 0.6915\n",
      "For Attention Module: 3.2\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.3220 - model_125_loss: 0.6598 - model_125_1_loss: 0.6044\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -5.9411 - model_124_loss: 0.3758 - model_125_loss: 0.6593 - model_125_1_loss: 0.6040\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -5.9546 - model_124_loss: 0.3749 - model_125_loss: 0.6599 - model_125_1_loss: 0.6060\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -5.9698 - model_124_loss: 0.3752 - model_125_loss: 0.6611 - model_125_1_loss: 0.6079\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -5.9748 - model_124_loss: 0.3759 - model_125_loss: 0.6604 - model_125_1_loss: 0.6098\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -5.9920 - model_124_loss: 0.3756 - model_125_loss: 0.6612 - model_125_1_loss: 0.6123\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.3793 - model_125_loss: 0.6621 - model_125_1_loss: 0.6136\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -5.9959 - model_124_loss: 0.3756 - model_125_loss: 0.6612 - model_125_1_loss: 0.6131\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.0081 - model_124_loss: 0.3777 - model_125_loss: 0.6617 - model_125_1_loss: 0.6154\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.0121 - model_124_loss: 0.3766 - model_125_loss: 0.6621 - model_125_1_loss: 0.6157\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.0262 - model_124_loss: 0.3792 - model_125_loss: 0.6633 - model_125_1_loss: 0.6178\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.0451 - model_124_loss: 0.3787 - model_125_loss: 0.6639 - model_125_1_loss: 0.6208\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.4377 - model_125_loss: 0.6651 - model_125_1_loss: 0.62230s - loss: 6.4300 - model_125_loss: 0.6633 - model_125_1_loss: 0.62\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.0527 - model_124_loss: 0.3804 - model_125_loss: 0.6643 - model_125_1_loss: 0.6224\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.0650 - model_124_loss: 0.3791 - model_125_loss: 0.6637 - model_125_1_loss: 0.6251\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.0781 - model_124_loss: 0.3815 - model_125_loss: 0.6664 - model_125_1_loss: 0.6256\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.0919 - model_124_loss: 0.3823 - model_125_loss: 0.6665 - model_125_1_loss: 0.6284\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1013 - model_124_loss: 0.3828 - model_125_loss: 0.6662 - model_125_1_loss: 0.6307\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.4974 - model_125_loss: 0.6663 - model_125_1_loss: 0.6324\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.1099 - model_124_loss: 0.3856 - model_125_loss: 0.6669 - model_125_1_loss: 0.6322\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.1318 - model_124_loss: 0.3863 - model_125_loss: 0.6685 - model_125_1_loss: 0.6351\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.1399 - model_124_loss: 0.3881 - model_125_loss: 0.6695 - model_125_1_loss: 0.6361\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.1531 - model_124_loss: 0.3901 - model_125_loss: 0.6690 - model_125_1_loss: 0.6397\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.1636 - model_124_loss: 0.3907 - model_125_loss: 0.6705 - model_125_1_loss: 0.6404\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.5647 - model_125_loss: 0.6713 - model_125_1_loss: 0.6415\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1681 - model_124_loss: 0.3926 - model_125_loss: 0.6706 - model_125_1_loss: 0.6416\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.1905 - model_124_loss: 0.3936 - model_125_loss: 0.6725 - model_125_1_loss: 0.6443\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.1839 - model_124_loss: 0.3955 - model_125_loss: 0.6713 - model_125_1_loss: 0.6445\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.2028 - model_124_loss: 0.3974 - model_125_loss: 0.6736 - model_125_1_loss: 0.6465\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.2173 - model_124_loss: 0.3980 - model_125_loss: 0.6733 - model_125_1_loss: 0.6498\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.6223 - model_125_loss: 0.6741 - model_125_1_loss: 0.6497\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.2212 - model_124_loss: 0.3994 - model_125_loss: 0.6738 - model_125_1_loss: 0.6503\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.2330 - model_124_loss: 0.4004 - model_125_loss: 0.6751 - model_125_1_loss: 0.6516\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.2538 - model_124_loss: 0.4030 - model_125_loss: 0.6763 - model_125_1_loss: 0.6551\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.2515 - model_124_loss: 0.4064 - model_125_loss: 0.6766 - model_125_1_loss: 0.6549\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.2661 - model_124_loss: 0.4076 - model_125_loss: 0.6770 - model_125_1_loss: 0.6577\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.6893 - model_125_loss: 0.6782 - model_125_1_loss: 0.6596\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2704 - model_124_loss: 0.4124 - model_125_loss: 0.6779 - model_125_1_loss: 0.6586\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.2741 - model_124_loss: 0.4144 - model_125_loss: 0.6784 - model_125_1_loss: 0.6593\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.2912 - model_124_loss: 0.4149 - model_125_loss: 0.6801 - model_125_1_loss: 0.6611\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3041 - model_124_loss: 0.4175 - model_125_loss: 0.6810 - model_125_1_loss: 0.6633\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3085 - model_124_loss: 0.4178 - model_125_loss: 0.6808 - model_125_1_loss: 0.6645\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.7346 - model_125_loss: 0.6813 - model_125_1_loss: 0.6659\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3112 - model_124_loss: 0.4202 - model_125_loss: 0.6814 - model_125_1_loss: 0.6648\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3252 - model_124_loss: 0.4231 - model_125_loss: 0.6827 - model_125_1_loss: 0.6670\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3325 - model_124_loss: 0.4270 - model_125_loss: 0.6834 - model_125_1_loss: 0.6685\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3426 - model_124_loss: 0.4287 - model_125_loss: 0.6843 - model_125_1_loss: 0.6700\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3446 - model_124_loss: 0.4306 - model_125_loss: 0.6840 - model_125_1_loss: 0.6710\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.7878 - model_125_loss: 0.6850 - model_125_1_loss: 0.6727\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3594 - model_124_loss: 0.4336 - model_125_loss: 0.6858 - model_125_1_loss: 0.6728\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3588 - model_124_loss: 0.4374 - model_125_loss: 0.6859 - model_125_1_loss: 0.6734\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3660 - model_124_loss: 0.4386 - model_125_loss: 0.6863 - model_125_1_loss: 0.6746\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3718 - model_124_loss: 0.4429 - model_125_loss: 0.6876 - model_125_1_loss: 0.6753\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3808 - model_124_loss: 0.4447 - model_125_loss: 0.6883 - model_125_1_loss: 0.6768\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.8341 - model_125_loss: 0.6882 - model_125_1_loss: 0.6785\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3797 - model_124_loss: 0.4473 - model_125_loss: 0.6874 - model_125_1_loss: 0.6780\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3803 - model_124_loss: 0.4517 - model_125_loss: 0.6876 - model_125_1_loss: 0.6788\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3884 - model_124_loss: 0.4540 - model_125_loss: 0.6883 - model_125_1_loss: 0.6802\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3984 - model_124_loss: 0.4584 - model_125_loss: 0.6893 - model_125_1_loss: 0.6821\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.3993 - model_124_loss: 0.4598 - model_125_loss: 0.6889 - model_125_1_loss: 0.6829\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.8672 - model_125_loss: 0.6899 - model_125_1_loss: 0.6831\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4003 - model_124_loss: 0.4629 - model_125_loss: 0.6897 - model_125_1_loss: 0.6829\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4004 - model_124_loss: 0.4662 - model_125_loss: 0.6896 - model_125_1_loss: 0.6837\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4090 - model_124_loss: 0.4700 - model_125_loss: 0.6904 - model_125_1_loss: 0.6853\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4150 - model_124_loss: 0.4704 - model_125_loss: 0.6910 - model_125_1_loss: 0.6861\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4181 - model_124_loss: 0.4750 - model_125_loss: 0.6916 - model_125_1_loss: 0.6870\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.8959 - model_125_loss: 0.6919 - model_125_1_loss: 0.68730s - loss: 6.8814 - model_125_loss: 0.6875 - model_125_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4135 - model_124_loss: 0.4784 - model_125_loss: 0.6914 - model_125_1_loss: 0.6870\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4185 - model_124_loss: 0.4784 - model_125_loss: 0.6920 - model_125_1_loss: 0.6874\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4173 - model_124_loss: 0.4802 - model_125_loss: 0.6918 - model_125_1_loss: 0.6877\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4215 - model_124_loss: 0.4854 - model_125_loss: 0.6923 - model_125_1_loss: 0.6891\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4254 - model_124_loss: 0.4851 - model_125_loss: 0.6927 - model_125_1_loss: 0.6894\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: 6.9090 - model_125_loss: 0.6922 - model_125_1_loss: 0.6890\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4206 - model_124_loss: 0.4849 - model_125_loss: 0.6920 - model_125_1_loss: 0.6891\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 13us/sample - loss: -6.4211 - model_124_loss: 0.4885 - model_125_loss: 0.6924 - model_125_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4201 - model_124_loss: 0.4889 - model_125_loss: 0.6923 - model_125_1_loss: 0.6895\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4258 - model_124_loss: 0.4892 - model_125_loss: 0.6928 - model_125_1_loss: 0.6902\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4257 - model_124_loss: 0.4906 - model_125_loss: 0.6928 - model_125_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9179 - model_125_loss: 0.6928 - model_125_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4236 - model_124_loss: 0.4921 - model_125_loss: 0.6928 - model_125_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4244 - model_124_loss: 0.4916 - model_125_loss: 0.6928 - model_125_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4272 - model_124_loss: 0.4909 - model_125_loss: 0.6929 - model_125_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4275 - model_124_loss: 0.4943 - model_125_loss: 0.6933 - model_125_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4268 - model_124_loss: 0.4925 - model_125_loss: 0.6933 - model_125_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9209 - model_125_loss: 0.6926 - model_125_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4277 - model_124_loss: 0.4939 - model_125_loss: 0.6932 - model_125_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4312 - model_124_loss: 0.4910 - model_125_loss: 0.6934 - model_125_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4294 - model_124_loss: 0.4881 - model_125_loss: 0.6926 - model_125_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4283 - model_124_loss: 0.4912 - model_125_loss: 0.6929 - model_125_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4311 - model_124_loss: 0.4896 - model_125_loss: 0.6928 - model_125_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.9216 - model_125_loss: 0.6928 - model_125_1_loss: 0.69130s - loss: 6.9170 - model_125_loss: 0.6902 - model_125_1_lo\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4317 - model_124_loss: 0.4893 - model_125_loss: 0.6927 - model_125_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4327 - model_124_loss: 0.4873 - model_125_loss: 0.6926 - model_125_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4350 - model_124_loss: 0.4883 - model_125_loss: 0.6927 - model_125_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4357 - model_124_loss: 0.4859 - model_125_loss: 0.6928 - model_125_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4351 - model_124_loss: 0.4861 - model_125_loss: 0.6927 - model_125_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: 6.9184 - model_125_loss: 0.6934 - model_125_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4397 - model_124_loss: 0.4832 - model_125_loss: 0.6930 - model_125_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4366 - model_124_loss: 0.4870 - model_125_loss: 0.6935 - model_125_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4378 - model_124_loss: 0.4834 - model_125_loss: 0.6931 - model_125_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4369 - model_124_loss: 0.4854 - model_125_loss: 0.6930 - model_125_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4387 - model_124_loss: 0.4851 - model_125_loss: 0.6929 - model_125_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9197 - model_125_loss: 0.6929 - model_125_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4353 - model_124_loss: 0.4853 - model_125_loss: 0.6927 - model_125_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4348 - model_124_loss: 0.4836 - model_125_loss: 0.6925 - model_125_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4347 - model_124_loss: 0.4828 - model_125_loss: 0.6926 - model_125_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4355 - model_124_loss: 0.4838 - model_125_loss: 0.6923 - model_125_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4339 - model_124_loss: 0.4843 - model_125_loss: 0.6925 - model_125_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: 6.9235 - model_125_loss: 0.6925 - model_125_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4386 - model_124_loss: 0.4820 - model_125_loss: 0.6926 - model_125_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4367 - model_124_loss: 0.4823 - model_125_loss: 0.6924 - model_125_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4383 - model_124_loss: 0.4811 - model_125_loss: 0.6924 - model_125_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4424 - model_124_loss: 0.4805 - model_125_loss: 0.6929 - model_125_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4373 - model_124_loss: 0.4825 - model_125_loss: 0.6926 - model_125_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9199 - model_125_loss: 0.6928 - model_125_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4397 - model_124_loss: 0.4823 - model_125_loss: 0.6924 - model_125_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4352 - model_124_loss: 0.4819 - model_125_loss: 0.6921 - model_125_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4430 - model_124_loss: 0.4790 - model_125_loss: 0.6925 - model_125_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4410 - model_124_loss: 0.4805 - model_125_loss: 0.6924 - model_125_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4406 - model_124_loss: 0.4809 - model_125_loss: 0.6922 - model_125_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9239 - model_125_loss: 0.6931 - model_125_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4416 - model_124_loss: 0.4799 - model_125_loss: 0.6926 - model_125_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4412 - model_124_loss: 0.4803 - model_125_loss: 0.6924 - model_125_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4376 - model_124_loss: 0.4816 - model_125_loss: 0.6921 - model_125_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4386 - model_124_loss: 0.4833 - model_125_loss: 0.6925 - model_125_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4389 - model_124_loss: 0.4817 - model_125_loss: 0.6924 - model_125_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9216 - model_125_loss: 0.6924 - model_125_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4398 - model_124_loss: 0.4814 - model_125_loss: 0.6923 - model_125_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4417 - model_124_loss: 0.4821 - model_125_loss: 0.6930 - model_125_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4431 - model_124_loss: 0.4800 - model_125_loss: 0.6925 - model_125_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4399 - model_124_loss: 0.4819 - model_125_loss: 0.6927 - model_125_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4393 - model_124_loss: 0.4842 - model_125_loss: 0.6926 - model_125_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9261 - model_125_loss: 0.6928 - model_125_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4371 - model_124_loss: 0.4837 - model_125_loss: 0.6921 - model_125_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4393 - model_124_loss: 0.4824 - model_125_loss: 0.6923 - model_125_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4385 - model_124_loss: 0.4830 - model_125_loss: 0.6923 - model_125_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4409 - model_124_loss: 0.4819 - model_125_loss: 0.6924 - model_125_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4439 - model_124_loss: 0.4806 - model_125_loss: 0.6928 - model_125_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: 6.9240 - model_125_loss: 0.6927 - model_125_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4387 - model_124_loss: 0.4811 - model_125_loss: 0.6924 - model_125_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4409 - model_124_loss: 0.4795 - model_125_loss: 0.6925 - model_125_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4420 - model_124_loss: 0.4793 - model_125_loss: 0.6927 - model_125_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4440 - model_124_loss: 0.4772 - model_125_loss: 0.6924 - model_125_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 14us/sample - loss: -6.4449 - model_124_loss: 0.4764 - model_125_loss: 0.6924 - model_125_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9216 - model_125_loss: 0.6930 - model_125_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4451 - model_124_loss: 0.4764 - model_125_loss: 0.6928 - model_125_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4448 - model_124_loss: 0.4742 - model_125_loss: 0.6924 - model_125_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4451 - model_124_loss: 0.4734 - model_125_loss: 0.6923 - model_125_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4467 - model_124_loss: 0.4719 - model_125_loss: 0.6925 - model_125_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4444 - model_124_loss: 0.4730 - model_125_loss: 0.6927 - model_125_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9167 - model_125_loss: 0.6925 - model_125_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4429 - model_124_loss: 0.4711 - model_125_loss: 0.6920 - model_125_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4471 - model_124_loss: 0.4679 - model_125_loss: 0.6921 - model_125_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4440 - model_124_loss: 0.4700 - model_125_loss: 0.6921 - model_125_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4446 - model_124_loss: 0.4685 - model_125_loss: 0.6919 - model_125_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4435 - model_124_loss: 0.4701 - model_125_loss: 0.6921 - model_125_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9140 - model_125_loss: 0.6928 - model_125_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4451 - model_124_loss: 0.4690 - model_125_loss: 0.6922 - model_125_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4445 - model_124_loss: 0.4678 - model_125_loss: 0.6917 - model_125_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4449 - model_124_loss: 0.4687 - model_125_loss: 0.6922 - model_125_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4477 - model_124_loss: 0.4674 - model_125_loss: 0.6922 - model_125_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4470 - model_124_loss: 0.4650 - model_125_loss: 0.6918 - model_125_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9125 - model_125_loss: 0.6917 - model_125_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4446 - model_124_loss: 0.4657 - model_125_loss: 0.6917 - model_125_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4434 - model_124_loss: 0.4678 - model_125_loss: 0.6919 - model_125_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4427 - model_124_loss: 0.4668 - model_125_loss: 0.6916 - model_125_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4446 - model_124_loss: 0.4665 - model_125_loss: 0.6922 - model_125_1_loss: 0.6900\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4444 - model_124_loss: 0.4674 - model_125_loss: 0.6920 - model_125_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9125 - model_125_loss: 0.6923 - model_125_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4474 - model_124_loss: 0.4663 - model_125_loss: 0.6924 - model_125_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4457 - model_124_loss: 0.4669 - model_125_loss: 0.6921 - model_125_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4455 - model_124_loss: 0.4684 - model_125_loss: 0.6922 - model_125_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4468 - model_124_loss: 0.4684 - model_125_loss: 0.6923 - model_125_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4493 - model_124_loss: 0.4671 - model_125_loss: 0.6924 - model_125_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: 6.9180 - model_125_loss: 0.6926 - model_125_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4452 - model_124_loss: 0.4698 - model_125_loss: 0.6922 - model_125_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4442 - model_124_loss: 0.4708 - model_125_loss: 0.6923 - model_125_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4473 - model_124_loss: 0.4683 - model_125_loss: 0.6926 - model_125_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4454 - model_124_loss: 0.4702 - model_125_loss: 0.6921 - model_125_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4477 - model_124_loss: 0.4704 - model_125_loss: 0.6922 - model_125_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9215 - model_125_loss: 0.6919 - model_125_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4465 - model_124_loss: 0.4714 - model_125_loss: 0.6924 - model_125_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4455 - model_124_loss: 0.4722 - model_125_loss: 0.6925 - model_125_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4471 - model_124_loss: 0.4727 - model_125_loss: 0.6927 - model_125_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4487 - model_124_loss: 0.4726 - model_125_loss: 0.6927 - model_125_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4487 - model_124_loss: 0.4727 - model_125_loss: 0.6925 - model_125_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9224 - model_125_loss: 0.6927 - model_125_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4480 - model_124_loss: 0.4738 - model_125_loss: 0.6927 - model_125_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4478 - model_124_loss: 0.4740 - model_125_loss: 0.6928 - model_125_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4477 - model_124_loss: 0.4738 - model_125_loss: 0.6928 - model_125_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4475 - model_124_loss: 0.4751 - model_125_loss: 0.6927 - model_125_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4457 - model_124_loss: 0.4769 - model_125_loss: 0.6928 - model_125_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.9250 - model_125_loss: 0.6936 - model_125_1_loss: 0.69230s - loss: 6.9364 - model_125_loss: 0.6949 - model_125_1_lo\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4457 - model_124_loss: 0.4761 - model_125_loss: 0.6925 - model_125_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4468 - model_124_loss: 0.4746 - model_125_loss: 0.6926 - model_125_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4499 - model_124_loss: 0.4755 - model_125_loss: 0.6929 - model_125_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4484 - model_124_loss: 0.4744 - model_125_loss: 0.6926 - model_125_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4486 - model_124_loss: 0.4741 - model_125_loss: 0.6927 - model_125_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9233 - model_125_loss: 0.6931 - model_125_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4471 - model_124_loss: 0.4759 - model_125_loss: 0.6927 - model_125_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4501 - model_124_loss: 0.4725 - model_125_loss: 0.6926 - model_125_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4491 - model_124_loss: 0.4715 - model_125_loss: 0.6925 - model_125_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4490 - model_124_loss: 0.4726 - model_125_loss: 0.6926 - model_125_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4519 - model_124_loss: 0.4691 - model_125_loss: 0.6928 - model_125_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9204 - model_125_loss: 0.6922 - model_125_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4494 - model_124_loss: 0.4707 - model_125_loss: 0.6925 - model_125_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4492 - model_124_loss: 0.4715 - model_125_loss: 0.6927 - model_125_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4494 - model_124_loss: 0.4715 - model_125_loss: 0.6926 - model_125_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4482 - model_124_loss: 0.4710 - model_125_loss: 0.6925 - model_125_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4517 - model_124_loss: 0.4702 - model_125_loss: 0.6927 - model_125_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9203 - model_125_loss: 0.6920 - model_125_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4494 - model_124_loss: 0.4703 - model_125_loss: 0.6925 - model_125_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4474 - model_124_loss: 0.4716 - model_125_loss: 0.6923 - model_125_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4501 - model_124_loss: 0.4701 - model_125_loss: 0.6925 - model_125_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4496 - model_124_loss: 0.4711 - model_125_loss: 0.6925 - model_125_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4508 - model_124_loss: 0.4703 - model_125_loss: 0.6927 - model_125_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9212 - model_125_loss: 0.6928 - model_125_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4487 - model_124_loss: 0.4703 - model_125_loss: 0.6924 - model_125_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4463 - model_124_loss: 0.4706 - model_125_loss: 0.6923 - model_125_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4500 - model_124_loss: 0.4688 - model_125_loss: 0.6926 - model_125_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4510 - model_124_loss: 0.4707 - model_125_loss: 0.6926 - model_125_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4500 - model_124_loss: 0.4694 - model_125_loss: 0.6924 - model_125_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9192 - model_125_loss: 0.6923 - model_125_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4468 - model_124_loss: 0.4712 - model_125_loss: 0.6926 - model_125_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4491 - model_124_loss: 0.4689 - model_125_loss: 0.6921 - model_125_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4470 - model_124_loss: 0.4713 - model_125_loss: 0.6921 - model_125_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4516 - model_124_loss: 0.4717 - model_125_loss: 0.6930 - model_125_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4499 - model_124_loss: 0.4714 - model_125_loss: 0.6925 - model_125_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9238 - model_125_loss: 0.6926 - model_125_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4491 - model_124_loss: 0.4710 - model_125_loss: 0.6925 - model_125_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4488 - model_124_loss: 0.4714 - model_125_loss: 0.6925 - model_125_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4518 - model_124_loss: 0.4704 - model_125_loss: 0.6926 - model_125_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4486 - model_124_loss: 0.4728 - model_125_loss: 0.6927 - model_125_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4505 - model_124_loss: 0.4716 - model_125_loss: 0.6927 - model_125_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9221 - model_125_loss: 0.6922 - model_125_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4502 - model_124_loss: 0.4705 - model_125_loss: 0.6925 - model_125_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4505 - model_124_loss: 0.4717 - model_125_loss: 0.6926 - model_125_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4507 - model_124_loss: 0.4706 - model_125_loss: 0.6924 - model_125_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4497 - model_124_loss: 0.4699 - model_125_loss: 0.6923 - model_125_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4517 - model_124_loss: 0.4699 - model_125_loss: 0.6925 - model_125_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9226 - model_125_loss: 0.6928 - model_125_1_loss: 0.69160s - loss: 6.9083 - model_125_loss: 0.6886 - model_125_1_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4487 - model_124_loss: 0.4714 - model_125_loss: 0.6923 - model_125_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4513 - model_124_loss: 0.4701 - model_125_loss: 0.6926 - model_125_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4529 - model_124_loss: 0.4698 - model_125_loss: 0.6927 - model_125_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4531 - model_124_loss: 0.4685 - model_125_loss: 0.6927 - model_125_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4512 - model_124_loss: 0.4690 - model_125_loss: 0.6925 - model_125_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9213 - model_125_loss: 0.6928 - model_125_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4500 - model_124_loss: 0.4681 - model_125_loss: 0.6921 - model_125_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4535 - model_124_loss: 0.4679 - model_125_loss: 0.6925 - model_125_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4515 - model_124_loss: 0.4676 - model_125_loss: 0.6924 - model_125_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4540 - model_124_loss: 0.4669 - model_125_loss: 0.6926 - model_125_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4545 - model_124_loss: 0.4665 - model_125_loss: 0.6927 - model_125_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9212 - model_125_loss: 0.6933 - model_125_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4532 - model_124_loss: 0.4665 - model_125_loss: 0.6923 - model_125_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4526 - model_124_loss: 0.4668 - model_125_loss: 0.6923 - model_125_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4521 - model_124_loss: 0.4667 - model_125_loss: 0.6922 - model_125_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4560 - model_124_loss: 0.4658 - model_125_loss: 0.6928 - model_125_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4576 - model_124_loss: 0.4650 - model_125_loss: 0.6929 - model_125_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9183 - model_125_loss: 0.6915 - model_125_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4525 - model_124_loss: 0.4652 - model_125_loss: 0.6924 - model_125_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4523 - model_124_loss: 0.4649 - model_125_loss: 0.6923 - model_125_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4505 - model_124_loss: 0.4667 - model_125_loss: 0.6924 - model_125_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4525 - model_124_loss: 0.4644 - model_125_loss: 0.6923 - model_125_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4535 - model_124_loss: 0.4654 - model_125_loss: 0.6926 - model_125_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9168 - model_125_loss: 0.6920 - model_125_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4525 - model_124_loss: 0.4655 - model_125_loss: 0.6924 - model_125_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4526 - model_124_loss: 0.4650 - model_125_loss: 0.6923 - model_125_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4538 - model_124_loss: 0.4638 - model_125_loss: 0.6923 - model_125_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4531 - model_124_loss: 0.4644 - model_125_loss: 0.6920 - model_125_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4552 - model_124_loss: 0.4643 - model_125_loss: 0.6925 - model_125_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.9218 - model_125_loss: 0.6922 - model_125_1_loss: 0.69130s - loss: 6.9171 - model_125_loss: 0.6933 - model_125_1_loss: \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4552 - model_124_loss: 0.4641 - model_125_loss: 0.6926 - model_125_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4536 - model_124_loss: 0.4659 - model_125_loss: 0.6926 - model_125_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4535 - model_124_loss: 0.4665 - model_125_loss: 0.6924 - model_125_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4516 - model_124_loss: 0.4659 - model_125_loss: 0.6924 - model_125_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4535 - model_124_loss: 0.4668 - model_125_loss: 0.6928 - model_125_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9198 - model_125_loss: 0.6934 - model_125_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4554 - model_124_loss: 0.4658 - model_125_loss: 0.6927 - model_125_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4545 - model_124_loss: 0.4675 - model_125_loss: 0.6927 - model_125_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4547 - model_124_loss: 0.4670 - model_125_loss: 0.6927 - model_125_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4542 - model_124_loss: 0.4663 - model_125_loss: 0.6924 - model_125_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4555 - model_124_loss: 0.4666 - model_125_loss: 0.6928 - model_125_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.9192 - model_125_loss: 0.6917 - model_125_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4545 - model_124_loss: 0.4671 - model_125_loss: 0.6929 - model_125_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4538 - model_124_loss: 0.4688 - model_125_loss: 0.6927 - model_125_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4553 - model_124_loss: 0.4677 - model_125_loss: 0.6930 - model_125_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4535 - model_124_loss: 0.4690 - model_125_loss: 0.6929 - model_125_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4537 - model_124_loss: 0.4694 - model_125_loss: 0.6929 - model_125_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9238 - model_125_loss: 0.6926 - model_125_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4542 - model_124_loss: 0.4701 - model_125_loss: 0.6925 - model_125_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4540 - model_124_loss: 0.4706 - model_125_loss: 0.6927 - model_125_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4553 - model_124_loss: 0.4699 - model_125_loss: 0.6927 - model_125_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4538 - model_124_loss: 0.4695 - model_125_loss: 0.6926 - model_125_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4560 - model_124_loss: 0.4699 - model_125_loss: 0.6927 - model_125_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9211 - model_125_loss: 0.6916 - model_125_1_loss: 0.69200s - loss: 6.9307 - model_125_loss: 0.6937 - model_125_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4517 - model_124_loss: 0.4711 - model_125_loss: 0.6923 - model_125_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4506 - model_124_loss: 0.4702 - model_125_loss: 0.6923 - model_125_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4523 - model_124_loss: 0.4701 - model_125_loss: 0.6924 - model_125_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4495 - model_124_loss: 0.4697 - model_125_loss: 0.6920 - model_125_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4512 - model_124_loss: 0.4689 - model_125_loss: 0.6919 - model_125_1_loss: 0.6921\n",
      "For Attention Module: 3.3000000000000003\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.3844 - model_129_loss: 0.6596 - model_129_1_loss: 0.6178\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: -6.0050 - model_128_loss: 0.3846 - model_129_loss: 0.6603 - model_129_1_loss: 0.6176\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.0178 - model_128_loss: 0.3850 - model_129_loss: 0.6604 - model_129_1_loss: 0.6201\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.0222 - model_128_loss: 0.3839 - model_129_loss: 0.6603 - model_129_1_loss: 0.6209\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.0456 - model_128_loss: 0.3844 - model_129_loss: 0.6625 - model_129_1_loss: 0.6235\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.0495 - model_128_loss: 0.3867 - model_129_loss: 0.6615 - model_129_1_loss: 0.6257\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.4456 - model_129_loss: 0.6620 - model_129_1_loss: 0.6266\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0632 - model_128_loss: 0.3875 - model_129_loss: 0.6625 - model_129_1_loss: 0.6276\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.0699 - model_128_loss: 0.3871 - model_129_loss: 0.6620 - model_129_1_loss: 0.6294\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.0811 - model_128_loss: 0.3865 - model_129_loss: 0.6627 - model_129_1_loss: 0.6308\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.0901 - model_128_loss: 0.3889 - model_129_loss: 0.6628 - model_129_1_loss: 0.6330\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.1010 - model_128_loss: 0.3893 - model_129_loss: 0.6626 - model_129_1_loss: 0.6354\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.5229 - model_129_loss: 0.6655 - model_129_1_loss: 0.6387\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1132 - model_128_loss: 0.3917 - model_129_loss: 0.6639 - model_129_1_loss: 0.6370\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.1341 - model_128_loss: 0.3939 - model_129_loss: 0.6654 - model_129_1_loss: 0.6402\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.1407 - model_128_loss: 0.3938 - model_129_loss: 0.6652 - model_129_1_loss: 0.6417\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.1619 - model_128_loss: 0.3947 - model_129_loss: 0.6674 - model_129_1_loss: 0.6439\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.1644 - model_128_loss: 0.3972 - model_129_loss: 0.6670 - model_129_1_loss: 0.6453\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.5874 - model_129_loss: 0.6683 - model_129_1_loss: 0.6494\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.1833 - model_128_loss: 0.3977 - model_129_loss: 0.6678 - model_129_1_loss: 0.6484\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.1916 - model_128_loss: 0.4013 - model_129_loss: 0.6678 - model_129_1_loss: 0.6508\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.2090 - model_128_loss: 0.4017 - model_129_loss: 0.6694 - model_129_1_loss: 0.6527\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.2313 - model_128_loss: 0.4043 - model_129_loss: 0.6720 - model_129_1_loss: 0.6552\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2383 - model_128_loss: 0.4061 - model_129_loss: 0.6704 - model_129_1_loss: 0.6584\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.6586 - model_129_loss: 0.6735 - model_129_1_loss: 0.6592\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.2411 - model_128_loss: 0.4081 - model_129_loss: 0.6711 - model_129_1_loss: 0.6587\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.2473 - model_128_loss: 0.4106 - model_129_loss: 0.6719 - model_129_1_loss: 0.6597\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2702 - model_128_loss: 0.4112 - model_129_loss: 0.6739 - model_129_1_loss: 0.6624\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2760 - model_128_loss: 0.4145 - model_129_loss: 0.6740 - model_129_1_loss: 0.6641\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.2846 - model_128_loss: 0.4161 - model_129_loss: 0.6742 - model_129_1_loss: 0.6659\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.7111 - model_129_loss: 0.6741 - model_129_1_loss: 0.6680\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2891 - model_128_loss: 0.4177 - model_129_loss: 0.6742 - model_129_1_loss: 0.6671\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.2999 - model_128_loss: 0.4211 - model_129_loss: 0.6764 - model_129_1_loss: 0.6678\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3125 - model_128_loss: 0.4244 - model_129_loss: 0.6770 - model_129_1_loss: 0.6703\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3176 - model_128_loss: 0.4227 - model_129_loss: 0.6773 - model_129_1_loss: 0.6708\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3275 - model_128_loss: 0.4295 - model_129_loss: 0.6783 - model_129_1_loss: 0.6731\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: 6.7619 - model_129_loss: 0.6783 - model_129_1_loss: 0.6737\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3272 - model_128_loss: 0.4309 - model_129_loss: 0.6779 - model_129_1_loss: 0.6737\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3338 - model_128_loss: 0.4340 - model_129_loss: 0.6790 - model_129_1_loss: 0.6746\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3493 - model_128_loss: 0.4345 - model_129_loss: 0.6811 - model_129_1_loss: 0.6756\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3583 - model_128_loss: 0.4397 - model_129_loss: 0.6822 - model_129_1_loss: 0.6775\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3595 - model_128_loss: 0.4422 - model_129_loss: 0.6825 - model_129_1_loss: 0.6779\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.8096 - model_129_loss: 0.6829 - model_129_1_loss: 0.67860s - loss: 6.7917 - model_129_loss: 0.6786 - model_129_1\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3618 - model_128_loss: 0.4438 - model_129_loss: 0.6828 - model_129_1_loss: 0.6784\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3638 - model_128_loss: 0.4504 - model_129_loss: 0.6837 - model_129_1_loss: 0.6791\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3721 - model_128_loss: 0.4506 - model_129_loss: 0.6840 - model_129_1_loss: 0.6805\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3729 - model_128_loss: 0.4536 - model_129_loss: 0.6844 - model_129_1_loss: 0.6809\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3785 - model_128_loss: 0.4580 - model_129_loss: 0.6853 - model_129_1_loss: 0.6820\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: 6.8429 - model_129_loss: 0.6875 - model_129_1_loss: 0.6822\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3674 - model_128_loss: 0.4606 - model_129_loss: 0.6849 - model_129_1_loss: 0.6807\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3794 - model_128_loss: 0.4637 - model_129_loss: 0.6869 - model_129_1_loss: 0.6818\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3852 - model_128_loss: 0.4640 - model_129_loss: 0.6868 - model_129_1_loss: 0.6830\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3866 - model_128_loss: 0.4681 - model_129_loss: 0.6876 - model_129_1_loss: 0.6834\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.3974 - model_128_loss: 0.4692 - model_129_loss: 0.6886 - model_129_1_loss: 0.6848\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.8712 - model_129_loss: 0.6895 - model_129_1_loss: 0.6854\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3930 - model_128_loss: 0.4710 - model_129_loss: 0.6884 - model_129_1_loss: 0.6844\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3938 - model_128_loss: 0.4725 - model_129_loss: 0.6887 - model_129_1_loss: 0.6846\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3992 - model_128_loss: 0.4758 - model_129_loss: 0.6898 - model_129_1_loss: 0.6852\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4029 - model_128_loss: 0.4768 - model_129_loss: 0.6893 - model_129_1_loss: 0.6867\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4053 - model_128_loss: 0.4799 - model_129_loss: 0.6901 - model_129_1_loss: 0.6870\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.8904 - model_129_loss: 0.6901 - model_129_1_loss: 0.6878\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4081 - model_128_loss: 0.4817 - model_129_loss: 0.6904 - model_129_1_loss: 0.6876\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4067 - model_128_loss: 0.4854 - model_129_loss: 0.6904 - model_129_1_loss: 0.6880\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4179 - model_128_loss: 0.4851 - model_129_loss: 0.6916 - model_129_1_loss: 0.6890\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4176 - model_128_loss: 0.4876 - model_129_loss: 0.6913 - model_129_1_loss: 0.6897\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4228 - model_128_loss: 0.4889 - model_129_loss: 0.6919 - model_129_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9150 - model_129_loss: 0.6922 - model_129_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4181 - model_128_loss: 0.4887 - model_129_loss: 0.6911 - model_129_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4162 - model_128_loss: 0.4903 - model_129_loss: 0.6911 - model_129_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4258 - model_128_loss: 0.4910 - model_129_loss: 0.6920 - model_129_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4236 - model_128_loss: 0.4917 - model_129_loss: 0.6918 - model_129_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4265 - model_128_loss: 0.4925 - model_129_loss: 0.6919 - model_129_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: 6.9193 - model_129_loss: 0.6914 - model_129_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4250 - model_128_loss: 0.4924 - model_129_loss: 0.6918 - model_129_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4251 - model_128_loss: 0.4918 - model_129_loss: 0.6917 - model_129_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4307 - model_128_loss: 0.4905 - model_129_loss: 0.6920 - model_129_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4310 - model_128_loss: 0.4942 - model_129_loss: 0.6924 - model_129_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4301 - model_128_loss: 0.4926 - model_129_loss: 0.6918 - model_129_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9238 - model_129_loss: 0.6922 - model_129_1_loss: 0.6930\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4339 - model_128_loss: 0.4905 - model_129_loss: 0.6919 - model_129_1_loss: 0.6929\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4338 - model_128_loss: 0.4920 - model_129_loss: 0.6923 - model_129_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4320 - model_128_loss: 0.4914 - model_129_loss: 0.6918 - model_129_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4302 - model_128_loss: 0.4930 - model_129_loss: 0.6918 - model_129_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4343 - model_128_loss: 0.4932 - model_129_loss: 0.6924 - model_129_1_loss: 0.6931\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: 6.9252 - model_129_loss: 0.6922 - model_129_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4262 - model_128_loss: 0.4922 - model_129_loss: 0.6912 - model_129_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4277 - model_128_loss: 0.4920 - model_129_loss: 0.6916 - model_129_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4284 - model_128_loss: 0.4930 - model_129_loss: 0.6918 - model_129_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4290 - model_128_loss: 0.4915 - model_129_loss: 0.6915 - model_129_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4308 - model_128_loss: 0.4905 - model_129_loss: 0.6914 - model_129_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9185 - model_129_loss: 0.6912 - model_129_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4314 - model_128_loss: 0.4890 - model_129_loss: 0.6913 - model_129_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4297 - model_128_loss: 0.4878 - model_129_loss: 0.6905 - model_129_1_loss: 0.6930\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4288 - model_128_loss: 0.4874 - model_129_loss: 0.6906 - model_129_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4282 - model_128_loss: 0.4859 - model_129_loss: 0.6905 - model_129_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4267 - model_128_loss: 0.4856 - model_129_loss: 0.6902 - model_129_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9144 - model_129_loss: 0.6904 - model_129_1_loss: 0.69220s - loss: 6.8850 - model_129_loss: 0.6856 - model_12\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4254 - model_128_loss: 0.4857 - model_129_loss: 0.6903 - model_129_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4233 - model_128_loss: 0.4863 - model_129_loss: 0.6900 - model_129_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4275 - model_128_loss: 0.4846 - model_129_loss: 0.6905 - model_129_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4269 - model_128_loss: 0.4844 - model_129_loss: 0.6904 - model_129_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 15us/sample - loss: -6.4266 - model_128_loss: 0.4834 - model_129_loss: 0.6906 - model_129_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 74us/sample - loss: 6.9097 - model_129_loss: 0.6902 - model_129_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4335 - model_128_loss: 0.4790 - model_129_loss: 0.6905 - model_129_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4279 - model_128_loss: 0.4819 - model_129_loss: 0.6901 - model_129_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4356 - model_128_loss: 0.4792 - model_129_loss: 0.6910 - model_129_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4325 - model_128_loss: 0.4780 - model_129_loss: 0.6908 - model_129_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4349 - model_128_loss: 0.4770 - model_129_loss: 0.6908 - model_129_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9084 - model_129_loss: 0.6903 - model_129_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4316 - model_128_loss: 0.4768 - model_129_loss: 0.6903 - model_129_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4325 - model_128_loss: 0.4769 - model_129_loss: 0.6905 - model_129_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4332 - model_128_loss: 0.4742 - model_129_loss: 0.6903 - model_129_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4352 - model_128_loss: 0.4748 - model_129_loss: 0.6901 - model_129_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4386 - model_128_loss: 0.4732 - model_129_loss: 0.6906 - model_129_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9092 - model_129_loss: 0.6909 - model_129_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4309 - model_128_loss: 0.4749 - model_129_loss: 0.6905 - model_129_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4287 - model_128_loss: 0.4747 - model_129_loss: 0.6901 - model_129_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4359 - model_128_loss: 0.4739 - model_129_loss: 0.6912 - model_129_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4314 - model_128_loss: 0.4743 - model_129_loss: 0.6902 - model_129_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4350 - model_128_loss: 0.4741 - model_129_loss: 0.6907 - model_129_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9080 - model_129_loss: 0.6902 - model_129_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: -6.4342 - model_128_loss: 0.4734 - model_129_loss: 0.6908 - model_129_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4311 - model_128_loss: 0.4739 - model_129_loss: 0.6903 - model_129_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4349 - model_128_loss: 0.4723 - model_129_loss: 0.6905 - model_129_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4333 - model_128_loss: 0.4726 - model_129_loss: 0.6905 - model_129_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4353 - model_128_loss: 0.4723 - model_129_loss: 0.6908 - model_129_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 72us/sample - loss: 6.9110 - model_129_loss: 0.6916 - model_129_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4302 - model_128_loss: 0.4731 - model_129_loss: 0.6906 - model_129_1_loss: 0.6901\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: -6.4351 - model_128_loss: 0.4733 - model_129_loss: 0.6910 - model_129_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: -6.4369 - model_128_loss: 0.4722 - model_129_loss: 0.6912 - model_129_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4395 - model_128_loss: 0.4713 - model_129_loss: 0.6916 - model_129_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4357 - model_128_loss: 0.4719 - model_129_loss: 0.6912 - model_129_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 73us/sample - loss: 6.9149 - model_129_loss: 0.6919 - model_129_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: -6.4383 - model_128_loss: 0.4725 - model_129_loss: 0.6913 - model_129_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4389 - model_128_loss: 0.4730 - model_129_loss: 0.6913 - model_129_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4408 - model_128_loss: 0.4715 - model_129_loss: 0.6913 - model_129_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4442 - model_128_loss: 0.4724 - model_129_loss: 0.6919 - model_129_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4437 - model_128_loss: 0.4733 - model_129_loss: 0.6918 - model_129_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9202 - model_129_loss: 0.6920 - model_129_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4430 - model_128_loss: 0.4740 - model_129_loss: 0.6916 - model_129_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4461 - model_128_loss: 0.4730 - model_129_loss: 0.6919 - model_129_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4459 - model_128_loss: 0.4747 - model_129_loss: 0.6922 - model_129_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4461 - model_128_loss: 0.4730 - model_129_loss: 0.6921 - model_129_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4502 - model_128_loss: 0.4726 - model_129_loss: 0.6923 - model_129_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9214 - model_129_loss: 0.6923 - model_129_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4460 - model_128_loss: 0.4716 - model_129_loss: 0.6920 - model_129_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4458 - model_128_loss: 0.4709 - model_129_loss: 0.6917 - model_129_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4468 - model_128_loss: 0.4716 - model_129_loss: 0.6918 - model_129_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4489 - model_128_loss: 0.4723 - model_129_loss: 0.6920 - model_129_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4442 - model_128_loss: 0.4729 - model_129_loss: 0.6916 - model_129_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9164 - model_129_loss: 0.6921 - model_129_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4454 - model_128_loss: 0.4714 - model_129_loss: 0.6916 - model_129_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4480 - model_128_loss: 0.4714 - model_129_loss: 0.6923 - model_129_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4482 - model_128_loss: 0.4731 - model_129_loss: 0.6923 - model_129_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4467 - model_128_loss: 0.4726 - model_129_loss: 0.6920 - model_129_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4465 - model_128_loss: 0.4730 - model_129_loss: 0.6920 - model_129_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9242 - model_129_loss: 0.6923 - model_129_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4453 - model_128_loss: 0.4735 - model_129_loss: 0.6922 - model_129_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4449 - model_128_loss: 0.4735 - model_129_loss: 0.6918 - model_129_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 16us/sample - loss: -6.4467 - model_128_loss: 0.4723 - model_129_loss: 0.6920 - model_129_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4490 - model_128_loss: 0.4730 - model_129_loss: 0.6923 - model_129_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4500 - model_128_loss: 0.4734 - model_129_loss: 0.6925 - model_129_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9237 - model_129_loss: 0.6932 - model_129_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4492 - model_128_loss: 0.4710 - model_129_loss: 0.6924 - model_129_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4502 - model_128_loss: 0.4735 - model_129_loss: 0.6927 - model_129_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4528 - model_128_loss: 0.4741 - model_129_loss: 0.6930 - model_129_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4494 - model_128_loss: 0.4730 - model_129_loss: 0.6926 - model_129_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4468 - model_128_loss: 0.4747 - model_129_loss: 0.6922 - model_129_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9252 - model_129_loss: 0.6927 - model_129_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4482 - model_128_loss: 0.4758 - model_129_loss: 0.6925 - model_129_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4472 - model_128_loss: 0.4748 - model_129_loss: 0.6921 - model_129_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4490 - model_128_loss: 0.4747 - model_129_loss: 0.6922 - model_129_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4475 - model_128_loss: 0.4757 - model_129_loss: 0.6925 - model_129_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4499 - model_128_loss: 0.4730 - model_129_loss: 0.6923 - model_129_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9250 - model_129_loss: 0.6929 - model_129_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4492 - model_128_loss: 0.4752 - model_129_loss: 0.6926 - model_129_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4515 - model_128_loss: 0.4717 - model_129_loss: 0.6925 - model_129_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4480 - model_128_loss: 0.4749 - model_129_loss: 0.6924 - model_129_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4516 - model_128_loss: 0.4734 - model_129_loss: 0.6925 - model_129_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4540 - model_128_loss: 0.4726 - model_129_loss: 0.6928 - model_129_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9246 - model_129_loss: 0.6922 - model_129_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4466 - model_128_loss: 0.4753 - model_129_loss: 0.6925 - model_129_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4489 - model_128_loss: 0.4739 - model_129_loss: 0.6923 - model_129_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4522 - model_128_loss: 0.4719 - model_129_loss: 0.6929 - model_129_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4515 - model_128_loss: 0.4726 - model_129_loss: 0.6926 - model_129_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4528 - model_128_loss: 0.4725 - model_129_loss: 0.6928 - model_129_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9228 - model_129_loss: 0.6928 - model_129_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4504 - model_128_loss: 0.4725 - model_129_loss: 0.6924 - model_129_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4499 - model_128_loss: 0.4725 - model_129_loss: 0.6924 - model_129_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4475 - model_128_loss: 0.4730 - model_129_loss: 0.6921 - model_129_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4510 - model_128_loss: 0.4718 - model_129_loss: 0.6927 - model_129_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4541 - model_128_loss: 0.4694 - model_129_loss: 0.6927 - model_129_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9144 - model_129_loss: 0.6913 - model_129_1_loss: 0.691 - 1s 43us/sample - loss: 6.9220 - model_129_loss: 0.6923 - model_129_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4521 - model_128_loss: 0.4704 - model_129_loss: 0.6927 - model_129_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4497 - model_128_loss: 0.4697 - model_129_loss: 0.6924 - model_129_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4497 - model_128_loss: 0.4682 - model_129_loss: 0.6923 - model_129_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4515 - model_128_loss: 0.4682 - model_129_loss: 0.6925 - model_129_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4519 - model_128_loss: 0.4677 - model_129_loss: 0.6924 - model_129_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9178 - model_129_loss: 0.6923 - model_129_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4519 - model_128_loss: 0.4684 - model_129_loss: 0.6925 - model_129_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4514 - model_128_loss: 0.4679 - model_129_loss: 0.6925 - model_129_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4520 - model_128_loss: 0.4668 - model_129_loss: 0.6925 - model_129_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4492 - model_128_loss: 0.4663 - model_129_loss: 0.6921 - model_129_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4512 - model_128_loss: 0.4662 - model_129_loss: 0.6921 - model_129_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9194 - model_129_loss: 0.6925 - model_129_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4498 - model_128_loss: 0.4668 - model_129_loss: 0.6922 - model_129_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4542 - model_128_loss: 0.4647 - model_129_loss: 0.6925 - model_129_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4522 - model_128_loss: 0.4662 - model_129_loss: 0.6923 - model_129_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4535 - model_128_loss: 0.4651 - model_129_loss: 0.6925 - model_129_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4523 - model_128_loss: 0.4659 - model_129_loss: 0.6925 - model_129_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9179 - model_129_loss: 0.6929 - model_129_1_loss: 0.69130s - loss: 6.8871 - model_129_loss: 0.6895 - model_129\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4544 - model_128_loss: 0.4642 - model_129_loss: 0.6925 - model_129_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4491 - model_128_loss: 0.4660 - model_129_loss: 0.6921 - model_129_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4505 - model_128_loss: 0.4664 - model_129_loss: 0.6921 - model_129_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4555 - model_128_loss: 0.4654 - model_129_loss: 0.6928 - model_129_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4567 - model_128_loss: 0.4659 - model_129_loss: 0.6930 - model_129_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9186 - model_129_loss: 0.6921 - model_129_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4529 - model_128_loss: 0.4653 - model_129_loss: 0.6923 - model_129_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4544 - model_128_loss: 0.4658 - model_129_loss: 0.6924 - model_129_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4505 - model_128_loss: 0.4662 - model_129_loss: 0.6921 - model_129_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4518 - model_128_loss: 0.4690 - model_129_loss: 0.6925 - model_129_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4517 - model_128_loss: 0.4693 - model_129_loss: 0.6925 - model_129_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9233 - model_129_loss: 0.6932 - model_129_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4549 - model_128_loss: 0.4667 - model_129_loss: 0.6927 - model_129_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4516 - model_128_loss: 0.4710 - model_129_loss: 0.6927 - model_129_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4550 - model_128_loss: 0.4678 - model_129_loss: 0.6928 - model_129_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4530 - model_128_loss: 0.4681 - model_129_loss: 0.6925 - model_129_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4534 - model_128_loss: 0.4692 - model_129_loss: 0.6927 - model_129_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9241 - model_129_loss: 0.6925 - model_129_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4535 - model_128_loss: 0.4691 - model_129_loss: 0.6927 - model_129_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4527 - model_128_loss: 0.4689 - model_129_loss: 0.6926 - model_129_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4545 - model_128_loss: 0.4689 - model_129_loss: 0.6930 - model_129_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4540 - model_128_loss: 0.4670 - model_129_loss: 0.6924 - model_129_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4576 - model_128_loss: 0.4677 - model_129_loss: 0.6930 - model_129_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9232 - model_129_loss: 0.6928 - model_129_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4565 - model_128_loss: 0.4672 - model_129_loss: 0.6926 - model_129_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4524 - model_128_loss: 0.4685 - model_129_loss: 0.6923 - model_129_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4528 - model_128_loss: 0.4684 - model_129_loss: 0.6924 - model_129_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4535 - model_128_loss: 0.4681 - model_129_loss: 0.6925 - model_129_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4536 - model_128_loss: 0.4671 - model_129_loss: 0.6926 - model_129_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9228 - model_129_loss: 0.6930 - model_129_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4541 - model_128_loss: 0.4685 - model_129_loss: 0.6929 - model_129_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4564 - model_128_loss: 0.4660 - model_129_loss: 0.6926 - model_129_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4555 - model_128_loss: 0.4673 - model_129_loss: 0.6927 - model_129_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4550 - model_128_loss: 0.4683 - model_129_loss: 0.6927 - model_129_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4545 - model_128_loss: 0.4661 - model_129_loss: 0.6925 - model_129_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9231 - model_129_loss: 0.6928 - model_129_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4546 - model_128_loss: 0.4666 - model_129_loss: 0.6925 - model_129_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4563 - model_128_loss: 0.4666 - model_129_loss: 0.6929 - model_129_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4578 - model_128_loss: 0.4651 - model_129_loss: 0.6928 - model_129_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4584 - model_128_loss: 0.4641 - model_129_loss: 0.6927 - model_129_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4570 - model_128_loss: 0.4642 - model_129_loss: 0.6925 - model_129_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9206 - model_129_loss: 0.6924 - model_129_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4572 - model_128_loss: 0.4636 - model_129_loss: 0.6926 - model_129_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4561 - model_128_loss: 0.4647 - model_129_loss: 0.6927 - model_129_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4527 - model_128_loss: 0.4644 - model_129_loss: 0.6922 - model_129_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4559 - model_128_loss: 0.4615 - model_129_loss: 0.6923 - model_129_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4573 - model_128_loss: 0.4629 - model_129_loss: 0.6928 - model_129_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9180 - model_129_loss: 0.6928 - model_129_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4538 - model_128_loss: 0.4642 - model_129_loss: 0.6927 - model_129_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4518 - model_128_loss: 0.4630 - model_129_loss: 0.6921 - model_129_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4547 - model_128_loss: 0.4635 - model_129_loss: 0.6925 - model_129_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4533 - model_128_loss: 0.4644 - model_129_loss: 0.6925 - model_129_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4561 - model_128_loss: 0.4634 - model_129_loss: 0.6926 - model_129_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9161 - model_129_loss: 0.6927 - model_129_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4553 - model_128_loss: 0.4639 - model_129_loss: 0.6926 - model_129_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4522 - model_128_loss: 0.4642 - model_129_loss: 0.6923 - model_129_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4563 - model_128_loss: 0.4638 - model_129_loss: 0.6926 - model_129_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4535 - model_128_loss: 0.4641 - model_129_loss: 0.6925 - model_129_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4550 - model_128_loss: 0.4658 - model_129_loss: 0.6924 - model_129_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9240 - model_129_loss: 0.6927 - model_129_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4533 - model_128_loss: 0.4666 - model_129_loss: 0.6924 - model_129_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4568 - model_128_loss: 0.4650 - model_129_loss: 0.6927 - model_129_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4536 - model_128_loss: 0.4662 - model_129_loss: 0.6924 - model_129_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4538 - model_128_loss: 0.4676 - model_129_loss: 0.6926 - model_129_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4550 - model_128_loss: 0.4659 - model_129_loss: 0.6925 - model_129_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9240 - model_129_loss: 0.6926 - model_129_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4553 - model_128_loss: 0.4673 - model_129_loss: 0.6927 - model_129_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4541 - model_128_loss: 0.4691 - model_129_loss: 0.6928 - model_129_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4553 - model_128_loss: 0.4678 - model_129_loss: 0.6926 - model_129_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4574 - model_128_loss: 0.4675 - model_129_loss: 0.6929 - model_129_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4560 - model_128_loss: 0.4683 - model_129_loss: 0.6928 - model_129_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9253 - model_129_loss: 0.6925 - model_129_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4565 - model_128_loss: 0.4675 - model_129_loss: 0.6928 - model_129_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4553 - model_128_loss: 0.4674 - model_129_loss: 0.6928 - model_129_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4564 - model_128_loss: 0.4674 - model_129_loss: 0.6928 - model_129_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4576 - model_128_loss: 0.4659 - model_129_loss: 0.6928 - model_129_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4561 - model_128_loss: 0.4669 - model_129_loss: 0.6928 - model_129_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9223 - model_129_loss: 0.6923 - model_129_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4566 - model_128_loss: 0.4650 - model_129_loss: 0.6925 - model_129_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4571 - model_128_loss: 0.4643 - model_129_loss: 0.6926 - model_129_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4529 - model_128_loss: 0.4665 - model_129_loss: 0.6925 - model_129_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4556 - model_128_loss: 0.4627 - model_129_loss: 0.6922 - model_129_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4557 - model_128_loss: 0.4622 - model_129_loss: 0.6921 - model_129_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9228 - model_129_loss: 0.6924 - model_129_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4551 - model_128_loss: 0.4643 - model_129_loss: 0.6923 - model_129_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4577 - model_128_loss: 0.4640 - model_129_loss: 0.6927 - model_129_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4580 - model_128_loss: 0.4628 - model_129_loss: 0.6926 - model_129_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4564 - model_128_loss: 0.4644 - model_129_loss: 0.6925 - model_129_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4550 - model_128_loss: 0.4644 - model_129_loss: 0.6926 - model_129_1_loss: 0.6913\n",
      "For Attention Module: 3.4000000000000004\n",
      "features X: 30940 samples, 69 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.3730 - model_133_loss: 0.6626 - model_133_1_loss: 0.6128\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: -5.9830 - model_132_loss: 0.3863 - model_133_loss: 0.6606 - model_133_1_loss: 0.6133\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -5.9969 - model_132_loss: 0.3867 - model_133_loss: 0.6614 - model_133_1_loss: 0.6154\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0102 - model_132_loss: 0.3864 - model_133_loss: 0.6613 - model_133_1_loss: 0.6180\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.0312 - model_132_loss: 0.3852 - model_133_loss: 0.6633 - model_133_1_loss: 0.6200\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0456 - model_132_loss: 0.3850 - model_133_loss: 0.6636 - model_133_1_loss: 0.6225\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.4316 - model_133_loss: 0.6644 - model_133_1_loss: 0.6221\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.0380 - model_132_loss: 0.3873 - model_133_loss: 0.6642 - model_133_1_loss: 0.6209\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.0450 - model_132_loss: 0.3868 - model_133_loss: 0.6638 - model_133_1_loss: 0.6225\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0555 - model_132_loss: 0.3881 - model_133_loss: 0.6631 - model_133_1_loss: 0.6256\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0658 - model_132_loss: 0.3875 - model_133_loss: 0.6637 - model_133_1_loss: 0.6270\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0840 - model_132_loss: 0.3882 - model_133_loss: 0.6654 - model_133_1_loss: 0.6290\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.4803 - model_133_loss: 0.6662 - model_133_1_loss: 0.6302\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.0862 - model_132_loss: 0.3898 - model_133_loss: 0.6652 - model_133_1_loss: 0.6300\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.0956 - model_132_loss: 0.3895 - model_133_loss: 0.6661 - model_133_1_loss: 0.6309\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1097 - model_132_loss: 0.3906 - model_133_loss: 0.6660 - model_133_1_loss: 0.6341\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.1252 - model_132_loss: 0.3897 - model_133_loss: 0.6682 - model_133_1_loss: 0.6348\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.1352 - model_132_loss: 0.3935 - model_133_loss: 0.6699 - model_133_1_loss: 0.6359\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.5367 - model_133_loss: 0.6690 - model_133_1_loss: 0.6388\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.1418 - model_132_loss: 0.3937 - model_133_loss: 0.6695 - model_133_1_loss: 0.6376\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.1680 - model_132_loss: 0.3968 - model_133_loss: 0.6722 - model_133_1_loss: 0.6407\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.1772 - model_132_loss: 0.3978 - model_133_loss: 0.6731 - model_133_1_loss: 0.6419\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.1900 - model_132_loss: 0.3998 - model_133_loss: 0.6718 - model_133_1_loss: 0.6461\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1964 - model_132_loss: 0.4007 - model_133_loss: 0.6726 - model_133_1_loss: 0.6468\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.6155 - model_133_loss: 0.6743 - model_133_1_loss: 0.6481\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.2190 - model_132_loss: 0.4009 - model_133_loss: 0.6748 - model_133_1_loss: 0.6492\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2189 - model_132_loss: 0.4048 - model_133_loss: 0.6737 - model_133_1_loss: 0.6510\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2407 - model_132_loss: 0.4069 - model_133_loss: 0.6754 - model_133_1_loss: 0.6542\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.2523 - model_132_loss: 0.4084 - model_133_loss: 0.6774 - model_133_1_loss: 0.6547\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2778 - model_132_loss: 0.4068 - model_133_loss: 0.6782 - model_133_1_loss: 0.6587\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.6792 - model_133_loss: 0.6776 - model_133_1_loss: 0.65810s - loss: 6.5259 - model_133_loss: 0.6535 - model_1\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.2757 - model_132_loss: 0.4130 - model_133_loss: 0.6787 - model_133_1_loss: 0.6590\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2788 - model_132_loss: 0.4130 - model_133_loss: 0.6783 - model_133_1_loss: 0.6601\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.2907 - model_132_loss: 0.4164 - model_133_loss: 0.6790 - model_133_1_loss: 0.6624\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3123 - model_132_loss: 0.4184 - model_133_loss: 0.6809 - model_133_1_loss: 0.6652\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3085 - model_132_loss: 0.4217 - model_133_loss: 0.6804 - model_133_1_loss: 0.6656\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.7485 - model_133_loss: 0.6823 - model_133_1_loss: 0.6674\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3146 - model_132_loss: 0.4219 - model_133_loss: 0.6807 - model_133_1_loss: 0.6665\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3292 - model_132_loss: 0.4255 - model_133_loss: 0.6821 - model_133_1_loss: 0.6689\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3360 - model_132_loss: 0.4291 - model_133_loss: 0.6829 - model_133_1_loss: 0.6701\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3490 - model_132_loss: 0.4311 - model_133_loss: 0.6836 - model_133_1_loss: 0.6725\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3560 - model_132_loss: 0.4357 - model_133_loss: 0.6840 - model_133_1_loss: 0.6743\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.8013 - model_133_loss: 0.6857 - model_133_1_loss: 0.6746\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3541 - model_132_loss: 0.4361 - model_133_loss: 0.6848 - model_133_1_loss: 0.6733\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3637 - model_132_loss: 0.4389 - model_133_loss: 0.6854 - model_133_1_loss: 0.6751\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3665 - model_132_loss: 0.4424 - model_133_loss: 0.6863 - model_133_1_loss: 0.6755\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3654 - model_132_loss: 0.4466 - model_133_loss: 0.6859 - model_133_1_loss: 0.6765\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3857 - model_132_loss: 0.4475 - model_133_loss: 0.6875 - model_133_1_loss: 0.6791\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.8360 - model_133_loss: 0.6877 - model_133_1_loss: 0.6792\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.3772 - model_132_loss: 0.4501 - model_133_loss: 0.6873 - model_133_1_loss: 0.6782\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3863 - model_132_loss: 0.4532 - model_133_loss: 0.6885 - model_133_1_loss: 0.6794\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3963 - model_132_loss: 0.4552 - model_133_loss: 0.6888 - model_133_1_loss: 0.6815\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3974 - model_132_loss: 0.4581 - model_133_loss: 0.6891 - model_133_1_loss: 0.6820\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4043 - model_132_loss: 0.4599 - model_133_loss: 0.6894 - model_133_1_loss: 0.6835\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.8661 - model_133_loss: 0.6898 - model_133_1_loss: 0.6835\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4012 - model_132_loss: 0.4642 - model_133_loss: 0.6903 - model_133_1_loss: 0.6828\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4041 - model_132_loss: 0.4683 - model_133_loss: 0.6911 - model_133_1_loss: 0.6833\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4060 - model_132_loss: 0.4699 - model_133_loss: 0.6909 - model_133_1_loss: 0.6843\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4073 - model_132_loss: 0.4706 - model_133_loss: 0.6914 - model_133_1_loss: 0.6842\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4093 - model_132_loss: 0.4738 - model_133_loss: 0.6916 - model_133_1_loss: 0.6850\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.8871 - model_133_loss: 0.6916 - model_133_1_loss: 0.6856\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4091 - model_132_loss: 0.4753 - model_133_loss: 0.6916 - model_133_1_loss: 0.6853\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4100 - model_132_loss: 0.4776 - model_133_loss: 0.6914 - model_133_1_loss: 0.6861\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4109 - model_132_loss: 0.4806 - model_133_loss: 0.6919 - model_133_1_loss: 0.6864\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4136 - model_132_loss: 0.4797 - model_133_loss: 0.6917 - model_133_1_loss: 0.6870\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4143 - model_132_loss: 0.4833 - model_133_loss: 0.6922 - model_133_1_loss: 0.6873\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.8999 - model_133_loss: 0.6924 - model_133_1_loss: 0.6872\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4129 - model_132_loss: 0.4850 - model_133_loss: 0.6921 - model_133_1_loss: 0.6875\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4153 - model_132_loss: 0.4834 - model_133_loss: 0.6923 - model_133_1_loss: 0.6874\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4185 - model_132_loss: 0.4852 - model_133_loss: 0.6924 - model_133_1_loss: 0.6884\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4214 - model_132_loss: 0.4871 - model_133_loss: 0.6931 - model_133_1_loss: 0.6886\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4246 - model_132_loss: 0.4863 - model_133_loss: 0.6930 - model_133_1_loss: 0.6892\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9124 - model_133_loss: 0.6936 - model_133_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4262 - model_132_loss: 0.4858 - model_133_loss: 0.6928 - model_133_1_loss: 0.6896\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4270 - model_132_loss: 0.4872 - model_133_loss: 0.6930 - model_133_1_loss: 0.6898\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4245 - model_132_loss: 0.4880 - model_133_loss: 0.6930 - model_133_1_loss: 0.6895\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4265 - model_132_loss: 0.4895 - model_133_loss: 0.6931 - model_133_1_loss: 0.6901\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4274 - model_132_loss: 0.4893 - model_133_loss: 0.6930 - model_133_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9186 - model_133_loss: 0.6930 - model_133_1_loss: 0.69060s - loss: 6.9025 - model_133_loss: 0.6916 - model_133_1_loss:\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4292 - model_132_loss: 0.4873 - model_133_loss: 0.6928 - model_133_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4316 - model_132_loss: 0.4869 - model_133_loss: 0.6929 - model_133_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4294 - model_132_loss: 0.4907 - model_133_loss: 0.6929 - model_133_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4344 - model_132_loss: 0.4868 - model_133_loss: 0.6929 - model_133_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4325 - model_132_loss: 0.4885 - model_133_loss: 0.6930 - model_133_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9220 - model_133_loss: 0.6926 - model_133_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4298 - model_132_loss: 0.4884 - model_133_loss: 0.6926 - model_133_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4304 - model_132_loss: 0.4868 - model_133_loss: 0.6925 - model_133_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4357 - model_132_loss: 0.4839 - model_133_loss: 0.6926 - model_133_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4313 - model_132_loss: 0.4870 - model_133_loss: 0.6926 - model_133_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4332 - model_132_loss: 0.4853 - model_133_loss: 0.6927 - model_133_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9188 - model_133_loss: 0.6931 - model_133_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4307 - model_132_loss: 0.4848 - model_133_loss: 0.6924 - model_133_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4339 - model_132_loss: 0.4826 - model_133_loss: 0.6923 - model_133_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4338 - model_132_loss: 0.4820 - model_133_loss: 0.6923 - model_133_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4332 - model_132_loss: 0.4811 - model_133_loss: 0.6921 - model_133_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4328 - model_132_loss: 0.4820 - model_133_loss: 0.6921 - model_133_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9147 - model_133_loss: 0.6926 - model_133_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4331 - model_132_loss: 0.4791 - model_133_loss: 0.6920 - model_133_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4339 - model_132_loss: 0.4807 - model_133_loss: 0.6922 - model_133_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4347 - model_132_loss: 0.4778 - model_133_loss: 0.6921 - model_133_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4355 - model_132_loss: 0.4775 - model_133_loss: 0.6920 - model_133_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4375 - model_132_loss: 0.4762 - model_133_loss: 0.6922 - model_133_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9109 - model_133_loss: 0.6914 - model_133_1_loss: 0.69050s - loss: 6.9174 - model_133_loss: 0.6932 - model_133_1_loss: 0.690\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4325 - model_132_loss: 0.4765 - model_133_loss: 0.6917 - model_133_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4359 - model_132_loss: 0.4742 - model_133_loss: 0.6917 - model_133_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4358 - model_132_loss: 0.4744 - model_133_loss: 0.6919 - model_133_1_loss: 0.6901\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4359 - model_132_loss: 0.4733 - model_133_loss: 0.6919 - model_133_1_loss: 0.6900\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4349 - model_132_loss: 0.4736 - model_133_loss: 0.6916 - model_133_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9120 - model_133_loss: 0.6929 - model_133_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4325 - model_132_loss: 0.4738 - model_133_loss: 0.6917 - model_133_1_loss: 0.6896\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4318 - model_132_loss: 0.4734 - model_133_loss: 0.6913 - model_133_1_loss: 0.6897\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4366 - model_132_loss: 0.4717 - model_133_loss: 0.6920 - model_133_1_loss: 0.6897\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4337 - model_132_loss: 0.4729 - model_133_loss: 0.6917 - model_133_1_loss: 0.6896\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4377 - model_132_loss: 0.4699 - model_133_loss: 0.6919 - model_133_1_loss: 0.6896\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9084 - model_133_loss: 0.6915 - model_133_1_loss: 0.68980s - loss: 6.9119 - model_133_loss: 0.6930 - model_133_1_loss: 0.68\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4344 - model_132_loss: 0.4705 - model_133_loss: 0.6915 - model_133_1_loss: 0.6895\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4326 - model_132_loss: 0.4722 - model_133_loss: 0.6916 - model_133_1_loss: 0.6894\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4329 - model_132_loss: 0.4716 - model_133_loss: 0.6912 - model_133_1_loss: 0.6897\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4371 - model_132_loss: 0.4709 - model_133_loss: 0.6917 - model_133_1_loss: 0.6899\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4355 - model_132_loss: 0.4707 - model_133_loss: 0.6915 - model_133_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9083 - model_133_loss: 0.6919 - model_133_1_loss: 0.68980s - loss: 6.9120 - model_133_loss: 0.6919 - model_133_1_loss: 0.690\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4355 - model_132_loss: 0.4711 - model_133_loss: 0.6913 - model_133_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4365 - model_132_loss: 0.4729 - model_133_loss: 0.6918 - model_133_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4376 - model_132_loss: 0.4723 - model_133_loss: 0.6919 - model_133_1_loss: 0.6901\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4382 - model_132_loss: 0.4719 - model_133_loss: 0.6918 - model_133_1_loss: 0.6902\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4427 - model_132_loss: 0.4724 - model_133_loss: 0.6924 - model_133_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9108 - model_133_loss: 0.6912 - model_133_1_loss: 0.69040s - loss: 6.9193 - model_133_loss: 0.6932 - model_133_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4403 - model_132_loss: 0.4725 - model_133_loss: 0.6922 - model_133_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4387 - model_132_loss: 0.4744 - model_133_loss: 0.6922 - model_133_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4410 - model_132_loss: 0.4746 - model_133_loss: 0.6927 - model_133_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4379 - model_132_loss: 0.4747 - model_133_loss: 0.6917 - model_133_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4394 - model_132_loss: 0.4742 - model_133_loss: 0.6917 - model_133_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9198 - model_133_loss: 0.6923 - model_133_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4391 - model_132_loss: 0.4767 - model_133_loss: 0.6921 - model_133_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4413 - model_132_loss: 0.4754 - model_133_loss: 0.6925 - model_133_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4416 - model_132_loss: 0.4751 - model_133_loss: 0.6922 - model_133_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4411 - model_132_loss: 0.4781 - model_133_loss: 0.6924 - model_133_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4437 - model_132_loss: 0.4773 - model_133_loss: 0.6927 - model_133_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9207 - model_133_loss: 0.6924 - model_133_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4447 - model_132_loss: 0.4777 - model_133_loss: 0.6930 - model_133_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4430 - model_132_loss: 0.4775 - model_133_loss: 0.6928 - model_133_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4410 - model_132_loss: 0.4787 - model_133_loss: 0.6926 - model_133_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4429 - model_132_loss: 0.4779 - model_133_loss: 0.6924 - model_133_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4452 - model_132_loss: 0.4783 - model_133_loss: 0.6932 - model_133_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9240 - model_133_loss: 0.6933 - model_133_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4447 - model_132_loss: 0.4777 - model_133_loss: 0.6931 - model_133_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4439 - model_132_loss: 0.4791 - model_133_loss: 0.6932 - model_133_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4455 - model_132_loss: 0.4785 - model_133_loss: 0.6932 - model_133_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4437 - model_132_loss: 0.4798 - model_133_loss: 0.6931 - model_133_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4451 - model_132_loss: 0.4773 - model_133_loss: 0.6929 - model_133_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9227 - model_133_loss: 0.6940 - model_133_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4460 - model_132_loss: 0.4762 - model_133_loss: 0.6930 - model_133_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4427 - model_132_loss: 0.4775 - model_133_loss: 0.6925 - model_133_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4416 - model_132_loss: 0.4784 - model_133_loss: 0.6927 - model_133_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4446 - model_132_loss: 0.4771 - model_133_loss: 0.6930 - model_133_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4434 - model_132_loss: 0.4778 - model_133_loss: 0.6927 - model_133_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9204 - model_133_loss: 0.6939 - model_133_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4437 - model_132_loss: 0.4766 - model_133_loss: 0.6927 - model_133_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4455 - model_132_loss: 0.4753 - model_133_loss: 0.6929 - model_133_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4430 - model_132_loss: 0.4776 - model_133_loss: 0.6928 - model_133_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4450 - model_132_loss: 0.4753 - model_133_loss: 0.6927 - model_133_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4457 - model_132_loss: 0.4742 - model_133_loss: 0.6927 - model_133_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9225 - model_133_loss: 0.6933 - model_133_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4446 - model_132_loss: 0.4748 - model_133_loss: 0.6925 - model_133_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4460 - model_132_loss: 0.4749 - model_133_loss: 0.6927 - model_133_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4438 - model_132_loss: 0.4747 - model_133_loss: 0.6924 - model_133_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4454 - model_132_loss: 0.4746 - model_133_loss: 0.6925 - model_133_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4475 - model_132_loss: 0.4728 - model_133_loss: 0.6927 - model_133_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9196 - model_133_loss: 0.6925 - model_133_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4456 - model_132_loss: 0.4741 - model_133_loss: 0.6924 - model_133_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4517 - model_132_loss: 0.4721 - model_133_loss: 0.6926 - model_133_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4475 - model_132_loss: 0.4730 - model_133_loss: 0.6925 - model_133_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4492 - model_132_loss: 0.4728 - model_133_loss: 0.6926 - model_133_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4486 - model_132_loss: 0.4725 - model_133_loss: 0.6926 - model_133_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9221 - model_133_loss: 0.6938 - model_133_1_loss: 0.69180s - loss: 6.9359 - model_133_loss: 0.6966 - model_13\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4478 - model_132_loss: 0.4725 - model_133_loss: 0.6925 - model_133_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4457 - model_132_loss: 0.4729 - model_133_loss: 0.6922 - model_133_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4451 - model_132_loss: 0.4741 - model_133_loss: 0.6923 - model_133_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4455 - model_132_loss: 0.4754 - model_133_loss: 0.6927 - model_133_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4496 - model_132_loss: 0.4717 - model_133_loss: 0.6926 - model_133_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9219 - model_133_loss: 0.6928 - model_133_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4462 - model_132_loss: 0.4731 - model_133_loss: 0.6924 - model_133_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4464 - model_132_loss: 0.4733 - model_133_loss: 0.6925 - model_133_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4478 - model_132_loss: 0.4718 - model_133_loss: 0.6926 - model_133_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4445 - model_132_loss: 0.4735 - model_133_loss: 0.6921 - model_133_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4457 - model_132_loss: 0.4725 - model_133_loss: 0.6924 - model_133_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9206 - model_133_loss: 0.6920 - model_133_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4462 - model_132_loss: 0.4714 - model_133_loss: 0.6922 - model_133_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4464 - model_132_loss: 0.4717 - model_133_loss: 0.6922 - model_133_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4458 - model_132_loss: 0.4714 - model_133_loss: 0.6924 - model_133_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4466 - model_132_loss: 0.4700 - model_133_loss: 0.6922 - model_133_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4458 - model_132_loss: 0.4724 - model_133_loss: 0.6925 - model_133_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9203 - model_133_loss: 0.6920 - model_133_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4489 - model_132_loss: 0.4715 - model_133_loss: 0.6925 - model_133_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4449 - model_132_loss: 0.4724 - model_133_loss: 0.6926 - model_133_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4481 - model_132_loss: 0.4702 - model_133_loss: 0.6923 - model_133_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4470 - model_132_loss: 0.4698 - model_133_loss: 0.6922 - model_133_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4499 - model_132_loss: 0.4692 - model_133_loss: 0.6925 - model_133_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9199 - model_133_loss: 0.6918 - model_133_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4488 - model_132_loss: 0.4707 - model_133_loss: 0.6925 - model_133_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4461 - model_132_loss: 0.4710 - model_133_loss: 0.6924 - model_133_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4500 - model_132_loss: 0.4707 - model_133_loss: 0.6927 - model_133_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4473 - model_132_loss: 0.4723 - model_133_loss: 0.6926 - model_133_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4446 - model_132_loss: 0.4731 - model_133_loss: 0.6923 - model_133_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9217 - model_133_loss: 0.6932 - model_133_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4496 - model_132_loss: 0.4699 - model_133_loss: 0.6924 - model_133_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4513 - model_132_loss: 0.4712 - model_133_loss: 0.6927 - model_133_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4486 - model_132_loss: 0.4713 - model_133_loss: 0.6925 - model_133_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4471 - model_132_loss: 0.4731 - model_133_loss: 0.6928 - model_133_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4450 - model_132_loss: 0.4740 - model_133_loss: 0.6924 - model_133_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9234 - model_133_loss: 0.6931 - model_133_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4488 - model_132_loss: 0.4728 - model_133_loss: 0.6927 - model_133_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4481 - model_132_loss: 0.4725 - model_133_loss: 0.6928 - model_133_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4477 - model_132_loss: 0.4720 - model_133_loss: 0.6924 - model_133_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4473 - model_132_loss: 0.4720 - model_133_loss: 0.6923 - model_133_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4487 - model_132_loss: 0.4728 - model_133_loss: 0.6926 - model_133_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9222 - model_133_loss: 0.6927 - model_133_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4533 - model_132_loss: 0.4702 - model_133_loss: 0.6930 - model_133_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4534 - model_132_loss: 0.4718 - model_133_loss: 0.6931 - model_133_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4519 - model_132_loss: 0.4718 - model_133_loss: 0.6929 - model_133_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4513 - model_132_loss: 0.4713 - model_133_loss: 0.6927 - model_133_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4484 - model_132_loss: 0.4721 - model_133_loss: 0.6926 - model_133_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9212 - model_133_loss: 0.6925 - model_133_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4500 - model_132_loss: 0.4691 - model_133_loss: 0.6925 - model_133_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4519 - model_132_loss: 0.4686 - model_133_loss: 0.6927 - model_133_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4509 - model_132_loss: 0.4693 - model_133_loss: 0.6928 - model_133_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4519 - model_132_loss: 0.4705 - model_133_loss: 0.6929 - model_133_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4498 - model_132_loss: 0.4695 - model_133_loss: 0.6926 - model_133_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9221 - model_133_loss: 0.6926 - model_133_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4509 - model_132_loss: 0.4693 - model_133_loss: 0.6925 - model_133_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4498 - model_132_loss: 0.4681 - model_133_loss: 0.6924 - model_133_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4507 - model_132_loss: 0.4680 - model_133_loss: 0.6926 - model_133_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4510 - model_132_loss: 0.4685 - model_133_loss: 0.6927 - model_133_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4520 - model_132_loss: 0.4664 - model_133_loss: 0.6924 - model_133_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9213 - model_133_loss: 0.6930 - model_133_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4516 - model_132_loss: 0.4679 - model_133_loss: 0.6924 - model_133_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4478 - model_132_loss: 0.4683 - model_133_loss: 0.6923 - model_133_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4476 - model_132_loss: 0.4685 - model_133_loss: 0.6922 - model_133_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4521 - model_132_loss: 0.4673 - model_133_loss: 0.6925 - model_133_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4531 - model_132_loss: 0.4654 - model_133_loss: 0.6925 - model_133_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 63us/sample - loss: 6.9207 - model_133_loss: 0.6928 - model_133_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4516 - model_132_loss: 0.4664 - model_133_loss: 0.6924 - model_133_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4517 - model_132_loss: 0.4662 - model_133_loss: 0.6925 - model_133_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4513 - model_132_loss: 0.4673 - model_133_loss: 0.6926 - model_133_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4529 - model_132_loss: 0.4660 - model_133_loss: 0.6924 - model_133_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4506 - model_132_loss: 0.4669 - model_133_loss: 0.6922 - model_133_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9188 - model_133_loss: 0.6922 - model_133_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4505 - model_132_loss: 0.4660 - model_133_loss: 0.6922 - model_133_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4508 - model_132_loss: 0.4661 - model_133_loss: 0.6922 - model_133_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4509 - model_132_loss: 0.4666 - model_133_loss: 0.6922 - model_133_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4502 - model_132_loss: 0.4671 - model_133_loss: 0.6922 - model_133_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4519 - model_132_loss: 0.4657 - model_133_loss: 0.6921 - model_133_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9201 - model_133_loss: 0.6924 - model_133_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4497 - model_132_loss: 0.4678 - model_133_loss: 0.6922 - model_133_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4534 - model_132_loss: 0.4666 - model_133_loss: 0.6927 - model_133_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4515 - model_132_loss: 0.4670 - model_133_loss: 0.6924 - model_133_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4521 - model_132_loss: 0.4663 - model_133_loss: 0.6923 - model_133_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4524 - model_132_loss: 0.4675 - model_133_loss: 0.6928 - model_133_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9219 - model_133_loss: 0.6930 - model_133_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4517 - model_132_loss: 0.4676 - model_133_loss: 0.6925 - model_133_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4541 - model_132_loss: 0.4659 - model_133_loss: 0.6926 - model_133_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4553 - model_132_loss: 0.4673 - model_133_loss: 0.6928 - model_133_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4546 - model_132_loss: 0.4668 - model_133_loss: 0.6926 - model_133_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4526 - model_132_loss: 0.4676 - model_133_loss: 0.6927 - model_133_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9223 - model_133_loss: 0.6927 - model_133_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4535 - model_132_loss: 0.4680 - model_133_loss: 0.6927 - model_133_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4534 - model_132_loss: 0.4672 - model_133_loss: 0.6927 - model_133_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4555 - model_132_loss: 0.4669 - model_133_loss: 0.6928 - model_133_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4526 - model_132_loss: 0.4685 - model_133_loss: 0.6925 - model_133_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4546 - model_132_loss: 0.4680 - model_133_loss: 0.6927 - model_133_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9237 - model_133_loss: 0.6929 - model_133_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4522 - model_132_loss: 0.4699 - model_133_loss: 0.6928 - model_133_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4552 - model_132_loss: 0.4680 - model_133_loss: 0.6928 - model_133_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4529 - model_132_loss: 0.4688 - model_133_loss: 0.6929 - model_133_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4549 - model_132_loss: 0.4685 - model_133_loss: 0.6930 - model_133_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4528 - model_132_loss: 0.4690 - model_133_loss: 0.6929 - model_133_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9229 - model_133_loss: 0.6927 - model_133_1_loss: 0.69150s - loss: 6.8671 - model_133_loss: 0.6831 - model_133_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4520 - model_132_loss: 0.4703 - model_133_loss: 0.6927 - model_133_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4543 - model_132_loss: 0.4672 - model_133_loss: 0.6926 - model_133_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4548 - model_132_loss: 0.4681 - model_133_loss: 0.6927 - model_133_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4540 - model_132_loss: 0.4686 - model_133_loss: 0.6929 - model_133_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4528 - model_132_loss: 0.4685 - model_133_loss: 0.6923 - model_133_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9222 - model_133_loss: 0.6927 - model_133_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4517 - model_132_loss: 0.4671 - model_133_loss: 0.6926 - model_133_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4532 - model_132_loss: 0.4672 - model_133_loss: 0.6927 - model_133_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4522 - model_132_loss: 0.4673 - model_133_loss: 0.6923 - model_133_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4544 - model_132_loss: 0.4655 - model_133_loss: 0.6926 - model_133_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4533 - model_132_loss: 0.4653 - model_133_loss: 0.6927 - model_133_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9206 - model_133_loss: 0.6928 - model_133_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4530 - model_132_loss: 0.4669 - model_133_loss: 0.6926 - model_133_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4569 - model_132_loss: 0.4648 - model_133_loss: 0.6928 - model_133_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4569 - model_132_loss: 0.4633 - model_133_loss: 0.6926 - model_133_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4536 - model_132_loss: 0.4648 - model_133_loss: 0.6924 - model_133_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4547 - model_132_loss: 0.4638 - model_133_loss: 0.6925 - model_133_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9231 - model_133_loss: 0.6926 - model_133_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4562 - model_132_loss: 0.4639 - model_133_loss: 0.6926 - model_133_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4547 - model_132_loss: 0.4637 - model_133_loss: 0.6926 - model_133_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4576 - model_132_loss: 0.4636 - model_133_loss: 0.6926 - model_133_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4557 - model_132_loss: 0.4643 - model_133_loss: 0.6926 - model_133_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4584 - model_132_loss: 0.4616 - model_133_loss: 0.6927 - model_133_1_loss: 0.6913\n",
      "For Attention Module: 3.5000000000000004\n",
      "features X: 30940 samples, 69 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 64us/sample - loss: 6.3584 - model_137_loss: 0.6598 - model_137_1_loss: 0.6115\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: -5.9706 - model_136_loss: 0.3850 - model_137_loss: 0.6597 - model_137_1_loss: 0.6114\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -5.9891 - model_136_loss: 0.3848 - model_137_loss: 0.6609 - model_137_1_loss: 0.6139\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -5.9883 - model_136_loss: 0.3845 - model_137_loss: 0.6613 - model_137_1_loss: 0.6133\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -5.9983 - model_136_loss: 0.3848 - model_137_loss: 0.6619 - model_137_1_loss: 0.6147\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.0061 - model_136_loss: 0.3847 - model_137_loss: 0.6612 - model_137_1_loss: 0.6170\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.4014 - model_137_loss: 0.6611 - model_137_1_loss: 0.6180\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.0100 - model_136_loss: 0.3857 - model_137_loss: 0.6618 - model_137_1_loss: 0.6174\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.0265 - model_136_loss: 0.3860 - model_137_loss: 0.6623 - model_137_1_loss: 0.6202\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.0392 - model_136_loss: 0.3860 - model_137_loss: 0.6631 - model_137_1_loss: 0.6219\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.0427 - model_136_loss: 0.3869 - model_137_loss: 0.6629 - model_137_1_loss: 0.6231\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.0643 - model_136_loss: 0.3863 - model_137_loss: 0.6647 - model_137_1_loss: 0.6254\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.4561 - model_137_loss: 0.6641 - model_137_1_loss: 0.6271\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.0626 - model_136_loss: 0.3855 - model_137_loss: 0.6643 - model_137_1_loss: 0.6253\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0730 - model_136_loss: 0.3859 - model_137_loss: 0.6644 - model_137_1_loss: 0.6274\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.0873 - model_136_loss: 0.3879 - model_137_loss: 0.6658 - model_137_1_loss: 0.6292\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.0972 - model_136_loss: 0.3893 - model_137_loss: 0.6657 - model_137_1_loss: 0.6316\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1046 - model_136_loss: 0.3883 - model_137_loss: 0.6663 - model_137_1_loss: 0.6323\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.5117 - model_137_loss: 0.6661 - model_137_1_loss: 0.6358\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.1138 - model_136_loss: 0.3892 - model_137_loss: 0.6655 - model_137_1_loss: 0.6351\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.1253 - model_136_loss: 0.3912 - model_137_loss: 0.6664 - model_137_1_loss: 0.6369\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.1413 - model_136_loss: 0.3910 - model_137_loss: 0.6674 - model_137_1_loss: 0.6391\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.1493 - model_136_loss: 0.3949 - model_137_loss: 0.6677 - model_137_1_loss: 0.6411\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.1805 - model_136_loss: 0.3945 - model_137_loss: 0.6701 - model_137_1_loss: 0.6449\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.5695 - model_137_loss: 0.6674 - model_137_1_loss: 0.6459\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.1800 - model_136_loss: 0.3956 - model_137_loss: 0.6700 - model_137_1_loss: 0.6451\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1885 - model_136_loss: 0.3961 - model_137_loss: 0.6702 - model_137_1_loss: 0.6467\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.2005 - model_136_loss: 0.4001 - model_137_loss: 0.6714 - model_137_1_loss: 0.6487\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.2038 - model_136_loss: 0.4004 - model_137_loss: 0.6710 - model_137_1_loss: 0.6498\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.2234 - model_136_loss: 0.4010 - model_137_loss: 0.6719 - model_137_1_loss: 0.6529\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.6426 - model_137_loss: 0.6738 - model_137_1_loss: 0.6547\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.2299 - model_136_loss: 0.4053 - model_137_loss: 0.6728 - model_137_1_loss: 0.6542\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.2328 - model_136_loss: 0.4065 - model_137_loss: 0.6728 - model_137_1_loss: 0.6551\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2546 - model_136_loss: 0.4081 - model_137_loss: 0.6740 - model_137_1_loss: 0.6586\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2652 - model_136_loss: 0.4100 - model_137_loss: 0.6747 - model_137_1_loss: 0.6604\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2782 - model_136_loss: 0.4115 - model_137_loss: 0.6772 - model_137_1_loss: 0.6608\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.6844 - model_137_loss: 0.6757 - model_137_1_loss: 0.661 - 1s 46us/sample - loss: 6.6875 - model_137_loss: 0.6758 - model_137_1_loss: 0.6617\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.2833 - model_136_loss: 0.4147 - model_137_loss: 0.6764 - model_137_1_loss: 0.6632\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2920 - model_136_loss: 0.4159 - model_137_loss: 0.6769 - model_137_1_loss: 0.6647\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3055 - model_136_loss: 0.4180 - model_137_loss: 0.6783 - model_137_1_loss: 0.6664\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3184 - model_136_loss: 0.4183 - model_137_loss: 0.6786 - model_137_1_loss: 0.6688\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3247 - model_136_loss: 0.4212 - model_137_loss: 0.6786 - model_137_1_loss: 0.6706\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.7488 - model_137_loss: 0.6800 - model_137_1_loss: 0.6698\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3354 - model_136_loss: 0.4252 - model_137_loss: 0.6808 - model_137_1_loss: 0.6714\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3363 - model_136_loss: 0.4269 - model_137_loss: 0.6811 - model_137_1_loss: 0.6715\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3555 - model_136_loss: 0.4282 - model_137_loss: 0.6828 - model_137_1_loss: 0.6739\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3581 - model_136_loss: 0.4316 - model_137_loss: 0.6840 - model_137_1_loss: 0.6739\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3672 - model_136_loss: 0.4344 - model_137_loss: 0.6846 - model_137_1_loss: 0.6757\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.8024 - model_137_loss: 0.6833 - model_137_1_loss: 0.6768\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.3602 - model_136_loss: 0.4398 - model_137_loss: 0.6834 - model_137_1_loss: 0.6766\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3633 - model_136_loss: 0.4415 - model_137_loss: 0.6834 - model_137_1_loss: 0.6776\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3741 - model_136_loss: 0.4427 - model_137_loss: 0.6846 - model_137_1_loss: 0.6787\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3860 - model_136_loss: 0.4441 - model_137_loss: 0.6859 - model_137_1_loss: 0.6801\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.3813 - model_136_loss: 0.4507 - model_137_loss: 0.6863 - model_137_1_loss: 0.6801\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.8320 - model_137_loss: 0.6858 - model_137_1_loss: 0.68070s - loss: 6.8493 - model_137_loss: 0.6876 - model_137_1_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3873 - model_136_loss: 0.4501 - model_137_loss: 0.6861 - model_137_1_loss: 0.6814\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3884 - model_136_loss: 0.4537 - model_137_loss: 0.6867 - model_137_1_loss: 0.6817\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3929 - model_136_loss: 0.4555 - model_137_loss: 0.6862 - model_137_1_loss: 0.6834\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4067 - model_136_loss: 0.4591 - model_137_loss: 0.6887 - model_137_1_loss: 0.6845\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3993 - model_136_loss: 0.4618 - model_137_loss: 0.6876 - model_137_1_loss: 0.6846\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.8717 - model_137_loss: 0.6894 - model_137_1_loss: 0.6852\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4045 - model_136_loss: 0.4642 - model_137_loss: 0.6891 - model_137_1_loss: 0.6847\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4083 - model_136_loss: 0.4666 - model_137_loss: 0.6894 - model_137_1_loss: 0.6855\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4093 - model_136_loss: 0.4682 - model_137_loss: 0.6897 - model_137_1_loss: 0.6858\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4128 - model_136_loss: 0.4706 - model_137_loss: 0.6903 - model_137_1_loss: 0.6864\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4132 - model_136_loss: 0.4728 - model_137_loss: 0.6904 - model_137_1_loss: 0.6868\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.8891 - model_137_loss: 0.6913 - model_137_1_loss: 0.6872\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4145 - model_136_loss: 0.4742 - model_137_loss: 0.6907 - model_137_1_loss: 0.6870\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4126 - model_136_loss: 0.4776 - model_137_loss: 0.6907 - model_137_1_loss: 0.6874\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4194 - model_136_loss: 0.4763 - model_137_loss: 0.6912 - model_137_1_loss: 0.6880\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4157 - model_136_loss: 0.4803 - model_137_loss: 0.6911 - model_137_1_loss: 0.6881\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4183 - model_136_loss: 0.4816 - model_137_loss: 0.6916 - model_137_1_loss: 0.6884\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9060 - model_137_loss: 0.6915 - model_137_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4197 - model_136_loss: 0.4822 - model_137_loss: 0.6915 - model_137_1_loss: 0.6889\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4213 - model_136_loss: 0.4821 - model_137_loss: 0.6915 - model_137_1_loss: 0.6892\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4197 - model_136_loss: 0.4842 - model_137_loss: 0.6916 - model_137_1_loss: 0.6892\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4208 - model_136_loss: 0.4852 - model_137_loss: 0.6914 - model_137_1_loss: 0.6898\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4255 - model_136_loss: 0.4826 - model_137_loss: 0.6919 - model_137_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9073 - model_137_loss: 0.6922 - model_137_1_loss: 0.68970s - loss: 6.9194 - model_137_loss: 0.6952 - model_13\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4239 - model_136_loss: 0.4837 - model_137_loss: 0.6918 - model_137_1_loss: 0.6898\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4233 - model_136_loss: 0.4843 - model_137_loss: 0.6916 - model_137_1_loss: 0.6899\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4286 - model_136_loss: 0.4842 - model_137_loss: 0.6920 - model_137_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4303 - model_136_loss: 0.4859 - model_137_loss: 0.6923 - model_137_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4291 - model_136_loss: 0.4845 - model_137_loss: 0.6921 - model_137_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9170 - model_137_loss: 0.6939 - model_137_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4297 - model_136_loss: 0.4836 - model_137_loss: 0.6916 - model_137_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4298 - model_136_loss: 0.4839 - model_137_loss: 0.6917 - model_137_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4306 - model_136_loss: 0.4836 - model_137_loss: 0.6916 - model_137_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4315 - model_136_loss: 0.4849 - model_137_loss: 0.6919 - model_137_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4361 - model_136_loss: 0.4814 - model_137_loss: 0.6920 - model_137_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9183 - model_137_loss: 0.6916 - model_137_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4339 - model_136_loss: 0.4831 - model_137_loss: 0.6917 - model_137_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4357 - model_136_loss: 0.4829 - model_137_loss: 0.6920 - model_137_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4342 - model_136_loss: 0.4836 - model_137_loss: 0.6917 - model_137_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4348 - model_136_loss: 0.4824 - model_137_loss: 0.6918 - model_137_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4357 - model_136_loss: 0.4808 - model_137_loss: 0.6917 - model_137_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9172 - model_137_loss: 0.6913 - model_137_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4307 - model_136_loss: 0.4830 - model_137_loss: 0.6911 - model_137_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4331 - model_136_loss: 0.4795 - model_137_loss: 0.6906 - model_137_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4362 - model_136_loss: 0.4799 - model_137_loss: 0.6912 - model_137_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4321 - model_136_loss: 0.4810 - model_137_loss: 0.6909 - model_137_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4359 - model_136_loss: 0.4793 - model_137_loss: 0.6913 - model_137_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9151 - model_137_loss: 0.6910 - model_137_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4318 - model_136_loss: 0.4801 - model_137_loss: 0.6907 - model_137_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4323 - model_136_loss: 0.4790 - model_137_loss: 0.6907 - model_137_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4333 - model_136_loss: 0.4787 - model_137_loss: 0.6907 - model_137_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4338 - model_136_loss: 0.4781 - model_137_loss: 0.6907 - model_137_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4323 - model_136_loss: 0.4785 - model_137_loss: 0.6906 - model_137_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: 6.9119 - model_137_loss: 0.6907 - model_137_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4322 - model_136_loss: 0.4775 - model_137_loss: 0.6904 - model_137_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4361 - model_136_loss: 0.4769 - model_137_loss: 0.6909 - model_137_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4356 - model_136_loss: 0.4770 - model_137_loss: 0.6906 - model_137_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4379 - model_136_loss: 0.4757 - model_137_loss: 0.6910 - model_137_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4396 - model_136_loss: 0.4742 - model_137_loss: 0.6911 - model_137_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9125 - model_137_loss: 0.6904 - model_137_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4304 - model_136_loss: 0.4750 - model_137_loss: 0.6900 - model_137_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4334 - model_136_loss: 0.4744 - model_137_loss: 0.6907 - model_137_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4325 - model_136_loss: 0.4746 - model_137_loss: 0.6906 - model_137_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4395 - model_136_loss: 0.4740 - model_137_loss: 0.6911 - model_137_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4351 - model_136_loss: 0.4743 - model_137_loss: 0.6909 - model_137_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9129 - model_137_loss: 0.6916 - model_137_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4351 - model_136_loss: 0.4739 - model_137_loss: 0.6903 - model_137_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4323 - model_136_loss: 0.4741 - model_137_loss: 0.6902 - model_137_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4330 - model_136_loss: 0.4735 - model_137_loss: 0.6905 - model_137_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4352 - model_136_loss: 0.4738 - model_137_loss: 0.6904 - model_137_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4363 - model_136_loss: 0.4730 - model_137_loss: 0.6911 - model_137_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9144 - model_137_loss: 0.6916 - model_137_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4379 - model_136_loss: 0.4725 - model_137_loss: 0.6910 - model_137_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4393 - model_136_loss: 0.4727 - model_137_loss: 0.6912 - model_137_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4373 - model_136_loss: 0.4743 - model_137_loss: 0.6912 - model_137_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4391 - model_136_loss: 0.4740 - model_137_loss: 0.6913 - model_137_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4371 - model_136_loss: 0.4744 - model_137_loss: 0.6909 - model_137_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9180 - model_137_loss: 0.6917 - model_137_1_loss: 0.69160s - loss: 6.9398 - model_137_loss: 0.6956 - model_137_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4439 - model_136_loss: 0.4729 - model_137_loss: 0.6917 - model_137_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4446 - model_136_loss: 0.4729 - model_137_loss: 0.6922 - model_137_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4440 - model_136_loss: 0.4738 - model_137_loss: 0.6919 - model_137_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4430 - model_136_loss: 0.4743 - model_137_loss: 0.6918 - model_137_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4440 - model_136_loss: 0.4747 - model_137_loss: 0.6920 - model_137_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9206 - model_137_loss: 0.6917 - model_137_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4414 - model_136_loss: 0.4756 - model_137_loss: 0.6920 - model_137_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4436 - model_136_loss: 0.4753 - model_137_loss: 0.6920 - model_137_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4430 - model_136_loss: 0.4739 - model_137_loss: 0.6918 - model_137_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4453 - model_136_loss: 0.4757 - model_137_loss: 0.6922 - model_137_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4420 - model_136_loss: 0.4766 - model_137_loss: 0.6920 - model_137_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9181 - model_137_loss: 0.6918 - model_137_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4464 - model_136_loss: 0.4749 - model_137_loss: 0.6924 - model_137_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4427 - model_136_loss: 0.4761 - model_137_loss: 0.6921 - model_137_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4421 - model_136_loss: 0.4757 - model_137_loss: 0.6921 - model_137_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4441 - model_136_loss: 0.4759 - model_137_loss: 0.6921 - model_137_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4446 - model_136_loss: 0.4758 - model_137_loss: 0.6922 - model_137_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9238 - model_137_loss: 0.6933 - model_137_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4452 - model_136_loss: 0.4768 - model_137_loss: 0.6926 - model_137_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4445 - model_136_loss: 0.4760 - model_137_loss: 0.6925 - model_137_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4441 - model_136_loss: 0.4757 - model_137_loss: 0.6922 - model_137_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4444 - model_136_loss: 0.4753 - model_137_loss: 0.6925 - model_137_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4440 - model_136_loss: 0.4765 - model_137_loss: 0.6925 - model_137_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9233 - model_137_loss: 0.6923 - model_137_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4474 - model_136_loss: 0.4735 - model_137_loss: 0.6923 - model_137_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4454 - model_136_loss: 0.4745 - model_137_loss: 0.6922 - model_137_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4450 - model_136_loss: 0.4743 - model_137_loss: 0.6921 - model_137_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4442 - model_136_loss: 0.4755 - model_137_loss: 0.6922 - model_137_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4466 - model_136_loss: 0.4741 - model_137_loss: 0.6925 - model_137_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9228 - model_137_loss: 0.6922 - model_137_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4440 - model_136_loss: 0.4760 - model_137_loss: 0.6922 - model_137_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4444 - model_136_loss: 0.4750 - model_137_loss: 0.6921 - model_137_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4477 - model_136_loss: 0.4734 - model_137_loss: 0.6922 - model_137_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4469 - model_136_loss: 0.4743 - model_137_loss: 0.6922 - model_137_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4465 - model_136_loss: 0.4747 - model_137_loss: 0.6923 - model_137_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9219 - model_137_loss: 0.6924 - model_137_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4468 - model_136_loss: 0.4737 - model_137_loss: 0.6921 - model_137_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4488 - model_136_loss: 0.4740 - model_137_loss: 0.6928 - model_137_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4478 - model_136_loss: 0.4748 - model_137_loss: 0.6926 - model_137_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4547 - model_136_loss: 0.4718 - model_137_loss: 0.6931 - model_137_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4479 - model_136_loss: 0.4734 - model_137_loss: 0.6924 - model_137_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9236 - model_137_loss: 0.6929 - model_137_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4452 - model_136_loss: 0.4734 - model_137_loss: 0.6920 - model_137_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4490 - model_136_loss: 0.4718 - model_137_loss: 0.6924 - model_137_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4481 - model_136_loss: 0.4736 - model_137_loss: 0.6925 - model_137_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4483 - model_136_loss: 0.4723 - model_137_loss: 0.6924 - model_137_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4485 - model_136_loss: 0.4723 - model_137_loss: 0.6923 - model_137_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9242 - model_137_loss: 0.6923 - model_137_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4493 - model_136_loss: 0.4724 - model_137_loss: 0.6923 - model_137_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4514 - model_136_loss: 0.4713 - model_137_loss: 0.6925 - model_137_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4490 - model_136_loss: 0.4725 - model_137_loss: 0.6924 - model_137_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4504 - model_136_loss: 0.4702 - model_137_loss: 0.6923 - model_137_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4492 - model_136_loss: 0.4714 - model_137_loss: 0.6923 - model_137_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9203 - model_137_loss: 0.6917 - model_137_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4466 - model_136_loss: 0.4720 - model_137_loss: 0.6922 - model_137_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4492 - model_136_loss: 0.4687 - model_137_loss: 0.6923 - model_137_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4488 - model_136_loss: 0.4693 - model_137_loss: 0.6921 - model_137_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4495 - model_136_loss: 0.4668 - model_137_loss: 0.6920 - model_137_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4485 - model_136_loss: 0.4685 - model_137_loss: 0.6922 - model_137_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9174 - model_137_loss: 0.6924 - model_137_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4481 - model_136_loss: 0.4670 - model_137_loss: 0.6920 - model_137_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4479 - model_136_loss: 0.4666 - model_137_loss: 0.6919 - model_137_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4523 - model_136_loss: 0.4655 - model_137_loss: 0.6923 - model_137_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4499 - model_136_loss: 0.4654 - model_137_loss: 0.6919 - model_137_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4494 - model_136_loss: 0.4656 - model_137_loss: 0.6922 - model_137_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9159 - model_137_loss: 0.6917 - model_137_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4489 - model_136_loss: 0.4654 - model_137_loss: 0.6919 - model_137_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4482 - model_136_loss: 0.4652 - model_137_loss: 0.6918 - model_137_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4476 - model_136_loss: 0.4641 - model_137_loss: 0.6916 - model_137_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4529 - model_136_loss: 0.4644 - model_137_loss: 0.6921 - model_137_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4535 - model_136_loss: 0.4644 - model_137_loss: 0.6926 - model_137_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9174 - model_137_loss: 0.6927 - model_137_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4512 - model_136_loss: 0.4641 - model_137_loss: 0.6920 - model_137_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4498 - model_136_loss: 0.4664 - model_137_loss: 0.6921 - model_137_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4507 - model_136_loss: 0.4656 - model_137_loss: 0.6920 - model_137_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4528 - model_136_loss: 0.4666 - model_137_loss: 0.6925 - model_137_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4521 - model_136_loss: 0.4666 - model_137_loss: 0.6925 - model_137_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9198 - model_137_loss: 0.6925 - model_137_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4541 - model_136_loss: 0.4676 - model_137_loss: 0.6927 - model_137_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4513 - model_136_loss: 0.4686 - model_137_loss: 0.6924 - model_137_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4492 - model_136_loss: 0.4694 - model_137_loss: 0.6921 - model_137_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4531 - model_136_loss: 0.4706 - model_137_loss: 0.6926 - model_137_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4509 - model_136_loss: 0.4701 - model_137_loss: 0.6928 - model_137_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9229 - model_137_loss: 0.6922 - model_137_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4514 - model_136_loss: 0.4708 - model_137_loss: 0.6926 - model_137_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4520 - model_136_loss: 0.4721 - model_137_loss: 0.6928 - model_137_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4517 - model_136_loss: 0.4722 - model_137_loss: 0.6925 - model_137_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4501 - model_136_loss: 0.4721 - model_137_loss: 0.6925 - model_137_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4518 - model_136_loss: 0.4723 - model_137_loss: 0.6925 - model_137_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9247 - model_137_loss: 0.6925 - model_137_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4528 - model_136_loss: 0.4714 - model_137_loss: 0.6926 - model_137_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4522 - model_136_loss: 0.4719 - model_137_loss: 0.6926 - model_137_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4533 - model_136_loss: 0.4727 - model_137_loss: 0.6929 - model_137_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4524 - model_136_loss: 0.4729 - model_137_loss: 0.6926 - model_137_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4512 - model_136_loss: 0.4735 - model_137_loss: 0.6926 - model_137_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9268 - model_137_loss: 0.6931 - model_137_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4520 - model_136_loss: 0.4728 - model_137_loss: 0.6926 - model_137_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4542 - model_136_loss: 0.4715 - model_137_loss: 0.6929 - model_137_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4535 - model_136_loss: 0.4710 - model_137_loss: 0.6925 - model_137_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4494 - model_136_loss: 0.4733 - model_137_loss: 0.6923 - model_137_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4542 - model_136_loss: 0.4698 - model_137_loss: 0.6927 - model_137_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: 6.9229 - model_137_loss: 0.6926 - model_137_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4526 - model_136_loss: 0.4695 - model_137_loss: 0.6925 - model_137_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4532 - model_136_loss: 0.4698 - model_137_loss: 0.6925 - model_137_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4546 - model_136_loss: 0.4685 - model_137_loss: 0.6925 - model_137_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4519 - model_136_loss: 0.4689 - model_137_loss: 0.6923 - model_137_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4543 - model_136_loss: 0.4654 - model_137_loss: 0.6924 - model_137_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9234 - model_137_loss: 0.6923 - model_137_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4553 - model_136_loss: 0.4660 - model_137_loss: 0.6926 - model_137_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4529 - model_136_loss: 0.4672 - model_137_loss: 0.6923 - model_137_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4515 - model_136_loss: 0.4671 - model_137_loss: 0.6922 - model_137_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4526 - model_136_loss: 0.4663 - model_137_loss: 0.6923 - model_137_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4544 - model_136_loss: 0.4655 - model_137_loss: 0.6924 - model_137_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9199 - model_137_loss: 0.6924 - model_137_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4522 - model_136_loss: 0.4651 - model_137_loss: 0.6923 - model_137_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4524 - model_136_loss: 0.4645 - model_137_loss: 0.6921 - model_137_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4511 - model_136_loss: 0.4646 - model_137_loss: 0.6921 - model_137_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4536 - model_136_loss: 0.4631 - model_137_loss: 0.6923 - model_137_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4514 - model_136_loss: 0.4652 - model_137_loss: 0.6923 - model_137_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9177 - model_137_loss: 0.6928 - model_137_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4525 - model_136_loss: 0.4639 - model_137_loss: 0.6919 - model_137_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4545 - model_136_loss: 0.4626 - model_137_loss: 0.6922 - model_137_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4554 - model_136_loss: 0.4624 - model_137_loss: 0.6924 - model_137_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4514 - model_136_loss: 0.4628 - model_137_loss: 0.6918 - model_137_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4548 - model_136_loss: 0.4617 - model_137_loss: 0.6922 - model_137_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9211 - model_137_loss: 0.6927 - model_137_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4560 - model_136_loss: 0.4626 - model_137_loss: 0.6924 - model_137_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4526 - model_136_loss: 0.4627 - model_137_loss: 0.6921 - model_137_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4561 - model_136_loss: 0.4615 - model_137_loss: 0.6925 - model_137_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4571 - model_136_loss: 0.4606 - model_137_loss: 0.6925 - model_137_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4563 - model_136_loss: 0.4606 - model_137_loss: 0.6923 - model_137_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9180 - model_137_loss: 0.6930 - model_137_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4567 - model_136_loss: 0.4613 - model_137_loss: 0.6923 - model_137_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4551 - model_136_loss: 0.4613 - model_137_loss: 0.6923 - model_137_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4541 - model_136_loss: 0.4617 - model_137_loss: 0.6924 - model_137_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4566 - model_136_loss: 0.4606 - model_137_loss: 0.6924 - model_137_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4541 - model_136_loss: 0.4614 - model_137_loss: 0.6920 - model_137_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9177 - model_137_loss: 0.6922 - model_137_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4541 - model_136_loss: 0.4609 - model_137_loss: 0.6922 - model_137_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4561 - model_136_loss: 0.4609 - model_137_loss: 0.6924 - model_137_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4551 - model_136_loss: 0.4638 - model_137_loss: 0.6926 - model_137_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4530 - model_136_loss: 0.4625 - model_137_loss: 0.6920 - model_137_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4518 - model_136_loss: 0.4645 - model_137_loss: 0.6921 - model_137_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9205 - model_137_loss: 0.6928 - model_137_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4546 - model_136_loss: 0.4627 - model_137_loss: 0.6923 - model_137_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4568 - model_136_loss: 0.4632 - model_137_loss: 0.6925 - model_137_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4543 - model_136_loss: 0.4631 - model_137_loss: 0.6921 - model_137_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4569 - model_136_loss: 0.4615 - model_137_loss: 0.6922 - model_137_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4542 - model_136_loss: 0.4645 - model_137_loss: 0.6922 - model_137_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9208 - model_137_loss: 0.6924 - model_137_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4559 - model_136_loss: 0.4638 - model_137_loss: 0.6924 - model_137_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4552 - model_136_loss: 0.4647 - model_137_loss: 0.6923 - model_137_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4540 - model_136_loss: 0.4653 - model_137_loss: 0.6922 - model_137_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4554 - model_136_loss: 0.4638 - model_137_loss: 0.6921 - model_137_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4569 - model_136_loss: 0.4651 - model_137_loss: 0.6924 - model_137_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9233 - model_137_loss: 0.6936 - model_137_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4539 - model_136_loss: 0.4665 - model_137_loss: 0.6923 - model_137_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4573 - model_136_loss: 0.4652 - model_137_loss: 0.6927 - model_137_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4542 - model_136_loss: 0.4670 - model_137_loss: 0.6924 - model_137_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4538 - model_136_loss: 0.4683 - model_137_loss: 0.6925 - model_137_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4546 - model_136_loss: 0.4658 - model_137_loss: 0.6923 - model_137_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9226 - model_137_loss: 0.6923 - model_137_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4569 - model_136_loss: 0.4650 - model_137_loss: 0.6926 - model_137_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4567 - model_136_loss: 0.4662 - model_137_loss: 0.6926 - model_137_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4581 - model_136_loss: 0.4647 - model_137_loss: 0.6926 - model_137_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4584 - model_136_loss: 0.4640 - model_137_loss: 0.6925 - model_137_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4585 - model_136_loss: 0.4634 - model_137_loss: 0.6928 - model_137_1_loss: 0.6916\n",
      "For Attention Module: 3.6\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: 6.3323 - model_141_loss: 0.6601 - model_141_1_loss: 0.60680s - loss: 6.3342 - model_141_loss: 0.6605 - model_141_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: -5.9559 - model_140_loss: 0.3756 - model_141_loss: 0.6590 - model_141_1_loss: 0.6073\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -5.9656 - model_140_loss: 0.3773 - model_141_loss: 0.6594 - model_141_1_loss: 0.6092\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -5.9758 - model_140_loss: 0.3761 - model_141_loss: 0.6601 - model_141_1_loss: 0.6102\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -5.9848 - model_140_loss: 0.3761 - model_141_loss: 0.6594 - model_141_1_loss: 0.6128\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0019 - model_140_loss: 0.3754 - model_141_loss: 0.6606 - model_141_1_loss: 0.6149\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.3891 - model_141_loss: 0.6617 - model_141_1_loss: 0.6165\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.0076 - model_140_loss: 0.3756 - model_141_loss: 0.6606 - model_141_1_loss: 0.6160\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.0209 - model_140_loss: 0.3777 - model_141_loss: 0.6629 - model_141_1_loss: 0.6168\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0297 - model_140_loss: 0.3783 - model_141_loss: 0.6625 - model_141_1_loss: 0.6191\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.0544 - model_140_loss: 0.3780 - model_141_loss: 0.6648 - model_141_1_loss: 0.6216\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0568 - model_140_loss: 0.3800 - model_141_loss: 0.6637 - model_141_1_loss: 0.6236\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.4491 - model_141_loss: 0.6644 - model_141_1_loss: 0.6257\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0568 - model_140_loss: 0.3804 - model_141_loss: 0.6631 - model_141_1_loss: 0.6244\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.0664 - model_140_loss: 0.3822 - model_141_loss: 0.6633 - model_141_1_loss: 0.6264\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.0835 - model_140_loss: 0.3820 - model_141_loss: 0.6648 - model_141_1_loss: 0.6283\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1042 - model_140_loss: 0.3834 - model_141_loss: 0.6660 - model_141_1_loss: 0.6315\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.1192 - model_140_loss: 0.3854 - model_141_loss: 0.6673 - model_141_1_loss: 0.6336\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.5129 - model_141_loss: 0.6669 - model_141_1_loss: 0.6361\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.1272 - model_140_loss: 0.3871 - model_141_loss: 0.6672 - model_141_1_loss: 0.6357\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.1411 - model_140_loss: 0.3874 - model_141_loss: 0.6683 - model_141_1_loss: 0.6374\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.1588 - model_140_loss: 0.3901 - model_141_loss: 0.6682 - model_141_1_loss: 0.6415\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.1638 - model_140_loss: 0.3919 - model_141_loss: 0.6684 - model_141_1_loss: 0.6428\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1935 - model_140_loss: 0.3929 - model_141_loss: 0.6707 - model_141_1_loss: 0.6466\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.5908 - model_141_loss: 0.6707 - model_141_1_loss: 0.6477\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.1910 - model_140_loss: 0.3953 - model_141_loss: 0.6706 - model_141_1_loss: 0.6467\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.2041 - model_140_loss: 0.3964 - model_141_loss: 0.6717 - model_141_1_loss: 0.6484\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2229 - model_140_loss: 0.3996 - model_141_loss: 0.6726 - model_141_1_loss: 0.6519\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.2349 - model_140_loss: 0.4019 - model_141_loss: 0.6732 - model_141_1_loss: 0.6542\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2480 - model_140_loss: 0.4020 - model_141_loss: 0.6740 - model_141_1_loss: 0.6560\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.6678 - model_141_loss: 0.6757 - model_141_1_loss: 0.6582\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.2654 - model_140_loss: 0.4065 - model_141_loss: 0.6760 - model_141_1_loss: 0.6584\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2715 - model_140_loss: 0.4077 - model_141_loss: 0.6768 - model_141_1_loss: 0.6590\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.2820 - model_140_loss: 0.4112 - model_141_loss: 0.6760 - model_141_1_loss: 0.6626\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.2959 - model_140_loss: 0.4150 - model_141_loss: 0.6772 - model_141_1_loss: 0.6650\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3091 - model_140_loss: 0.4168 - model_141_loss: 0.6783 - model_141_1_loss: 0.6669\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.7394 - model_141_loss: 0.6797 - model_141_1_loss: 0.66830s - loss: 6.7668 - model_141_loss: 0.6843 - model_1\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3029 - model_140_loss: 0.4224 - model_141_loss: 0.6782 - model_141_1_loss: 0.6668\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3199 - model_140_loss: 0.4228 - model_141_loss: 0.6800 - model_141_1_loss: 0.6686\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3311 - model_140_loss: 0.4290 - model_141_loss: 0.6820 - model_141_1_loss: 0.6701\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3432 - model_140_loss: 0.4293 - model_141_loss: 0.6822 - model_141_1_loss: 0.6723\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3539 - model_140_loss: 0.4329 - model_141_loss: 0.6832 - model_141_1_loss: 0.6742\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.7965 - model_141_loss: 0.6838 - model_141_1_loss: 0.6749\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3524 - model_140_loss: 0.4369 - model_141_loss: 0.6829 - model_141_1_loss: 0.6750\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3558 - model_140_loss: 0.4411 - model_141_loss: 0.6840 - model_141_1_loss: 0.6754\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3664 - model_140_loss: 0.4435 - model_141_loss: 0.6844 - model_141_1_loss: 0.6775\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.3734 - model_140_loss: 0.4474 - model_141_loss: 0.6857 - model_141_1_loss: 0.6785\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3969 - model_140_loss: 0.4490 - model_141_loss: 0.6876 - model_141_1_loss: 0.6815\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.8398 - model_141_loss: 0.6875 - model_141_1_loss: 0.6801\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3777 - model_140_loss: 0.4542 - model_141_loss: 0.6862 - model_141_1_loss: 0.6802\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3871 - model_140_loss: 0.4572 - model_141_loss: 0.6874 - model_141_1_loss: 0.6814\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3927 - model_140_loss: 0.4594 - model_141_loss: 0.6882 - model_141_1_loss: 0.6822\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3951 - model_140_loss: 0.4623 - model_141_loss: 0.6886 - model_141_1_loss: 0.6829\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3960 - model_140_loss: 0.4679 - model_141_loss: 0.6889 - model_141_1_loss: 0.6839\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.8687 - model_141_loss: 0.6898 - model_141_1_loss: 0.68450s - loss: 6.8678 - model_141_loss: 0.6893 - model_141_1_lo\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3999 - model_140_loss: 0.4692 - model_141_loss: 0.6893 - model_141_1_loss: 0.6845\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3995 - model_140_loss: 0.4729 - model_141_loss: 0.6897 - model_141_1_loss: 0.6847\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4049 - model_140_loss: 0.4769 - model_141_loss: 0.6903 - model_141_1_loss: 0.6861\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4122 - model_140_loss: 0.4772 - model_141_loss: 0.6914 - model_141_1_loss: 0.6864\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4063 - model_140_loss: 0.4778 - model_141_loss: 0.6908 - model_141_1_loss: 0.6860\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.8967 - model_141_loss: 0.6922 - model_141_1_loss: 0.6877\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4131 - model_140_loss: 0.4808 - model_141_loss: 0.6915 - model_141_1_loss: 0.6872\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4095 - model_140_loss: 0.4831 - model_141_loss: 0.6913 - model_141_1_loss: 0.6873\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4131 - model_140_loss: 0.4854 - model_141_loss: 0.6915 - model_141_1_loss: 0.6881\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4154 - model_140_loss: 0.4852 - model_141_loss: 0.6917 - model_141_1_loss: 0.6884\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4183 - model_140_loss: 0.4846 - model_141_loss: 0.6920 - model_141_1_loss: 0.6886\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9074 - model_141_loss: 0.6927 - model_141_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4171 - model_140_loss: 0.4861 - model_141_loss: 0.6917 - model_141_1_loss: 0.6889\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4214 - model_140_loss: 0.4862 - model_141_loss: 0.6922 - model_141_1_loss: 0.6893\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4234 - model_140_loss: 0.4866 - model_141_loss: 0.6928 - model_141_1_loss: 0.6892\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4282 - model_140_loss: 0.4869 - model_141_loss: 0.6932 - model_141_1_loss: 0.6899\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4304 - model_140_loss: 0.4872 - model_141_loss: 0.6931 - model_141_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9157 - model_141_loss: 0.6928 - model_141_1_loss: 0.69000s - loss: 6.9283 - model_141_loss: 0.6949 - model_141_1_loss:\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4301 - model_140_loss: 0.4873 - model_141_loss: 0.6930 - model_141_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4311 - model_140_loss: 0.4894 - model_141_loss: 0.6934 - model_141_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4282 - model_140_loss: 0.4900 - model_141_loss: 0.6929 - model_141_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4342 - model_140_loss: 0.4911 - model_141_loss: 0.6934 - model_141_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4402 - model_140_loss: 0.4894 - model_141_loss: 0.6940 - model_141_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9287 - model_141_loss: 0.6938 - model_141_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4343 - model_140_loss: 0.4900 - model_141_loss: 0.6931 - model_141_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4358 - model_140_loss: 0.4896 - model_141_loss: 0.6936 - model_141_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4416 - model_140_loss: 0.4877 - model_141_loss: 0.6936 - model_141_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4395 - model_140_loss: 0.4881 - model_141_loss: 0.6932 - model_141_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4380 - model_140_loss: 0.4898 - model_141_loss: 0.6937 - model_141_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9287 - model_141_loss: 0.6934 - model_141_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4365 - model_140_loss: 0.4876 - model_141_loss: 0.6930 - model_141_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4369 - model_140_loss: 0.4890 - model_141_loss: 0.6929 - model_141_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4374 - model_140_loss: 0.4890 - model_141_loss: 0.6932 - model_141_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4386 - model_140_loss: 0.4869 - model_141_loss: 0.6930 - model_141_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4425 - model_140_loss: 0.4843 - model_141_loss: 0.6930 - model_141_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9234 - model_141_loss: 0.6922 - model_141_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4344 - model_140_loss: 0.4858 - model_141_loss: 0.6924 - model_141_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4368 - model_140_loss: 0.4851 - model_141_loss: 0.6927 - model_141_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4325 - model_140_loss: 0.4859 - model_141_loss: 0.6924 - model_141_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4339 - model_140_loss: 0.4835 - model_141_loss: 0.6924 - model_141_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4365 - model_140_loss: 0.4829 - model_141_loss: 0.6926 - model_141_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9222 - model_141_loss: 0.6922 - model_141_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4313 - model_140_loss: 0.4833 - model_141_loss: 0.6920 - model_141_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4357 - model_140_loss: 0.4815 - model_141_loss: 0.6923 - model_141_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4356 - model_140_loss: 0.4813 - model_141_loss: 0.6925 - model_141_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4374 - model_140_loss: 0.4813 - model_141_loss: 0.6926 - model_141_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4392 - model_140_loss: 0.4788 - model_141_loss: 0.6927 - model_141_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9146 - model_141_loss: 0.6920 - model_141_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4409 - model_140_loss: 0.4781 - model_141_loss: 0.6925 - model_141_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4433 - model_140_loss: 0.4773 - model_141_loss: 0.6924 - model_141_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4339 - model_140_loss: 0.4808 - model_141_loss: 0.6920 - model_141_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4422 - model_140_loss: 0.4793 - model_141_loss: 0.6925 - model_141_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4432 - model_140_loss: 0.4786 - model_141_loss: 0.6924 - model_141_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9195 - model_141_loss: 0.6928 - model_141_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4369 - model_140_loss: 0.4778 - model_141_loss: 0.6919 - model_141_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4387 - model_140_loss: 0.4776 - model_141_loss: 0.6921 - model_141_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4454 - model_140_loss: 0.4773 - model_141_loss: 0.6929 - model_141_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4443 - model_140_loss: 0.4788 - model_141_loss: 0.6928 - model_141_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4485 - model_140_loss: 0.4780 - model_141_loss: 0.6929 - model_141_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9239 - model_141_loss: 0.6925 - model_141_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4417 - model_140_loss: 0.4790 - model_141_loss: 0.6925 - model_141_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4459 - model_140_loss: 0.4796 - model_141_loss: 0.6928 - model_141_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4444 - model_140_loss: 0.4785 - model_141_loss: 0.6925 - model_141_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4469 - model_140_loss: 0.4773 - model_141_loss: 0.6927 - model_141_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4439 - model_140_loss: 0.4790 - model_141_loss: 0.6925 - model_141_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9266 - model_141_loss: 0.6932 - model_141_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4465 - model_140_loss: 0.4771 - model_141_loss: 0.6925 - model_141_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4483 - model_140_loss: 0.4780 - model_141_loss: 0.6930 - model_141_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4454 - model_140_loss: 0.4789 - model_141_loss: 0.6928 - model_141_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4449 - model_140_loss: 0.4800 - model_141_loss: 0.6928 - model_141_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4452 - model_140_loss: 0.4796 - model_141_loss: 0.6930 - model_141_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9248 - model_141_loss: 0.6926 - model_141_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4440 - model_140_loss: 0.4774 - model_141_loss: 0.6928 - model_141_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4426 - model_140_loss: 0.4763 - model_141_loss: 0.6925 - model_141_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4448 - model_140_loss: 0.4757 - model_141_loss: 0.6927 - model_141_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4451 - model_140_loss: 0.4766 - model_141_loss: 0.6929 - model_141_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4467 - model_140_loss: 0.4742 - model_141_loss: 0.6929 - model_141_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9193 - model_141_loss: 0.6927 - model_141_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4435 - model_140_loss: 0.4742 - model_141_loss: 0.6926 - model_141_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4449 - model_140_loss: 0.4731 - model_141_loss: 0.6926 - model_141_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4422 - model_140_loss: 0.4745 - model_141_loss: 0.6926 - model_141_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4455 - model_140_loss: 0.4717 - model_141_loss: 0.6927 - model_141_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4435 - model_140_loss: 0.4736 - model_141_loss: 0.6928 - model_141_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9160 - model_141_loss: 0.6926 - model_141_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4449 - model_140_loss: 0.4695 - model_141_loss: 0.6925 - model_141_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4447 - model_140_loss: 0.4713 - model_141_loss: 0.6925 - model_141_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4458 - model_140_loss: 0.4695 - model_141_loss: 0.6925 - model_141_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4461 - model_140_loss: 0.4694 - model_141_loss: 0.6925 - model_141_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4441 - model_140_loss: 0.4696 - model_141_loss: 0.6923 - model_141_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9129 - model_141_loss: 0.6923 - model_141_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4439 - model_140_loss: 0.4683 - model_141_loss: 0.6924 - model_141_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4452 - model_140_loss: 0.4677 - model_141_loss: 0.6925 - model_141_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4471 - model_140_loss: 0.4666 - model_141_loss: 0.6924 - model_141_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4465 - model_140_loss: 0.4673 - model_141_loss: 0.6923 - model_141_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4482 - model_140_loss: 0.4679 - model_141_loss: 0.6925 - model_141_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9174 - model_141_loss: 0.6931 - model_141_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4460 - model_140_loss: 0.4682 - model_141_loss: 0.6924 - model_141_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4468 - model_140_loss: 0.4679 - model_141_loss: 0.6925 - model_141_1_loss: 0.6905\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4498 - model_140_loss: 0.4671 - model_141_loss: 0.6925 - model_141_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4471 - model_140_loss: 0.4702 - model_141_loss: 0.6925 - model_141_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4467 - model_140_loss: 0.4696 - model_141_loss: 0.6921 - model_141_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9142 - model_141_loss: 0.6918 - model_141_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4503 - model_140_loss: 0.4681 - model_141_loss: 0.6924 - model_141_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4483 - model_140_loss: 0.4680 - model_141_loss: 0.6923 - model_141_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4473 - model_140_loss: 0.4695 - model_141_loss: 0.6925 - model_141_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4480 - model_140_loss: 0.4689 - model_141_loss: 0.6921 - model_141_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4526 - model_140_loss: 0.4709 - model_141_loss: 0.6928 - model_141_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9228 - model_141_loss: 0.6925 - model_141_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4472 - model_140_loss: 0.4719 - model_141_loss: 0.6925 - model_141_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4472 - model_140_loss: 0.4727 - model_141_loss: 0.6924 - model_141_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4501 - model_140_loss: 0.4708 - model_141_loss: 0.6923 - model_141_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4478 - model_140_loss: 0.4738 - model_141_loss: 0.6924 - model_141_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4512 - model_140_loss: 0.4732 - model_141_loss: 0.6926 - model_141_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9223 - model_141_loss: 0.6925 - model_141_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4477 - model_140_loss: 0.4761 - model_141_loss: 0.6928 - model_141_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4447 - model_140_loss: 0.4751 - model_141_loss: 0.6918 - model_141_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4474 - model_140_loss: 0.4747 - model_141_loss: 0.6924 - model_141_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4485 - model_140_loss: 0.4737 - model_141_loss: 0.6925 - model_141_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4500 - model_140_loss: 0.4751 - model_141_loss: 0.6928 - model_141_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9236 - model_141_loss: 0.6926 - model_141_1_loss: 0.69180s - loss: 6.8904 - model_141_loss: 0.6891 - model_141_1_lo\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4471 - model_140_loss: 0.4753 - model_141_loss: 0.6924 - model_141_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4497 - model_140_loss: 0.4734 - model_141_loss: 0.6925 - model_141_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4454 - model_140_loss: 0.4766 - model_141_loss: 0.6923 - model_141_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4508 - model_140_loss: 0.4740 - model_141_loss: 0.6927 - model_141_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4515 - model_140_loss: 0.4733 - model_141_loss: 0.6928 - model_141_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9253 - model_141_loss: 0.6926 - model_141_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4499 - model_140_loss: 0.4725 - model_141_loss: 0.6926 - model_141_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4508 - model_140_loss: 0.4737 - model_141_loss: 0.6926 - model_141_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4490 - model_140_loss: 0.4733 - model_141_loss: 0.6926 - model_141_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4513 - model_140_loss: 0.4699 - model_141_loss: 0.6924 - model_141_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4502 - model_140_loss: 0.4714 - model_141_loss: 0.6925 - model_141_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9241 - model_141_loss: 0.6931 - model_141_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4509 - model_140_loss: 0.4717 - model_141_loss: 0.6926 - model_141_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4523 - model_140_loss: 0.4700 - model_141_loss: 0.6928 - model_141_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4516 - model_140_loss: 0.4690 - model_141_loss: 0.6927 - model_141_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4544 - model_140_loss: 0.4686 - model_141_loss: 0.6928 - model_141_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4536 - model_140_loss: 0.4684 - model_141_loss: 0.6928 - model_141_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9216 - model_141_loss: 0.6921 - model_141_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4502 - model_140_loss: 0.4684 - model_141_loss: 0.6925 - model_141_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4531 - model_140_loss: 0.4663 - model_141_loss: 0.6927 - model_141_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4526 - model_140_loss: 0.4672 - model_141_loss: 0.6927 - model_141_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4520 - model_140_loss: 0.4669 - model_141_loss: 0.6928 - model_141_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4508 - model_140_loss: 0.4661 - model_141_loss: 0.6925 - model_141_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9177 - model_141_loss: 0.6927 - model_141_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4517 - model_140_loss: 0.4650 - model_141_loss: 0.6924 - model_141_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4511 - model_140_loss: 0.4665 - model_141_loss: 0.6924 - model_141_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4506 - model_140_loss: 0.4669 - model_141_loss: 0.6925 - model_141_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4506 - model_140_loss: 0.4661 - model_141_loss: 0.6926 - model_141_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4493 - model_140_loss: 0.4662 - model_141_loss: 0.6924 - model_141_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9178 - model_141_loss: 0.6932 - model_141_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4508 - model_140_loss: 0.4657 - model_141_loss: 0.6924 - model_141_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4473 - model_140_loss: 0.4690 - model_141_loss: 0.6924 - model_141_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4489 - model_140_loss: 0.4677 - model_141_loss: 0.6924 - model_141_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4495 - model_140_loss: 0.4676 - model_141_loss: 0.6922 - model_141_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4480 - model_140_loss: 0.4692 - model_141_loss: 0.6924 - model_141_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9189 - model_141_loss: 0.6927 - model_141_1_loss: 0.69130s - loss: 6.8907 - model_141_loss: 0.6887 - model_141_1_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4522 - model_140_loss: 0.4674 - model_141_loss: 0.6926 - model_141_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4493 - model_140_loss: 0.4680 - model_141_loss: 0.6922 - model_141_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4502 - model_140_loss: 0.4684 - model_141_loss: 0.6924 - model_141_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4500 - model_140_loss: 0.4690 - model_141_loss: 0.6925 - model_141_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4518 - model_140_loss: 0.4696 - model_141_loss: 0.6926 - model_141_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.9231 - model_141_loss: 0.6927 - model_141_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4536 - model_140_loss: 0.4672 - model_141_loss: 0.6926 - model_141_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4528 - model_140_loss: 0.4673 - model_141_loss: 0.6926 - model_141_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4528 - model_140_loss: 0.4693 - model_141_loss: 0.6926 - model_141_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4539 - model_140_loss: 0.4678 - model_141_loss: 0.6926 - model_141_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4534 - model_140_loss: 0.4684 - model_141_loss: 0.6927 - model_141_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9220 - model_141_loss: 0.6929 - model_141_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4534 - model_140_loss: 0.4671 - model_141_loss: 0.6925 - model_141_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4533 - model_140_loss: 0.4669 - model_141_loss: 0.6923 - model_141_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4519 - model_140_loss: 0.4675 - model_141_loss: 0.6923 - model_141_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4531 - model_140_loss: 0.4650 - model_141_loss: 0.6924 - model_141_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4535 - model_140_loss: 0.4660 - model_141_loss: 0.6924 - model_141_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9187 - model_141_loss: 0.6924 - model_141_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4479 - model_140_loss: 0.4657 - model_141_loss: 0.6921 - model_141_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4514 - model_140_loss: 0.4648 - model_141_loss: 0.6920 - model_141_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4515 - model_140_loss: 0.4634 - model_141_loss: 0.6921 - model_141_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4506 - model_140_loss: 0.4639 - model_141_loss: 0.6921 - model_141_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4533 - model_140_loss: 0.4621 - model_141_loss: 0.6923 - model_141_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9167 - model_141_loss: 0.6922 - model_141_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4517 - model_140_loss: 0.4633 - model_141_loss: 0.6921 - model_141_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4540 - model_140_loss: 0.4617 - model_141_loss: 0.6924 - model_141_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4520 - model_140_loss: 0.4633 - model_141_loss: 0.6922 - model_141_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4497 - model_140_loss: 0.4626 - model_141_loss: 0.6918 - model_141_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4499 - model_140_loss: 0.4618 - model_141_loss: 0.6918 - model_141_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: 6.9179 - model_141_loss: 0.6918 - model_141_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4510 - model_140_loss: 0.4642 - model_141_loss: 0.6922 - model_141_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4501 - model_140_loss: 0.4627 - model_141_loss: 0.6920 - model_141_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4526 - model_140_loss: 0.4624 - model_141_loss: 0.6921 - model_141_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4487 - model_140_loss: 0.4648 - model_141_loss: 0.6918 - model_141_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4535 - model_140_loss: 0.4638 - model_141_loss: 0.6920 - model_141_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9183 - model_141_loss: 0.6923 - model_141_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4542 - model_140_loss: 0.4644 - model_141_loss: 0.6923 - model_141_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4546 - model_140_loss: 0.4659 - model_141_loss: 0.6925 - model_141_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4559 - model_140_loss: 0.4648 - model_141_loss: 0.6924 - model_141_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4551 - model_140_loss: 0.4665 - model_141_loss: 0.6925 - model_141_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4564 - model_140_loss: 0.4662 - model_141_loss: 0.6926 - model_141_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9209 - model_141_loss: 0.6926 - model_141_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4513 - model_140_loss: 0.4684 - model_141_loss: 0.6924 - model_141_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4523 - model_140_loss: 0.4681 - model_141_loss: 0.6923 - model_141_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4515 - model_140_loss: 0.4693 - model_141_loss: 0.6923 - model_141_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4538 - model_140_loss: 0.4687 - model_141_loss: 0.6925 - model_141_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4531 - model_140_loss: 0.4689 - model_141_loss: 0.6924 - model_141_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9204 - model_141_loss: 0.6927 - model_141_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4552 - model_140_loss: 0.4678 - model_141_loss: 0.6926 - model_141_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4518 - model_140_loss: 0.4684 - model_141_loss: 0.6924 - model_141_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4539 - model_140_loss: 0.4689 - model_141_loss: 0.6926 - model_141_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4528 - model_140_loss: 0.4705 - model_141_loss: 0.6926 - model_141_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4541 - model_140_loss: 0.4676 - model_141_loss: 0.6924 - model_141_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9227 - model_141_loss: 0.6919 - model_141_1_loss: 0.69190s - loss: 6.9338 - model_141_loss: 0.6908 - model_14\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4515 - model_140_loss: 0.4705 - model_141_loss: 0.6925 - model_141_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4522 - model_140_loss: 0.4689 - model_141_loss: 0.6925 - model_141_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4532 - model_140_loss: 0.4684 - model_141_loss: 0.6924 - model_141_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4547 - model_140_loss: 0.4671 - model_141_loss: 0.6925 - model_141_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4514 - model_140_loss: 0.4699 - model_141_loss: 0.6926 - model_141_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9219 - model_141_loss: 0.6925 - model_141_1_loss: 0.69150s - loss: 6.9179 - model_141_loss: 0.6923 - model_141_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4530 - model_140_loss: 0.4686 - model_141_loss: 0.6927 - model_141_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4527 - model_140_loss: 0.4666 - model_141_loss: 0.6926 - model_141_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4528 - model_140_loss: 0.4676 - model_141_loss: 0.6927 - model_141_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4520 - model_140_loss: 0.4668 - model_141_loss: 0.6925 - model_141_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4530 - model_140_loss: 0.4667 - model_141_loss: 0.6927 - model_141_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9206 - model_141_loss: 0.6923 - model_141_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4522 - model_140_loss: 0.4668 - model_141_loss: 0.6924 - model_141_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4547 - model_140_loss: 0.4650 - model_141_loss: 0.6926 - model_141_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4535 - model_140_loss: 0.4660 - model_141_loss: 0.6926 - model_141_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4562 - model_140_loss: 0.4638 - model_141_loss: 0.6926 - model_141_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4576 - model_140_loss: 0.4631 - model_141_loss: 0.6926 - model_141_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9208 - model_141_loss: 0.6933 - model_141_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4545 - model_140_loss: 0.4643 - model_141_loss: 0.6926 - model_141_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4568 - model_140_loss: 0.4624 - model_141_loss: 0.6925 - model_141_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4544 - model_140_loss: 0.4644 - model_141_loss: 0.6925 - model_141_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4559 - model_140_loss: 0.4647 - model_141_loss: 0.6925 - model_141_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4574 - model_140_loss: 0.4623 - model_141_loss: 0.6927 - model_141_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9191 - model_141_loss: 0.6924 - model_141_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4534 - model_140_loss: 0.4635 - model_141_loss: 0.6922 - model_141_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4543 - model_140_loss: 0.4641 - model_141_loss: 0.6923 - model_141_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4531 - model_140_loss: 0.4661 - model_141_loss: 0.6924 - model_141_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4578 - model_140_loss: 0.4633 - model_141_loss: 0.6927 - model_141_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4558 - model_140_loss: 0.4643 - model_141_loss: 0.6924 - model_141_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9219 - model_141_loss: 0.6935 - model_141_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4565 - model_140_loss: 0.4647 - model_141_loss: 0.6925 - model_141_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4548 - model_140_loss: 0.4647 - model_141_loss: 0.6924 - model_141_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4528 - model_140_loss: 0.4662 - model_141_loss: 0.6924 - model_141_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4532 - model_140_loss: 0.4658 - model_141_loss: 0.6925 - model_141_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4524 - model_140_loss: 0.4650 - model_141_loss: 0.6922 - model_141_1_loss: 0.6913\n",
      "For Attention Module: 3.7\n",
      "features X: 30940 samples, 69 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.3657 - model_145_loss: 0.6620 - model_145_1_loss: 0.6122\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: -5.9768 - model_144_loss: 0.3848 - model_145_loss: 0.6616 - model_145_1_loss: 0.6108\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -5.9908 - model_144_loss: 0.3864 - model_145_loss: 0.6615 - model_145_1_loss: 0.6139\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.0025 - model_144_loss: 0.3846 - model_145_loss: 0.6619 - model_145_1_loss: 0.6155\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0070 - model_144_loss: 0.3854 - model_145_loss: 0.6623 - model_145_1_loss: 0.6162\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0165 - model_144_loss: 0.3874 - model_145_loss: 0.6619 - model_145_1_loss: 0.6189\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.4135 - model_145_loss: 0.6624 - model_145_1_loss: 0.620 - 1s 47us/sample - loss: 6.4132 - model_145_loss: 0.6632 - model_145_1_loss: 0.6200\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.0121 - model_144_loss: 0.3865 - model_145_loss: 0.6613 - model_145_1_loss: 0.6184\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0375 - model_144_loss: 0.3861 - model_145_loss: 0.6634 - model_145_1_loss: 0.6213\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0494 - model_144_loss: 0.3873 - model_145_loss: 0.6631 - model_145_1_loss: 0.6242\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.0515 - model_144_loss: 0.3862 - model_145_loss: 0.6635 - model_145_1_loss: 0.6240\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.0715 - model_144_loss: 0.3867 - model_145_loss: 0.6638 - model_145_1_loss: 0.6279\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.4672 - model_145_loss: 0.6648 - model_145_1_loss: 0.6289\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.0803 - model_144_loss: 0.3876 - model_145_loss: 0.6644 - model_145_1_loss: 0.6292\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.0905 - model_144_loss: 0.3883 - model_145_loss: 0.6653 - model_145_1_loss: 0.6304\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.0991 - model_144_loss: 0.3914 - model_145_loss: 0.6659 - model_145_1_loss: 0.6322\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.1073 - model_144_loss: 0.3923 - model_145_loss: 0.6661 - model_145_1_loss: 0.6338\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.1384 - model_144_loss: 0.3935 - model_145_loss: 0.6669 - model_145_1_loss: 0.6395\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.5305 - model_145_loss: 0.6677 - model_145_1_loss: 0.638 - 1s 49us/sample - loss: 6.5337 - model_145_loss: 0.6673 - model_145_1_loss: 0.6396\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.1337 - model_144_loss: 0.3952 - model_145_loss: 0.6662 - model_145_1_loss: 0.6396\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1531 - model_144_loss: 0.3956 - model_145_loss: 0.6681 - model_145_1_loss: 0.6416\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1792 - model_144_loss: 0.4003 - model_145_loss: 0.6697 - model_145_1_loss: 0.6462\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1809 - model_144_loss: 0.4014 - model_145_loss: 0.6699 - model_145_1_loss: 0.6466\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.1813 - model_144_loss: 0.4023 - model_145_loss: 0.6683 - model_145_1_loss: 0.6485\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: 6.6077 - model_145_loss: 0.6713 - model_145_1_loss: 0.6513\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.1897 - model_144_loss: 0.4052 - model_145_loss: 0.6688 - model_145_1_loss: 0.6501\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.2142 - model_144_loss: 0.4093 - model_145_loss: 0.6712 - model_145_1_loss: 0.6535\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.2267 - model_144_loss: 0.4098 - model_145_loss: 0.6726 - model_145_1_loss: 0.6547\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.2481 - model_144_loss: 0.4138 - model_145_loss: 0.6739 - model_145_1_loss: 0.6585\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.2532 - model_144_loss: 0.4172 - model_145_loss: 0.6735 - model_145_1_loss: 0.6606\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.6786 - model_145_loss: 0.6738 - model_145_1_loss: 0.6614\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.2583 - model_144_loss: 0.4173 - model_145_loss: 0.6744 - model_145_1_loss: 0.6608\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.2592 - model_144_loss: 0.4212 - model_145_loss: 0.6740 - model_145_1_loss: 0.6620\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.2760 - model_144_loss: 0.4244 - model_145_loss: 0.6758 - model_145_1_loss: 0.6643\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.2917 - model_144_loss: 0.4254 - model_145_loss: 0.6767 - model_145_1_loss: 0.6667\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.2970 - model_144_loss: 0.4286 - model_145_loss: 0.6777 - model_145_1_loss: 0.6674\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.7378 - model_145_loss: 0.6775 - model_145_1_loss: 0.6693\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3050 - model_144_loss: 0.4290 - model_145_loss: 0.6786 - model_145_1_loss: 0.6682\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3140 - model_144_loss: 0.4306 - model_145_loss: 0.6785 - model_145_1_loss: 0.6704\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3241 - model_144_loss: 0.4341 - model_145_loss: 0.6799 - model_145_1_loss: 0.6717\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3317 - model_144_loss: 0.4370 - model_145_loss: 0.6808 - model_145_1_loss: 0.6729\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3382 - model_144_loss: 0.4397 - model_145_loss: 0.6806 - model_145_1_loss: 0.6749\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.7846 - model_145_loss: 0.6812 - model_145_1_loss: 0.6755\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3392 - model_144_loss: 0.4412 - model_145_loss: 0.6813 - model_145_1_loss: 0.6747\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3463 - model_144_loss: 0.4423 - model_145_loss: 0.6819 - model_145_1_loss: 0.6758\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3577 - model_144_loss: 0.4467 - model_145_loss: 0.6830 - model_145_1_loss: 0.6779\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3634 - model_144_loss: 0.4495 - model_145_loss: 0.6837 - model_145_1_loss: 0.6789\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3699 - model_144_loss: 0.4505 - model_145_loss: 0.6843 - model_145_1_loss: 0.6798\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.8226 - model_145_loss: 0.6836 - model_145_1_loss: 0.6806\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3759 - model_144_loss: 0.4524 - model_145_loss: 0.6850 - model_145_1_loss: 0.6806\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3778 - model_144_loss: 0.4536 - model_145_loss: 0.6851 - model_145_1_loss: 0.6812\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3850 - model_144_loss: 0.4565 - model_145_loss: 0.6856 - model_145_1_loss: 0.6827\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3938 - model_144_loss: 0.4599 - model_145_loss: 0.6874 - model_145_1_loss: 0.6834\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3969 - model_144_loss: 0.4625 - model_145_loss: 0.6875 - model_145_1_loss: 0.6843\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.8596 - model_145_loss: 0.6882 - model_145_1_loss: 0.6844\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3940 - model_144_loss: 0.4667 - model_145_loss: 0.6881 - model_145_1_loss: 0.6840\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3991 - model_144_loss: 0.4668 - model_145_loss: 0.6882 - model_145_1_loss: 0.6850\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4028 - model_144_loss: 0.4711 - model_145_loss: 0.6890 - model_145_1_loss: 0.6858\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4018 - model_144_loss: 0.4758 - model_145_loss: 0.6894 - model_145_1_loss: 0.6861\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4057 - model_144_loss: 0.4768 - model_145_loss: 0.6889 - model_145_1_loss: 0.6876\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.8891 - model_145_loss: 0.6905 - model_145_1_loss: 0.6871\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4076 - model_144_loss: 0.4787 - model_145_loss: 0.6900 - model_145_1_loss: 0.6873\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4119 - model_144_loss: 0.4800 - model_145_loss: 0.6903 - model_145_1_loss: 0.6881\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4079 - model_144_loss: 0.4848 - model_145_loss: 0.6909 - model_145_1_loss: 0.6877\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4114 - model_144_loss: 0.4841 - model_145_loss: 0.6905 - model_145_1_loss: 0.6886\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4191 - model_144_loss: 0.4835 - model_145_loss: 0.6918 - model_145_1_loss: 0.6887\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9018 - model_145_loss: 0.6907 - model_145_1_loss: 0.6890\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4159 - model_144_loss: 0.4866 - model_145_loss: 0.6914 - model_145_1_loss: 0.6891\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4174 - model_144_loss: 0.4893 - model_145_loss: 0.6913 - model_145_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4201 - model_144_loss: 0.4918 - model_145_loss: 0.6919 - model_145_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4233 - model_144_loss: 0.4893 - model_145_loss: 0.6922 - model_145_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4267 - model_144_loss: 0.4907 - model_145_loss: 0.6925 - model_145_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9148 - model_145_loss: 0.6924 - model_145_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4222 - model_144_loss: 0.4928 - model_145_loss: 0.6920 - model_145_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4219 - model_144_loss: 0.4931 - model_145_loss: 0.6921 - model_145_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4271 - model_144_loss: 0.4930 - model_145_loss: 0.6925 - model_145_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4296 - model_144_loss: 0.4933 - model_145_loss: 0.6928 - model_145_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4309 - model_144_loss: 0.4937 - model_145_loss: 0.6928 - model_145_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9197 - model_145_loss: 0.6919 - model_145_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4229 - model_144_loss: 0.4943 - model_145_loss: 0.6918 - model_145_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4247 - model_144_loss: 0.4938 - model_145_loss: 0.6916 - model_145_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4258 - model_144_loss: 0.4933 - model_145_loss: 0.6917 - model_145_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4282 - model_144_loss: 0.4943 - model_145_loss: 0.6922 - model_145_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4296 - model_144_loss: 0.4931 - model_145_loss: 0.6921 - model_145_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9211 - model_145_loss: 0.6922 - model_145_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4242 - model_144_loss: 0.4933 - model_145_loss: 0.6913 - model_145_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4285 - model_144_loss: 0.4904 - model_145_loss: 0.6915 - model_145_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4249 - model_144_loss: 0.4909 - model_145_loss: 0.6911 - model_145_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4259 - model_144_loss: 0.4902 - model_145_loss: 0.6909 - model_145_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4257 - model_144_loss: 0.4899 - model_145_loss: 0.6908 - model_145_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9175 - model_145_loss: 0.6915 - model_145_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4255 - model_144_loss: 0.4882 - model_145_loss: 0.6906 - model_145_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4233 - model_144_loss: 0.4884 - model_145_loss: 0.6902 - model_145_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4241 - model_144_loss: 0.4886 - model_145_loss: 0.6903 - model_145_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4241 - model_144_loss: 0.4883 - model_145_loss: 0.6904 - model_145_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4268 - model_144_loss: 0.4865 - model_145_loss: 0.6905 - model_145_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9102 - model_145_loss: 0.6906 - model_145_1_loss: 0.69180s - loss: 6.9238 - model_145_loss: 0.6946 - model_145_1\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4190 - model_144_loss: 0.4871 - model_145_loss: 0.6896 - model_145_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4175 - model_144_loss: 0.4850 - model_145_loss: 0.6892 - model_145_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4202 - model_144_loss: 0.4846 - model_145_loss: 0.6894 - model_145_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4201 - model_144_loss: 0.4839 - model_145_loss: 0.6895 - model_145_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4213 - model_144_loss: 0.4827 - model_145_loss: 0.6894 - model_145_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9056 - model_145_loss: 0.6900 - model_145_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4164 - model_144_loss: 0.4824 - model_145_loss: 0.6885 - model_145_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4205 - model_144_loss: 0.4819 - model_145_loss: 0.6891 - model_145_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4160 - model_144_loss: 0.4837 - model_145_loss: 0.6889 - model_145_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4250 - model_144_loss: 0.4800 - model_145_loss: 0.6895 - model_145_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4224 - model_144_loss: 0.4816 - model_145_loss: 0.6896 - model_145_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9029 - model_145_loss: 0.6889 - model_145_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4234 - model_144_loss: 0.4778 - model_145_loss: 0.6894 - model_145_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4198 - model_144_loss: 0.4797 - model_145_loss: 0.6890 - model_145_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4244 - model_144_loss: 0.4794 - model_145_loss: 0.6896 - model_145_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4250 - model_144_loss: 0.4777 - model_145_loss: 0.6898 - model_145_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4258 - model_144_loss: 0.4782 - model_145_loss: 0.6896 - model_145_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9029 - model_145_loss: 0.6898 - model_145_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4262 - model_144_loss: 0.4815 - model_145_loss: 0.6902 - model_145_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4280 - model_144_loss: 0.4786 - model_145_loss: 0.6898 - model_145_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4305 - model_144_loss: 0.4789 - model_145_loss: 0.6901 - model_145_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4299 - model_144_loss: 0.4794 - model_145_loss: 0.6901 - model_145_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4364 - model_144_loss: 0.4791 - model_145_loss: 0.6912 - model_145_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9104 - model_145_loss: 0.6901 - model_145_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4292 - model_144_loss: 0.4793 - model_145_loss: 0.6902 - model_145_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4321 - model_144_loss: 0.4794 - model_145_loss: 0.6907 - model_145_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4281 - model_144_loss: 0.4817 - model_145_loss: 0.6903 - model_145_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4372 - model_144_loss: 0.4775 - model_145_loss: 0.6910 - model_145_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4366 - model_144_loss: 0.4815 - model_145_loss: 0.6918 - model_145_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9149 - model_145_loss: 0.6916 - model_145_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4372 - model_144_loss: 0.4803 - model_145_loss: 0.6916 - model_145_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4345 - model_144_loss: 0.4801 - model_145_loss: 0.6912 - model_145_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4350 - model_144_loss: 0.4800 - model_145_loss: 0.6914 - model_145_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4373 - model_144_loss: 0.4808 - model_145_loss: 0.6919 - model_145_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4384 - model_144_loss: 0.4811 - model_145_loss: 0.6920 - model_145_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9167 - model_145_loss: 0.6912 - model_145_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4338 - model_144_loss: 0.4797 - model_145_loss: 0.6913 - model_145_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4336 - model_144_loss: 0.4794 - model_145_loss: 0.6914 - model_145_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4345 - model_144_loss: 0.4794 - model_145_loss: 0.6918 - model_145_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4344 - model_144_loss: 0.4797 - model_145_loss: 0.6917 - model_145_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4388 - model_144_loss: 0.4770 - model_145_loss: 0.6920 - model_145_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9147 - model_145_loss: 0.6920 - model_145_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4335 - model_144_loss: 0.4762 - model_145_loss: 0.6911 - model_145_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4344 - model_144_loss: 0.4778 - model_145_loss: 0.6915 - model_145_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4358 - model_144_loss: 0.4765 - model_145_loss: 0.6915 - model_145_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4382 - model_144_loss: 0.4758 - model_145_loss: 0.6918 - model_145_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4399 - model_144_loss: 0.4737 - model_145_loss: 0.6917 - model_145_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9134 - model_145_loss: 0.6928 - model_145_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4347 - model_144_loss: 0.4757 - model_145_loss: 0.6913 - model_145_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4375 - model_144_loss: 0.4742 - model_145_loss: 0.6915 - model_145_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4401 - model_144_loss: 0.4730 - model_145_loss: 0.6918 - model_145_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4392 - model_144_loss: 0.4731 - model_145_loss: 0.6916 - model_145_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4380 - model_144_loss: 0.4736 - model_145_loss: 0.6916 - model_145_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9111 - model_145_loss: 0.6918 - model_145_1_loss: 0.69070s - loss: 6.9576 - model_145_loss: 0.7020 - model_145_1\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4415 - model_144_loss: 0.4737 - model_145_loss: 0.6922 - model_145_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4395 - model_144_loss: 0.4734 - model_145_loss: 0.6919 - model_145_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4425 - model_144_loss: 0.4719 - model_145_loss: 0.6919 - model_145_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4468 - model_144_loss: 0.4728 - model_145_loss: 0.6925 - model_145_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4418 - model_144_loss: 0.4745 - model_145_loss: 0.6920 - model_145_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9128 - model_145_loss: 0.6911 - model_145_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4434 - model_144_loss: 0.4721 - model_145_loss: 0.6922 - model_145_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4397 - model_144_loss: 0.4742 - model_145_loss: 0.6914 - model_145_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4445 - model_144_loss: 0.4738 - model_145_loss: 0.6921 - model_145_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4452 - model_144_loss: 0.4749 - model_145_loss: 0.6924 - model_145_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4445 - model_144_loss: 0.4752 - model_145_loss: 0.6922 - model_145_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9243 - model_145_loss: 0.6928 - model_145_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4432 - model_144_loss: 0.4762 - model_145_loss: 0.6921 - model_145_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4440 - model_144_loss: 0.4765 - model_145_loss: 0.6924 - model_145_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4456 - model_144_loss: 0.4756 - model_145_loss: 0.6924 - model_145_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4414 - model_144_loss: 0.4780 - model_145_loss: 0.6923 - model_145_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4475 - model_144_loss: 0.4749 - model_145_loss: 0.6927 - model_145_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9212 - model_145_loss: 0.6925 - model_145_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4460 - model_144_loss: 0.4765 - model_145_loss: 0.6928 - model_145_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4438 - model_144_loss: 0.4770 - model_145_loss: 0.6925 - model_145_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4427 - model_144_loss: 0.4771 - model_145_loss: 0.6923 - model_145_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4436 - model_144_loss: 0.4778 - model_145_loss: 0.6924 - model_145_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4441 - model_144_loss: 0.4762 - model_145_loss: 0.6924 - model_145_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9245 - model_145_loss: 0.6922 - model_145_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4461 - model_144_loss: 0.4755 - model_145_loss: 0.6923 - model_145_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4454 - model_144_loss: 0.4778 - model_145_loss: 0.6925 - model_145_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4482 - model_144_loss: 0.4752 - model_145_loss: 0.6926 - model_145_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4467 - model_144_loss: 0.4771 - model_145_loss: 0.6925 - model_145_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4460 - model_144_loss: 0.4768 - model_145_loss: 0.6927 - model_145_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9242 - model_145_loss: 0.6934 - model_145_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4471 - model_144_loss: 0.4769 - model_145_loss: 0.6927 - model_145_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4473 - model_144_loss: 0.4751 - model_145_loss: 0.6922 - model_145_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4482 - model_144_loss: 0.4755 - model_145_loss: 0.6924 - model_145_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4500 - model_144_loss: 0.4752 - model_145_loss: 0.6927 - model_145_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4493 - model_144_loss: 0.4752 - model_145_loss: 0.6926 - model_145_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9253 - model_145_loss: 0.6932 - model_145_1_loss: 0.69230s - loss: 6.9610 - model_145_loss: 0.6995 - model_145_1_los\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4513 - model_144_loss: 0.4746 - model_145_loss: 0.6930 - model_145_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4508 - model_144_loss: 0.4731 - model_145_loss: 0.6925 - model_145_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4486 - model_144_loss: 0.4736 - model_145_loss: 0.6924 - model_145_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4492 - model_144_loss: 0.4750 - model_145_loss: 0.6925 - model_145_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4514 - model_144_loss: 0.4740 - model_145_loss: 0.6928 - model_145_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9255 - model_145_loss: 0.6922 - model_145_1_loss: 0.69230s - loss: 6.9241 - model_145_loss: 0.6924 - model_145_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4455 - model_144_loss: 0.4742 - model_145_loss: 0.6921 - model_145_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4474 - model_144_loss: 0.4747 - model_145_loss: 0.6924 - model_145_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4518 - model_144_loss: 0.4701 - model_145_loss: 0.6925 - model_145_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4498 - model_144_loss: 0.4715 - model_145_loss: 0.6924 - model_145_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4506 - model_144_loss: 0.4724 - model_145_loss: 0.6927 - model_145_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9227 - model_145_loss: 0.6936 - model_145_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4474 - model_144_loss: 0.4725 - model_145_loss: 0.6923 - model_145_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4477 - model_144_loss: 0.4713 - model_145_loss: 0.6922 - model_145_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4486 - model_144_loss: 0.4715 - model_145_loss: 0.6923 - model_145_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4462 - model_144_loss: 0.4724 - model_145_loss: 0.6922 - model_145_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4511 - model_144_loss: 0.4718 - model_145_loss: 0.6926 - model_145_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9196 - model_145_loss: 0.6935 - model_145_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4481 - model_144_loss: 0.4692 - model_145_loss: 0.6924 - model_145_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4441 - model_144_loss: 0.4715 - model_145_loss: 0.6921 - model_145_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4484 - model_144_loss: 0.4707 - model_145_loss: 0.6924 - model_145_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4485 - model_144_loss: 0.4712 - model_145_loss: 0.6925 - model_145_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4480 - model_144_loss: 0.4706 - model_145_loss: 0.6921 - model_145_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9211 - model_145_loss: 0.6918 - model_145_1_loss: 0.69150s - loss: 6.9235 - model_145_loss: 0.6932 - model_145_1_loss: 0.691\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4484 - model_144_loss: 0.4713 - model_145_loss: 0.6926 - model_145_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4480 - model_144_loss: 0.4708 - model_145_loss: 0.6924 - model_145_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4470 - model_144_loss: 0.4712 - model_145_loss: 0.6924 - model_145_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4461 - model_144_loss: 0.4726 - model_145_loss: 0.6924 - model_145_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4479 - model_144_loss: 0.4714 - model_145_loss: 0.6924 - model_145_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9205 - model_145_loss: 0.6930 - model_145_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4479 - model_144_loss: 0.4703 - model_145_loss: 0.6925 - model_145_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4500 - model_144_loss: 0.4709 - model_145_loss: 0.6927 - model_145_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4498 - model_144_loss: 0.4716 - model_145_loss: 0.6928 - model_145_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4496 - model_144_loss: 0.4718 - model_145_loss: 0.6926 - model_145_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4495 - model_144_loss: 0.4717 - model_145_loss: 0.6925 - model_145_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9221 - model_145_loss: 0.6925 - model_145_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4484 - model_144_loss: 0.4713 - model_145_loss: 0.6922 - model_145_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4509 - model_144_loss: 0.4697 - model_145_loss: 0.6924 - model_145_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4478 - model_144_loss: 0.4723 - model_145_loss: 0.6924 - model_145_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4506 - model_144_loss: 0.4694 - model_145_loss: 0.6924 - model_145_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4491 - model_144_loss: 0.4712 - model_145_loss: 0.6923 - model_145_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.9216 - model_145_loss: 0.6924 - model_145_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4482 - model_144_loss: 0.4724 - model_145_loss: 0.6922 - model_145_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4477 - model_144_loss: 0.4703 - model_145_loss: 0.6922 - model_145_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4511 - model_144_loss: 0.4705 - model_145_loss: 0.6923 - model_145_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4532 - model_144_loss: 0.4688 - model_145_loss: 0.6925 - model_145_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4514 - model_144_loss: 0.4710 - model_145_loss: 0.6926 - model_145_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9225 - model_145_loss: 0.6923 - model_145_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4516 - model_144_loss: 0.4708 - model_145_loss: 0.6924 - model_145_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4522 - model_144_loss: 0.4690 - model_145_loss: 0.6921 - model_145_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4526 - model_144_loss: 0.4690 - model_145_loss: 0.6925 - model_145_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4510 - model_144_loss: 0.4694 - model_145_loss: 0.6922 - model_145_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4516 - model_144_loss: 0.4686 - model_145_loss: 0.6921 - model_145_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9232 - model_145_loss: 0.6927 - model_145_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4493 - model_144_loss: 0.4696 - model_145_loss: 0.6923 - model_145_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4540 - model_144_loss: 0.4685 - model_145_loss: 0.6926 - model_145_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4502 - model_144_loss: 0.4702 - model_145_loss: 0.6924 - model_145_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4537 - model_144_loss: 0.4698 - model_145_loss: 0.6928 - model_145_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4538 - model_144_loss: 0.4692 - model_145_loss: 0.6926 - model_145_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9252 - model_145_loss: 0.6929 - model_145_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4550 - model_144_loss: 0.4688 - model_145_loss: 0.6928 - model_145_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4554 - model_144_loss: 0.4690 - model_145_loss: 0.6926 - model_145_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4557 - model_144_loss: 0.4687 - model_145_loss: 0.6926 - model_145_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4512 - model_144_loss: 0.4693 - model_145_loss: 0.6921 - model_145_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4517 - model_144_loss: 0.4688 - model_145_loss: 0.6921 - model_145_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9225 - model_145_loss: 0.6926 - model_145_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4539 - model_144_loss: 0.4690 - model_145_loss: 0.6926 - model_145_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4547 - model_144_loss: 0.4702 - model_145_loss: 0.6928 - model_145_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4528 - model_144_loss: 0.4694 - model_145_loss: 0.6925 - model_145_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4494 - model_144_loss: 0.4699 - model_145_loss: 0.6920 - model_145_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4517 - model_144_loss: 0.4689 - model_145_loss: 0.6922 - model_145_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9239 - model_145_loss: 0.6919 - model_145_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4526 - model_144_loss: 0.4713 - model_145_loss: 0.6927 - model_145_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4542 - model_144_loss: 0.4696 - model_145_loss: 0.6925 - model_145_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4549 - model_144_loss: 0.4694 - model_145_loss: 0.6926 - model_145_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4541 - model_144_loss: 0.4689 - model_145_loss: 0.6924 - model_145_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4517 - model_144_loss: 0.4699 - model_145_loss: 0.6922 - model_145_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9223 - model_145_loss: 0.6925 - model_145_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4525 - model_144_loss: 0.4689 - model_145_loss: 0.6921 - model_145_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4508 - model_144_loss: 0.4673 - model_145_loss: 0.6919 - model_145_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4516 - model_144_loss: 0.4681 - model_145_loss: 0.6923 - model_145_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4528 - model_144_loss: 0.4670 - model_145_loss: 0.6922 - model_145_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4532 - model_144_loss: 0.4668 - model_145_loss: 0.6924 - model_145_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.9211 - model_145_loss: 0.6928 - model_145_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4529 - model_144_loss: 0.4662 - model_145_loss: 0.6920 - model_145_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4539 - model_144_loss: 0.4667 - model_145_loss: 0.6923 - model_145_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4529 - model_144_loss: 0.4670 - model_145_loss: 0.6923 - model_145_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4488 - model_144_loss: 0.4687 - model_145_loss: 0.6920 - model_145_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4543 - model_144_loss: 0.4662 - model_145_loss: 0.6924 - model_145_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9200 - model_145_loss: 0.6919 - model_145_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4532 - model_144_loss: 0.4654 - model_145_loss: 0.6922 - model_145_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4555 - model_144_loss: 0.4650 - model_145_loss: 0.6923 - model_145_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4533 - model_144_loss: 0.4659 - model_145_loss: 0.6921 - model_145_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4549 - model_144_loss: 0.4639 - model_145_loss: 0.6922 - model_145_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4543 - model_144_loss: 0.4647 - model_145_loss: 0.6922 - model_145_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 64us/sample - loss: 6.9181 - model_145_loss: 0.6915 - model_145_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4547 - model_144_loss: 0.4629 - model_145_loss: 0.6922 - model_145_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4522 - model_144_loss: 0.4646 - model_145_loss: 0.6919 - model_145_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4532 - model_144_loss: 0.4651 - model_145_loss: 0.6921 - model_145_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4554 - model_144_loss: 0.4630 - model_145_loss: 0.6923 - model_145_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4531 - model_144_loss: 0.4656 - model_145_loss: 0.6922 - model_145_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9218 - model_145_loss: 0.6926 - model_145_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4514 - model_144_loss: 0.4674 - model_145_loss: 0.6924 - model_145_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4524 - model_144_loss: 0.4663 - model_145_loss: 0.6922 - model_145_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4535 - model_144_loss: 0.4662 - model_145_loss: 0.6923 - model_145_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4531 - model_144_loss: 0.4673 - model_145_loss: 0.6923 - model_145_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4535 - model_144_loss: 0.4668 - model_145_loss: 0.6924 - model_145_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9233 - model_145_loss: 0.6920 - model_145_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4531 - model_144_loss: 0.4671 - model_145_loss: 0.6926 - model_145_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4559 - model_144_loss: 0.4670 - model_145_loss: 0.6927 - model_145_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4567 - model_144_loss: 0.4675 - model_145_loss: 0.6925 - model_145_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4568 - model_144_loss: 0.4675 - model_145_loss: 0.6928 - model_145_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4558 - model_144_loss: 0.4687 - model_145_loss: 0.6927 - model_145_1_loss: 0.6922\n",
      "For Attention Module: 3.8000000000000003\n",
      "features X: 30940 samples, 69 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.3765 - model_149_loss: 0.6619 - model_149_1_loss: 0.61430s - loss: 6.4300 - model_149_loss: 0.6692 - model_149_1_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: -5.9921 - model_148_loss: 0.3841 - model_149_loss: 0.6606 - model_149_1_loss: 0.6147\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.0046 - model_148_loss: 0.3842 - model_149_loss: 0.6613 - model_149_1_loss: 0.6165\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.0230 - model_148_loss: 0.3847 - model_149_loss: 0.6620 - model_149_1_loss: 0.6195\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.0306 - model_148_loss: 0.3867 - model_149_loss: 0.6622 - model_149_1_loss: 0.6212\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.0333 - model_148_loss: 0.3871 - model_149_loss: 0.6610 - model_149_1_loss: 0.6231\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 72us/sample - loss: 6.4284 - model_149_loss: 0.6624 - model_149_1_loss: 0.6226\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.0339 - model_148_loss: 0.3876 - model_149_loss: 0.6617 - model_149_1_loss: 0.6227\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.0413 - model_148_loss: 0.3901 - model_149_loss: 0.6631 - model_149_1_loss: 0.6231\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.0541 - model_148_loss: 0.3907 - model_149_loss: 0.6629 - model_149_1_loss: 0.6261\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.0695 - model_148_loss: 0.3926 - model_149_loss: 0.6646 - model_149_1_loss: 0.6279\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.0798 - model_148_loss: 0.3937 - model_149_loss: 0.6648 - model_149_1_loss: 0.6299\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.4775 - model_149_loss: 0.6648 - model_149_1_loss: 0.6301\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.0842 - model_148_loss: 0.3944 - model_149_loss: 0.6656 - model_149_1_loss: 0.6301\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.0964 - model_148_loss: 0.3956 - model_149_loss: 0.6658 - model_149_1_loss: 0.6326\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.1095 - model_148_loss: 0.3972 - model_149_loss: 0.6663 - model_149_1_loss: 0.6351\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.1075 - model_148_loss: 0.4003 - model_149_loss: 0.6663 - model_149_1_loss: 0.6353\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1253 - model_148_loss: 0.4017 - model_149_loss: 0.6676 - model_149_1_loss: 0.6378\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.5374 - model_149_loss: 0.6679 - model_149_1_loss: 0.6398\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.1401 - model_148_loss: 0.4006 - model_149_loss: 0.6684 - model_149_1_loss: 0.6397\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.1467 - model_148_loss: 0.4020 - model_149_loss: 0.6689 - model_149_1_loss: 0.6409\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.1613 - model_148_loss: 0.4040 - model_149_loss: 0.6704 - model_149_1_loss: 0.6427\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.1669 - model_148_loss: 0.4051 - model_149_loss: 0.6698 - model_149_1_loss: 0.6446\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.1804 - model_148_loss: 0.4050 - model_149_loss: 0.6703 - model_149_1_loss: 0.6468\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.5996 - model_149_loss: 0.6707 - model_149_1_loss: 0.6487\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1892 - model_148_loss: 0.4076 - model_149_loss: 0.6709 - model_149_1_loss: 0.6484\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.1948 - model_148_loss: 0.4088 - model_149_loss: 0.6719 - model_149_1_loss: 0.6489\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.2185 - model_148_loss: 0.4092 - model_149_loss: 0.6732 - model_149_1_loss: 0.6523\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.2348 - model_148_loss: 0.4111 - model_149_loss: 0.6750 - model_149_1_loss: 0.6542\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.2336 - model_148_loss: 0.4124 - model_149_loss: 0.6736 - model_149_1_loss: 0.6556\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.6691 - model_149_loss: 0.6753 - model_149_1_loss: 0.6581\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.2522 - model_148_loss: 0.4136 - model_149_loss: 0.6752 - model_149_1_loss: 0.6580\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.2530 - model_148_loss: 0.4169 - model_149_loss: 0.6757 - model_149_1_loss: 0.6583\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.2651 - model_148_loss: 0.4173 - model_149_loss: 0.6760 - model_149_1_loss: 0.6605\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.2917 - model_148_loss: 0.4186 - model_149_loss: 0.6780 - model_149_1_loss: 0.6640\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.2979 - model_148_loss: 0.4217 - model_149_loss: 0.6782 - model_149_1_loss: 0.6657\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.7293 - model_149_loss: 0.6786 - model_149_1_loss: 0.66670s - loss: 6.7581 - model_149_loss: 0.6847 - model_149_1_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.2971 - model_148_loss: 0.4237 - model_149_loss: 0.6777 - model_149_1_loss: 0.6665\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3143 - model_148_loss: 0.4256 - model_149_loss: 0.6799 - model_149_1_loss: 0.6681\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3176 - model_148_loss: 0.4283 - model_149_loss: 0.6805 - model_149_1_loss: 0.6687\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3276 - model_148_loss: 0.4305 - model_149_loss: 0.6809 - model_149_1_loss: 0.6708\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3397 - model_148_loss: 0.4347 - model_149_loss: 0.6819 - model_149_1_loss: 0.6730\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.7782 - model_149_loss: 0.6825 - model_149_1_loss: 0.6735\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3422 - model_148_loss: 0.4346 - model_149_loss: 0.6820 - model_149_1_loss: 0.6733\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3544 - model_148_loss: 0.4387 - model_149_loss: 0.6834 - model_149_1_loss: 0.6752\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.3584 - model_148_loss: 0.4428 - model_149_loss: 0.6832 - model_149_1_loss: 0.6770\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3708 - model_148_loss: 0.4431 - model_149_loss: 0.6848 - model_149_1_loss: 0.6780\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3712 - model_148_loss: 0.4488 - model_149_loss: 0.6844 - model_149_1_loss: 0.6796\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.8326 - model_149_loss: 0.6859 - model_149_1_loss: 0.6809\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3750 - model_148_loss: 0.4492 - model_149_loss: 0.6845 - model_149_1_loss: 0.6803\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3759 - model_148_loss: 0.4536 - model_149_loss: 0.6855 - model_149_1_loss: 0.6804\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.3882 - model_148_loss: 0.4554 - model_149_loss: 0.6870 - model_149_1_loss: 0.6817\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3933 - model_148_loss: 0.4579 - model_149_loss: 0.6871 - model_149_1_loss: 0.6832\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3979 - model_148_loss: 0.4623 - model_149_loss: 0.6881 - model_149_1_loss: 0.6840\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.8643 - model_149_loss: 0.6884 - model_149_1_loss: 0.6843\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4029 - model_148_loss: 0.4617 - model_149_loss: 0.6887 - model_149_1_loss: 0.6842\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3984 - model_148_loss: 0.4664 - model_149_loss: 0.6882 - model_149_1_loss: 0.6848\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3997 - model_148_loss: 0.4698 - model_149_loss: 0.6884 - model_149_1_loss: 0.6855\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3985 - model_148_loss: 0.4710 - model_149_loss: 0.6883 - model_149_1_loss: 0.6856\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4096 - model_148_loss: 0.4736 - model_149_loss: 0.6895 - model_149_1_loss: 0.6871\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: 6.8837 - model_149_loss: 0.6894 - model_149_1_loss: 0.6871\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4059 - model_148_loss: 0.4759 - model_149_loss: 0.6897 - model_149_1_loss: 0.6866\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4115 - model_148_loss: 0.4790 - model_149_loss: 0.6905 - model_149_1_loss: 0.6876\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4088 - model_148_loss: 0.4791 - model_149_loss: 0.6901 - model_149_1_loss: 0.6875\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4145 - model_148_loss: 0.4801 - model_149_loss: 0.6910 - model_149_1_loss: 0.6879\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4141 - model_148_loss: 0.4819 - model_149_loss: 0.6909 - model_149_1_loss: 0.6883\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 64us/sample - loss: 6.9020 - model_149_loss: 0.6923 - model_149_1_loss: 0.6887\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4129 - model_148_loss: 0.4826 - model_149_loss: 0.6911 - model_149_1_loss: 0.6880\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4148 - model_148_loss: 0.4847 - model_149_loss: 0.6916 - model_149_1_loss: 0.6883\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4165 - model_148_loss: 0.4870 - model_149_loss: 0.6918 - model_149_1_loss: 0.6890\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4179 - model_148_loss: 0.4843 - model_149_loss: 0.6915 - model_149_1_loss: 0.6890\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4191 - model_148_loss: 0.4850 - model_149_loss: 0.6917 - model_149_1_loss: 0.6891\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 71us/sample - loss: 6.9082 - model_149_loss: 0.6937 - model_149_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4161 - model_148_loss: 0.4849 - model_149_loss: 0.6910 - model_149_1_loss: 0.6892\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4236 - model_148_loss: 0.4867 - model_149_loss: 0.6925 - model_149_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4248 - model_148_loss: 0.4850 - model_149_loss: 0.6922 - model_149_1_loss: 0.6898\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4242 - model_148_loss: 0.4863 - model_149_loss: 0.6926 - model_149_1_loss: 0.6895\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4256 - model_148_loss: 0.4836 - model_149_loss: 0.6924 - model_149_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 70us/sample - loss: 6.9116 - model_149_loss: 0.6925 - model_149_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4259 - model_148_loss: 0.4833 - model_149_loss: 0.6923 - model_149_1_loss: 0.6895\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4304 - model_148_loss: 0.4821 - model_149_loss: 0.6928 - model_149_1_loss: 0.6897\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4311 - model_148_loss: 0.4842 - model_149_loss: 0.6929 - model_149_1_loss: 0.6901\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4364 - model_148_loss: 0.4789 - model_149_loss: 0.6929 - model_149_1_loss: 0.6902\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4373 - model_148_loss: 0.4795 - model_149_loss: 0.6931 - model_149_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9178 - model_149_loss: 0.6931 - model_149_1_loss: 0.690 - 2s 71us/sample - loss: 6.9164 - model_149_loss: 0.6928 - model_149_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4345 - model_148_loss: 0.4790 - model_149_loss: 0.6927 - model_149_1_loss: 0.6900\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4324 - model_148_loss: 0.4817 - model_149_loss: 0.6927 - model_149_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4351 - model_148_loss: 0.4790 - model_149_loss: 0.6926 - model_149_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4364 - model_148_loss: 0.4785 - model_149_loss: 0.6927 - model_149_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4429 - model_148_loss: 0.4781 - model_149_loss: 0.6931 - model_149_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.9180 - model_149_loss: 0.6926 - model_149_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4354 - model_148_loss: 0.4821 - model_149_loss: 0.6926 - model_149_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4371 - model_148_loss: 0.4816 - model_149_loss: 0.6929 - model_149_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4402 - model_148_loss: 0.4793 - model_149_loss: 0.6929 - model_149_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4427 - model_148_loss: 0.4793 - model_149_loss: 0.6932 - model_149_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4432 - model_148_loss: 0.4798 - model_149_loss: 0.6932 - model_149_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 78us/sample - loss: 6.9246 - model_149_loss: 0.6940 - model_149_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4401 - model_148_loss: 0.4788 - model_149_loss: 0.6929 - model_149_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4369 - model_148_loss: 0.4813 - model_149_loss: 0.6925 - model_149_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4422 - model_148_loss: 0.4838 - model_149_loss: 0.6934 - model_149_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4388 - model_148_loss: 0.4816 - model_149_loss: 0.6929 - model_149_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4403 - model_148_loss: 0.4814 - model_149_loss: 0.6929 - model_149_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9228 - model_149_loss: 0.6930 - model_149_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4377 - model_148_loss: 0.4819 - model_149_loss: 0.6925 - model_149_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4373 - model_148_loss: 0.4823 - model_149_loss: 0.6924 - model_149_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4374 - model_148_loss: 0.4822 - model_149_loss: 0.6925 - model_149_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4362 - model_148_loss: 0.4833 - model_149_loss: 0.6925 - model_149_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4394 - model_148_loss: 0.4816 - model_149_loss: 0.6925 - model_149_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9178 - model_149_loss: 0.6920 - model_149_1_loss: 0.69130s - loss: 6.8532 - model_149_loss: 0.6805 - mode\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4373 - model_148_loss: 0.4811 - model_149_loss: 0.6923 - model_149_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4363 - model_148_loss: 0.4804 - model_149_loss: 0.6919 - model_149_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4396 - model_148_loss: 0.4809 - model_149_loss: 0.6921 - model_149_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4373 - model_148_loss: 0.4809 - model_149_loss: 0.6920 - model_149_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4401 - model_148_loss: 0.4790 - model_149_loss: 0.6923 - model_149_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9149 - model_149_loss: 0.6920 - model_149_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4388 - model_148_loss: 0.4768 - model_149_loss: 0.6920 - model_149_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4410 - model_148_loss: 0.4759 - model_149_loss: 0.6920 - model_149_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4391 - model_148_loss: 0.4778 - model_149_loss: 0.6920 - model_149_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4379 - model_148_loss: 0.4770 - model_149_loss: 0.6919 - model_149_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4420 - model_148_loss: 0.4766 - model_149_loss: 0.6923 - model_149_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9174 - model_149_loss: 0.6921 - model_149_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4414 - model_148_loss: 0.4755 - model_149_loss: 0.6920 - model_149_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4384 - model_148_loss: 0.4776 - model_149_loss: 0.6917 - model_149_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4439 - model_148_loss: 0.4757 - model_149_loss: 0.6921 - model_149_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4425 - model_148_loss: 0.4775 - model_149_loss: 0.6922 - model_149_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4434 - model_148_loss: 0.4762 - model_149_loss: 0.6921 - model_149_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9227 - model_149_loss: 0.6924 - model_149_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4436 - model_148_loss: 0.4771 - model_149_loss: 0.6922 - model_149_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4404 - model_148_loss: 0.4793 - model_149_loss: 0.6922 - model_149_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4401 - model_148_loss: 0.4775 - model_149_loss: 0.6916 - model_149_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4441 - model_148_loss: 0.4783 - model_149_loss: 0.6924 - model_149_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4471 - model_148_loss: 0.4786 - model_149_loss: 0.6927 - model_149_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 95us/sample - loss: 6.9204 - model_149_loss: 0.6921 - model_149_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4441 - model_148_loss: 0.4781 - model_149_loss: 0.6922 - model_149_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4451 - model_148_loss: 0.4771 - model_149_loss: 0.6923 - model_149_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4449 - model_148_loss: 0.4758 - model_149_loss: 0.6921 - model_149_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4416 - model_148_loss: 0.4788 - model_149_loss: 0.6921 - model_149_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4442 - model_148_loss: 0.4776 - model_149_loss: 0.6923 - model_149_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9236 - model_149_loss: 0.6928 - model_149_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4475 - model_148_loss: 0.4768 - model_149_loss: 0.6927 - model_149_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4462 - model_148_loss: 0.4769 - model_149_loss: 0.6928 - model_149_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4439 - model_148_loss: 0.4778 - model_149_loss: 0.6924 - model_149_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4428 - model_148_loss: 0.4776 - model_149_loss: 0.6921 - model_149_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4471 - model_148_loss: 0.4764 - model_149_loss: 0.6926 - model_149_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9220 - model_149_loss: 0.6926 - model_149_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4444 - model_148_loss: 0.4757 - model_149_loss: 0.6925 - model_149_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4448 - model_148_loss: 0.4754 - model_149_loss: 0.6925 - model_149_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4448 - model_148_loss: 0.4752 - model_149_loss: 0.6923 - model_149_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4444 - model_148_loss: 0.4754 - model_149_loss: 0.6926 - model_149_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4452 - model_148_loss: 0.4749 - model_149_loss: 0.6927 - model_149_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9178 - model_149_loss: 0.6921 - model_149_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4460 - model_148_loss: 0.4728 - model_149_loss: 0.6925 - model_149_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4476 - model_148_loss: 0.4724 - model_149_loss: 0.6925 - model_149_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4494 - model_148_loss: 0.4720 - model_149_loss: 0.6927 - model_149_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4429 - model_148_loss: 0.4740 - model_149_loss: 0.6921 - model_149_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4458 - model_148_loss: 0.4723 - model_149_loss: 0.6922 - model_149_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9178 - model_149_loss: 0.6920 - model_149_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4455 - model_148_loss: 0.4702 - model_149_loss: 0.6921 - model_149_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4456 - model_148_loss: 0.4720 - model_149_loss: 0.6923 - model_149_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4451 - model_148_loss: 0.4730 - model_149_loss: 0.6922 - model_149_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4466 - model_148_loss: 0.4724 - model_149_loss: 0.6922 - model_149_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4475 - model_148_loss: 0.4716 - model_149_loss: 0.6925 - model_149_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: 6.9187 - model_149_loss: 0.6917 - model_149_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4464 - model_148_loss: 0.4716 - model_149_loss: 0.6919 - model_149_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4474 - model_148_loss: 0.4703 - model_149_loss: 0.6920 - model_149_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4454 - model_148_loss: 0.4725 - model_149_loss: 0.6919 - model_149_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4469 - model_148_loss: 0.4724 - model_149_loss: 0.6922 - model_149_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4478 - model_148_loss: 0.4717 - model_149_loss: 0.6920 - model_149_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9175 - model_149_loss: 0.6928 - model_149_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4460 - model_148_loss: 0.4708 - model_149_loss: 0.6917 - model_149_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4484 - model_148_loss: 0.4703 - model_149_loss: 0.6921 - model_149_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4470 - model_148_loss: 0.4713 - model_149_loss: 0.6918 - model_149_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4506 - model_148_loss: 0.4708 - model_149_loss: 0.6922 - model_149_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4524 - model_148_loss: 0.4696 - model_149_loss: 0.6924 - model_149_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9200 - model_149_loss: 0.6920 - model_149_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4474 - model_148_loss: 0.4717 - model_149_loss: 0.6920 - model_149_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4491 - model_148_loss: 0.4700 - model_149_loss: 0.6923 - model_149_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4482 - model_148_loss: 0.4722 - model_149_loss: 0.6925 - model_149_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4502 - model_148_loss: 0.4690 - model_149_loss: 0.6919 - model_149_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4478 - model_148_loss: 0.4717 - model_149_loss: 0.6920 - model_149_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: 6.9216 - model_149_loss: 0.6925 - model_149_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4474 - model_148_loss: 0.4714 - model_149_loss: 0.6918 - model_149_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4495 - model_148_loss: 0.4712 - model_149_loss: 0.6922 - model_149_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4464 - model_148_loss: 0.4702 - model_149_loss: 0.6917 - model_149_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4477 - model_148_loss: 0.4696 - model_149_loss: 0.6919 - model_149_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4514 - model_148_loss: 0.4697 - model_149_loss: 0.6923 - model_149_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9203 - model_149_loss: 0.6917 - model_149_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4497 - model_148_loss: 0.4706 - model_149_loss: 0.6925 - model_149_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4503 - model_148_loss: 0.4701 - model_149_loss: 0.6923 - model_149_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4483 - model_148_loss: 0.4710 - model_149_loss: 0.6923 - model_149_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4487 - model_148_loss: 0.4701 - model_149_loss: 0.6923 - model_149_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4504 - model_148_loss: 0.4685 - model_149_loss: 0.6922 - model_149_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9192 - model_149_loss: 0.6919 - model_149_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4520 - model_148_loss: 0.4672 - model_149_loss: 0.6923 - model_149_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4479 - model_148_loss: 0.4694 - model_149_loss: 0.6919 - model_149_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4495 - model_148_loss: 0.4667 - model_149_loss: 0.6920 - model_149_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4498 - model_148_loss: 0.4678 - model_149_loss: 0.6922 - model_149_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4511 - model_148_loss: 0.4660 - model_149_loss: 0.6923 - model_149_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 51us/sample - loss: 6.9187 - model_149_loss: 0.6921 - model_149_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4524 - model_148_loss: 0.4649 - model_149_loss: 0.6922 - model_149_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4518 - model_148_loss: 0.4650 - model_149_loss: 0.6922 - model_149_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4535 - model_148_loss: 0.4659 - model_149_loss: 0.6923 - model_149_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4552 - model_148_loss: 0.4656 - model_149_loss: 0.6926 - model_149_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4545 - model_148_loss: 0.4655 - model_149_loss: 0.6923 - model_149_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9196 - model_149_loss: 0.6922 - model_149_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4544 - model_148_loss: 0.4665 - model_149_loss: 0.6926 - model_149_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4515 - model_148_loss: 0.4665 - model_149_loss: 0.6921 - model_149_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4559 - model_148_loss: 0.4658 - model_149_loss: 0.6923 - model_149_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4509 - model_148_loss: 0.4673 - model_149_loss: 0.6920 - model_149_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4539 - model_148_loss: 0.4665 - model_149_loss: 0.6923 - model_149_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9226 - model_149_loss: 0.6920 - model_149_1_loss: 0.69180s - loss: 6.9207 - model_149_loss: 0.6921 - model_149_1\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4542 - model_148_loss: 0.4671 - model_149_loss: 0.6924 - model_149_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4551 - model_148_loss: 0.4677 - model_149_loss: 0.6925 - model_149_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4503 - model_148_loss: 0.4692 - model_149_loss: 0.6921 - model_149_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4523 - model_148_loss: 0.4673 - model_149_loss: 0.6921 - model_149_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4531 - model_148_loss: 0.4693 - model_149_loss: 0.6924 - model_149_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9222 - model_149_loss: 0.6929 - model_149_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4527 - model_148_loss: 0.4696 - model_149_loss: 0.6923 - model_149_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4501 - model_148_loss: 0.4710 - model_149_loss: 0.6922 - model_149_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4512 - model_148_loss: 0.4694 - model_149_loss: 0.6921 - model_149_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4521 - model_148_loss: 0.4700 - model_149_loss: 0.6924 - model_149_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4526 - model_148_loss: 0.4699 - model_149_loss: 0.6925 - model_149_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9236 - model_149_loss: 0.6919 - model_149_1_loss: 0.69220s - loss: 6.9529 - model_149_loss: 0.6977 - model_149_1_l\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4538 - model_148_loss: 0.4686 - model_149_loss: 0.6921 - model_149_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4559 - model_148_loss: 0.4686 - model_149_loss: 0.6927 - model_149_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4541 - model_148_loss: 0.4692 - model_149_loss: 0.6925 - model_149_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4538 - model_148_loss: 0.4715 - model_149_loss: 0.6926 - model_149_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4570 - model_148_loss: 0.4693 - model_149_loss: 0.6927 - model_149_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9260 - model_149_loss: 0.6930 - model_149_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4556 - model_148_loss: 0.4686 - model_149_loss: 0.6927 - model_149_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4524 - model_148_loss: 0.4716 - model_149_loss: 0.6927 - model_149_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4538 - model_148_loss: 0.4717 - model_149_loss: 0.6928 - model_149_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4525 - model_148_loss: 0.4701 - model_149_loss: 0.6925 - model_149_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4511 - model_148_loss: 0.4721 - model_149_loss: 0.6926 - model_149_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9268 - model_149_loss: 0.6925 - model_149_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4542 - model_148_loss: 0.4719 - model_149_loss: 0.6930 - model_149_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4528 - model_148_loss: 0.4717 - model_149_loss: 0.6927 - model_149_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4577 - model_148_loss: 0.4699 - model_149_loss: 0.6931 - model_149_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4539 - model_148_loss: 0.4710 - model_149_loss: 0.6928 - model_149_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4529 - model_148_loss: 0.4710 - model_149_loss: 0.6927 - model_149_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9239 - model_149_loss: 0.6935 - model_149_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4569 - model_148_loss: 0.4683 - model_149_loss: 0.6927 - model_149_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4523 - model_148_loss: 0.4704 - model_149_loss: 0.6927 - model_149_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4535 - model_148_loss: 0.4695 - model_149_loss: 0.6927 - model_149_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4566 - model_148_loss: 0.4680 - model_149_loss: 0.6927 - model_149_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4541 - model_148_loss: 0.4691 - model_149_loss: 0.6928 - model_149_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9231 - model_149_loss: 0.6933 - model_149_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4523 - model_148_loss: 0.4684 - model_149_loss: 0.6926 - model_149_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4526 - model_148_loss: 0.4690 - model_149_loss: 0.6929 - model_149_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4528 - model_148_loss: 0.4681 - model_149_loss: 0.6925 - model_149_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4544 - model_148_loss: 0.4673 - model_149_loss: 0.6926 - model_149_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4547 - model_148_loss: 0.4689 - model_149_loss: 0.6928 - model_149_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9219 - model_149_loss: 0.6930 - model_149_1_loss: 0.69191s - loss: 7.0270 - model_149_loss: 0.7071 - m\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4551 - model_148_loss: 0.4661 - model_149_loss: 0.6923 - model_149_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4537 - model_148_loss: 0.4661 - model_149_loss: 0.6925 - model_149_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4579 - model_148_loss: 0.4666 - model_149_loss: 0.6929 - model_149_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4562 - model_148_loss: 0.4650 - model_149_loss: 0.6926 - model_149_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4526 - model_148_loss: 0.4671 - model_149_loss: 0.6922 - model_149_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9243 - model_149_loss: 0.6928 - model_149_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4571 - model_148_loss: 0.4646 - model_149_loss: 0.6927 - model_149_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4577 - model_148_loss: 0.4640 - model_149_loss: 0.6925 - model_149_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4547 - model_148_loss: 0.4651 - model_149_loss: 0.6924 - model_149_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4576 - model_148_loss: 0.4654 - model_149_loss: 0.6926 - model_149_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4550 - model_148_loss: 0.4673 - model_149_loss: 0.6925 - model_149_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9252 - model_149_loss: 0.6935 - model_149_1_loss: 0.691 - 1s 51us/sample - loss: 6.9223 - model_149_loss: 0.6918 - model_149_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4589 - model_148_loss: 0.4635 - model_149_loss: 0.6928 - model_149_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4556 - model_148_loss: 0.4661 - model_149_loss: 0.6926 - model_149_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4560 - model_148_loss: 0.4659 - model_149_loss: 0.6925 - model_149_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4548 - model_148_loss: 0.4663 - model_149_loss: 0.6924 - model_149_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4552 - model_148_loss: 0.4655 - model_149_loss: 0.6924 - model_149_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9217 - model_149_loss: 0.6927 - model_149_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4541 - model_148_loss: 0.4654 - model_149_loss: 0.6926 - model_149_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4561 - model_148_loss: 0.4634 - model_149_loss: 0.6925 - model_149_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4556 - model_148_loss: 0.4642 - model_149_loss: 0.6925 - model_149_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4530 - model_148_loss: 0.4658 - model_149_loss: 0.6924 - model_149_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4542 - model_148_loss: 0.4651 - model_149_loss: 0.6925 - model_149_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9192 - model_149_loss: 0.6925 - model_149_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4547 - model_148_loss: 0.4634 - model_149_loss: 0.6926 - model_149_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4555 - model_148_loss: 0.4623 - model_149_loss: 0.6926 - model_149_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4544 - model_148_loss: 0.4632 - model_149_loss: 0.6924 - model_149_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4544 - model_148_loss: 0.4633 - model_149_loss: 0.6925 - model_149_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4554 - model_148_loss: 0.4616 - model_149_loss: 0.6925 - model_149_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9172 - model_149_loss: 0.6918 - model_149_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4542 - model_148_loss: 0.4628 - model_149_loss: 0.6926 - model_149_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4519 - model_148_loss: 0.4621 - model_149_loss: 0.6922 - model_149_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4546 - model_148_loss: 0.4597 - model_149_loss: 0.6921 - model_149_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4542 - model_148_loss: 0.4614 - model_149_loss: 0.6922 - model_149_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4556 - model_148_loss: 0.4591 - model_149_loss: 0.6924 - model_149_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9147 - model_149_loss: 0.6917 - model_149_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4552 - model_148_loss: 0.4597 - model_149_loss: 0.6921 - model_149_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4546 - model_148_loss: 0.4598 - model_149_loss: 0.6923 - model_149_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4550 - model_148_loss: 0.4597 - model_149_loss: 0.6921 - model_149_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4572 - model_148_loss: 0.4590 - model_149_loss: 0.6924 - model_149_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4544 - model_148_loss: 0.4601 - model_149_loss: 0.6921 - model_149_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9171 - model_149_loss: 0.6924 - model_149_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4537 - model_148_loss: 0.4619 - model_149_loss: 0.6919 - model_149_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4578 - model_148_loss: 0.4603 - model_149_loss: 0.6923 - model_149_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4583 - model_148_loss: 0.4614 - model_149_loss: 0.6927 - model_149_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4585 - model_148_loss: 0.4604 - model_149_loss: 0.6924 - model_149_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4586 - model_148_loss: 0.4611 - model_149_loss: 0.6923 - model_149_1_loss: 0.6916\n",
      "For Attention Module: 3.9000000000000004\n",
      "features X: 30940 samples, 64 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.8191 - model_153_loss: 0.6745 - model_153_1_loss: 0.6894\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: -6.3354 - model_152_loss: 0.4834 - model_153_loss: 0.6745 - model_153_1_loss: 0.6893\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3437 - model_152_loss: 0.4850 - model_153_loss: 0.6765 - model_153_1_loss: 0.6893\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.3404 - model_152_loss: 0.4838 - model_153_loss: 0.6753 - model_153_1_loss: 0.6895\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3353 - model_152_loss: 0.4845 - model_153_loss: 0.6743 - model_153_1_loss: 0.6896\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3425 - model_152_loss: 0.4850 - model_153_loss: 0.6759 - model_153_1_loss: 0.6896\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.8252 - model_153_loss: 0.6752 - model_153_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3471 - model_152_loss: 0.4844 - model_153_loss: 0.6767 - model_153_1_loss: 0.6896\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3460 - model_152_loss: 0.4840 - model_153_loss: 0.6762 - model_153_1_loss: 0.6899\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3543 - model_152_loss: 0.4828 - model_153_loss: 0.6775 - model_153_1_loss: 0.6899\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.3556 - model_152_loss: 0.4827 - model_153_loss: 0.6779 - model_153_1_loss: 0.6898\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.3601 - model_152_loss: 0.4836 - model_153_loss: 0.6786 - model_153_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.8418 - model_153_loss: 0.6782 - model_153_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3542 - model_152_loss: 0.4844 - model_153_loss: 0.6777 - model_153_1_loss: 0.6901\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3525 - model_152_loss: 0.4849 - model_153_loss: 0.6774 - model_153_1_loss: 0.6900\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3508 - model_152_loss: 0.4850 - model_153_loss: 0.6770 - model_153_1_loss: 0.6902\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.3643 - model_152_loss: 0.4836 - model_153_loss: 0.6792 - model_153_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3603 - model_152_loss: 0.4843 - model_153_loss: 0.6784 - model_153_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 63us/sample - loss: 6.8508 - model_153_loss: 0.6793 - model_153_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3628 - model_152_loss: 0.4858 - model_153_loss: 0.6792 - model_153_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3631 - model_152_loss: 0.4860 - model_153_loss: 0.6792 - model_153_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3634 - model_152_loss: 0.4865 - model_153_loss: 0.6794 - model_153_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3598 - model_152_loss: 0.4873 - model_153_loss: 0.6789 - model_153_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3678 - model_152_loss: 0.4878 - model_153_loss: 0.6804 - model_153_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.8601 - model_153_loss: 0.6804 - model_153_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3706 - model_152_loss: 0.4868 - model_153_loss: 0.6806 - model_153_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3749 - model_152_loss: 0.4867 - model_153_loss: 0.6815 - model_153_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3710 - model_152_loss: 0.4870 - model_153_loss: 0.6805 - model_153_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3825 - model_152_loss: 0.4863 - model_153_loss: 0.6825 - model_153_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.3837 - model_152_loss: 0.4868 - model_153_loss: 0.6830 - model_153_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: 6.8708 - model_153_loss: 0.6823 - model_153_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3789 - model_152_loss: 0.4880 - model_153_loss: 0.6822 - model_153_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3822 - model_152_loss: 0.4876 - model_153_loss: 0.6828 - model_153_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3879 - model_152_loss: 0.4887 - model_153_loss: 0.6839 - model_153_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3872 - model_152_loss: 0.4892 - model_153_loss: 0.6838 - model_153_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3889 - model_152_loss: 0.4879 - model_153_loss: 0.6838 - model_153_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 62us/sample - loss: 6.8804 - model_153_loss: 0.6847 - model_153_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3870 - model_152_loss: 0.4901 - model_153_loss: 0.6841 - model_153_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3868 - model_152_loss: 0.4910 - model_153_loss: 0.6840 - model_153_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3974 - model_152_loss: 0.4918 - model_153_loss: 0.6862 - model_153_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.3929 - model_152_loss: 0.4928 - model_153_loss: 0.6855 - model_153_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.3997 - model_152_loss: 0.4921 - model_153_loss: 0.6866 - model_153_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.8994 - model_153_loss: 0.6876 - model_153_1_loss: 0.69180s - loss: 6.8784 - model_153_loss: 0.6848 - model_153_1_loss:\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4065 - model_152_loss: 0.4931 - model_153_loss: 0.6879 - model_153_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4035 - model_152_loss: 0.4931 - model_153_loss: 0.6873 - model_153_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4069 - model_152_loss: 0.4939 - model_153_loss: 0.6880 - model_153_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4079 - model_152_loss: 0.4937 - model_153_loss: 0.6882 - model_153_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4160 - model_152_loss: 0.4935 - model_153_loss: 0.6898 - model_153_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9110 - model_153_loss: 0.6904 - model_153_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4066 - model_152_loss: 0.4953 - model_153_loss: 0.6882 - model_153_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4166 - model_152_loss: 0.4943 - model_153_loss: 0.6899 - model_153_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4155 - model_152_loss: 0.4939 - model_153_loss: 0.6899 - model_153_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4183 - model_152_loss: 0.4935 - model_153_loss: 0.6901 - model_153_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4239 - model_152_loss: 0.4968 - model_153_loss: 0.6916 - model_153_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9186 - model_153_loss: 0.6913 - model_153_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4258 - model_152_loss: 0.4960 - model_153_loss: 0.6919 - model_153_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4234 - model_152_loss: 0.4970 - model_153_loss: 0.6915 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4257 - model_152_loss: 0.4980 - model_153_loss: 0.6921 - model_153_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4261 - model_152_loss: 0.4988 - model_153_loss: 0.6924 - model_153_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4280 - model_152_loss: 0.4999 - model_153_loss: 0.6929 - model_153_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 111us/sample - loss: 6.9283 - model_153_loss: 0.6927 - model_153_1_loss: 0.6930\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4247 - model_152_loss: 0.4995 - model_153_loss: 0.6922 - model_153_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4263 - model_152_loss: 0.5001 - model_153_loss: 0.6925 - model_153_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4250 - model_152_loss: 0.5013 - model_153_loss: 0.6925 - model_153_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4278 - model_152_loss: 0.5005 - model_153_loss: 0.6928 - model_153_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4278 - model_152_loss: 0.5029 - model_153_loss: 0.6933 - model_153_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: 6.9295 - model_153_loss: 0.6933 - model_153_1_loss: 0.6931\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4237 - model_152_loss: 0.5023 - model_153_loss: 0.6924 - model_153_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4250 - model_152_loss: 0.5026 - model_153_loss: 0.6926 - model_153_1_loss: 0.6929\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4250 - model_152_loss: 0.5023 - model_153_loss: 0.6926 - model_153_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4258 - model_152_loss: 0.5030 - model_153_loss: 0.6928 - model_153_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4245 - model_152_loss: 0.5024 - model_153_loss: 0.6926 - model_153_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9266 - model_153_loss: 0.6922 - model_153_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4228 - model_152_loss: 0.5027 - model_153_loss: 0.6922 - model_153_1_loss: 0.6929\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4224 - model_152_loss: 0.5033 - model_153_loss: 0.6921 - model_153_1_loss: 0.6930\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4225 - model_152_loss: 0.5034 - model_153_loss: 0.6923 - model_153_1_loss: 0.6929\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4220 - model_152_loss: 0.5039 - model_153_loss: 0.6923 - model_153_1_loss: 0.6929\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4220 - model_152_loss: 0.5037 - model_153_loss: 0.6923 - model_153_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9277 - model_153_loss: 0.6924 - model_153_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4212 - model_152_loss: 0.5042 - model_153_loss: 0.6923 - model_153_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4222 - model_152_loss: 0.5028 - model_153_loss: 0.6922 - model_153_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4207 - model_152_loss: 0.5034 - model_153_loss: 0.6921 - model_153_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4217 - model_152_loss: 0.5030 - model_153_loss: 0.6922 - model_153_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4200 - model_152_loss: 0.5036 - model_153_loss: 0.6920 - model_153_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 68us/sample - loss: 6.9256 - model_153_loss: 0.6933 - model_153_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4216 - model_152_loss: 0.5017 - model_153_loss: 0.6918 - model_153_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4246 - model_152_loss: 0.5006 - model_153_loss: 0.6922 - model_153_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4238 - model_152_loss: 0.5006 - model_153_loss: 0.6921 - model_153_1_loss: 0.6928\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4224 - model_152_loss: 0.5014 - model_153_loss: 0.6920 - model_153_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4258 - model_152_loss: 0.4991 - model_153_loss: 0.6922 - model_153_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9236 - model_153_loss: 0.6921 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4250 - model_152_loss: 0.4985 - model_153_loss: 0.6921 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4241 - model_152_loss: 0.4989 - model_153_loss: 0.6920 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4225 - model_152_loss: 0.4998 - model_153_loss: 0.6919 - model_153_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4242 - model_152_loss: 0.4987 - model_153_loss: 0.6920 - model_153_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4271 - model_152_loss: 0.4973 - model_153_loss: 0.6923 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.9234 - model_153_loss: 0.6918 - model_153_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4256 - model_152_loss: 0.4986 - model_153_loss: 0.6920 - model_153_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4273 - model_152_loss: 0.4977 - model_153_loss: 0.6922 - model_153_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4295 - model_152_loss: 0.4958 - model_153_loss: 0.6922 - model_153_1_loss: 0.6929\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4254 - model_152_loss: 0.4984 - model_153_loss: 0.6921 - model_153_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4273 - model_152_loss: 0.4979 - model_153_loss: 0.6923 - model_153_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.9251 - model_153_loss: 0.6922 - model_153_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4291 - model_152_loss: 0.4956 - model_153_loss: 0.6925 - model_153_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4257 - model_152_loss: 0.4968 - model_153_loss: 0.6921 - model_153_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4287 - model_152_loss: 0.4957 - model_153_loss: 0.6924 - model_153_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4288 - model_152_loss: 0.4954 - model_153_loss: 0.6925 - model_153_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4267 - model_152_loss: 0.4967 - model_153_loss: 0.6924 - model_153_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.9259 - model_153_loss: 0.6927 - model_153_1_loss: 0.69250s - loss: 6.9334 - model_153_loss: 0.6938 - model_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4279 - model_152_loss: 0.4959 - model_153_loss: 0.6922 - model_153_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4255 - model_152_loss: 0.4959 - model_153_loss: 0.6919 - model_153_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4269 - model_152_loss: 0.4958 - model_153_loss: 0.6921 - model_153_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4285 - model_152_loss: 0.4953 - model_153_loss: 0.6923 - model_153_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4289 - model_152_loss: 0.4951 - model_153_loss: 0.6923 - model_153_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9226 - model_153_loss: 0.6927 - model_153_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4288 - model_152_loss: 0.4959 - model_153_loss: 0.6926 - model_153_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4287 - model_152_loss: 0.4948 - model_153_loss: 0.6924 - model_153_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4301 - model_152_loss: 0.4957 - model_153_loss: 0.6930 - model_153_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4317 - model_152_loss: 0.4956 - model_153_loss: 0.6931 - model_153_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4300 - model_152_loss: 0.4951 - model_153_loss: 0.6927 - model_153_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9255 - model_153_loss: 0.6918 - model_153_1_loss: 0.69271s - loss: 6.9474 - model_153_loss: 0.6948 - m\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4292 - model_152_loss: 0.4951 - model_153_loss: 0.6926 - model_153_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4256 - model_152_loss: 0.4961 - model_153_loss: 0.6920 - model_153_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4277 - model_152_loss: 0.4958 - model_153_loss: 0.6923 - model_153_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4272 - model_152_loss: 0.4965 - model_153_loss: 0.6923 - model_153_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4269 - model_152_loss: 0.4964 - model_153_loss: 0.6923 - model_153_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9259 - model_153_loss: 0.6925 - model_153_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4313 - model_152_loss: 0.4951 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4295 - model_152_loss: 0.4969 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4282 - model_152_loss: 0.4966 - model_153_loss: 0.6924 - model_153_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4292 - model_152_loss: 0.4966 - model_153_loss: 0.6926 - model_153_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4296 - model_152_loss: 0.4968 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9270 - model_153_loss: 0.6928 - model_153_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4285 - model_152_loss: 0.4969 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4290 - model_152_loss: 0.4954 - model_153_loss: 0.6925 - model_153_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4281 - model_152_loss: 0.4960 - model_153_loss: 0.6923 - model_153_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4270 - model_152_loss: 0.4979 - model_153_loss: 0.6925 - model_153_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4278 - model_152_loss: 0.4965 - model_153_loss: 0.6924 - model_153_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9266 - model_153_loss: 0.6936 - model_153_1_loss: 0.69261s - loss: 6.8477 - model_153_loss: 0.6762 - mo\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4292 - model_152_loss: 0.4966 - model_153_loss: 0.6927 - model_153_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4300 - model_152_loss: 0.4971 - model_153_loss: 0.6928 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4295 - model_152_loss: 0.4968 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4300 - model_152_loss: 0.4974 - model_153_loss: 0.6929 - model_153_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4305 - model_152_loss: 0.4960 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9266 - model_153_loss: 0.6932 - model_153_1_loss: 0.69250s - loss: 6.9666 - model_153_loss: 0.7001 - model_153_1_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4279 - model_152_loss: 0.4970 - model_153_loss: 0.6925 - model_153_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4287 - model_152_loss: 0.4974 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4292 - model_152_loss: 0.4968 - model_153_loss: 0.6927 - model_153_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4280 - model_152_loss: 0.4972 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4284 - model_152_loss: 0.4976 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9293 - model_153_loss: 0.6933 - model_153_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4280 - model_152_loss: 0.4981 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4311 - model_152_loss: 0.4958 - model_153_loss: 0.6928 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4290 - model_152_loss: 0.4974 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4306 - model_152_loss: 0.4962 - model_153_loss: 0.6926 - model_153_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4296 - model_152_loss: 0.4975 - model_153_loss: 0.6928 - model_153_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.9275 - model_153_loss: 0.6928 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4287 - model_152_loss: 0.4971 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4297 - model_152_loss: 0.4963 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4294 - model_152_loss: 0.4971 - model_153_loss: 0.6926 - model_153_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4306 - model_152_loss: 0.4963 - model_153_loss: 0.6927 - model_153_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4291 - model_152_loss: 0.4975 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9282 - model_153_loss: 0.6932 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4293 - model_152_loss: 0.4972 - model_153_loss: 0.6926 - model_153_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4299 - model_152_loss: 0.4964 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4289 - model_152_loss: 0.4977 - model_153_loss: 0.6926 - model_153_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4301 - model_152_loss: 0.4965 - model_153_loss: 0.6926 - model_153_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4309 - model_152_loss: 0.4952 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9263 - model_153_loss: 0.6924 - model_153_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4313 - model_152_loss: 0.4957 - model_153_loss: 0.6927 - model_153_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4305 - model_152_loss: 0.4959 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4300 - model_152_loss: 0.4957 - model_153_loss: 0.6926 - model_153_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4308 - model_152_loss: 0.4954 - model_153_loss: 0.6926 - model_153_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4304 - model_152_loss: 0.4957 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9268 - model_153_loss: 0.6933 - model_153_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4303 - model_152_loss: 0.4955 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4298 - model_152_loss: 0.4953 - model_153_loss: 0.6925 - model_153_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4291 - model_152_loss: 0.4959 - model_153_loss: 0.6924 - model_153_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4304 - model_152_loss: 0.4959 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4304 - model_152_loss: 0.4949 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9259 - model_153_loss: 0.6929 - model_153_1_loss: 0.69281s - loss: 7.1182 - model_153_loss: 0.7277 - mod\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4291 - model_152_loss: 0.4955 - model_153_loss: 0.6924 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4280 - model_152_loss: 0.4959 - model_153_loss: 0.6922 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4304 - model_152_loss: 0.4959 - model_153_loss: 0.6925 - model_153_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4297 - model_152_loss: 0.4966 - model_153_loss: 0.6925 - model_153_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4292 - model_152_loss: 0.4964 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9270 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4293 - model_152_loss: 0.4954 - model_153_loss: 0.6924 - model_153_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4296 - model_152_loss: 0.4968 - model_153_loss: 0.6926 - model_153_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4278 - model_152_loss: 0.4964 - model_153_loss: 0.6923 - model_153_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4296 - model_152_loss: 0.4953 - model_153_loss: 0.6924 - model_153_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4302 - model_152_loss: 0.4956 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: 6.9268 - model_153_loss: 0.6925 - model_153_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4310 - model_152_loss: 0.4955 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4301 - model_152_loss: 0.4962 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4300 - model_152_loss: 0.4958 - model_153_loss: 0.6926 - model_153_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4303 - model_152_loss: 0.4948 - model_153_loss: 0.6925 - model_153_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4307 - model_152_loss: 0.4957 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9267 - model_153_loss: 0.6937 - model_153_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4319 - model_152_loss: 0.4960 - model_153_loss: 0.6926 - model_153_1_loss: 0.6930\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4346 - model_152_loss: 0.4933 - model_153_loss: 0.6927 - model_153_1_loss: 0.6929\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4331 - model_152_loss: 0.4948 - model_153_loss: 0.6927 - model_153_1_loss: 0.6929\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4337 - model_152_loss: 0.4955 - model_153_loss: 0.6928 - model_153_1_loss: 0.6930\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4341 - model_152_loss: 0.4947 - model_153_loss: 0.6927 - model_153_1_loss: 0.6930\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9252 - model_153_loss: 0.6919 - model_153_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4299 - model_152_loss: 0.4964 - model_153_loss: 0.6926 - model_153_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4297 - model_152_loss: 0.4953 - model_153_loss: 0.6923 - model_153_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4287 - model_152_loss: 0.4961 - model_153_loss: 0.6922 - model_153_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4315 - model_152_loss: 0.4957 - model_153_loss: 0.6927 - model_153_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4304 - model_152_loss: 0.4950 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9286 - model_153_loss: 0.6928 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4311 - model_152_loss: 0.4941 - model_153_loss: 0.6925 - model_153_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4309 - model_152_loss: 0.4946 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4301 - model_152_loss: 0.4954 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4320 - model_152_loss: 0.4944 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4297 - model_152_loss: 0.4958 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: 6.9270 - model_153_loss: 0.6934 - model_153_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4302 - model_152_loss: 0.4960 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4315 - model_152_loss: 0.4952 - model_153_loss: 0.6927 - model_153_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4306 - model_152_loss: 0.4959 - model_153_loss: 0.6926 - model_153_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4302 - model_152_loss: 0.4955 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4307 - model_152_loss: 0.4959 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9273 - model_153_loss: 0.6927 - model_153_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4296 - model_152_loss: 0.4954 - model_153_loss: 0.6923 - model_153_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 17us/sample - loss: -6.4300 - model_152_loss: 0.4955 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4313 - model_152_loss: 0.4945 - model_153_loss: 0.6925 - model_153_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4302 - model_152_loss: 0.4957 - model_153_loss: 0.6925 - model_153_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4297 - model_152_loss: 0.4954 - model_153_loss: 0.6924 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 44us/sample - loss: 6.9271 - model_153_loss: 0.6943 - model_153_1_loss: 0.69290s - loss: 6.9174 - model_153_loss: 0.6904 - model_153_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4303 - model_152_loss: 0.4951 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4300 - model_152_loss: 0.4949 - model_153_loss: 0.6924 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4292 - model_152_loss: 0.4957 - model_153_loss: 0.6924 - model_153_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4309 - model_152_loss: 0.4954 - model_153_loss: 0.6927 - model_153_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4302 - model_152_loss: 0.4953 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9279 - model_153_loss: 0.6923 - model_153_1_loss: 0.69270s - loss: 6.9128 - model_153_loss: 0.6901 - model_153_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4312 - model_152_loss: 0.4940 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4316 - model_152_loss: 0.4943 - model_153_loss: 0.6927 - model_153_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4304 - model_152_loss: 0.4951 - model_153_loss: 0.6926 - model_153_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4306 - model_152_loss: 0.4950 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4311 - model_152_loss: 0.4952 - model_153_loss: 0.6928 - model_153_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9280 - model_153_loss: 0.6931 - model_153_1_loss: 0.6930\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4289 - model_152_loss: 0.4941 - model_153_loss: 0.6922 - model_153_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4306 - model_152_loss: 0.4945 - model_153_loss: 0.6925 - model_153_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4328 - model_152_loss: 0.4934 - model_153_loss: 0.6927 - model_153_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4300 - model_152_loss: 0.4946 - model_153_loss: 0.6924 - model_153_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4316 - model_152_loss: 0.4936 - model_153_loss: 0.6926 - model_153_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9261 - model_153_loss: 0.6918 - model_153_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4310 - model_152_loss: 0.4939 - model_153_loss: 0.6924 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4317 - model_152_loss: 0.4934 - model_153_loss: 0.6924 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4316 - model_152_loss: 0.4940 - model_153_loss: 0.6926 - model_153_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4305 - model_152_loss: 0.4944 - model_153_loss: 0.6925 - model_153_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4308 - model_152_loss: 0.4940 - model_153_loss: 0.6924 - model_153_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9253 - model_153_loss: 0.6924 - model_153_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4319 - model_152_loss: 0.4946 - model_153_loss: 0.6928 - model_153_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4312 - model_152_loss: 0.4955 - model_153_loss: 0.6928 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4307 - model_152_loss: 0.4946 - model_153_loss: 0.6925 - model_153_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4310 - model_152_loss: 0.4949 - model_153_loss: 0.6927 - model_153_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4309 - model_152_loss: 0.4957 - model_153_loss: 0.6928 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: 6.9300 - model_153_loss: 0.6934 - model_153_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4307 - model_152_loss: 0.4952 - model_153_loss: 0.6926 - model_153_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4302 - model_152_loss: 0.4952 - model_153_loss: 0.6925 - model_153_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4308 - model_152_loss: 0.4942 - model_153_loss: 0.6925 - model_153_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4310 - model_152_loss: 0.4955 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4307 - model_152_loss: 0.4953 - model_153_loss: 0.6926 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9263 - model_153_loss: 0.6933 - model_153_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 20us/sample - loss: -6.4325 - model_152_loss: 0.4942 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4324 - model_152_loss: 0.4952 - model_153_loss: 0.6929 - model_153_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4317 - model_152_loss: 0.4954 - model_153_loss: 0.6928 - model_153_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4322 - model_152_loss: 0.4946 - model_153_loss: 0.6927 - model_153_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4322 - model_152_loss: 0.4950 - model_153_loss: 0.6927 - model_153_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9264 - model_153_loss: 0.6918 - model_153_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4324 - model_152_loss: 0.4950 - model_153_loss: 0.6929 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4315 - model_152_loss: 0.4957 - model_153_loss: 0.6929 - model_153_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4316 - model_152_loss: 0.4959 - model_153_loss: 0.6929 - model_153_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4324 - model_152_loss: 0.4959 - model_153_loss: 0.6929 - model_153_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4332 - model_152_loss: 0.4959 - model_153_loss: 0.6933 - model_153_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 46us/sample - loss: 6.9292 - model_153_loss: 0.6925 - model_153_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4316 - model_152_loss: 0.4954 - model_153_loss: 0.6925 - model_153_1_loss: 0.6928\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4325 - model_152_loss: 0.4949 - model_153_loss: 0.6927 - model_153_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4319 - model_152_loss: 0.4954 - model_153_loss: 0.6927 - model_153_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4323 - model_152_loss: 0.4960 - model_153_loss: 0.6928 - model_153_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4314 - model_152_loss: 0.4951 - model_153_loss: 0.6925 - model_153_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9264 - model_153_loss: 0.6929 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4318 - model_152_loss: 0.4951 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4329 - model_152_loss: 0.4943 - model_153_loss: 0.6927 - model_153_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4335 - model_152_loss: 0.4937 - model_153_loss: 0.6928 - model_153_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4314 - model_152_loss: 0.4949 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4315 - model_152_loss: 0.4952 - model_153_loss: 0.6927 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9274 - model_153_loss: 0.6924 - model_153_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4343 - model_152_loss: 0.4940 - model_153_loss: 0.6930 - model_153_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4351 - model_152_loss: 0.4939 - model_153_loss: 0.6931 - model_153_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 20us/sample - loss: -6.4330 - model_152_loss: 0.4952 - model_153_loss: 0.6930 - model_153_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4357 - model_152_loss: 0.4936 - model_153_loss: 0.6932 - model_153_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4338 - model_152_loss: 0.4951 - model_153_loss: 0.6931 - model_153_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: 6.9273 - model_153_loss: 0.6926 - model_153_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4314 - model_152_loss: 0.4941 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4326 - model_152_loss: 0.4947 - model_153_loss: 0.6928 - model_153_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4299 - model_152_loss: 0.4954 - model_153_loss: 0.6925 - model_153_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 0s 18us/sample - loss: -6.4306 - model_152_loss: 0.4948 - model_153_loss: 0.6926 - model_153_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 0s 19us/sample - loss: -6.4330 - model_152_loss: 0.4936 - model_153_loss: 0.6928 - model_153_1_loss: 0.6926\n",
      "For Attention Module: 4.0\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 61us/sample - loss: 6.3391 - model_157_loss: 0.6586 - model_157_1_loss: 0.6095\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: -5.9536 - model_156_loss: 0.3771 - model_157_loss: 0.6583 - model_157_1_loss: 0.6078\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -5.9760 - model_156_loss: 0.3754 - model_157_loss: 0.6588 - model_157_1_loss: 0.6115\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -5.9849 - model_156_loss: 0.3752 - model_157_loss: 0.6591 - model_157_1_loss: 0.6129\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.0015 - model_156_loss: 0.3744 - model_157_loss: 0.6599 - model_157_1_loss: 0.6153\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.0030 - model_156_loss: 0.3747 - model_157_loss: 0.6587 - model_157_1_loss: 0.6168\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: 6.3988 - model_157_loss: 0.6621 - model_157_1_loss: 0.6178\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.0136 - model_156_loss: 0.3777 - model_157_loss: 0.6617 - model_157_1_loss: 0.6165\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.0284 - model_156_loss: 0.3777 - model_157_loss: 0.6621 - model_157_1_loss: 0.6191\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.0368 - model_156_loss: 0.3756 - model_157_loss: 0.6621 - model_157_1_loss: 0.6204\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.0418 - model_156_loss: 0.3786 - model_157_loss: 0.6621 - model_157_1_loss: 0.6219\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.0546 - model_156_loss: 0.3792 - model_157_loss: 0.6626 - model_157_1_loss: 0.6241\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.4515 - model_157_loss: 0.6648 - model_157_1_loss: 0.6261\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0622 - model_156_loss: 0.3815 - model_157_loss: 0.6628 - model_157_1_loss: 0.6259\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.0713 - model_156_loss: 0.3816 - model_157_loss: 0.6628 - model_157_1_loss: 0.6278\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.0926 - model_156_loss: 0.3795 - model_157_loss: 0.6635 - model_157_1_loss: 0.6309\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.1057 - model_156_loss: 0.3834 - model_157_loss: 0.6651 - model_157_1_loss: 0.6327\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.1223 - model_156_loss: 0.3854 - model_157_loss: 0.6656 - model_157_1_loss: 0.6359\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.5186 - model_157_loss: 0.6665 - model_157_1_loss: 0.6370\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.1282 - model_156_loss: 0.3857 - model_157_loss: 0.6667 - model_157_1_loss: 0.6361\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.1485 - model_156_loss: 0.3861 - model_157_loss: 0.6677 - model_157_1_loss: 0.6392\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.1584 - model_156_loss: 0.3869 - model_157_loss: 0.6684 - model_157_1_loss: 0.6406\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.1702 - model_156_loss: 0.3888 - model_157_loss: 0.6683 - model_157_1_loss: 0.6435\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.1823 - model_156_loss: 0.3932 - model_157_loss: 0.6693 - model_157_1_loss: 0.6458\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.5871 - model_157_loss: 0.6699 - model_157_1_loss: 0.64710s - loss: 6.5673 - model_157_loss: 0.6654 - model_1\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.1918 - model_156_loss: 0.3936 - model_157_loss: 0.6695 - model_157_1_loss: 0.6476\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.2060 - model_156_loss: 0.3954 - model_157_loss: 0.6708 - model_157_1_loss: 0.6495\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.2141 - model_156_loss: 0.4000 - model_157_loss: 0.6723 - model_157_1_loss: 0.6506\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.2392 - model_156_loss: 0.3968 - model_157_loss: 0.6732 - model_157_1_loss: 0.6540\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.2463 - model_156_loss: 0.4030 - model_157_loss: 0.6737 - model_157_1_loss: 0.6562\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 66us/sample - loss: 6.6552 - model_157_loss: 0.6735 - model_157_1_loss: 0.6564\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.2484 - model_156_loss: 0.4030 - model_157_loss: 0.6742 - model_157_1_loss: 0.6560\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.2647 - model_156_loss: 0.4053 - model_157_loss: 0.6755 - model_157_1_loss: 0.6586\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.2660 - model_156_loss: 0.4060 - model_157_loss: 0.6757 - model_157_1_loss: 0.6587\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.2847 - model_156_loss: 0.4095 - model_157_loss: 0.6771 - model_157_1_loss: 0.6617\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2963 - model_156_loss: 0.4142 - model_157_loss: 0.6775 - model_157_1_loss: 0.6646\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.7157 - model_157_loss: 0.6798 - model_157_1_loss: 0.6637\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.2968 - model_156_loss: 0.4146 - model_157_loss: 0.6783 - model_157_1_loss: 0.6640\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.2960 - model_156_loss: 0.4167 - model_157_loss: 0.6770 - model_157_1_loss: 0.6656\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3168 - model_156_loss: 0.4203 - model_157_loss: 0.6806 - model_157_1_loss: 0.6669\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3255 - model_156_loss: 0.4235 - model_157_loss: 0.6804 - model_157_1_loss: 0.6694\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3313 - model_156_loss: 0.4263 - model_157_loss: 0.6812 - model_157_1_loss: 0.6703\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.7599 - model_157_loss: 0.6817 - model_157_1_loss: 0.6701\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.3374 - model_156_loss: 0.4270 - model_157_loss: 0.6822 - model_157_1_loss: 0.6706\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3426 - model_156_loss: 0.4296 - model_157_loss: 0.6825 - model_157_1_loss: 0.6719\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3505 - model_156_loss: 0.4327 - model_157_loss: 0.6831 - model_157_1_loss: 0.6736\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3506 - model_156_loss: 0.4335 - model_157_loss: 0.6833 - model_157_1_loss: 0.6735\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3705 - model_156_loss: 0.4375 - model_157_loss: 0.6853 - model_157_1_loss: 0.6763\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: 6.7995 - model_157_loss: 0.6841 - model_157_1_loss: 0.6759\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3701 - model_156_loss: 0.4371 - model_157_loss: 0.6857 - model_157_1_loss: 0.6758\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3735 - model_156_loss: 0.4401 - model_157_loss: 0.6856 - model_157_1_loss: 0.6771\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3766 - model_156_loss: 0.4429 - model_157_loss: 0.6857 - model_157_1_loss: 0.6782\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.3830 - model_156_loss: 0.4482 - model_157_loss: 0.6870 - model_157_1_loss: 0.6792\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.3942 - model_156_loss: 0.4494 - model_157_loss: 0.6879 - model_157_1_loss: 0.6809\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.8447 - model_157_loss: 0.6880 - model_157_1_loss: 0.68081s - loss: 6.7910 - model_157_loss: 0.6785 - model_157_1_loss: 0 - ETA: 0s - loss: 6.8117 - model_157_loss: 0.6815 - model_15\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.3852 - model_156_loss: 0.4529 - model_157_loss: 0.6869 - model_157_1_loss: 0.6808\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3918 - model_156_loss: 0.4548 - model_157_loss: 0.6878 - model_157_1_loss: 0.6815\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.3973 - model_156_loss: 0.4574 - model_157_loss: 0.6883 - model_157_1_loss: 0.6826\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4020 - model_156_loss: 0.4605 - model_157_loss: 0.6893 - model_157_1_loss: 0.6832\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4137 - model_156_loss: 0.4606 - model_157_loss: 0.6898 - model_157_1_loss: 0.6850\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.8732 - model_157_loss: 0.6898 - model_157_1_loss: 0.6846\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4057 - model_156_loss: 0.4647 - model_157_loss: 0.6895 - model_157_1_loss: 0.6846\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4130 - model_156_loss: 0.4654 - model_157_loss: 0.6906 - model_157_1_loss: 0.6851\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4126 - model_156_loss: 0.4696 - model_157_loss: 0.6903 - model_157_1_loss: 0.6862\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4152 - model_156_loss: 0.4710 - model_157_loss: 0.6908 - model_157_1_loss: 0.6864\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4133 - model_156_loss: 0.4744 - model_157_loss: 0.6912 - model_157_1_loss: 0.6863\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.8991 - model_157_loss: 0.6920 - model_157_1_loss: 0.6885\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4230 - model_156_loss: 0.4743 - model_157_loss: 0.6917 - model_157_1_loss: 0.6878\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4252 - model_156_loss: 0.4743 - model_157_loss: 0.6919 - model_157_1_loss: 0.6880\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4277 - model_156_loss: 0.4758 - model_157_loss: 0.6920 - model_157_1_loss: 0.6887\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4226 - model_156_loss: 0.4799 - model_157_loss: 0.6918 - model_157_1_loss: 0.6887\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4301 - model_156_loss: 0.4803 - model_157_loss: 0.6927 - model_157_1_loss: 0.6894\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9125 - model_157_loss: 0.6932 - model_157_1_loss: 0.6898\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4319 - model_156_loss: 0.4788 - model_157_loss: 0.6925 - model_157_1_loss: 0.6896\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4259 - model_156_loss: 0.4840 - model_157_loss: 0.6925 - model_157_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4309 - model_156_loss: 0.4834 - model_157_loss: 0.6925 - model_157_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4349 - model_156_loss: 0.4824 - model_157_loss: 0.6931 - model_157_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4322 - model_156_loss: 0.4834 - model_157_loss: 0.6927 - model_157_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: 6.9201 - model_157_loss: 0.6928 - model_157_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4316 - model_156_loss: 0.4830 - model_157_loss: 0.6925 - model_157_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4341 - model_156_loss: 0.4833 - model_157_loss: 0.6927 - model_157_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4360 - model_156_loss: 0.4825 - model_157_loss: 0.6930 - model_157_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4351 - model_156_loss: 0.4844 - model_157_loss: 0.6930 - model_157_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4356 - model_156_loss: 0.4844 - model_157_loss: 0.6928 - model_157_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: 6.9247 - model_157_loss: 0.6931 - model_157_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4398 - model_156_loss: 0.4813 - model_157_loss: 0.6929 - model_157_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4380 - model_156_loss: 0.4845 - model_157_loss: 0.6932 - model_157_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4374 - model_156_loss: 0.4835 - model_157_loss: 0.6929 - model_157_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4421 - model_156_loss: 0.4838 - model_157_loss: 0.6936 - model_157_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4410 - model_156_loss: 0.4833 - model_157_loss: 0.6932 - model_157_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.9212 - model_157_loss: 0.6933 - model_157_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4404 - model_156_loss: 0.4812 - model_157_loss: 0.6929 - model_157_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4406 - model_156_loss: 0.4805 - model_157_loss: 0.6925 - model_157_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4412 - model_156_loss: 0.4825 - model_157_loss: 0.6930 - model_157_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4418 - model_156_loss: 0.4808 - model_157_loss: 0.6929 - model_157_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4381 - model_156_loss: 0.4827 - model_157_loss: 0.6925 - model_157_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9216 - model_157_loss: 0.6930 - model_157_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4372 - model_156_loss: 0.4816 - model_157_loss: 0.6926 - model_157_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4357 - model_156_loss: 0.4825 - model_157_loss: 0.6925 - model_157_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4402 - model_156_loss: 0.4804 - model_157_loss: 0.6928 - model_157_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4388 - model_156_loss: 0.4828 - model_157_loss: 0.6926 - model_157_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4378 - model_156_loss: 0.4806 - model_157_loss: 0.6923 - model_157_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9196 - model_157_loss: 0.6916 - model_157_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4396 - model_156_loss: 0.4826 - model_157_loss: 0.6926 - model_157_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4389 - model_156_loss: 0.4825 - model_157_loss: 0.6923 - model_157_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4396 - model_156_loss: 0.4809 - model_157_loss: 0.6923 - model_157_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4402 - model_156_loss: 0.4813 - model_157_loss: 0.6929 - model_157_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4370 - model_156_loss: 0.4814 - model_157_loss: 0.6920 - model_157_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9249 - model_157_loss: 0.6934 - model_157_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4409 - model_156_loss: 0.4802 - model_157_loss: 0.6923 - model_157_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4415 - model_156_loss: 0.4825 - model_157_loss: 0.6926 - model_157_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4428 - model_156_loss: 0.4792 - model_157_loss: 0.6925 - model_157_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4386 - model_156_loss: 0.4828 - model_157_loss: 0.6922 - model_157_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4409 - model_156_loss: 0.4815 - model_157_loss: 0.6922 - model_157_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 61us/sample - loss: 6.9249 - model_157_loss: 0.6938 - model_157_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4416 - model_156_loss: 0.4802 - model_157_loss: 0.6922 - model_157_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4381 - model_156_loss: 0.4821 - model_157_loss: 0.6923 - model_157_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4409 - model_156_loss: 0.4811 - model_157_loss: 0.6921 - model_157_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4427 - model_156_loss: 0.4801 - model_157_loss: 0.6922 - model_157_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4421 - model_156_loss: 0.4806 - model_157_loss: 0.6922 - model_157_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9237 - model_157_loss: 0.6930 - model_157_1_loss: 0.69230s - loss: 6.9005 - model_157_loss: 0.6898 - model_157_1\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4399 - model_156_loss: 0.4819 - model_157_loss: 0.6920 - model_157_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4406 - model_156_loss: 0.4822 - model_157_loss: 0.6923 - model_157_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4433 - model_156_loss: 0.4792 - model_157_loss: 0.6922 - model_157_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4422 - model_156_loss: 0.4800 - model_157_loss: 0.6924 - model_157_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4419 - model_156_loss: 0.4789 - model_157_loss: 0.6922 - model_157_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.9231 - model_157_loss: 0.6927 - model_157_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4428 - model_156_loss: 0.4763 - model_157_loss: 0.6921 - model_157_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4411 - model_156_loss: 0.4776 - model_157_loss: 0.6920 - model_157_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4436 - model_156_loss: 0.4762 - model_157_loss: 0.6922 - model_157_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4423 - model_156_loss: 0.4765 - model_157_loss: 0.6920 - model_157_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4436 - model_156_loss: 0.4765 - model_157_loss: 0.6923 - model_157_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 74us/sample - loss: 6.9209 - model_157_loss: 0.6925 - model_157_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4441 - model_156_loss: 0.4745 - model_157_loss: 0.6922 - model_157_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4433 - model_156_loss: 0.4729 - model_157_loss: 0.6920 - model_157_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4441 - model_156_loss: 0.4730 - model_157_loss: 0.6921 - model_157_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4461 - model_156_loss: 0.4722 - model_157_loss: 0.6924 - model_157_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4449 - model_156_loss: 0.4702 - model_157_loss: 0.6924 - model_157_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9163 - model_157_loss: 0.6919 - model_157_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4442 - model_156_loss: 0.4704 - model_157_loss: 0.6916 - model_157_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4458 - model_156_loss: 0.4706 - model_157_loss: 0.6922 - model_157_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4434 - model_156_loss: 0.4699 - model_157_loss: 0.6918 - model_157_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4459 - model_156_loss: 0.4683 - model_157_loss: 0.6920 - model_157_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4471 - model_156_loss: 0.4677 - model_157_loss: 0.6921 - model_157_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: 6.9137 - model_157_loss: 0.6924 - model_157_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4464 - model_156_loss: 0.4676 - model_157_loss: 0.6920 - model_157_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4460 - model_156_loss: 0.4687 - model_157_loss: 0.6919 - model_157_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4459 - model_156_loss: 0.4673 - model_157_loss: 0.6919 - model_157_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4484 - model_156_loss: 0.4676 - model_157_loss: 0.6923 - model_157_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4474 - model_156_loss: 0.4657 - model_157_loss: 0.6921 - model_157_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9164 - model_157_loss: 0.6925 - model_157_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4451 - model_156_loss: 0.4676 - model_157_loss: 0.6920 - model_157_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4479 - model_156_loss: 0.4674 - model_157_loss: 0.6923 - model_157_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4454 - model_156_loss: 0.4667 - model_157_loss: 0.6918 - model_157_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4452 - model_156_loss: 0.4669 - model_157_loss: 0.6919 - model_157_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4442 - model_156_loss: 0.4679 - model_157_loss: 0.6918 - model_157_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9185 - model_157_loss: 0.6929 - model_157_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4484 - model_156_loss: 0.4667 - model_157_loss: 0.6923 - model_157_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4477 - model_156_loss: 0.4678 - model_157_loss: 0.6923 - model_157_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4499 - model_156_loss: 0.4672 - model_157_loss: 0.6924 - model_157_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4474 - model_156_loss: 0.4686 - model_157_loss: 0.6921 - model_157_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4471 - model_156_loss: 0.4683 - model_157_loss: 0.6921 - model_157_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9194 - model_157_loss: 0.6918 - model_157_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4491 - model_156_loss: 0.4672 - model_157_loss: 0.6925 - model_157_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4511 - model_156_loss: 0.4670 - model_157_loss: 0.6926 - model_157_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4494 - model_156_loss: 0.4676 - model_157_loss: 0.6923 - model_157_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4475 - model_156_loss: 0.4689 - model_157_loss: 0.6923 - model_157_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4530 - model_156_loss: 0.4678 - model_157_loss: 0.6925 - model_157_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9207 - model_157_loss: 0.6927 - model_157_1_loss: 0.69170s - loss: 6.9165 - model_157_loss: 0.6952 - model_1\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4466 - model_156_loss: 0.4686 - model_157_loss: 0.6919 - model_157_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4503 - model_156_loss: 0.4693 - model_157_loss: 0.6925 - model_157_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4497 - model_156_loss: 0.4697 - model_157_loss: 0.6925 - model_157_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4497 - model_156_loss: 0.4692 - model_157_loss: 0.6924 - model_157_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4505 - model_156_loss: 0.4693 - model_157_loss: 0.6925 - model_157_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9216 - model_157_loss: 0.6927 - model_157_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4519 - model_156_loss: 0.4683 - model_157_loss: 0.6923 - model_157_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4479 - model_156_loss: 0.4693 - model_157_loss: 0.6921 - model_157_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4536 - model_156_loss: 0.4687 - model_157_loss: 0.6926 - model_157_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4523 - model_156_loss: 0.4692 - model_157_loss: 0.6926 - model_157_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4515 - model_156_loss: 0.4701 - model_157_loss: 0.6927 - model_157_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9243 - model_157_loss: 0.6927 - model_157_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4506 - model_156_loss: 0.4709 - model_157_loss: 0.6925 - model_157_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4509 - model_156_loss: 0.4717 - model_157_loss: 0.6925 - model_157_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4513 - model_156_loss: 0.4714 - model_157_loss: 0.6926 - model_157_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4524 - model_156_loss: 0.4715 - model_157_loss: 0.6929 - model_157_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4510 - model_156_loss: 0.4704 - model_157_loss: 0.6925 - model_157_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9238 - model_157_loss: 0.6930 - model_157_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4522 - model_156_loss: 0.4707 - model_157_loss: 0.6927 - model_157_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4544 - model_156_loss: 0.4706 - model_157_loss: 0.6929 - model_157_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4521 - model_156_loss: 0.4708 - model_157_loss: 0.6926 - model_157_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4532 - model_156_loss: 0.4708 - model_157_loss: 0.6929 - model_157_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4539 - model_156_loss: 0.4707 - model_157_loss: 0.6928 - model_157_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9225 - model_157_loss: 0.6934 - model_157_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4520 - model_156_loss: 0.4725 - model_157_loss: 0.6927 - model_157_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4529 - model_156_loss: 0.4697 - model_157_loss: 0.6925 - model_157_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4556 - model_156_loss: 0.4687 - model_157_loss: 0.6928 - model_157_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4544 - model_156_loss: 0.4687 - model_157_loss: 0.6924 - model_157_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4530 - model_156_loss: 0.4675 - model_157_loss: 0.6922 - model_157_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9215 - model_157_loss: 0.6924 - model_157_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4517 - model_156_loss: 0.4679 - model_157_loss: 0.6926 - model_157_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4523 - model_156_loss: 0.4658 - model_157_loss: 0.6924 - model_157_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4527 - model_156_loss: 0.4670 - model_157_loss: 0.6925 - model_157_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4524 - model_156_loss: 0.4653 - model_157_loss: 0.6924 - model_157_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4546 - model_156_loss: 0.4650 - model_157_loss: 0.6924 - model_157_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9190 - model_157_loss: 0.6922 - model_157_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4521 - model_156_loss: 0.4651 - model_157_loss: 0.6922 - model_157_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4515 - model_156_loss: 0.4643 - model_157_loss: 0.6923 - model_157_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4523 - model_156_loss: 0.4655 - model_157_loss: 0.6923 - model_157_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4509 - model_156_loss: 0.4655 - model_157_loss: 0.6923 - model_157_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4535 - model_156_loss: 0.4643 - model_157_loss: 0.6924 - model_157_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.9188 - model_157_loss: 0.6923 - model_157_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4527 - model_156_loss: 0.4634 - model_157_loss: 0.6922 - model_157_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4545 - model_156_loss: 0.4633 - model_157_loss: 0.6926 - model_157_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4503 - model_156_loss: 0.4656 - model_157_loss: 0.6921 - model_157_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4512 - model_156_loss: 0.4642 - model_157_loss: 0.6921 - model_157_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4538 - model_156_loss: 0.4640 - model_157_loss: 0.6925 - model_157_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9203 - model_157_loss: 0.6927 - model_157_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4560 - model_156_loss: 0.4631 - model_157_loss: 0.6923 - model_157_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4525 - model_156_loss: 0.4642 - model_157_loss: 0.6921 - model_157_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4520 - model_156_loss: 0.4660 - model_157_loss: 0.6924 - model_157_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4547 - model_156_loss: 0.4665 - model_157_loss: 0.6925 - model_157_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4546 - model_156_loss: 0.4652 - model_157_loss: 0.6924 - model_157_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9214 - model_157_loss: 0.6926 - model_157_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4513 - model_156_loss: 0.4672 - model_157_loss: 0.6921 - model_157_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4539 - model_156_loss: 0.4683 - model_157_loss: 0.6924 - model_157_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4535 - model_156_loss: 0.4668 - model_157_loss: 0.6921 - model_157_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4556 - model_156_loss: 0.4673 - model_157_loss: 0.6923 - model_157_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4544 - model_156_loss: 0.4691 - model_157_loss: 0.6925 - model_157_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9240 - model_157_loss: 0.6923 - model_157_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4570 - model_156_loss: 0.4683 - model_157_loss: 0.6928 - model_157_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4544 - model_156_loss: 0.4681 - model_157_loss: 0.6926 - model_157_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4534 - model_156_loss: 0.4717 - model_157_loss: 0.6927 - model_157_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4536 - model_156_loss: 0.4714 - model_157_loss: 0.6925 - model_157_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4569 - model_156_loss: 0.4706 - model_157_loss: 0.6929 - model_157_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9268 - model_157_loss: 0.6920 - model_157_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4539 - model_156_loss: 0.4706 - model_157_loss: 0.6928 - model_157_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4521 - model_156_loss: 0.4707 - model_157_loss: 0.6924 - model_157_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4550 - model_156_loss: 0.4711 - model_157_loss: 0.6928 - model_157_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4533 - model_156_loss: 0.4710 - model_157_loss: 0.6926 - model_157_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4535 - model_156_loss: 0.4698 - model_157_loss: 0.6926 - model_157_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9270 - model_157_loss: 0.6921 - model_157_1_loss: 0.69250s - loss: 6.9319 - model_157_loss: 0.6919 - model_157_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4549 - model_156_loss: 0.4708 - model_157_loss: 0.6926 - model_157_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4541 - model_156_loss: 0.4704 - model_157_loss: 0.6926 - model_157_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4544 - model_156_loss: 0.4711 - model_157_loss: 0.6928 - model_157_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4554 - model_156_loss: 0.4694 - model_157_loss: 0.6929 - model_157_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4572 - model_156_loss: 0.4694 - model_157_loss: 0.6931 - model_157_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9256 - model_157_loss: 0.6935 - model_157_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4555 - model_156_loss: 0.4677 - model_157_loss: 0.6925 - model_157_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4550 - model_156_loss: 0.4688 - model_157_loss: 0.6927 - model_157_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4567 - model_156_loss: 0.4667 - model_157_loss: 0.6926 - model_157_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4549 - model_156_loss: 0.4679 - model_157_loss: 0.6926 - model_157_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4533 - model_156_loss: 0.4670 - model_157_loss: 0.6924 - model_157_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9229 - model_157_loss: 0.6928 - model_157_1_loss: 0.69180s - loss: 6.9185 - model_157_loss: 0.6928 - model_157_1_loss:\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4555 - model_156_loss: 0.4661 - model_157_loss: 0.6926 - model_157_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4550 - model_156_loss: 0.4670 - model_157_loss: 0.6927 - model_157_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4554 - model_156_loss: 0.4651 - model_157_loss: 0.6925 - model_157_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4549 - model_156_loss: 0.4651 - model_157_loss: 0.6924 - model_157_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4561 - model_156_loss: 0.4647 - model_157_loss: 0.6926 - model_157_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9206 - model_157_loss: 0.6933 - model_157_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4565 - model_156_loss: 0.4626 - model_157_loss: 0.6922 - model_157_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4545 - model_156_loss: 0.4640 - model_157_loss: 0.6924 - model_157_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4577 - model_156_loss: 0.4613 - model_157_loss: 0.6923 - model_157_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4557 - model_156_loss: 0.4625 - model_157_loss: 0.6926 - model_157_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4563 - model_156_loss: 0.4615 - model_157_loss: 0.6923 - model_157_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9190 - model_157_loss: 0.6922 - model_157_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4530 - model_156_loss: 0.4606 - model_157_loss: 0.6917 - model_157_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4549 - model_156_loss: 0.4610 - model_157_loss: 0.6920 - model_157_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4527 - model_156_loss: 0.4627 - model_157_loss: 0.6921 - model_157_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4544 - model_156_loss: 0.4618 - model_157_loss: 0.6924 - model_157_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4531 - model_156_loss: 0.4615 - model_157_loss: 0.6921 - model_157_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9162 - model_157_loss: 0.6924 - model_157_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4585 - model_156_loss: 0.4602 - model_157_loss: 0.6924 - model_157_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4566 - model_156_loss: 0.4615 - model_157_loss: 0.6924 - model_157_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4593 - model_156_loss: 0.4590 - model_157_loss: 0.6926 - model_157_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4572 - model_156_loss: 0.4604 - model_157_loss: 0.6925 - model_157_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4539 - model_156_loss: 0.4615 - model_157_loss: 0.6921 - model_157_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 85us/sample - loss: 6.9186 - model_157_loss: 0.6929 - model_157_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4553 - model_156_loss: 0.4605 - model_157_loss: 0.6922 - model_157_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4552 - model_156_loss: 0.4603 - model_157_loss: 0.6922 - model_157_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4535 - model_156_loss: 0.4606 - model_157_loss: 0.6922 - model_157_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4559 - model_156_loss: 0.4612 - model_157_loss: 0.6925 - model_157_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4596 - model_156_loss: 0.4588 - model_157_loss: 0.6922 - model_157_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 62us/sample - loss: 6.9182 - model_157_loss: 0.6928 - model_157_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4589 - model_156_loss: 0.4592 - model_157_loss: 0.6925 - model_157_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4578 - model_156_loss: 0.4604 - model_157_loss: 0.6927 - model_157_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4552 - model_156_loss: 0.4616 - model_157_loss: 0.6923 - model_157_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4553 - model_156_loss: 0.4615 - model_157_loss: 0.6925 - model_157_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4578 - model_156_loss: 0.4604 - model_157_loss: 0.6926 - model_157_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9199 - model_157_loss: 0.6929 - model_157_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4588 - model_156_loss: 0.4612 - model_157_loss: 0.6927 - model_157_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4582 - model_156_loss: 0.4605 - model_157_loss: 0.6926 - model_157_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4586 - model_156_loss: 0.4611 - model_157_loss: 0.6928 - model_157_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4597 - model_156_loss: 0.4610 - model_157_loss: 0.6927 - model_157_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4566 - model_156_loss: 0.4604 - model_157_loss: 0.6922 - model_157_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: 6.9218 - model_157_loss: 0.6924 - model_157_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4554 - model_156_loss: 0.4632 - model_157_loss: 0.6925 - model_157_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4587 - model_156_loss: 0.4636 - model_157_loss: 0.6927 - model_157_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4604 - model_156_loss: 0.4608 - model_157_loss: 0.6929 - model_157_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4584 - model_156_loss: 0.4633 - model_157_loss: 0.6929 - model_157_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4590 - model_156_loss: 0.4634 - model_157_loss: 0.6928 - model_157_1_loss: 0.6916\n",
      "For Attention Module: 4.1\n",
      "features X: 30940 samples, 63 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.8168 - model_161_loss: 0.6754 - model_161_1_loss: 0.6886\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: -6.3364 - model_160_loss: 0.4839 - model_161_loss: 0.6749 - model_161_1_loss: 0.6891\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3359 - model_160_loss: 0.4833 - model_161_loss: 0.6746 - model_161_1_loss: 0.6892\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3332 - model_160_loss: 0.4854 - model_161_loss: 0.6748 - model_161_1_loss: 0.6889\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3350 - model_160_loss: 0.4849 - model_161_loss: 0.6745 - model_161_1_loss: 0.6895\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3351 - model_160_loss: 0.4840 - model_161_loss: 0.6748 - model_161_1_loss: 0.6890\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.8256 - model_161_loss: 0.6757 - model_161_1_loss: 0.6898\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3401 - model_160_loss: 0.4850 - model_161_loss: 0.6755 - model_161_1_loss: 0.6895\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3492 - model_160_loss: 0.4850 - model_161_loss: 0.6776 - model_161_1_loss: 0.6893\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3413 - model_160_loss: 0.4862 - model_161_loss: 0.6762 - model_161_1_loss: 0.6893\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3460 - model_160_loss: 0.4853 - model_161_loss: 0.6770 - model_161_1_loss: 0.6893\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3450 - model_160_loss: 0.4858 - model_161_loss: 0.6765 - model_161_1_loss: 0.6896\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.8370 - model_161_loss: 0.6774 - model_161_1_loss: 0.68961s - loss: 6.8845 - model_161_loss: 0.6865 - m\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.3549 - model_160_loss: 0.4854 - model_161_loss: 0.6785 - model_161_1_loss: 0.6896\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3548 - model_160_loss: 0.4855 - model_161_loss: 0.6782 - model_161_1_loss: 0.6899\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3532 - model_160_loss: 0.4866 - model_161_loss: 0.6783 - model_161_1_loss: 0.6896\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3593 - model_160_loss: 0.4851 - model_161_loss: 0.6790 - model_161_1_loss: 0.6899\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3586 - model_160_loss: 0.4862 - model_161_loss: 0.6792 - model_161_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.8471 - model_161_loss: 0.6794 - model_161_1_loss: 0.6896\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3615 - model_160_loss: 0.4860 - model_161_loss: 0.6796 - model_161_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3653 - model_160_loss: 0.4861 - model_161_loss: 0.6804 - model_161_1_loss: 0.6899\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.3711 - model_160_loss: 0.4851 - model_161_loss: 0.6812 - model_161_1_loss: 0.6900\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.3764 - model_160_loss: 0.4874 - model_161_loss: 0.6825 - model_161_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3763 - model_160_loss: 0.4877 - model_161_loss: 0.6823 - model_161_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.8711 - model_161_loss: 0.6833 - model_161_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.3793 - model_160_loss: 0.4870 - model_161_loss: 0.6827 - model_161_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3804 - model_160_loss: 0.4880 - model_161_loss: 0.6830 - model_161_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3920 - model_160_loss: 0.4874 - model_161_loss: 0.6850 - model_161_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3896 - model_160_loss: 0.4882 - model_161_loss: 0.6852 - model_161_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4004 - model_160_loss: 0.4886 - model_161_loss: 0.6870 - model_161_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.8885 - model_161_loss: 0.6865 - model_161_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.3947 - model_160_loss: 0.4889 - model_161_loss: 0.6863 - model_161_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4015 - model_160_loss: 0.4910 - model_161_loss: 0.6877 - model_161_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.3987 - model_160_loss: 0.4903 - model_161_loss: 0.6868 - model_161_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4022 - model_160_loss: 0.4900 - model_161_loss: 0.6877 - model_161_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4115 - model_160_loss: 0.4906 - model_161_loss: 0.6892 - model_161_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 62us/sample - loss: 6.9060 - model_161_loss: 0.6894 - model_161_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4110 - model_160_loss: 0.4897 - model_161_loss: 0.6892 - model_161_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4151 - model_160_loss: 0.4927 - model_161_loss: 0.6902 - model_161_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4159 - model_160_loss: 0.4926 - model_161_loss: 0.6902 - model_161_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4128 - model_160_loss: 0.4930 - model_161_loss: 0.6899 - model_161_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4224 - model_160_loss: 0.4936 - model_161_loss: 0.6917 - model_161_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 65us/sample - loss: 6.9162 - model_161_loss: 0.6919 - model_161_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4180 - model_160_loss: 0.4946 - model_161_loss: 0.6910 - model_161_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4217 - model_160_loss: 0.4952 - model_161_loss: 0.6921 - model_161_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4208 - model_160_loss: 0.4944 - model_161_loss: 0.6916 - model_161_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4213 - model_160_loss: 0.4950 - model_161_loss: 0.6918 - model_161_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4280 - model_160_loss: 0.4966 - model_161_loss: 0.6932 - model_161_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9229 - model_161_loss: 0.6925 - model_161_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4241 - model_160_loss: 0.4951 - model_161_loss: 0.6921 - model_161_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4255 - model_160_loss: 0.4980 - model_161_loss: 0.6929 - model_161_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4249 - model_160_loss: 0.4984 - model_161_loss: 0.6929 - model_161_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4244 - model_160_loss: 0.5000 - model_161_loss: 0.6928 - model_161_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4278 - model_160_loss: 0.4989 - model_161_loss: 0.6933 - model_161_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9263 - model_161_loss: 0.6927 - model_161_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4224 - model_160_loss: 0.4988 - model_161_loss: 0.6925 - model_161_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4242 - model_160_loss: 0.4994 - model_161_loss: 0.6927 - model_161_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4225 - model_160_loss: 0.5006 - model_161_loss: 0.6928 - model_161_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4227 - model_160_loss: 0.5012 - model_161_loss: 0.6928 - model_161_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4253 - model_160_loss: 0.5016 - model_161_loss: 0.6931 - model_161_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.9254 - model_161_loss: 0.6925 - model_161_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4243 - model_160_loss: 0.5004 - model_161_loss: 0.6927 - model_161_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4220 - model_160_loss: 0.5016 - model_161_loss: 0.6925 - model_161_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4235 - model_160_loss: 0.5010 - model_161_loss: 0.6927 - model_161_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4230 - model_160_loss: 0.5029 - model_161_loss: 0.6928 - model_161_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4258 - model_160_loss: 0.4994 - model_161_loss: 0.6928 - model_161_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9243 - model_161_loss: 0.6924 - model_161_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4205 - model_160_loss: 0.5027 - model_161_loss: 0.6924 - model_161_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4221 - model_160_loss: 0.5016 - model_161_loss: 0.6924 - model_161_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4212 - model_160_loss: 0.5016 - model_161_loss: 0.6923 - model_161_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4208 - model_160_loss: 0.5023 - model_161_loss: 0.6924 - model_161_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4225 - model_160_loss: 0.5009 - model_161_loss: 0.6923 - model_161_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 64us/sample - loss: 6.9257 - model_161_loss: 0.6923 - model_161_1_loss: 0.69271s - loss: 6.8960 - model_161_loss: 0.6845 \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4192 - model_160_loss: 0.5032 - model_161_loss: 0.6923 - model_161_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4220 - model_160_loss: 0.5019 - model_161_loss: 0.6922 - model_161_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4219 - model_160_loss: 0.5009 - model_161_loss: 0.6921 - model_161_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4236 - model_160_loss: 0.5013 - model_161_loss: 0.6924 - model_161_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4213 - model_160_loss: 0.5017 - model_161_loss: 0.6924 - model_161_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 64us/sample - loss: 6.9244 - model_161_loss: 0.6926 - model_161_1_loss: 0.69231s - loss: 6.9547 - model_161_loss: 0.6949 - mo\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: -6.4204 - model_160_loss: 0.5015 - model_161_loss: 0.6920 - model_161_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4201 - model_160_loss: 0.5010 - model_161_loss: 0.6919 - model_161_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4206 - model_160_loss: 0.5009 - model_161_loss: 0.6921 - model_161_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4243 - model_160_loss: 0.4980 - model_161_loss: 0.6922 - model_161_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4246 - model_160_loss: 0.4999 - model_161_loss: 0.6924 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9232 - model_161_loss: 0.6929 - model_161_1_loss: 0.69240s - loss: 6.9257 - model_161_loss: 0.6923 - model_161_1_loss: - ETA: 0s - loss: 6.9201 - model_161_loss: 0.6915 - model_161_1_loss: \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4224 - model_160_loss: 0.4980 - model_161_loss: 0.6917 - model_161_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4232 - model_160_loss: 0.4989 - model_161_loss: 0.6921 - model_161_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4215 - model_160_loss: 0.4988 - model_161_loss: 0.6919 - model_161_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4246 - model_160_loss: 0.4983 - model_161_loss: 0.6923 - model_161_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4214 - model_160_loss: 0.4984 - model_161_loss: 0.6920 - model_161_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 55us/sample - loss: 6.9248 - model_161_loss: 0.6928 - model_161_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4233 - model_160_loss: 0.4980 - model_161_loss: 0.6921 - model_161_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4233 - model_160_loss: 0.4984 - model_161_loss: 0.6921 - model_161_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4240 - model_160_loss: 0.4983 - model_161_loss: 0.6922 - model_161_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4229 - model_160_loss: 0.4991 - model_161_loss: 0.6921 - model_161_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4238 - model_160_loss: 0.4991 - model_161_loss: 0.6924 - model_161_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 64us/sample - loss: 6.9232 - model_161_loss: 0.6926 - model_161_1_loss: 0.69221s - loss: 6.8998 - model_161_loss: 0.6903 - model_161_1_loss: - ETA: 0s - loss: 6.9283 - model_161_loss: 0.6934 - model_161_1\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4250 - model_160_loss: 0.4966 - model_161_loss: 0.6921 - model_161_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4225 - model_160_loss: 0.4971 - model_161_loss: 0.6918 - model_161_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4244 - model_160_loss: 0.4978 - model_161_loss: 0.6923 - model_161_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4263 - model_160_loss: 0.4959 - model_161_loss: 0.6923 - model_161_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4232 - model_160_loss: 0.4999 - model_161_loss: 0.6924 - model_161_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: 6.9254 - model_161_loss: 0.6927 - model_161_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4258 - model_160_loss: 0.4980 - model_161_loss: 0.6925 - model_161_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4278 - model_160_loss: 0.4983 - model_161_loss: 0.6928 - model_161_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4248 - model_160_loss: 0.4991 - model_161_loss: 0.6925 - model_161_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4272 - model_160_loss: 0.4976 - model_161_loss: 0.6926 - model_161_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4272 - model_160_loss: 0.4973 - model_161_loss: 0.6925 - model_161_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 69us/sample - loss: 6.9261 - model_161_loss: 0.6935 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4269 - model_160_loss: 0.4978 - model_161_loss: 0.6926 - model_161_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4261 - model_160_loss: 0.4984 - model_161_loss: 0.6926 - model_161_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4255 - model_160_loss: 0.4982 - model_161_loss: 0.6925 - model_161_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4257 - model_160_loss: 0.4990 - model_161_loss: 0.6926 - model_161_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4267 - model_160_loss: 0.4988 - model_161_loss: 0.6928 - model_161_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 63us/sample - loss: 6.9270 - model_161_loss: 0.6928 - model_161_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4253 - model_160_loss: 0.4991 - model_161_loss: 0.6925 - model_161_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4270 - model_160_loss: 0.4977 - model_161_loss: 0.6924 - model_161_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4265 - model_160_loss: 0.4982 - model_161_loss: 0.6925 - model_161_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4283 - model_160_loss: 0.4965 - model_161_loss: 0.6926 - model_161_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4279 - model_160_loss: 0.4975 - model_161_loss: 0.6928 - model_161_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: 6.9250 - model_161_loss: 0.6917 - model_161_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4258 - model_160_loss: 0.4981 - model_161_loss: 0.6925 - model_161_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 21us/sample - loss: -6.4279 - model_160_loss: 0.4972 - model_161_loss: 0.6927 - model_161_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4265 - model_160_loss: 0.4986 - model_161_loss: 0.6926 - model_161_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4285 - model_160_loss: 0.4977 - model_161_loss: 0.6929 - model_161_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4305 - model_160_loss: 0.4967 - model_161_loss: 0.6931 - model_161_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: 6.9268 - model_161_loss: 0.6926 - model_161_1_loss: 0.69231s - loss: 6.9698 - model_161_loss: 0.7057 - model_16 - ETA: 0s - loss: 6.9748 - model_161_loss: 0.7010 - model_161_1_loss:\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4286 - model_160_loss: 0.4966 - model_161_loss: 0.6926 - model_161_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 22us/sample - loss: -6.4277 - model_160_loss: 0.4968 - model_161_loss: 0.6925 - model_161_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4283 - model_160_loss: 0.4975 - model_161_loss: 0.6927 - model_161_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4284 - model_160_loss: 0.4980 - model_161_loss: 0.6927 - model_161_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4285 - model_160_loss: 0.4983 - model_161_loss: 0.6928 - model_161_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 74us/sample - loss: 6.9275 - model_161_loss: 0.6925 - model_161_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4263 - model_160_loss: 0.4977 - model_161_loss: 0.6924 - model_161_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4264 - model_160_loss: 0.4970 - model_161_loss: 0.6923 - model_161_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4259 - model_160_loss: 0.4979 - model_161_loss: 0.6924 - model_161_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4268 - model_160_loss: 0.4971 - model_161_loss: 0.6924 - model_161_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4268 - model_160_loss: 0.4981 - model_161_loss: 0.6925 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 67us/sample - loss: 6.9265 - model_161_loss: 0.6940 - model_161_1_loss: 0.69260s - loss: 6.9749 - model_161_loss: 0.7012 - model\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4283 - model_160_loss: 0.4967 - model_161_loss: 0.6926 - model_161_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4281 - model_160_loss: 0.4971 - model_161_loss: 0.6927 - model_161_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4277 - model_160_loss: 0.4977 - model_161_loss: 0.6926 - model_161_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4262 - model_160_loss: 0.4990 - model_161_loss: 0.6927 - model_161_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4288 - model_160_loss: 0.4958 - model_161_loss: 0.6926 - model_161_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9265 - model_161_loss: 0.6929 - model_161_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4276 - model_160_loss: 0.4967 - model_161_loss: 0.6925 - model_161_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4266 - model_160_loss: 0.4975 - model_161_loss: 0.6924 - model_161_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4279 - model_160_loss: 0.4964 - model_161_loss: 0.6925 - model_161_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4283 - model_160_loss: 0.4966 - model_161_loss: 0.6926 - model_161_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4287 - model_160_loss: 0.4963 - model_161_loss: 0.6927 - model_161_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: 6.9264 - model_161_loss: 0.6925 - model_161_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -6.4266 - model_160_loss: 0.4969 - model_161_loss: 0.6925 - model_161_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4304 - model_160_loss: 0.4952 - model_161_loss: 0.6927 - model_161_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4275 - model_160_loss: 0.4971 - model_161_loss: 0.6926 - model_161_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4278 - model_160_loss: 0.4961 - model_161_loss: 0.6925 - model_161_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4279 - model_160_loss: 0.4959 - model_161_loss: 0.6924 - model_161_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.9268 - model_161_loss: 0.6932 - model_161_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4275 - model_160_loss: 0.4964 - model_161_loss: 0.6924 - model_161_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4306 - model_160_loss: 0.4951 - model_161_loss: 0.6925 - model_161_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4294 - model_160_loss: 0.4957 - model_161_loss: 0.6926 - model_161_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4291 - model_160_loss: 0.4962 - model_161_loss: 0.6926 - model_161_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4281 - model_160_loss: 0.4961 - model_161_loss: 0.6925 - model_161_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9266 - model_161_loss: 0.6929 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4285 - model_160_loss: 0.4955 - model_161_loss: 0.6925 - model_161_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4278 - model_160_loss: 0.4962 - model_161_loss: 0.6924 - model_161_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4276 - model_160_loss: 0.4977 - model_161_loss: 0.6924 - model_161_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4294 - model_160_loss: 0.4960 - model_161_loss: 0.6926 - model_161_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4294 - model_160_loss: 0.4960 - model_161_loss: 0.6925 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9259 - model_161_loss: 0.6929 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4288 - model_160_loss: 0.4968 - model_161_loss: 0.6927 - model_161_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4298 - model_160_loss: 0.4964 - model_161_loss: 0.6927 - model_161_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4305 - model_160_loss: 0.4958 - model_161_loss: 0.6929 - model_161_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4288 - model_160_loss: 0.4957 - model_161_loss: 0.6925 - model_161_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4307 - model_160_loss: 0.4958 - model_161_loss: 0.6928 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 64us/sample - loss: 6.9279 - model_161_loss: 0.6932 - model_161_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4299 - model_160_loss: 0.4957 - model_161_loss: 0.6925 - model_161_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4306 - model_160_loss: 0.4954 - model_161_loss: 0.6927 - model_161_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4298 - model_160_loss: 0.4961 - model_161_loss: 0.6926 - model_161_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4307 - model_160_loss: 0.4943 - model_161_loss: 0.6925 - model_161_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4304 - model_160_loss: 0.4950 - model_161_loss: 0.6926 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 63us/sample - loss: 6.9269 - model_161_loss: 0.6930 - model_161_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4274 - model_160_loss: 0.4958 - model_161_loss: 0.6923 - model_161_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4294 - model_160_loss: 0.4960 - model_161_loss: 0.6927 - model_161_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4293 - model_160_loss: 0.4959 - model_161_loss: 0.6925 - model_161_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4298 - model_160_loss: 0.4936 - model_161_loss: 0.6923 - model_161_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4293 - model_160_loss: 0.4950 - model_161_loss: 0.6924 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 64us/sample - loss: 6.9264 - model_161_loss: 0.6925 - model_161_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4293 - model_160_loss: 0.4950 - model_161_loss: 0.6924 - model_161_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4288 - model_160_loss: 0.4958 - model_161_loss: 0.6926 - model_161_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4286 - model_160_loss: 0.4950 - model_161_loss: 0.6923 - model_161_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4286 - model_160_loss: 0.4952 - model_161_loss: 0.6923 - model_161_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4277 - model_160_loss: 0.4959 - model_161_loss: 0.6923 - model_161_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.9257 - model_161_loss: 0.6924 - model_161_1_loss: 0.69250s - loss: 6.9184 - model_161_loss: 0.6908 - model_161_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4284 - model_160_loss: 0.4951 - model_161_loss: 0.6924 - model_161_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4294 - model_160_loss: 0.4953 - model_161_loss: 0.6925 - model_161_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4269 - model_160_loss: 0.4962 - model_161_loss: 0.6922 - model_161_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4280 - model_160_loss: 0.4961 - model_161_loss: 0.6924 - model_161_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4300 - model_160_loss: 0.4951 - model_161_loss: 0.6925 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.9265 - model_161_loss: 0.6928 - model_161_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4313 - model_160_loss: 0.4941 - model_161_loss: 0.6926 - model_161_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4299 - model_160_loss: 0.4953 - model_161_loss: 0.6925 - model_161_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4284 - model_160_loss: 0.4962 - model_161_loss: 0.6924 - model_161_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4292 - model_160_loss: 0.4962 - model_161_loss: 0.6927 - model_161_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4287 - model_160_loss: 0.4977 - model_161_loss: 0.6929 - model_161_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 74us/sample - loss: 6.9273 - model_161_loss: 0.6932 - model_161_1_loss: 0.6930\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4296 - model_160_loss: 0.4949 - model_161_loss: 0.6925 - model_161_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4297 - model_160_loss: 0.4957 - model_161_loss: 0.6926 - model_161_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4294 - model_160_loss: 0.4968 - model_161_loss: 0.6926 - model_161_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4303 - model_160_loss: 0.4959 - model_161_loss: 0.6927 - model_161_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 23us/sample - loss: -6.4303 - model_160_loss: 0.4957 - model_161_loss: 0.6926 - model_161_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 90us/sample - loss: 6.9280 - model_161_loss: 0.6924 - model_161_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4302 - model_160_loss: 0.4961 - model_161_loss: 0.6927 - model_161_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4300 - model_160_loss: 0.4959 - model_161_loss: 0.6925 - model_161_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4293 - model_160_loss: 0.4963 - model_161_loss: 0.6926 - model_161_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4291 - model_160_loss: 0.4969 - model_161_loss: 0.6926 - model_161_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4294 - model_160_loss: 0.4966 - model_161_loss: 0.6927 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 71us/sample - loss: 6.9266 - model_161_loss: 0.6926 - model_161_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4315 - model_160_loss: 0.4948 - model_161_loss: 0.6927 - model_161_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4307 - model_160_loss: 0.4965 - model_161_loss: 0.6928 - model_161_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4322 - model_160_loss: 0.4950 - model_161_loss: 0.6927 - model_161_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4336 - model_160_loss: 0.4944 - model_161_loss: 0.6929 - model_161_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4314 - model_160_loss: 0.4957 - model_161_loss: 0.6927 - model_161_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 77us/sample - loss: 6.9277 - model_161_loss: 0.6936 - model_161_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4302 - model_160_loss: 0.4954 - model_161_loss: 0.6926 - model_161_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4309 - model_160_loss: 0.4956 - model_161_loss: 0.6927 - model_161_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4312 - model_160_loss: 0.4946 - model_161_loss: 0.6926 - model_161_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4310 - model_160_loss: 0.4939 - model_161_loss: 0.6924 - model_161_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4300 - model_160_loss: 0.4955 - model_161_loss: 0.6925 - model_161_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 66us/sample - loss: 6.9269 - model_161_loss: 0.6929 - model_161_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4308 - model_160_loss: 0.4952 - model_161_loss: 0.6927 - model_161_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4301 - model_160_loss: 0.4950 - model_161_loss: 0.6926 - model_161_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4308 - model_160_loss: 0.4958 - model_161_loss: 0.6928 - model_161_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4319 - model_160_loss: 0.4960 - model_161_loss: 0.6930 - model_161_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4317 - model_160_loss: 0.4945 - model_161_loss: 0.6928 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 71us/sample - loss: 6.9272 - model_161_loss: 0.6923 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4305 - model_160_loss: 0.4954 - model_161_loss: 0.6926 - model_161_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4310 - model_160_loss: 0.4953 - model_161_loss: 0.6926 - model_161_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4296 - model_160_loss: 0.4959 - model_161_loss: 0.6926 - model_161_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4300 - model_160_loss: 0.4954 - model_161_loss: 0.6925 - model_161_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4303 - model_160_loss: 0.4952 - model_161_loss: 0.6926 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 65us/sample - loss: 6.9278 - model_161_loss: 0.6934 - model_161_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4302 - model_160_loss: 0.4960 - model_161_loss: 0.6927 - model_161_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4316 - model_160_loss: 0.4937 - model_161_loss: 0.6926 - model_161_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4315 - model_160_loss: 0.4940 - model_161_loss: 0.6926 - model_161_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4316 - model_160_loss: 0.4942 - model_161_loss: 0.6927 - model_161_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4323 - model_160_loss: 0.4942 - model_161_loss: 0.6927 - model_161_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 72us/sample - loss: 6.9269 - model_161_loss: 0.6928 - model_161_1_loss: 0.69250s - loss: 6.9268 - model_161_loss: 0.6919 - model_161_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4312 - model_160_loss: 0.4951 - model_161_loss: 0.6928 - model_161_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4293 - model_160_loss: 0.4953 - model_161_loss: 0.6925 - model_161_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4310 - model_160_loss: 0.4949 - model_161_loss: 0.6927 - model_161_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4320 - model_160_loss: 0.4944 - model_161_loss: 0.6927 - model_161_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4310 - model_160_loss: 0.4953 - model_161_loss: 0.6927 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 68us/sample - loss: 6.9269 - model_161_loss: 0.6932 - model_161_1_loss: 0.69260s - loss: 6.8679 - model_161_loss: 0.6817 - model_161\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4309 - model_160_loss: 0.4952 - model_161_loss: 0.6928 - model_161_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4329 - model_160_loss: 0.4945 - model_161_loss: 0.6929 - model_161_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4332 - model_160_loss: 0.4942 - model_161_loss: 0.6929 - model_161_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4328 - model_160_loss: 0.4933 - model_161_loss: 0.6927 - model_161_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4325 - model_160_loss: 0.4949 - model_161_loss: 0.6928 - model_161_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 69us/sample - loss: 6.9280 - model_161_loss: 0.6930 - model_161_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4317 - model_160_loss: 0.4940 - model_161_loss: 0.6926 - model_161_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4315 - model_160_loss: 0.4938 - model_161_loss: 0.6925 - model_161_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4306 - model_160_loss: 0.4949 - model_161_loss: 0.6925 - model_161_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4285 - model_160_loss: 0.4956 - model_161_loss: 0.6924 - model_161_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4305 - model_160_loss: 0.4950 - model_161_loss: 0.6925 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 68us/sample - loss: 6.9259 - model_161_loss: 0.6932 - model_161_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4301 - model_160_loss: 0.4946 - model_161_loss: 0.6925 - model_161_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4307 - model_160_loss: 0.4942 - model_161_loss: 0.6925 - model_161_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4313 - model_160_loss: 0.4935 - model_161_loss: 0.6926 - model_161_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4309 - model_160_loss: 0.4943 - model_161_loss: 0.6926 - model_161_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4315 - model_160_loss: 0.4938 - model_161_loss: 0.6925 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 65us/sample - loss: 6.9262 - model_161_loss: 0.6923 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4306 - model_160_loss: 0.4951 - model_161_loss: 0.6925 - model_161_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4295 - model_160_loss: 0.4950 - model_161_loss: 0.6924 - model_161_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4303 - model_160_loss: 0.4951 - model_161_loss: 0.6927 - model_161_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4299 - model_160_loss: 0.4954 - model_161_loss: 0.6926 - model_161_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4316 - model_160_loss: 0.4946 - model_161_loss: 0.6928 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 65us/sample - loss: 6.9284 - model_161_loss: 0.6931 - model_161_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4324 - model_160_loss: 0.4945 - model_161_loss: 0.6927 - model_161_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4321 - model_160_loss: 0.4949 - model_161_loss: 0.6928 - model_161_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4327 - model_160_loss: 0.4946 - model_161_loss: 0.6930 - model_161_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4303 - model_160_loss: 0.4952 - model_161_loss: 0.6926 - model_161_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4305 - model_160_loss: 0.4956 - model_161_loss: 0.6927 - model_161_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 76us/sample - loss: 6.9273 - model_161_loss: 0.6935 - model_161_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: -6.4319 - model_160_loss: 0.4954 - model_161_loss: 0.6928 - model_161_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4323 - model_160_loss: 0.4954 - model_161_loss: 0.6929 - model_161_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4316 - model_160_loss: 0.4960 - model_161_loss: 0.6929 - model_161_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4308 - model_160_loss: 0.4957 - model_161_loss: 0.6927 - model_161_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4346 - model_160_loss: 0.4940 - model_161_loss: 0.6931 - model_161_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 76us/sample - loss: 6.9274 - model_161_loss: 0.6930 - model_161_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4310 - model_160_loss: 0.4965 - model_161_loss: 0.6928 - model_161_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4333 - model_160_loss: 0.4944 - model_161_loss: 0.6928 - model_161_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4315 - model_160_loss: 0.4951 - model_161_loss: 0.6927 - model_161_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4318 - model_160_loss: 0.4954 - model_161_loss: 0.6928 - model_161_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4328 - model_160_loss: 0.4948 - model_161_loss: 0.6928 - model_161_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 69us/sample - loss: 6.9279 - model_161_loss: 0.6923 - model_161_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4309 - model_160_loss: 0.4951 - model_161_loss: 0.6926 - model_161_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4312 - model_160_loss: 0.4945 - model_161_loss: 0.6926 - model_161_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4317 - model_160_loss: 0.4946 - model_161_loss: 0.6927 - model_161_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4320 - model_160_loss: 0.4942 - model_161_loss: 0.6926 - model_161_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4290 - model_160_loss: 0.4957 - model_161_loss: 0.6925 - model_161_1_loss: 0.6925\n",
      "For Attention Module: 4.2\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 81us/sample - loss: 6.3314 - model_165_loss: 0.6605 - model_165_1_loss: 0.6061\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: -5.9496 - model_164_loss: 0.3765 - model_165_loss: 0.6604 - model_165_1_loss: 0.6049\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -5.9626 - model_164_loss: 0.3773 - model_165_loss: 0.6610 - model_165_1_loss: 0.6070\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -5.9827 - model_164_loss: 0.3765 - model_165_loss: 0.6621 - model_165_1_loss: 0.6098\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -5.9859 - model_164_loss: 0.3744 - model_165_loss: 0.6615 - model_165_1_loss: 0.6106\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -5.9989 - model_164_loss: 0.3755 - model_165_loss: 0.6622 - model_165_1_loss: 0.6126\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 77us/sample - loss: 6.3817 - model_165_loss: 0.6619 - model_165_1_loss: 0.61411s - loss: 6.4370 - model_165_loss: 0.6648\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: -6.0047 - model_164_loss: 0.3764 - model_165_loss: 0.6636 - model_165_1_loss: 0.6127\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.0226 - model_164_loss: 0.3766 - model_165_loss: 0.6634 - model_165_1_loss: 0.6164\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0329 - model_164_loss: 0.3788 - model_165_loss: 0.6647 - model_165_1_loss: 0.6176\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0510 - model_164_loss: 0.3775 - model_165_loss: 0.6652 - model_165_1_loss: 0.6205\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0626 - model_164_loss: 0.3802 - model_165_loss: 0.6657 - model_165_1_loss: 0.6229\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 64us/sample - loss: 6.4511 - model_165_loss: 0.6660 - model_165_1_loss: 0.6243\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0739 - model_164_loss: 0.3809 - model_165_loss: 0.6660 - model_165_1_loss: 0.6250\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.0897 - model_164_loss: 0.3818 - model_165_loss: 0.6667 - model_165_1_loss: 0.6276\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.1008 - model_164_loss: 0.3812 - model_165_loss: 0.6676 - model_165_1_loss: 0.6288\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1153 - model_164_loss: 0.3827 - model_165_loss: 0.6690 - model_165_1_loss: 0.6306\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.1230 - model_164_loss: 0.3863 - model_165_loss: 0.6687 - model_165_1_loss: 0.6332\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 82us/sample - loss: 6.5199 - model_165_loss: 0.6701 - model_165_1_loss: 0.63380s - loss: 6.5225 - model_165_loss: 0.6703 - model_165_1_loss: 0.634 - ETA: 0s - loss: 6.5305 - model_165_loss: 0.6717 - model_165_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.1254 - model_164_loss: 0.3855 - model_165_loss: 0.6684 - model_165_1_loss: 0.6337\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.1433 - model_164_loss: 0.3894 - model_165_loss: 0.6698 - model_165_1_loss: 0.6367\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1593 - model_164_loss: 0.3896 - model_165_loss: 0.6711 - model_165_1_loss: 0.6387\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1622 - model_164_loss: 0.3925 - model_165_loss: 0.6714 - model_165_1_loss: 0.6395\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.1931 - model_164_loss: 0.3924 - model_165_loss: 0.6740 - model_165_1_loss: 0.6431\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 72us/sample - loss: 6.5952 - model_165_loss: 0.6734 - model_165_1_loss: 0.6453\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.1871 - model_164_loss: 0.3967 - model_165_loss: 0.6736 - model_165_1_loss: 0.6431\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2086 - model_164_loss: 0.3988 - model_165_loss: 0.6753 - model_165_1_loss: 0.6462\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2231 - model_164_loss: 0.4003 - model_165_loss: 0.6755 - model_165_1_loss: 0.6492\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2344 - model_164_loss: 0.4035 - model_165_loss: 0.6763 - model_165_1_loss: 0.6513\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2429 - model_164_loss: 0.4056 - model_165_loss: 0.6768 - model_165_1_loss: 0.6529\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 79us/sample - loss: 6.6587 - model_165_loss: 0.6774 - model_165_1_loss: 0.65420s - loss: 6.6349 - model_165_loss: 0.6693 - model_165_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.2569 - model_164_loss: 0.4068 - model_165_loss: 0.6781 - model_165_1_loss: 0.6546\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.2618 - model_164_loss: 0.4086 - model_165_loss: 0.6771 - model_165_1_loss: 0.6570\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.2710 - model_164_loss: 0.4123 - model_165_loss: 0.6796 - model_165_1_loss: 0.6571\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.2855 - model_164_loss: 0.4140 - model_165_loss: 0.6795 - model_165_1_loss: 0.6604\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3084 - model_164_loss: 0.4137 - model_165_loss: 0.6825 - model_165_1_loss: 0.6619\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 76us/sample - loss: 6.7233 - model_165_loss: 0.6825 - model_165_1_loss: 0.6626\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2985 - model_164_loss: 0.4174 - model_165_loss: 0.6813 - model_165_1_loss: 0.6619\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3121 - model_164_loss: 0.4190 - model_165_loss: 0.6819 - model_165_1_loss: 0.6643\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3226 - model_164_loss: 0.4229 - model_165_loss: 0.6833 - model_165_1_loss: 0.6658\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3383 - model_164_loss: 0.4221 - model_165_loss: 0.6838 - model_165_1_loss: 0.6683\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3473 - model_164_loss: 0.4255 - model_165_loss: 0.6843 - model_165_1_loss: 0.6703\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 79us/sample - loss: 6.7755 - model_165_loss: 0.6859 - model_165_1_loss: 0.66970s - loss: 6.7498 - model_165_loss: 0.6797 - model_165_1_lo\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.3470 - model_164_loss: 0.4300 - model_165_loss: 0.6851 - model_165_1_loss: 0.6703\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3516 - model_164_loss: 0.4324 - model_165_loss: 0.6852 - model_165_1_loss: 0.6716\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3613 - model_164_loss: 0.4359 - model_165_loss: 0.6862 - model_165_1_loss: 0.6732\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3711 - model_164_loss: 0.4376 - model_165_loss: 0.6868 - model_165_1_loss: 0.6749\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3749 - model_164_loss: 0.4396 - model_165_loss: 0.6870 - model_165_1_loss: 0.6759\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 78us/sample - loss: 6.8240 - model_165_loss: 0.6872 - model_165_1_loss: 0.6773\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3797 - model_164_loss: 0.4418 - model_165_loss: 0.6880 - model_165_1_loss: 0.6763\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3860 - model_164_loss: 0.4448 - model_165_loss: 0.6886 - model_165_1_loss: 0.6776\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.3882 - model_164_loss: 0.4467 - model_165_loss: 0.6887 - model_165_1_loss: 0.6782\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.3967 - model_164_loss: 0.4487 - model_165_loss: 0.6891 - model_165_1_loss: 0.6800\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.3998 - model_164_loss: 0.4539 - model_165_loss: 0.6896 - model_165_1_loss: 0.6812\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 66us/sample - loss: 6.8625 - model_165_loss: 0.6904 - model_165_1_loss: 0.68261s - loss: 6.8808 - model_165_loss: 0.6934 - mode\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4001 - model_164_loss: 0.4582 - model_165_loss: 0.6899 - model_165_1_loss: 0.6818\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4014 - model_164_loss: 0.4606 - model_165_loss: 0.6897 - model_165_1_loss: 0.6827\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4080 - model_164_loss: 0.4648 - model_165_loss: 0.6905 - model_165_1_loss: 0.6840\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4174 - model_164_loss: 0.4644 - model_165_loss: 0.6917 - model_165_1_loss: 0.6847\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4125 - model_164_loss: 0.4671 - model_165_loss: 0.6910 - model_165_1_loss: 0.6850\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 68us/sample - loss: 6.8875 - model_165_loss: 0.6914 - model_165_1_loss: 0.6859\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4178 - model_164_loss: 0.4707 - model_165_loss: 0.6913 - model_165_1_loss: 0.6864\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4203 - model_164_loss: 0.4716 - model_165_loss: 0.6916 - model_165_1_loss: 0.6868\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4213 - model_164_loss: 0.4764 - model_165_loss: 0.6917 - model_165_1_loss: 0.6878\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4239 - model_164_loss: 0.4761 - model_165_loss: 0.6921 - model_165_1_loss: 0.6879\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4260 - model_164_loss: 0.4769 - model_165_loss: 0.6922 - model_165_1_loss: 0.6884\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 87us/sample - loss: 6.9092 - model_165_loss: 0.6929 - model_165_1_loss: 0.68911s - loss: 6.8776 - model_165_loss: 0.6854 - model_16 - ETA: 0s - loss: 6.9070 - model_165_loss: 0.6924 - model_165_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: -6.4290 - model_164_loss: 0.4791 - model_165_loss: 0.6926 - model_165_1_loss: 0.6890\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: -6.4274 - model_164_loss: 0.4823 - model_165_loss: 0.6927 - model_165_1_loss: 0.6892\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4263 - model_164_loss: 0.4847 - model_165_loss: 0.6927 - model_165_1_loss: 0.6895\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4298 - model_164_loss: 0.4839 - model_165_loss: 0.6930 - model_165_1_loss: 0.6897\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4279 - model_164_loss: 0.4840 - model_165_loss: 0.6927 - model_165_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 74us/sample - loss: 6.9183 - model_165_loss: 0.6929 - model_165_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4322 - model_164_loss: 0.4843 - model_165_loss: 0.6929 - model_165_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4305 - model_164_loss: 0.4881 - model_165_loss: 0.6931 - model_165_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4340 - model_164_loss: 0.4844 - model_165_loss: 0.6927 - model_165_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4339 - model_164_loss: 0.4862 - model_165_loss: 0.6928 - model_165_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4334 - model_164_loss: 0.4890 - model_165_loss: 0.6932 - model_165_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 76us/sample - loss: 6.9243 - model_165_loss: 0.6929 - model_165_1_loss: 0.69180s - loss: 6.9383 - model_165_loss: 0.6952 - model_165_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4320 - model_164_loss: 0.4914 - model_165_loss: 0.6931 - model_165_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4350 - model_164_loss: 0.4887 - model_165_loss: 0.6931 - model_165_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4361 - model_164_loss: 0.4879 - model_165_loss: 0.6930 - model_165_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4404 - model_164_loss: 0.4876 - model_165_loss: 0.6934 - model_165_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4386 - model_164_loss: 0.4884 - model_165_loss: 0.6932 - model_165_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 71us/sample - loss: 6.9246 - model_165_loss: 0.6925 - model_165_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4377 - model_164_loss: 0.4896 - model_165_loss: 0.6933 - model_165_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4388 - model_164_loss: 0.4891 - model_165_loss: 0.6934 - model_165_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4438 - model_164_loss: 0.4862 - model_165_loss: 0.6938 - model_165_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4400 - model_164_loss: 0.4864 - model_165_loss: 0.6931 - model_165_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4373 - model_164_loss: 0.4887 - model_165_loss: 0.6930 - model_165_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 74us/sample - loss: 6.9240 - model_165_loss: 0.6931 - model_165_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4328 - model_164_loss: 0.4879 - model_165_loss: 0.6925 - model_165_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4362 - model_164_loss: 0.4867 - model_165_loss: 0.6925 - model_165_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4377 - model_164_loss: 0.4865 - model_165_loss: 0.6928 - model_165_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4367 - model_164_loss: 0.4849 - model_165_loss: 0.6926 - model_165_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4385 - model_164_loss: 0.4840 - model_165_loss: 0.6927 - model_165_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 73us/sample - loss: 6.9227 - model_165_loss: 0.6924 - model_165_1_loss: 0.69201s - loss: 6.9365 - model_165_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4370 - model_164_loss: 0.4837 - model_165_loss: 0.6929 - model_165_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4350 - model_164_loss: 0.4834 - model_165_loss: 0.6925 - model_165_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4373 - model_164_loss: 0.4836 - model_165_loss: 0.6926 - model_165_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4376 - model_164_loss: 0.4811 - model_165_loss: 0.6924 - model_165_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4396 - model_164_loss: 0.4789 - model_165_loss: 0.6928 - model_165_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 72us/sample - loss: 6.9185 - model_165_loss: 0.6919 - model_165_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4347 - model_164_loss: 0.4817 - model_165_loss: 0.6921 - model_165_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4379 - model_164_loss: 0.4794 - model_165_loss: 0.6923 - model_165_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4377 - model_164_loss: 0.4797 - model_165_loss: 0.6923 - model_165_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4391 - model_164_loss: 0.4779 - model_165_loss: 0.6922 - model_165_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4357 - model_164_loss: 0.4787 - model_165_loss: 0.6921 - model_165_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9068 - model_165_loss: 0.6904 - model_165_1_loss: 0.690 - 2s 73us/sample - loss: 6.9171 - model_165_loss: 0.6925 - model_165_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4350 - model_164_loss: 0.4783 - model_165_loss: 0.6918 - model_165_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4375 - model_164_loss: 0.4792 - model_165_loss: 0.6921 - model_165_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4365 - model_164_loss: 0.4769 - model_165_loss: 0.6918 - model_165_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4403 - model_164_loss: 0.4767 - model_165_loss: 0.6923 - model_165_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4375 - model_164_loss: 0.4762 - model_165_loss: 0.6920 - model_165_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 76us/sample - loss: 6.9158 - model_165_loss: 0.6921 - model_165_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4389 - model_164_loss: 0.4760 - model_165_loss: 0.6917 - model_165_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4371 - model_164_loss: 0.4776 - model_165_loss: 0.6915 - model_165_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4407 - model_164_loss: 0.4756 - model_165_loss: 0.6917 - model_165_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4388 - model_164_loss: 0.4755 - model_165_loss: 0.6916 - model_165_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4375 - model_164_loss: 0.4770 - model_165_loss: 0.6916 - model_165_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 75us/sample - loss: 6.9155 - model_165_loss: 0.6919 - model_165_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4370 - model_164_loss: 0.4761 - model_165_loss: 0.6916 - model_165_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4398 - model_164_loss: 0.4751 - model_165_loss: 0.6917 - model_165_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4395 - model_164_loss: 0.4752 - model_165_loss: 0.6918 - model_165_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4384 - model_164_loss: 0.4767 - model_165_loss: 0.6919 - model_165_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4433 - model_164_loss: 0.4749 - model_165_loss: 0.6919 - model_165_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 87us/sample - loss: 6.9165 - model_165_loss: 0.6920 - model_165_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4371 - model_164_loss: 0.4773 - model_165_loss: 0.6916 - model_165_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4364 - model_164_loss: 0.4774 - model_165_loss: 0.6916 - model_165_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4410 - model_164_loss: 0.4749 - model_165_loss: 0.6917 - model_165_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4409 - model_164_loss: 0.4775 - model_165_loss: 0.6921 - model_165_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4421 - model_164_loss: 0.4754 - model_165_loss: 0.6921 - model_165_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 83us/sample - loss: 6.9175 - model_165_loss: 0.6933 - model_165_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4403 - model_164_loss: 0.4750 - model_165_loss: 0.6922 - model_165_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4370 - model_164_loss: 0.4770 - model_165_loss: 0.6918 - model_165_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4412 - model_164_loss: 0.4763 - model_165_loss: 0.6923 - model_165_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4398 - model_164_loss: 0.4762 - model_165_loss: 0.6921 - model_165_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4409 - model_164_loss: 0.4762 - model_165_loss: 0.6922 - model_165_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 73us/sample - loss: 6.9192 - model_165_loss: 0.6928 - model_165_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4423 - model_164_loss: 0.4745 - model_165_loss: 0.6921 - model_165_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4440 - model_164_loss: 0.4741 - model_165_loss: 0.6922 - model_165_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4409 - model_164_loss: 0.4763 - model_165_loss: 0.6921 - model_165_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4445 - model_164_loss: 0.4742 - model_165_loss: 0.6924 - model_165_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4436 - model_164_loss: 0.4748 - model_165_loss: 0.6924 - model_165_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 74us/sample - loss: 6.9164 - model_165_loss: 0.6924 - model_165_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4420 - model_164_loss: 0.4748 - model_165_loss: 0.6922 - model_165_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4448 - model_164_loss: 0.4729 - model_165_loss: 0.6922 - model_165_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4444 - model_164_loss: 0.4728 - model_165_loss: 0.6922 - model_165_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4479 - model_164_loss: 0.4714 - model_165_loss: 0.6923 - model_165_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4445 - model_164_loss: 0.4724 - model_165_loss: 0.6919 - model_165_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 75us/sample - loss: 6.9211 - model_165_loss: 0.6934 - model_165_1_loss: 0.69160s - loss: 6.8766 - model_165_loss: 0.6865 - model_165\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4480 - model_164_loss: 0.4715 - model_165_loss: 0.6924 - model_165_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4478 - model_164_loss: 0.4729 - model_165_loss: 0.6923 - model_165_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4487 - model_164_loss: 0.4711 - model_165_loss: 0.6923 - model_165_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4490 - model_164_loss: 0.4717 - model_165_loss: 0.6925 - model_165_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4506 - model_164_loss: 0.4701 - model_165_loss: 0.6924 - model_165_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 78us/sample - loss: 6.9193 - model_165_loss: 0.6928 - model_165_1_loss: 0.69130s - loss: 6.9725 - model_165_loss: 0.7008 - model_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4480 - model_164_loss: 0.4695 - model_165_loss: 0.6922 - model_165_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4481 - model_164_loss: 0.4691 - model_165_loss: 0.6922 - model_165_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4482 - model_164_loss: 0.4695 - model_165_loss: 0.6922 - model_165_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4498 - model_164_loss: 0.4687 - model_165_loss: 0.6925 - model_165_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4482 - model_164_loss: 0.4693 - model_165_loss: 0.6922 - model_165_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 73us/sample - loss: 6.9154 - model_165_loss: 0.6918 - model_165_1_loss: 0.69111s - loss: 7.0218 - model_165_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4449 - model_164_loss: 0.4697 - model_165_loss: 0.6919 - model_165_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4459 - model_164_loss: 0.4688 - model_165_loss: 0.6920 - model_165_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4469 - model_164_loss: 0.4704 - model_165_loss: 0.6923 - model_165_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4455 - model_164_loss: 0.4696 - model_165_loss: 0.6920 - model_165_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4474 - model_164_loss: 0.4689 - model_165_loss: 0.6924 - model_165_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 76us/sample - loss: 6.9191 - model_165_loss: 0.6928 - model_165_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4478 - model_164_loss: 0.4693 - model_165_loss: 0.6920 - model_165_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4472 - model_164_loss: 0.4686 - model_165_loss: 0.6922 - model_165_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4442 - model_164_loss: 0.4697 - model_165_loss: 0.6918 - model_165_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4470 - model_164_loss: 0.4674 - model_165_loss: 0.6922 - model_165_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4455 - model_164_loss: 0.4700 - model_165_loss: 0.6921 - model_165_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 72us/sample - loss: 6.9132 - model_165_loss: 0.6925 - model_165_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4473 - model_164_loss: 0.4686 - model_165_loss: 0.6923 - model_165_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4493 - model_164_loss: 0.4676 - model_165_loss: 0.6925 - model_165_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4518 - model_164_loss: 0.4663 - model_165_loss: 0.6925 - model_165_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4499 - model_164_loss: 0.4676 - model_165_loss: 0.6925 - model_165_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4506 - model_164_loss: 0.4665 - model_165_loss: 0.6923 - model_165_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 75us/sample - loss: 6.9180 - model_165_loss: 0.6920 - model_165_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4478 - model_164_loss: 0.4670 - model_165_loss: 0.6923 - model_165_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4469 - model_164_loss: 0.4664 - model_165_loss: 0.6919 - model_165_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4474 - model_164_loss: 0.4690 - model_165_loss: 0.6922 - model_165_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4485 - model_164_loss: 0.4692 - model_165_loss: 0.6921 - model_165_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4499 - model_164_loss: 0.4673 - model_165_loss: 0.6922 - model_165_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 88us/sample - loss: 6.9202 - model_165_loss: 0.6923 - model_165_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4496 - model_164_loss: 0.4669 - model_165_loss: 0.6921 - model_165_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4530 - model_164_loss: 0.4670 - model_165_loss: 0.6927 - model_165_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4552 - model_164_loss: 0.4653 - model_165_loss: 0.6926 - model_165_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4517 - model_164_loss: 0.4689 - model_165_loss: 0.6924 - model_165_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4542 - model_164_loss: 0.4669 - model_165_loss: 0.6923 - model_165_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 75us/sample - loss: 6.9225 - model_165_loss: 0.6923 - model_165_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4517 - model_164_loss: 0.4668 - model_165_loss: 0.6923 - model_165_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4497 - model_164_loss: 0.4673 - model_165_loss: 0.6925 - model_165_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4532 - model_164_loss: 0.4660 - model_165_loss: 0.6923 - model_165_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4509 - model_164_loss: 0.4681 - model_165_loss: 0.6925 - model_165_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4507 - model_164_loss: 0.4681 - model_165_loss: 0.6921 - model_165_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 70us/sample - loss: 6.9205 - model_165_loss: 0.6929 - model_165_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4472 - model_164_loss: 0.4701 - model_165_loss: 0.6924 - model_165_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4506 - model_164_loss: 0.4675 - model_165_loss: 0.6924 - model_165_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4507 - model_164_loss: 0.4694 - model_165_loss: 0.6924 - model_165_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4500 - model_164_loss: 0.4707 - model_165_loss: 0.6924 - model_165_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4522 - model_164_loss: 0.4697 - model_165_loss: 0.6925 - model_165_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 74us/sample - loss: 6.9228 - model_165_loss: 0.6930 - model_165_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4494 - model_164_loss: 0.4713 - model_165_loss: 0.6926 - model_165_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4504 - model_164_loss: 0.4720 - model_165_loss: 0.6928 - model_165_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4527 - model_164_loss: 0.4700 - model_165_loss: 0.6925 - model_165_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4523 - model_164_loss: 0.4719 - model_165_loss: 0.6927 - model_165_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4515 - model_164_loss: 0.4723 - model_165_loss: 0.6927 - model_165_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 78us/sample - loss: 6.9257 - model_165_loss: 0.6927 - model_165_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4550 - model_164_loss: 0.4705 - model_165_loss: 0.6929 - model_165_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4516 - model_164_loss: 0.4712 - model_165_loss: 0.6925 - model_165_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4526 - model_164_loss: 0.4727 - model_165_loss: 0.6930 - model_165_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4519 - model_164_loss: 0.4725 - model_165_loss: 0.6925 - model_165_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4518 - model_164_loss: 0.4732 - model_165_loss: 0.6927 - model_165_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 91us/sample - loss: 6.9253 - model_165_loss: 0.6921 - model_165_1_loss: 0.69231s - loss: 6.9548 - model_165_loss: 0.6991 - \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4537 - model_164_loss: 0.4719 - model_165_loss: 0.6928 - model_165_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4532 - model_164_loss: 0.4716 - model_165_loss: 0.6926 - model_165_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4522 - model_164_loss: 0.4722 - model_165_loss: 0.6926 - model_165_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4537 - model_164_loss: 0.4707 - model_165_loss: 0.6927 - model_165_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4541 - model_164_loss: 0.4718 - model_165_loss: 0.6929 - model_165_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 76us/sample - loss: 6.9228 - model_165_loss: 0.6927 - model_165_1_loss: 0.69180s - loss: 6.9106 - model_165_loss: 0.6904 - model_165_1_loss: \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4512 - model_164_loss: 0.4690 - model_165_loss: 0.6924 - model_165_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4488 - model_164_loss: 0.4707 - model_165_loss: 0.6925 - model_165_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4529 - model_164_loss: 0.4682 - model_165_loss: 0.6926 - model_165_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4512 - model_164_loss: 0.4694 - model_165_loss: 0.6925 - model_165_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4500 - model_164_loss: 0.4674 - model_165_loss: 0.6923 - model_165_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 73us/sample - loss: 6.9183 - model_165_loss: 0.6924 - model_165_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4487 - model_164_loss: 0.4679 - model_165_loss: 0.6922 - model_165_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4538 - model_164_loss: 0.4662 - model_165_loss: 0.6926 - model_165_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4522 - model_164_loss: 0.4652 - model_165_loss: 0.6924 - model_165_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4499 - model_164_loss: 0.4659 - model_165_loss: 0.6922 - model_165_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4516 - model_164_loss: 0.4648 - model_165_loss: 0.6923 - model_165_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 79us/sample - loss: 6.9191 - model_165_loss: 0.6922 - model_165_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4484 - model_164_loss: 0.4665 - model_165_loss: 0.6921 - model_165_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4518 - model_164_loss: 0.4661 - model_165_loss: 0.6923 - model_165_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4547 - model_164_loss: 0.4633 - model_165_loss: 0.6927 - model_165_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4523 - model_164_loss: 0.4642 - model_165_loss: 0.6923 - model_165_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4534 - model_164_loss: 0.4622 - model_165_loss: 0.6922 - model_165_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 83us/sample - loss: 6.9165 - model_165_loss: 0.6919 - model_165_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: -6.4554 - model_164_loss: 0.4634 - model_165_loss: 0.6924 - model_165_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: -6.4544 - model_164_loss: 0.4633 - model_165_loss: 0.6924 - model_165_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4521 - model_164_loss: 0.4643 - model_165_loss: 0.6924 - model_165_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4536 - model_164_loss: 0.4643 - model_165_loss: 0.6922 - model_165_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4551 - model_164_loss: 0.4615 - model_165_loss: 0.6922 - model_165_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 85us/sample - loss: 6.9200 - model_165_loss: 0.6930 - model_165_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4504 - model_164_loss: 0.4646 - model_165_loss: 0.6924 - model_165_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4538 - model_164_loss: 0.4632 - model_165_loss: 0.6924 - model_165_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4576 - model_164_loss: 0.4631 - model_165_loss: 0.6926 - model_165_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4571 - model_164_loss: 0.4649 - model_165_loss: 0.6928 - model_165_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4556 - model_164_loss: 0.4654 - model_165_loss: 0.6926 - model_165_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 83us/sample - loss: 6.9207 - model_165_loss: 0.6922 - model_165_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4536 - model_164_loss: 0.4660 - model_165_loss: 0.6926 - model_165_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4527 - model_164_loss: 0.4662 - model_165_loss: 0.6926 - model_165_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4534 - model_164_loss: 0.4696 - model_165_loss: 0.6928 - model_165_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4579 - model_164_loss: 0.4677 - model_165_loss: 0.6927 - model_165_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4587 - model_164_loss: 0.4693 - model_165_loss: 0.6929 - model_165_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 85us/sample - loss: 6.9272 - model_165_loss: 0.6928 - model_165_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4520 - model_164_loss: 0.4707 - model_165_loss: 0.6925 - model_165_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4513 - model_164_loss: 0.4724 - model_165_loss: 0.6928 - model_165_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4546 - model_164_loss: 0.4711 - model_165_loss: 0.6928 - model_165_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4535 - model_164_loss: 0.4732 - model_165_loss: 0.6929 - model_165_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4525 - model_164_loss: 0.4753 - model_165_loss: 0.6929 - model_165_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 86us/sample - loss: 6.9268 - model_165_loss: 0.6925 - model_165_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4565 - model_164_loss: 0.4716 - model_165_loss: 0.6929 - model_165_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4513 - model_164_loss: 0.4753 - model_165_loss: 0.6928 - model_165_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4518 - model_164_loss: 0.4752 - model_165_loss: 0.6928 - model_165_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4529 - model_164_loss: 0.4739 - model_165_loss: 0.6929 - model_165_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4564 - model_164_loss: 0.4714 - model_165_loss: 0.6931 - model_165_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 83us/sample - loss: 6.9272 - model_165_loss: 0.6930 - model_165_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4552 - model_164_loss: 0.4710 - model_165_loss: 0.6929 - model_165_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4551 - model_164_loss: 0.4709 - model_165_loss: 0.6929 - model_165_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4553 - model_164_loss: 0.4709 - model_165_loss: 0.6927 - model_165_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4561 - model_164_loss: 0.4690 - model_165_loss: 0.6927 - model_165_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4566 - model_164_loss: 0.4694 - model_165_loss: 0.6928 - model_165_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 79us/sample - loss: 6.9239 - model_165_loss: 0.6925 - model_165_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4559 - model_164_loss: 0.4662 - model_165_loss: 0.6927 - model_165_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4533 - model_164_loss: 0.4682 - model_165_loss: 0.6928 - model_165_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4551 - model_164_loss: 0.4652 - model_165_loss: 0.6925 - model_165_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4565 - model_164_loss: 0.4645 - model_165_loss: 0.6926 - model_165_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4542 - model_164_loss: 0.4638 - model_165_loss: 0.6923 - model_165_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 80us/sample - loss: 6.9190 - model_165_loss: 0.6923 - model_165_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4559 - model_164_loss: 0.4617 - model_165_loss: 0.6923 - model_165_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4515 - model_164_loss: 0.4628 - model_165_loss: 0.6922 - model_165_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4533 - model_164_loss: 0.4618 - model_165_loss: 0.6922 - model_165_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4547 - model_164_loss: 0.4599 - model_165_loss: 0.6920 - model_165_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4513 - model_164_loss: 0.4616 - model_165_loss: 0.6922 - model_165_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 88us/sample - loss: 6.9176 - model_165_loss: 0.6930 - model_165_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4573 - model_164_loss: 0.4606 - model_165_loss: 0.6922 - model_165_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4600 - model_164_loss: 0.4589 - model_165_loss: 0.6924 - model_165_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4593 - model_164_loss: 0.4602 - model_165_loss: 0.6923 - model_165_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4626 - model_164_loss: 0.4585 - model_165_loss: 0.6925 - model_165_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4628 - model_164_loss: 0.4584 - model_165_loss: 0.6926 - model_165_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 86us/sample - loss: 6.9162 - model_165_loss: 0.6924 - model_165_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4576 - model_164_loss: 0.4593 - model_165_loss: 0.6924 - model_165_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4554 - model_164_loss: 0.4605 - model_165_loss: 0.6922 - model_165_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4561 - model_164_loss: 0.4598 - model_165_loss: 0.6921 - model_165_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4534 - model_164_loss: 0.4601 - model_165_loss: 0.6921 - model_165_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4579 - model_164_loss: 0.4592 - model_165_loss: 0.6921 - model_165_1_loss: 0.6913\n",
      "For Attention Module: 4.3\n",
      "features X: 30940 samples, 69 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.3658 - model_169_loss: 0.6616 - model_169_1_loss: 0.6120\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: -5.9792 - model_168_loss: 0.3843 - model_169_loss: 0.6609 - model_169_1_loss: 0.6118\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -5.9918 - model_168_loss: 0.3858 - model_169_loss: 0.6616 - model_169_1_loss: 0.6139\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.0006 - model_168_loss: 0.3856 - model_169_loss: 0.6612 - model_169_1_loss: 0.6161\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.0172 - model_168_loss: 0.3847 - model_169_loss: 0.6626 - model_169_1_loss: 0.6178\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.0204 - model_168_loss: 0.3851 - model_169_loss: 0.6611 - model_169_1_loss: 0.6199\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.4172 - model_169_loss: 0.6622 - model_169_1_loss: 0.6205\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0334 - model_168_loss: 0.3860 - model_169_loss: 0.6632 - model_169_1_loss: 0.6206\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.0281 - model_168_loss: 0.3855 - model_169_loss: 0.6621 - model_169_1_loss: 0.6206\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.0552 - model_168_loss: 0.3887 - model_169_loss: 0.6638 - model_169_1_loss: 0.6250\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.0663 - model_168_loss: 0.3880 - model_169_loss: 0.6642 - model_169_1_loss: 0.6266\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.0857 - model_168_loss: 0.3891 - model_169_loss: 0.6655 - model_169_1_loss: 0.6295\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 62us/sample - loss: 6.4764 - model_169_loss: 0.6650 - model_169_1_loss: 0.6304\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.0720 - model_168_loss: 0.3895 - model_169_loss: 0.6643 - model_169_1_loss: 0.6280\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.0849 - model_168_loss: 0.3900 - model_169_loss: 0.6645 - model_169_1_loss: 0.6304\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.1004 - model_168_loss: 0.3919 - model_169_loss: 0.6659 - model_169_1_loss: 0.6326\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.1178 - model_168_loss: 0.3931 - model_169_loss: 0.6669 - model_169_1_loss: 0.6353\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.1234 - model_168_loss: 0.3962 - model_169_loss: 0.6672 - model_169_1_loss: 0.6368\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.5343 - model_169_loss: 0.6685 - model_169_1_loss: 0.63870s - loss: 6.5388 - model_169_loss: 0.6690 - model_169_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.1295 - model_168_loss: 0.3963 - model_169_loss: 0.6673 - model_169_1_loss: 0.6379\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.1428 - model_168_loss: 0.3970 - model_169_loss: 0.6684 - model_169_1_loss: 0.6396\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.1543 - model_168_loss: 0.3968 - model_169_loss: 0.6685 - model_169_1_loss: 0.6417\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.1677 - model_168_loss: 0.4008 - model_169_loss: 0.6695 - model_169_1_loss: 0.6442\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.1860 - model_168_loss: 0.4016 - model_169_loss: 0.6708 - model_169_1_loss: 0.6467\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.5841 - model_169_loss: 0.6700 - model_169_1_loss: 0.6469\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.1986 - model_168_loss: 0.4019 - model_169_loss: 0.6715 - model_169_1_loss: 0.6486\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.2055 - model_168_loss: 0.4053 - model_169_loss: 0.6712 - model_169_1_loss: 0.6509\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2135 - model_168_loss: 0.4072 - model_169_loss: 0.6728 - model_169_1_loss: 0.6514\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2410 - model_168_loss: 0.4079 - model_169_loss: 0.6741 - model_169_1_loss: 0.6557\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.2469 - model_168_loss: 0.4113 - model_169_loss: 0.6742 - model_169_1_loss: 0.6575\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.6583 - model_169_loss: 0.6742 - model_169_1_loss: 0.65660s - loss: 6.6706 - model_169_loss: 0.6793 - model_169_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2339 - model_168_loss: 0.4142 - model_169_loss: 0.6734 - model_169_1_loss: 0.6563\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.2484 - model_168_loss: 0.4139 - model_169_loss: 0.6747 - model_169_1_loss: 0.6578\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.2482 - model_168_loss: 0.4159 - model_169_loss: 0.6737 - model_169_1_loss: 0.6591\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.2659 - model_168_loss: 0.4172 - model_169_loss: 0.6754 - model_169_1_loss: 0.6612\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.2801 - model_168_loss: 0.4181 - model_169_loss: 0.6768 - model_169_1_loss: 0.6629\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.7076 - model_169_loss: 0.6767 - model_169_1_loss: 0.6645\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2753 - model_168_loss: 0.4210 - model_169_loss: 0.6757 - model_169_1_loss: 0.6636\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.2797 - model_168_loss: 0.4223 - model_169_loss: 0.6763 - model_169_1_loss: 0.6642\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3004 - model_168_loss: 0.4227 - model_169_loss: 0.6781 - model_169_1_loss: 0.6665\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3053 - model_168_loss: 0.4227 - model_169_loss: 0.6784 - model_169_1_loss: 0.6672\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3141 - model_168_loss: 0.4259 - model_169_loss: 0.6802 - model_169_1_loss: 0.6678\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.7447 - model_169_loss: 0.6803 - model_169_1_loss: 0.66880s - loss: 6.7725 - model_169_loss: 0.6847 - model_16\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3084 - model_168_loss: 0.4274 - model_169_loss: 0.6787 - model_169_1_loss: 0.6684\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3170 - model_168_loss: 0.4314 - model_169_loss: 0.6803 - model_169_1_loss: 0.6694\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3309 - model_168_loss: 0.4297 - model_169_loss: 0.6808 - model_169_1_loss: 0.6713\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3308 - model_168_loss: 0.4334 - model_169_loss: 0.6809 - model_169_1_loss: 0.6719\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3402 - model_168_loss: 0.4347 - model_169_loss: 0.6819 - model_169_1_loss: 0.6730\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 62us/sample - loss: 6.7812 - model_169_loss: 0.6825 - model_169_1_loss: 0.67350s - loss: 6.7956 - model_169_loss: 0.6843 - model_169_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3417 - model_168_loss: 0.4377 - model_169_loss: 0.6828 - model_169_1_loss: 0.6731\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3487 - model_168_loss: 0.4373 - model_169_loss: 0.6827 - model_169_1_loss: 0.6745\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3546 - model_168_loss: 0.4391 - model_169_loss: 0.6832 - model_169_1_loss: 0.6755\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3624 - model_168_loss: 0.4406 - model_169_loss: 0.6842 - model_169_1_loss: 0.6764\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.3687 - model_168_loss: 0.4438 - model_169_loss: 0.6858 - model_169_1_loss: 0.6767\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: 6.8130 - model_169_loss: 0.6856 - model_169_1_loss: 0.6773\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3657 - model_168_loss: 0.4455 - model_169_loss: 0.6851 - model_169_1_loss: 0.6772\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3692 - model_168_loss: 0.4488 - model_169_loss: 0.6854 - model_169_1_loss: 0.6782\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.3728 - model_168_loss: 0.4513 - model_169_loss: 0.6857 - model_169_1_loss: 0.6792\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3782 - model_168_loss: 0.4531 - model_169_loss: 0.6872 - model_169_1_loss: 0.6791\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3792 - model_168_loss: 0.4560 - model_169_loss: 0.6865 - model_169_1_loss: 0.6806\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.8506 - model_169_loss: 0.6879 - model_169_1_loss: 0.6818\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3873 - model_168_loss: 0.4562 - model_169_loss: 0.6875 - model_169_1_loss: 0.6812\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3899 - model_168_loss: 0.4598 - model_169_loss: 0.6878 - model_169_1_loss: 0.6821\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.3884 - model_168_loss: 0.4635 - model_169_loss: 0.6881 - model_169_1_loss: 0.6823\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.3974 - model_168_loss: 0.4646 - model_169_loss: 0.6889 - model_169_1_loss: 0.6835\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4025 - model_168_loss: 0.4660 - model_169_loss: 0.6894 - model_169_1_loss: 0.6843\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 59us/sample - loss: 6.8723 - model_169_loss: 0.6891 - model_169_1_loss: 0.6847\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4036 - model_168_loss: 0.4674 - model_169_loss: 0.6896 - model_169_1_loss: 0.6846\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4029 - model_168_loss: 0.4687 - model_169_loss: 0.6895 - model_169_1_loss: 0.6848\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4108 - model_168_loss: 0.4702 - model_169_loss: 0.6902 - model_169_1_loss: 0.6860\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4147 - model_168_loss: 0.4713 - model_169_loss: 0.6909 - model_169_1_loss: 0.6863\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4171 - model_168_loss: 0.4689 - model_169_loss: 0.6908 - model_169_1_loss: 0.6864\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.8920 - model_169_loss: 0.6912 - model_169_1_loss: 0.68731s - loss: 6.8790 - model_169_loss: 0.6903 - m\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4143 - model_168_loss: 0.4740 - model_169_loss: 0.6905 - model_169_1_loss: 0.6871\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4223 - model_168_loss: 0.4736 - model_169_loss: 0.6909 - model_169_1_loss: 0.6882\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4203 - model_168_loss: 0.4762 - model_169_loss: 0.6909 - model_169_1_loss: 0.6883\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4220 - model_168_loss: 0.4759 - model_169_loss: 0.6909 - model_169_1_loss: 0.6887\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4283 - model_168_loss: 0.4772 - model_169_loss: 0.6915 - model_169_1_loss: 0.6896\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: 6.9075 - model_169_loss: 0.6916 - model_169_1_loss: 0.6899\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4255 - model_168_loss: 0.4787 - model_169_loss: 0.6917 - model_169_1_loss: 0.6891\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4269 - model_168_loss: 0.4797 - model_169_loss: 0.6918 - model_169_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4243 - model_168_loss: 0.4815 - model_169_loss: 0.6913 - model_169_1_loss: 0.6899\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4319 - model_168_loss: 0.4808 - model_169_loss: 0.6922 - model_169_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4336 - model_168_loss: 0.4844 - model_169_loss: 0.6926 - model_169_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.9152 - model_169_loss: 0.6919 - model_169_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4292 - model_168_loss: 0.4844 - model_169_loss: 0.6924 - model_169_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4281 - model_168_loss: 0.4846 - model_169_loss: 0.6918 - model_169_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4320 - model_168_loss: 0.4846 - model_169_loss: 0.6919 - model_169_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4321 - model_168_loss: 0.4853 - model_169_loss: 0.6924 - model_169_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4327 - model_168_loss: 0.4860 - model_169_loss: 0.6923 - model_169_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.9200 - model_169_loss: 0.6931 - model_169_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4307 - model_168_loss: 0.4858 - model_169_loss: 0.6921 - model_169_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4307 - model_168_loss: 0.4884 - model_169_loss: 0.6918 - model_169_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4314 - model_168_loss: 0.4893 - model_169_loss: 0.6924 - model_169_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4327 - model_168_loss: 0.4896 - model_169_loss: 0.6925 - model_169_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4332 - model_168_loss: 0.4880 - model_169_loss: 0.6923 - model_169_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.9264 - model_169_loss: 0.6921 - model_169_1_loss: 0.69291s - loss: 6.9958 - model_169_loss: 0.7036 - model_169_1_ - ETA: 0s - loss: 6.9309 - model_169_loss: 0.6926 - model_169_1_loss:\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4286 - model_168_loss: 0.4885 - model_169_loss: 0.6915 - model_169_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4313 - model_168_loss: 0.4884 - model_169_loss: 0.6918 - model_169_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4315 - model_168_loss: 0.4890 - model_169_loss: 0.6920 - model_169_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4308 - model_168_loss: 0.4884 - model_169_loss: 0.6918 - model_169_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4304 - model_168_loss: 0.4901 - model_169_loss: 0.6922 - model_169_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9143 - model_169_loss: 0.6907 - model_169_1_loss: 0.692 - 1s 59us/sample - loss: 6.9209 - model_169_loss: 0.6926 - model_169_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4324 - model_168_loss: 0.4887 - model_169_loss: 0.6920 - model_169_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4293 - model_168_loss: 0.4890 - model_169_loss: 0.6915 - model_169_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4329 - model_168_loss: 0.4864 - model_169_loss: 0.6917 - model_169_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4316 - model_168_loss: 0.4891 - model_169_loss: 0.6922 - model_169_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4348 - model_168_loss: 0.4876 - model_169_loss: 0.6920 - model_169_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 63us/sample - loss: 6.9215 - model_169_loss: 0.6917 - model_169_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4334 - model_168_loss: 0.4872 - model_169_loss: 0.6919 - model_169_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4332 - model_168_loss: 0.4870 - model_169_loss: 0.6921 - model_169_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4322 - model_168_loss: 0.4880 - model_169_loss: 0.6919 - model_169_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4346 - model_168_loss: 0.4852 - model_169_loss: 0.6921 - model_169_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4368 - model_168_loss: 0.4827 - model_169_loss: 0.6919 - model_169_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9204 - model_169_loss: 0.6925 - model_169_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4345 - model_168_loss: 0.4837 - model_169_loss: 0.6916 - model_169_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4367 - model_168_loss: 0.4835 - model_169_loss: 0.6919 - model_169_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4349 - model_168_loss: 0.4812 - model_169_loss: 0.6913 - model_169_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4405 - model_168_loss: 0.4824 - model_169_loss: 0.6926 - model_169_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4378 - model_168_loss: 0.4812 - model_169_loss: 0.6919 - model_169_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 65us/sample - loss: 6.9183 - model_169_loss: 0.6917 - model_169_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4368 - model_168_loss: 0.4802 - model_169_loss: 0.6917 - model_169_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4380 - model_168_loss: 0.4789 - model_169_loss: 0.6917 - model_169_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4377 - model_168_loss: 0.4794 - model_169_loss: 0.6919 - model_169_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4389 - model_168_loss: 0.4774 - model_169_loss: 0.6917 - model_169_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4396 - model_168_loss: 0.4779 - model_169_loss: 0.6918 - model_169_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 61us/sample - loss: 6.9156 - model_169_loss: 0.6915 - model_169_1_loss: 0.69111s - loss: 7.0537 - model_169_loss: 0.7138 - model_169_ - ETA: 0s - loss: 6.9300 - model_169_loss: 0.6934 - model_169_1_l\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4398 - model_168_loss: 0.4766 - model_169_loss: 0.6919 - model_169_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4391 - model_168_loss: 0.4752 - model_169_loss: 0.6916 - model_169_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4387 - model_168_loss: 0.4751 - model_169_loss: 0.6919 - model_169_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4380 - model_168_loss: 0.4747 - model_169_loss: 0.6918 - model_169_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4415 - model_168_loss: 0.4726 - model_169_loss: 0.6917 - model_169_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 62us/sample - loss: 6.9180 - model_169_loss: 0.6921 - model_169_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4355 - model_168_loss: 0.4748 - model_169_loss: 0.6914 - model_169_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4388 - model_168_loss: 0.4725 - model_169_loss: 0.6917 - model_169_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4395 - model_168_loss: 0.4712 - model_169_loss: 0.6915 - model_169_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4383 - model_168_loss: 0.4712 - model_169_loss: 0.6914 - model_169_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4400 - model_168_loss: 0.4701 - model_169_loss: 0.6915 - model_169_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: 6.9137 - model_169_loss: 0.6916 - model_169_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4355 - model_168_loss: 0.4713 - model_169_loss: 0.6912 - model_169_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4373 - model_168_loss: 0.4713 - model_169_loss: 0.6917 - model_169_1_loss: 0.6900\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4387 - model_168_loss: 0.4696 - model_169_loss: 0.6917 - model_169_1_loss: 0.6900\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4364 - model_168_loss: 0.4711 - model_169_loss: 0.6911 - model_169_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4378 - model_168_loss: 0.4696 - model_169_loss: 0.6917 - model_169_1_loss: 0.6898\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 62us/sample - loss: 6.9104 - model_169_loss: 0.6917 - model_169_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4400 - model_168_loss: 0.4705 - model_169_loss: 0.6919 - model_169_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4411 - model_168_loss: 0.4683 - model_169_loss: 0.6918 - model_169_1_loss: 0.6901\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4405 - model_168_loss: 0.4680 - model_169_loss: 0.6914 - model_169_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4416 - model_168_loss: 0.4681 - model_169_loss: 0.6917 - model_169_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4421 - model_168_loss: 0.4685 - model_169_loss: 0.6920 - model_169_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: 6.9131 - model_169_loss: 0.6923 - model_169_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4404 - model_168_loss: 0.4685 - model_169_loss: 0.6916 - model_169_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 24us/sample - loss: -6.4412 - model_168_loss: 0.4683 - model_169_loss: 0.6916 - model_169_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4419 - model_168_loss: 0.4688 - model_169_loss: 0.6915 - model_169_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4415 - model_168_loss: 0.4677 - model_169_loss: 0.6914 - model_169_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4431 - model_168_loss: 0.4681 - model_169_loss: 0.6917 - model_169_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 62us/sample - loss: 6.9144 - model_169_loss: 0.6926 - model_169_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4468 - model_168_loss: 0.4689 - model_169_loss: 0.6923 - model_169_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4440 - model_168_loss: 0.4691 - model_169_loss: 0.6920 - model_169_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4439 - model_168_loss: 0.4690 - model_169_loss: 0.6919 - model_169_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4450 - model_168_loss: 0.4695 - model_169_loss: 0.6920 - model_169_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4476 - model_168_loss: 0.4701 - model_169_loss: 0.6925 - model_169_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 63us/sample - loss: 6.9153 - model_169_loss: 0.6920 - model_169_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4493 - model_168_loss: 0.4703 - model_169_loss: 0.6924 - model_169_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.4484 - model_168_loss: 0.4706 - model_169_loss: 0.6923 - model_169_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4514 - model_168_loss: 0.4703 - model_169_loss: 0.6925 - model_169_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4486 - model_168_loss: 0.4710 - model_169_loss: 0.6925 - model_169_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4515 - model_168_loss: 0.4703 - model_169_loss: 0.6925 - model_169_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 75us/sample - loss: 6.9203 - model_169_loss: 0.6921 - model_169_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4445 - model_168_loss: 0.4718 - model_169_loss: 0.6921 - model_169_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4448 - model_168_loss: 0.4725 - model_169_loss: 0.6921 - model_169_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4467 - model_168_loss: 0.4727 - model_169_loss: 0.6925 - model_169_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4473 - model_168_loss: 0.4729 - model_169_loss: 0.6925 - model_169_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4442 - model_168_loss: 0.4743 - model_169_loss: 0.6923 - model_169_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 80us/sample - loss: 6.9203 - model_169_loss: 0.6921 - model_169_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -6.4496 - model_168_loss: 0.4721 - model_169_loss: 0.6928 - model_169_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4439 - model_168_loss: 0.4744 - model_169_loss: 0.6923 - model_169_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4466 - model_168_loss: 0.4739 - model_169_loss: 0.6925 - model_169_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4433 - model_168_loss: 0.4750 - model_169_loss: 0.6923 - model_169_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4449 - model_168_loss: 0.4734 - model_169_loss: 0.6919 - model_169_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 76us/sample - loss: 6.9241 - model_169_loss: 0.6927 - model_169_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4501 - model_168_loss: 0.4733 - model_169_loss: 0.6925 - model_169_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4480 - model_168_loss: 0.4745 - model_169_loss: 0.6926 - model_169_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4477 - model_168_loss: 0.4733 - model_169_loss: 0.6924 - model_169_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4482 - model_168_loss: 0.4750 - model_169_loss: 0.6926 - model_169_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4479 - model_168_loss: 0.4758 - model_169_loss: 0.6927 - model_169_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 87us/sample - loss: 6.9241 - model_169_loss: 0.6928 - model_169_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4467 - model_168_loss: 0.4753 - model_169_loss: 0.6926 - model_169_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4497 - model_168_loss: 0.4737 - model_169_loss: 0.6927 - model_169_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4444 - model_168_loss: 0.4752 - model_169_loss: 0.6924 - model_169_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4479 - model_168_loss: 0.4749 - model_169_loss: 0.6924 - model_169_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4490 - model_168_loss: 0.4741 - model_169_loss: 0.6928 - model_169_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 76us/sample - loss: 6.9249 - model_169_loss: 0.6927 - model_169_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4450 - model_168_loss: 0.4756 - model_169_loss: 0.6926 - model_169_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4473 - model_168_loss: 0.4760 - model_169_loss: 0.6928 - model_169_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4484 - model_168_loss: 0.4744 - model_169_loss: 0.6925 - model_169_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4482 - model_168_loss: 0.4748 - model_169_loss: 0.6926 - model_169_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4476 - model_168_loss: 0.4745 - model_169_loss: 0.6924 - model_169_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 75us/sample - loss: 6.9217 - model_169_loss: 0.6931 - model_169_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4495 - model_168_loss: 0.4732 - model_169_loss: 0.6928 - model_169_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4497 - model_168_loss: 0.4725 - model_169_loss: 0.6925 - model_169_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4513 - model_168_loss: 0.4723 - model_169_loss: 0.6928 - model_169_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4472 - model_168_loss: 0.4735 - model_169_loss: 0.6928 - model_169_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4484 - model_168_loss: 0.4724 - model_169_loss: 0.6923 - model_169_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 76us/sample - loss: 6.9208 - model_169_loss: 0.6927 - model_169_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4459 - model_168_loss: 0.4718 - model_169_loss: 0.6922 - model_169_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4476 - model_168_loss: 0.4710 - model_169_loss: 0.6923 - model_169_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4505 - model_168_loss: 0.4695 - model_169_loss: 0.6925 - model_169_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4499 - model_168_loss: 0.4690 - model_169_loss: 0.6924 - model_169_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4493 - model_168_loss: 0.4709 - model_169_loss: 0.6926 - model_169_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 76us/sample - loss: 6.9183 - model_169_loss: 0.6919 - model_169_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4487 - model_168_loss: 0.4704 - model_169_loss: 0.6923 - model_169_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4487 - model_168_loss: 0.4691 - model_169_loss: 0.6923 - model_169_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4475 - model_168_loss: 0.4702 - model_169_loss: 0.6921 - model_169_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4501 - model_168_loss: 0.4689 - model_169_loss: 0.6924 - model_169_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4513 - model_168_loss: 0.4684 - model_169_loss: 0.6923 - model_169_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 69us/sample - loss: 6.9178 - model_169_loss: 0.6923 - model_169_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4498 - model_168_loss: 0.4671 - model_169_loss: 0.6922 - model_169_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4479 - model_168_loss: 0.4683 - model_169_loss: 0.6923 - model_169_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4480 - model_168_loss: 0.4681 - model_169_loss: 0.6922 - model_169_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4483 - model_168_loss: 0.4687 - model_169_loss: 0.6922 - model_169_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4468 - model_168_loss: 0.4701 - model_169_loss: 0.6920 - model_169_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 66us/sample - loss: 6.9192 - model_169_loss: 0.6920 - model_169_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4470 - model_168_loss: 0.4682 - model_169_loss: 0.6920 - model_169_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4464 - model_168_loss: 0.4680 - model_169_loss: 0.6919 - model_169_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4504 - model_168_loss: 0.4678 - model_169_loss: 0.6921 - model_169_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4530 - model_168_loss: 0.4663 - model_169_loss: 0.6922 - model_169_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4499 - model_168_loss: 0.4685 - model_169_loss: 0.6921 - model_169_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 65us/sample - loss: 6.9203 - model_169_loss: 0.6928 - model_169_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4485 - model_168_loss: 0.4705 - model_169_loss: 0.6924 - model_169_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4511 - model_168_loss: 0.4691 - model_169_loss: 0.6924 - model_169_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4487 - model_168_loss: 0.4715 - model_169_loss: 0.6923 - model_169_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4495 - model_168_loss: 0.4693 - model_169_loss: 0.6921 - model_169_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4510 - model_168_loss: 0.4710 - model_169_loss: 0.6927 - model_169_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 75us/sample - loss: 6.9239 - model_169_loss: 0.6924 - model_169_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4489 - model_168_loss: 0.4712 - model_169_loss: 0.6924 - model_169_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4478 - model_168_loss: 0.4720 - model_169_loss: 0.6924 - model_169_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4502 - model_168_loss: 0.4715 - model_169_loss: 0.6925 - model_169_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4522 - model_168_loss: 0.4712 - model_169_loss: 0.6926 - model_169_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4476 - model_168_loss: 0.4731 - model_169_loss: 0.6922 - model_169_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 78us/sample - loss: 6.9225 - model_169_loss: 0.6920 - model_169_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4510 - model_168_loss: 0.4724 - model_169_loss: 0.6927 - model_169_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4535 - model_168_loss: 0.4710 - model_169_loss: 0.6927 - model_169_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4494 - model_168_loss: 0.4742 - model_169_loss: 0.6925 - model_169_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4488 - model_168_loss: 0.4733 - model_169_loss: 0.6924 - model_169_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4535 - model_168_loss: 0.4727 - model_169_loss: 0.6931 - model_169_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 77us/sample - loss: 6.9241 - model_169_loss: 0.6931 - model_169_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4495 - model_168_loss: 0.4729 - model_169_loss: 0.6925 - model_169_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4501 - model_168_loss: 0.4733 - model_169_loss: 0.6927 - model_169_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4514 - model_168_loss: 0.4716 - model_169_loss: 0.6924 - model_169_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4489 - model_168_loss: 0.4732 - model_169_loss: 0.6923 - model_169_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4531 - model_168_loss: 0.4712 - model_169_loss: 0.6926 - model_169_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 88us/sample - loss: 6.9223 - model_169_loss: 0.6936 - model_169_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4516 - model_168_loss: 0.4716 - model_169_loss: 0.6926 - model_169_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4488 - model_168_loss: 0.4714 - model_169_loss: 0.6923 - model_169_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4516 - model_168_loss: 0.4704 - model_169_loss: 0.6926 - model_169_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4494 - model_168_loss: 0.4702 - model_169_loss: 0.6922 - model_169_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4501 - model_168_loss: 0.4698 - model_169_loss: 0.6923 - model_169_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 81us/sample - loss: 6.9208 - model_169_loss: 0.6920 - model_169_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4522 - model_168_loss: 0.4687 - model_169_loss: 0.6924 - model_169_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4512 - model_168_loss: 0.4685 - model_169_loss: 0.6924 - model_169_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4521 - model_168_loss: 0.4689 - model_169_loss: 0.6926 - model_169_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4536 - model_168_loss: 0.4673 - model_169_loss: 0.6927 - model_169_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4521 - model_168_loss: 0.4672 - model_169_loss: 0.6924 - model_169_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 79us/sample - loss: 6.9211 - model_169_loss: 0.6921 - model_169_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4502 - model_168_loss: 0.4682 - model_169_loss: 0.6922 - model_169_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4527 - model_168_loss: 0.4669 - model_169_loss: 0.6926 - model_169_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4544 - model_168_loss: 0.4659 - model_169_loss: 0.6924 - model_169_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4531 - model_168_loss: 0.4666 - model_169_loss: 0.6924 - model_169_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4532 - model_168_loss: 0.4661 - model_169_loss: 0.6925 - model_169_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 77us/sample - loss: 6.9203 - model_169_loss: 0.6929 - model_169_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4487 - model_168_loss: 0.4686 - model_169_loss: 0.6922 - model_169_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4516 - model_168_loss: 0.4653 - model_169_loss: 0.6922 - model_169_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4562 - model_168_loss: 0.4635 - model_169_loss: 0.6926 - model_169_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4536 - model_168_loss: 0.4647 - model_169_loss: 0.6925 - model_169_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4524 - model_168_loss: 0.4647 - model_169_loss: 0.6924 - model_169_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 83us/sample - loss: 6.9182 - model_169_loss: 0.6922 - model_169_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4526 - model_168_loss: 0.4648 - model_169_loss: 0.6923 - model_169_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4493 - model_168_loss: 0.4649 - model_169_loss: 0.6920 - model_169_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4492 - model_168_loss: 0.4649 - model_169_loss: 0.6921 - model_169_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4516 - model_168_loss: 0.4631 - model_169_loss: 0.6922 - model_169_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4522 - model_168_loss: 0.4643 - model_169_loss: 0.6923 - model_169_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 93us/sample - loss: 6.9175 - model_169_loss: 0.6927 - model_169_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 52us/sample - loss: -6.4525 - model_168_loss: 0.4627 - model_169_loss: 0.6923 - model_169_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -6.4513 - model_168_loss: 0.4645 - model_169_loss: 0.6924 - model_169_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4516 - model_168_loss: 0.4635 - model_169_loss: 0.6923 - model_169_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4530 - model_168_loss: 0.4644 - model_169_loss: 0.6926 - model_169_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4523 - model_168_loss: 0.4640 - model_169_loss: 0.6922 - model_169_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 84us/sample - loss: 6.9166 - model_169_loss: 0.6922 - model_169_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4510 - model_168_loss: 0.4644 - model_169_loss: 0.6921 - model_169_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4535 - model_168_loss: 0.4620 - model_169_loss: 0.6923 - model_169_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4529 - model_168_loss: 0.4640 - model_169_loss: 0.6924 - model_169_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4532 - model_168_loss: 0.4644 - model_169_loss: 0.6924 - model_169_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4509 - model_168_loss: 0.4648 - model_169_loss: 0.6922 - model_169_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 72us/sample - loss: 6.9166 - model_169_loss: 0.6925 - model_169_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4534 - model_168_loss: 0.4628 - model_169_loss: 0.6924 - model_169_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4526 - model_168_loss: 0.4636 - model_169_loss: 0.6921 - model_169_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4526 - model_168_loss: 0.4642 - model_169_loss: 0.6922 - model_169_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4526 - model_168_loss: 0.4634 - model_169_loss: 0.6922 - model_169_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4521 - model_168_loss: 0.4637 - model_169_loss: 0.6921 - model_169_1_loss: 0.6911\n",
      "For Attention Module: 4.3999999999999995\n",
      "features X: 30940 samples, 69 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 66us/sample - loss: 6.3757 - model_173_loss: 0.6615 - model_173_1_loss: 0.6144\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: -5.9961 - model_172_loss: 0.3864 - model_173_loss: 0.6605 - model_173_1_loss: 0.6160\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -5.9940 - model_172_loss: 0.3850 - model_173_loss: 0.6600 - model_173_1_loss: 0.6158\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0069 - model_172_loss: 0.3851 - model_173_loss: 0.6608 - model_173_1_loss: 0.6176\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0246 - model_172_loss: 0.3859 - model_173_loss: 0.6622 - model_173_1_loss: 0.6199\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0349 - model_172_loss: 0.3856 - model_173_loss: 0.6626 - model_173_1_loss: 0.6215\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 81us/sample - loss: 6.4249 - model_173_loss: 0.6630 - model_173_1_loss: 0.6217\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0429 - model_172_loss: 0.3873 - model_173_loss: 0.6631 - model_173_1_loss: 0.6229\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0468 - model_172_loss: 0.3858 - model_173_loss: 0.6633 - model_173_1_loss: 0.6232\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0586 - model_172_loss: 0.3894 - model_173_loss: 0.6647 - model_173_1_loss: 0.6249\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0736 - model_172_loss: 0.3886 - model_173_loss: 0.6641 - model_173_1_loss: 0.6284\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0732 - model_172_loss: 0.3921 - model_173_loss: 0.6642 - model_173_1_loss: 0.6289\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 83us/sample - loss: 6.4768 - model_173_loss: 0.6660 - model_173_1_loss: 0.6296\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.0907 - model_172_loss: 0.3905 - model_173_loss: 0.6652 - model_173_1_loss: 0.6310\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1019 - model_172_loss: 0.3914 - model_173_loss: 0.6664 - model_173_1_loss: 0.6323\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.1181 - model_172_loss: 0.3926 - model_173_loss: 0.6675 - model_173_1_loss: 0.6347\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.1193 - model_172_loss: 0.3935 - model_173_loss: 0.6664 - model_173_1_loss: 0.6361\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.1414 - model_172_loss: 0.3939 - model_173_loss: 0.6685 - model_173_1_loss: 0.6386\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 64us/sample - loss: 6.5466 - model_173_loss: 0.6689 - model_173_1_loss: 0.64090s - loss: 6.5948 - model_173_loss: 0.6756 - model_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.1431 - model_172_loss: 0.3960 - model_173_loss: 0.6677 - model_173_1_loss: 0.6401\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.1587 - model_172_loss: 0.3972 - model_173_loss: 0.6678 - model_173_1_loss: 0.6434\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.1785 - model_172_loss: 0.3984 - model_173_loss: 0.6706 - model_173_1_loss: 0.6448\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.1913 - model_172_loss: 0.3998 - model_173_loss: 0.6709 - model_173_1_loss: 0.6473\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.2129 - model_172_loss: 0.4014 - model_173_loss: 0.6727 - model_173_1_loss: 0.6502\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 68us/sample - loss: 6.6181 - model_173_loss: 0.6730 - model_173_1_loss: 0.6512\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.2170 - model_172_loss: 0.4022 - model_173_loss: 0.6725 - model_173_1_loss: 0.6514\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.2245 - model_172_loss: 0.4046 - model_173_loss: 0.6727 - model_173_1_loss: 0.6531\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.2484 - model_172_loss: 0.4056 - model_173_loss: 0.6748 - model_173_1_loss: 0.6560\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 25us/sample - loss: -6.2589 - model_172_loss: 0.4071 - model_173_loss: 0.6759 - model_173_1_loss: 0.6573\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.2683 - model_172_loss: 0.4081 - model_173_loss: 0.6755 - model_173_1_loss: 0.6598\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 67us/sample - loss: 6.6757 - model_173_loss: 0.6747 - model_173_1_loss: 0.66040s - loss: 6.6712 - model_173_loss: 0.6734 - model_173_1\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.2556 - model_172_loss: 0.4100 - model_173_loss: 0.6749 - model_173_1_loss: 0.6583\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2772 - model_172_loss: 0.4104 - model_173_loss: 0.6766 - model_173_1_loss: 0.6609\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.2824 - model_172_loss: 0.4133 - model_173_loss: 0.6761 - model_173_1_loss: 0.6630\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.3014 - model_172_loss: 0.4145 - model_173_loss: 0.6780 - model_173_1_loss: 0.6652\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.3098 - model_172_loss: 0.4179 - model_173_loss: 0.6789 - model_173_1_loss: 0.6666\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 80us/sample - loss: 6.7289 - model_173_loss: 0.6779 - model_173_1_loss: 0.66751s - loss: 6.7335 - model_173_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: -6.3012 - model_172_loss: 0.4204 - model_173_loss: 0.6784 - model_173_1_loss: 0.6659\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 49us/sample - loss: -6.3187 - model_172_loss: 0.4217 - model_173_loss: 0.6795 - model_173_1_loss: 0.6686\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: -6.3241 - model_172_loss: 0.4230 - model_173_loss: 0.6798 - model_173_1_loss: 0.6696\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.3332 - model_172_loss: 0.4267 - model_173_loss: 0.6808 - model_173_1_loss: 0.6711\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3408 - model_172_loss: 0.4291 - model_173_loss: 0.6818 - model_173_1_loss: 0.6722\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 132us/sample - loss: 6.7765 - model_173_loss: 0.6818 - model_173_1_loss: 0.6734\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.3370 - model_172_loss: 0.4315 - model_173_loss: 0.6814 - model_173_1_loss: 0.6723\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3524 - model_172_loss: 0.4339 - model_173_loss: 0.6828 - model_173_1_loss: 0.6745\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3494 - model_172_loss: 0.4354 - model_173_loss: 0.6823 - model_173_1_loss: 0.6747\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3547 - model_172_loss: 0.4406 - model_173_loss: 0.6830 - model_173_1_loss: 0.6760\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3612 - model_172_loss: 0.4405 - model_173_loss: 0.6834 - model_173_1_loss: 0.6770\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 133us/sample - loss: 6.8189 - model_173_loss: 0.6853 - model_173_1_loss: 0.6782\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.3675 - model_172_loss: 0.4444 - model_173_loss: 0.6844 - model_173_1_loss: 0.6780\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3710 - model_172_loss: 0.4465 - model_173_loss: 0.6849 - model_173_1_loss: 0.6786\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3795 - model_172_loss: 0.4465 - model_173_loss: 0.6862 - model_173_1_loss: 0.6790\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3822 - model_172_loss: 0.4497 - model_173_loss: 0.6864 - model_173_1_loss: 0.6800\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3815 - model_172_loss: 0.4527 - model_173_loss: 0.6869 - model_173_1_loss: 0.6799\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 135us/sample - loss: 6.8447 - model_173_loss: 0.6872 - model_173_1_loss: 0.6818\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3858 - model_172_loss: 0.4547 - model_173_loss: 0.6869 - model_173_1_loss: 0.6812\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3898 - model_172_loss: 0.4570 - model_173_loss: 0.6871 - model_173_1_loss: 0.6823\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3971 - model_172_loss: 0.4584 - model_173_loss: 0.6877 - model_173_1_loss: 0.6834\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4028 - model_172_loss: 0.4595 - model_173_loss: 0.6891 - model_173_1_loss: 0.6834\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4037 - model_172_loss: 0.4638 - model_173_loss: 0.6892 - model_173_1_loss: 0.6843\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.8710 - model_173_loss: 0.6887 - model_173_1_loss: 0.6849\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4048 - model_172_loss: 0.4653 - model_173_loss: 0.6895 - model_173_1_loss: 0.6845\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4018 - model_172_loss: 0.4684 - model_173_loss: 0.6895 - model_173_1_loss: 0.6845\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4066 - model_172_loss: 0.4685 - model_173_loss: 0.6896 - model_173_1_loss: 0.6854\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4111 - model_172_loss: 0.4722 - model_173_loss: 0.6905 - model_173_1_loss: 0.6861\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4151 - model_172_loss: 0.4729 - model_173_loss: 0.6907 - model_173_1_loss: 0.6869\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 109us/sample - loss: 6.8903 - model_173_loss: 0.6905 - model_173_1_loss: 0.6870\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4123 - model_172_loss: 0.4761 - model_173_loss: 0.6910 - model_173_1_loss: 0.6866\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4184 - model_172_loss: 0.4762 - model_173_loss: 0.6912 - model_173_1_loss: 0.6877\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4195 - model_172_loss: 0.4781 - model_173_loss: 0.6913 - model_173_1_loss: 0.6882\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4187 - model_172_loss: 0.4825 - model_173_loss: 0.6915 - model_173_1_loss: 0.6887\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4268 - model_172_loss: 0.4787 - model_173_loss: 0.6921 - model_173_1_loss: 0.6890\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 4s 150us/sample - loss: 6.9060 - model_173_loss: 0.6925 - model_173_1_loss: 0.6887\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4254 - model_172_loss: 0.4803 - model_173_loss: 0.6924 - model_173_1_loss: 0.6887\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4277 - model_172_loss: 0.4837 - model_173_loss: 0.6923 - model_173_1_loss: 0.6900\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4263 - model_172_loss: 0.4830 - model_173_loss: 0.6924 - model_173_1_loss: 0.6895\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4273 - model_172_loss: 0.4852 - model_173_loss: 0.6926 - model_173_1_loss: 0.6899\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4336 - model_172_loss: 0.4834 - model_173_loss: 0.6933 - model_173_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 90us/sample - loss: 6.9198 - model_173_loss: 0.6933 - model_173_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4362 - model_172_loss: 0.4874 - model_173_loss: 0.6932 - model_173_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4330 - model_172_loss: 0.4851 - model_173_loss: 0.6927 - model_173_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4371 - model_172_loss: 0.4884 - model_173_loss: 0.6936 - model_173_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4437 - model_172_loss: 0.4864 - model_173_loss: 0.6937 - model_173_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4447 - model_172_loss: 0.4861 - model_173_loss: 0.6936 - model_173_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.9300 - model_173_loss: 0.6936 - model_173_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4362 - model_172_loss: 0.4887 - model_173_loss: 0.6929 - model_173_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4396 - model_172_loss: 0.4885 - model_173_loss: 0.6932 - model_173_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4369 - model_172_loss: 0.4903 - model_173_loss: 0.6929 - model_173_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4406 - model_172_loss: 0.4879 - model_173_loss: 0.6931 - model_173_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4370 - model_172_loss: 0.4909 - model_173_loss: 0.6930 - model_173_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 116us/sample - loss: 6.9261 - model_173_loss: 0.6930 - model_173_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4309 - model_172_loss: 0.4901 - model_173_loss: 0.6923 - model_173_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4294 - model_172_loss: 0.4892 - model_173_loss: 0.6919 - model_173_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4360 - model_172_loss: 0.4874 - model_173_loss: 0.6923 - model_173_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4323 - model_172_loss: 0.4890 - model_173_loss: 0.6919 - model_173_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4331 - model_172_loss: 0.4868 - model_173_loss: 0.6921 - model_173_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 94us/sample - loss: 6.9177 - model_173_loss: 0.6907 - model_173_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4342 - model_172_loss: 0.4846 - model_173_loss: 0.6917 - model_173_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4324 - model_172_loss: 0.4856 - model_173_loss: 0.6916 - model_173_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4306 - model_172_loss: 0.4850 - model_173_loss: 0.6916 - model_173_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4330 - model_172_loss: 0.4851 - model_173_loss: 0.6917 - model_173_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4309 - model_172_loss: 0.4833 - model_173_loss: 0.6912 - model_173_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 127us/sample - loss: 6.9186 - model_173_loss: 0.6922 - model_173_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4344 - model_172_loss: 0.4809 - model_173_loss: 0.6913 - model_173_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4355 - model_172_loss: 0.4812 - model_173_loss: 0.6915 - model_173_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4323 - model_172_loss: 0.4831 - model_173_loss: 0.6911 - model_173_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4297 - model_172_loss: 0.4824 - model_173_loss: 0.6909 - model_173_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4361 - model_172_loss: 0.4800 - model_173_loss: 0.6918 - model_173_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 103us/sample - loss: 6.9150 - model_173_loss: 0.6907 - model_173_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4363 - model_172_loss: 0.4806 - model_173_loss: 0.6917 - model_173_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4372 - model_172_loss: 0.4787 - model_173_loss: 0.6913 - model_173_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4412 - model_172_loss: 0.4777 - model_173_loss: 0.6921 - model_173_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4405 - model_172_loss: 0.4781 - model_173_loss: 0.6922 - model_173_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4381 - model_172_loss: 0.4765 - model_173_loss: 0.6914 - model_173_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 98us/sample - loss: 6.9201 - model_173_loss: 0.6923 - model_173_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4382 - model_172_loss: 0.4767 - model_173_loss: 0.6914 - model_173_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4402 - model_172_loss: 0.4751 - model_173_loss: 0.6916 - model_173_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4396 - model_172_loss: 0.4755 - model_173_loss: 0.6918 - model_173_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4392 - model_172_loss: 0.4755 - model_173_loss: 0.6919 - model_173_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4411 - model_172_loss: 0.4717 - model_173_loss: 0.6914 - model_173_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 121us/sample - loss: 6.9183 - model_173_loss: 0.6924 - model_173_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4426 - model_172_loss: 0.4722 - model_173_loss: 0.6921 - model_173_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4443 - model_172_loss: 0.4722 - model_173_loss: 0.6924 - model_173_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4414 - model_172_loss: 0.4721 - model_173_loss: 0.6924 - model_173_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4426 - model_172_loss: 0.4713 - model_173_loss: 0.6921 - model_173_1_loss: 0.6907\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4411 - model_172_loss: 0.4714 - model_173_loss: 0.6920 - model_173_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 125us/sample - loss: 6.9142 - model_173_loss: 0.6919 - model_173_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4357 - model_172_loss: 0.4736 - model_173_loss: 0.6918 - model_173_1_loss: 0.6901\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4423 - model_172_loss: 0.4706 - model_173_loss: 0.6923 - model_173_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4402 - model_172_loss: 0.4700 - model_173_loss: 0.6917 - model_173_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4411 - model_172_loss: 0.4702 - model_173_loss: 0.6920 - model_173_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4410 - model_172_loss: 0.4699 - model_173_loss: 0.6922 - model_173_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 121us/sample - loss: 6.9091 - model_173_loss: 0.6921 - model_173_1_loss: 0.6899\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4369 - model_172_loss: 0.4696 - model_173_loss: 0.6916 - model_173_1_loss: 0.6897\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4401 - model_172_loss: 0.4682 - model_173_loss: 0.6919 - model_173_1_loss: 0.6898\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4406 - model_172_loss: 0.4695 - model_173_loss: 0.6922 - model_173_1_loss: 0.6899\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4419 - model_172_loss: 0.4690 - model_173_loss: 0.6921 - model_173_1_loss: 0.6900\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4421 - model_172_loss: 0.4683 - model_173_loss: 0.6921 - model_173_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 113us/sample - loss: 6.9093 - model_173_loss: 0.6921 - model_173_1_loss: 0.6899\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4387 - model_172_loss: 0.4701 - model_173_loss: 0.6918 - model_173_1_loss: 0.6899\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4433 - model_172_loss: 0.4675 - model_173_loss: 0.6919 - model_173_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4421 - model_172_loss: 0.4672 - model_173_loss: 0.6917 - model_173_1_loss: 0.6901\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4442 - model_172_loss: 0.4670 - model_173_loss: 0.6923 - model_173_1_loss: 0.6900\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4453 - model_172_loss: 0.4675 - model_173_loss: 0.6920 - model_173_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.9107 - model_173_loss: 0.6913 - model_173_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4439 - model_172_loss: 0.4665 - model_173_loss: 0.6918 - model_173_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4408 - model_172_loss: 0.4690 - model_173_loss: 0.6916 - model_173_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4446 - model_172_loss: 0.4667 - model_173_loss: 0.6918 - model_173_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4434 - model_172_loss: 0.4681 - model_173_loss: 0.6919 - model_173_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4457 - model_172_loss: 0.4667 - model_173_loss: 0.6917 - model_173_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 111us/sample - loss: 6.9136 - model_173_loss: 0.6918 - model_173_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4458 - model_172_loss: 0.4672 - model_173_loss: 0.6918 - model_173_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4462 - model_172_loss: 0.4672 - model_173_loss: 0.6918 - model_173_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4453 - model_172_loss: 0.4683 - model_173_loss: 0.6918 - model_173_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4466 - model_172_loss: 0.4676 - model_173_loss: 0.6918 - model_173_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4444 - model_172_loss: 0.4671 - model_173_loss: 0.6915 - model_173_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 119us/sample - loss: 6.9165 - model_173_loss: 0.6916 - model_173_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: -6.4457 - model_172_loss: 0.4676 - model_173_loss: 0.6917 - model_173_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: -6.4485 - model_172_loss: 0.4669 - model_173_loss: 0.6920 - model_173_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4464 - model_172_loss: 0.4681 - model_173_loss: 0.6919 - model_173_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4467 - model_172_loss: 0.4674 - model_173_loss: 0.6918 - model_173_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4490 - model_172_loss: 0.4688 - model_173_loss: 0.6921 - model_173_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 4s 169us/sample - loss: 6.9165 - model_173_loss: 0.6918 - model_173_1_loss: 0.6917s - loss: 6.8842 - model_173_loss: 0.6840 - \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 2s 71us/sample - loss: -6.4458 - model_172_loss: 0.4672 - model_173_loss: 0.6917 - model_173_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4477 - model_172_loss: 0.4684 - model_173_loss: 0.6921 - model_173_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4478 - model_172_loss: 0.4675 - model_173_loss: 0.6920 - model_173_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4466 - model_172_loss: 0.4683 - model_173_loss: 0.6918 - model_173_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4500 - model_172_loss: 0.4688 - model_173_loss: 0.6922 - model_173_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 121us/sample - loss: 6.9177 - model_173_loss: 0.6921 - model_173_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4470 - model_172_loss: 0.4700 - model_173_loss: 0.6921 - model_173_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4458 - model_172_loss: 0.4704 - model_173_loss: 0.6921 - model_173_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4420 - model_172_loss: 0.4739 - model_173_loss: 0.6918 - model_173_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4501 - model_172_loss: 0.4714 - model_173_loss: 0.6925 - model_173_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4491 - model_172_loss: 0.4700 - model_173_loss: 0.6921 - model_173_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9202 - model_173_loss: 0.6923 - model_173_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4437 - model_172_loss: 0.4745 - model_173_loss: 0.6919 - model_173_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4439 - model_172_loss: 0.4723 - model_173_loss: 0.6917 - model_173_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4467 - model_172_loss: 0.4728 - model_173_loss: 0.6922 - model_173_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4454 - model_172_loss: 0.4735 - model_173_loss: 0.6922 - model_173_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4473 - model_172_loss: 0.4754 - model_173_loss: 0.6926 - model_173_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 135us/sample - loss: 6.9232 - model_173_loss: 0.6926 - model_173_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4471 - model_172_loss: 0.4745 - model_173_loss: 0.6924 - model_173_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4459 - model_172_loss: 0.4754 - model_173_loss: 0.6922 - model_173_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4487 - model_172_loss: 0.4741 - model_173_loss: 0.6925 - model_173_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4477 - model_172_loss: 0.4747 - model_173_loss: 0.6927 - model_173_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4505 - model_172_loss: 0.4739 - model_173_loss: 0.6927 - model_173_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.9241 - model_173_loss: 0.6927 - model_173_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4450 - model_172_loss: 0.4759 - model_173_loss: 0.6923 - model_173_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4462 - model_172_loss: 0.4750 - model_173_loss: 0.6924 - model_173_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4500 - model_172_loss: 0.4732 - model_173_loss: 0.6924 - model_173_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4494 - model_172_loss: 0.4735 - model_173_loss: 0.6923 - model_173_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4521 - model_172_loss: 0.4721 - model_173_loss: 0.6926 - model_173_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.9235 - model_173_loss: 0.6924 - model_173_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4477 - model_172_loss: 0.4728 - model_173_loss: 0.6922 - model_173_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4502 - model_172_loss: 0.4712 - model_173_loss: 0.6922 - model_173_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4520 - model_172_loss: 0.4714 - model_173_loss: 0.6925 - model_173_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4532 - model_172_loss: 0.4711 - model_173_loss: 0.6926 - model_173_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4520 - model_172_loss: 0.4697 - model_173_loss: 0.6922 - model_173_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.9218 - model_173_loss: 0.6914 - model_173_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4509 - model_172_loss: 0.4711 - model_173_loss: 0.6926 - model_173_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4534 - model_172_loss: 0.4684 - model_173_loss: 0.6922 - model_173_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4513 - model_172_loss: 0.4701 - model_173_loss: 0.6925 - model_173_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4521 - model_172_loss: 0.4692 - model_173_loss: 0.6924 - model_173_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4524 - model_172_loss: 0.4701 - model_173_loss: 0.6926 - model_173_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9229 - model_173_loss: 0.6927 - model_173_1_loss: 0.6919s - loss: 6.9369 - model_173_loss: 0.6958 - model_173_1_lo\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4502 - model_172_loss: 0.4709 - model_173_loss: 0.6927 - model_173_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4489 - model_172_loss: 0.4679 - model_173_loss: 0.6920 - model_173_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4511 - model_172_loss: 0.4684 - model_173_loss: 0.6925 - model_173_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4502 - model_172_loss: 0.4666 - model_173_loss: 0.6920 - model_173_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4503 - model_172_loss: 0.4682 - model_173_loss: 0.6923 - model_173_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 123us/sample - loss: 6.9164 - model_173_loss: 0.6925 - model_173_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4504 - model_172_loss: 0.4657 - model_173_loss: 0.6919 - model_173_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4500 - model_172_loss: 0.4659 - model_173_loss: 0.6920 - model_173_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4530 - model_172_loss: 0.4663 - model_173_loss: 0.6924 - model_173_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4519 - model_172_loss: 0.4650 - model_173_loss: 0.6922 - model_173_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4479 - model_172_loss: 0.4667 - model_173_loss: 0.6919 - model_173_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 121us/sample - loss: 6.9182 - model_173_loss: 0.6924 - model_173_1_loss: 0.6910s - loss: 6.8719 - mo\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4488 - model_172_loss: 0.4652 - model_173_loss: 0.6920 - model_173_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4513 - model_172_loss: 0.4645 - model_173_loss: 0.6921 - model_173_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4517 - model_172_loss: 0.4658 - model_173_loss: 0.6923 - model_173_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4521 - model_172_loss: 0.4640 - model_173_loss: 0.6922 - model_173_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4478 - model_172_loss: 0.4658 - model_173_loss: 0.6919 - model_173_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 116us/sample - loss: 6.9176 - model_173_loss: 0.6922 - model_173_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -6.4512 - model_172_loss: 0.4668 - model_173_loss: 0.6923 - model_173_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4516 - model_172_loss: 0.4642 - model_173_loss: 0.6921 - model_173_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4516 - model_172_loss: 0.4659 - model_173_loss: 0.6922 - model_173_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4505 - model_172_loss: 0.4671 - model_173_loss: 0.6922 - model_173_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4532 - model_172_loss: 0.4652 - model_173_loss: 0.6922 - model_173_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 118us/sample - loss: 6.9191 - model_173_loss: 0.6922 - model_173_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4512 - model_172_loss: 0.4678 - model_173_loss: 0.6923 - model_173_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4549 - model_172_loss: 0.4657 - model_173_loss: 0.6926 - model_173_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4546 - model_172_loss: 0.4671 - model_173_loss: 0.6926 - model_173_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4526 - model_172_loss: 0.4681 - model_173_loss: 0.6926 - model_173_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4540 - model_172_loss: 0.4667 - model_173_loss: 0.6923 - model_173_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 130us/sample - loss: 6.9216 - model_173_loss: 0.6925 - model_173_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4551 - model_172_loss: 0.4671 - model_173_loss: 0.6925 - model_173_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4521 - model_172_loss: 0.4683 - model_173_loss: 0.6924 - model_173_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4571 - model_172_loss: 0.4670 - model_173_loss: 0.6927 - model_173_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4538 - model_172_loss: 0.4694 - model_173_loss: 0.6927 - model_173_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4549 - model_172_loss: 0.4688 - model_173_loss: 0.6926 - model_173_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 91us/sample - loss: 6.9224 - model_173_loss: 0.6923 - model_173_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4513 - model_172_loss: 0.4690 - model_173_loss: 0.6923 - model_173_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4509 - model_172_loss: 0.4695 - model_173_loss: 0.6923 - model_173_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4532 - model_172_loss: 0.4687 - model_173_loss: 0.6925 - model_173_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4521 - model_172_loss: 0.4694 - model_173_loss: 0.6923 - model_173_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4480 - model_172_loss: 0.4713 - model_173_loss: 0.6921 - model_173_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9212 - model_173_loss: 0.6925 - model_173_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4527 - model_172_loss: 0.4704 - model_173_loss: 0.6927 - model_173_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4516 - model_172_loss: 0.4688 - model_173_loss: 0.6922 - model_173_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4542 - model_172_loss: 0.4679 - model_173_loss: 0.6923 - model_173_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4558 - model_172_loss: 0.4696 - model_173_loss: 0.6930 - model_173_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4530 - model_172_loss: 0.4692 - model_173_loss: 0.6926 - model_173_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 111us/sample - loss: 6.9257 - model_173_loss: 0.6925 - model_173_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4539 - model_172_loss: 0.4682 - model_173_loss: 0.6926 - model_173_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4521 - model_172_loss: 0.4697 - model_173_loss: 0.6925 - model_173_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4552 - model_172_loss: 0.4668 - model_173_loss: 0.6926 - model_173_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4519 - model_172_loss: 0.4684 - model_173_loss: 0.6925 - model_173_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4552 - model_172_loss: 0.4673 - model_173_loss: 0.6926 - model_173_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.9210 - model_173_loss: 0.6932 - model_173_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4525 - model_172_loss: 0.4674 - model_173_loss: 0.6922 - model_173_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4536 - model_172_loss: 0.4684 - model_173_loss: 0.6928 - model_173_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4531 - model_172_loss: 0.4676 - model_173_loss: 0.6924 - model_173_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4513 - model_172_loss: 0.4667 - model_173_loss: 0.6921 - model_173_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4538 - model_172_loss: 0.4671 - model_173_loss: 0.6924 - model_173_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 117us/sample - loss: 6.9212 - model_173_loss: 0.6921 - model_173_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4565 - model_172_loss: 0.4645 - model_173_loss: 0.6925 - model_173_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4550 - model_172_loss: 0.4649 - model_173_loss: 0.6924 - model_173_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4536 - model_172_loss: 0.4663 - model_173_loss: 0.6925 - model_173_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4526 - model_172_loss: 0.4675 - model_173_loss: 0.6925 - model_173_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4524 - model_172_loss: 0.4668 - model_173_loss: 0.6924 - model_173_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 139us/sample - loss: 6.9208 - model_173_loss: 0.6929 - model_173_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: -6.4523 - model_172_loss: 0.4663 - model_173_loss: 0.6922 - model_173_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4519 - model_172_loss: 0.4665 - model_173_loss: 0.6920 - model_173_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4531 - model_172_loss: 0.4664 - model_173_loss: 0.6925 - model_173_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4562 - model_172_loss: 0.4660 - model_173_loss: 0.6926 - model_173_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4522 - model_172_loss: 0.4676 - model_173_loss: 0.6923 - model_173_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 111us/sample - loss: 6.9222 - model_173_loss: 0.6935 - model_173_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4547 - model_172_loss: 0.4660 - model_173_loss: 0.6924 - model_173_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4542 - model_172_loss: 0.4669 - model_173_loss: 0.6924 - model_173_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4556 - model_172_loss: 0.4652 - model_173_loss: 0.6924 - model_173_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4553 - model_172_loss: 0.4651 - model_173_loss: 0.6924 - model_173_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4533 - model_172_loss: 0.4675 - model_173_loss: 0.6925 - model_173_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.9239 - model_173_loss: 0.6937 - model_173_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4524 - model_172_loss: 0.4670 - model_173_loss: 0.6922 - model_173_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4535 - model_172_loss: 0.4671 - model_173_loss: 0.6926 - model_173_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4542 - model_172_loss: 0.4659 - model_173_loss: 0.6923 - model_173_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4554 - model_172_loss: 0.4655 - model_173_loss: 0.6926 - model_173_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4543 - model_172_loss: 0.4664 - model_173_loss: 0.6926 - model_173_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 116us/sample - loss: 6.9204 - model_173_loss: 0.6924 - model_173_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4556 - model_172_loss: 0.4645 - model_173_loss: 0.6925 - model_173_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4552 - model_172_loss: 0.4644 - model_173_loss: 0.6924 - model_173_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4525 - model_172_loss: 0.4655 - model_173_loss: 0.6923 - model_173_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4553 - model_172_loss: 0.4636 - model_173_loss: 0.6923 - model_173_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4555 - model_172_loss: 0.4635 - model_173_loss: 0.6924 - model_173_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9191 - model_173_loss: 0.6929 - model_173_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4543 - model_172_loss: 0.4628 - model_173_loss: 0.6920 - model_173_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4556 - model_172_loss: 0.4627 - model_173_loss: 0.6925 - model_173_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4558 - model_172_loss: 0.4618 - model_173_loss: 0.6922 - model_173_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4577 - model_172_loss: 0.4613 - model_173_loss: 0.6922 - model_173_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4577 - model_172_loss: 0.4628 - model_173_loss: 0.6926 - model_173_1_loss: 0.6915\n",
      "For Attention Module: 4.5\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 87us/sample - loss: 6.3384 - model_177_loss: 0.6612 - model_177_1_loss: 0.6065\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: -5.9632 - model_176_loss: 0.3748 - model_177_loss: 0.6609 - model_177_1_loss: 0.6067\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -5.9697 - model_176_loss: 0.3749 - model_177_loss: 0.6608 - model_177_1_loss: 0.6082\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -5.9698 - model_176_loss: 0.3772 - model_177_loss: 0.6607 - model_177_1_loss: 0.6087\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -5.9969 - model_176_loss: 0.3766 - model_177_loss: 0.6625 - model_177_1_loss: 0.6123\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -5.9952 - model_176_loss: 0.3766 - model_177_loss: 0.6618 - model_177_1_loss: 0.6126\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 134us/sample - loss: 6.3872 - model_177_loss: 0.6628 - model_177_1_loss: 0.6146\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.0074 - model_176_loss: 0.3782 - model_177_loss: 0.6636 - model_177_1_loss: 0.6135\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0265 - model_176_loss: 0.3764 - model_177_loss: 0.6629 - model_177_1_loss: 0.6177\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0319 - model_176_loss: 0.3786 - model_177_loss: 0.6633 - model_177_1_loss: 0.6188\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0416 - model_176_loss: 0.3776 - model_177_loss: 0.6641 - model_177_1_loss: 0.6198\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0514 - model_176_loss: 0.3786 - model_177_loss: 0.6644 - model_177_1_loss: 0.6216\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 97us/sample - loss: 6.4437 - model_177_loss: 0.6652 - model_177_1_loss: 0.6242\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.0748 - model_176_loss: 0.3804 - model_177_loss: 0.6661 - model_177_1_loss: 0.6250\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0783 - model_176_loss: 0.3802 - model_177_loss: 0.6661 - model_177_1_loss: 0.6256\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0862 - model_176_loss: 0.3834 - model_177_loss: 0.6659 - model_177_1_loss: 0.6280\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.1063 - model_176_loss: 0.3834 - model_177_loss: 0.6674 - model_177_1_loss: 0.6305\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.0999 - model_176_loss: 0.3841 - model_177_loss: 0.6665 - model_177_1_loss: 0.6303\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 116us/sample - loss: 6.5155 - model_177_loss: 0.6680 - model_177_1_loss: 0.6349s - loss: 6.5068 - model_177_loss: - ETA: 0s - loss: 6.5218 - model_177_loss: 0.6701 - model_177_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.1302 - model_176_loss: 0.3867 - model_177_loss: 0.6695 - model_177_1_loss: 0.6339\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1401 - model_176_loss: 0.3850 - model_177_loss: 0.6692 - model_177_1_loss: 0.6358\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1576 - model_176_loss: 0.3879 - model_177_loss: 0.6714 - model_177_1_loss: 0.6377\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.1731 - model_176_loss: 0.3913 - model_177_loss: 0.6721 - model_177_1_loss: 0.6407\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1800 - model_176_loss: 0.3902 - model_177_loss: 0.6717 - model_177_1_loss: 0.6423\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 103us/sample - loss: 6.5855 - model_177_loss: 0.6726 - model_177_1_loss: 0.6437\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.1921 - model_176_loss: 0.3939 - model_177_loss: 0.6731 - model_177_1_loss: 0.6440\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2105 - model_176_loss: 0.3967 - model_177_loss: 0.6741 - model_177_1_loss: 0.6473\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2204 - model_176_loss: 0.3984 - model_177_loss: 0.6747 - model_177_1_loss: 0.6491\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2297 - model_176_loss: 0.3995 - model_177_loss: 0.6745 - model_177_1_loss: 0.6513\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2445 - model_176_loss: 0.4028 - model_177_loss: 0.6764 - model_177_1_loss: 0.6531\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 111us/sample - loss: 6.6549 - model_177_loss: 0.6772 - model_177_1_loss: 0.6545\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.2577 - model_176_loss: 0.4018 - model_177_loss: 0.6771 - model_177_1_loss: 0.6548\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2631 - model_176_loss: 0.4063 - model_177_loss: 0.6780 - model_177_1_loss: 0.6559\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2775 - model_176_loss: 0.4081 - model_177_loss: 0.6787 - model_177_1_loss: 0.6584\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2929 - model_176_loss: 0.4091 - model_177_loss: 0.6796 - model_177_1_loss: 0.6608\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3007 - model_176_loss: 0.4108 - model_177_loss: 0.6796 - model_177_1_loss: 0.6627\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 98us/sample - loss: 6.7242 - model_177_loss: 0.6818 - model_177_1_loss: 0.6635\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3058 - model_176_loss: 0.4148 - model_177_loss: 0.6813 - model_177_1_loss: 0.6628\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3132 - model_176_loss: 0.4163 - model_177_loss: 0.6818 - model_177_1_loss: 0.6641\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3263 - model_176_loss: 0.4213 - model_177_loss: 0.6829 - model_177_1_loss: 0.6667\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3319 - model_176_loss: 0.4225 - model_177_loss: 0.6828 - model_177_1_loss: 0.6680\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3419 - model_176_loss: 0.4247 - model_177_loss: 0.6837 - model_177_1_loss: 0.6696\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 108us/sample - loss: 6.7756 - model_177_loss: 0.6847 - model_177_1_loss: 0.6705\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3431 - model_176_loss: 0.4272 - model_177_loss: 0.6841 - model_177_1_loss: 0.6700\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3478 - model_176_loss: 0.4312 - model_177_loss: 0.6847 - model_177_1_loss: 0.6711\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3601 - model_176_loss: 0.4315 - model_177_loss: 0.6850 - model_177_1_loss: 0.6733\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3594 - model_176_loss: 0.4356 - model_177_loss: 0.6855 - model_177_1_loss: 0.6735\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3759 - model_176_loss: 0.4355 - model_177_loss: 0.6868 - model_177_1_loss: 0.6755\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 95us/sample - loss: 6.8175 - model_177_loss: 0.6870 - model_177_1_loss: 0.6761\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3728 - model_176_loss: 0.4357 - model_177_loss: 0.6860 - model_177_1_loss: 0.6757\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3824 - model_176_loss: 0.4389 - model_177_loss: 0.6871 - model_177_1_loss: 0.6771\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3838 - model_176_loss: 0.4413 - model_177_loss: 0.6875 - model_177_1_loss: 0.6776\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3948 - model_176_loss: 0.4452 - model_177_loss: 0.6882 - model_177_1_loss: 0.6798\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3969 - model_176_loss: 0.4482 - model_177_loss: 0.6888 - model_177_1_loss: 0.6803\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 109us/sample - loss: 6.8477 - model_177_loss: 0.6888 - model_177_1_loss: 0.6806\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4000 - model_176_loss: 0.4480 - model_177_loss: 0.6895 - model_177_1_loss: 0.6802\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3993 - model_176_loss: 0.4505 - model_177_loss: 0.6889 - model_177_1_loss: 0.6811\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4044 - model_176_loss: 0.4547 - model_177_loss: 0.6899 - model_177_1_loss: 0.6819\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4110 - model_176_loss: 0.4566 - model_177_loss: 0.6902 - model_177_1_loss: 0.6833\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4117 - model_176_loss: 0.4580 - model_177_loss: 0.6903 - model_177_1_loss: 0.6836\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 101us/sample - loss: 6.8766 - model_177_loss: 0.6910 - model_177_1_loss: 0.6846\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4153 - model_176_loss: 0.4617 - model_177_loss: 0.6913 - model_177_1_loss: 0.6841\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4166 - model_176_loss: 0.4649 - model_177_loss: 0.6913 - model_177_1_loss: 0.6850\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4198 - model_176_loss: 0.4676 - model_177_loss: 0.6915 - model_177_1_loss: 0.6860\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4212 - model_176_loss: 0.4690 - model_177_loss: 0.6915 - model_177_1_loss: 0.6865\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4304 - model_176_loss: 0.4710 - model_177_loss: 0.6925 - model_177_1_loss: 0.6878\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 97us/sample - loss: 6.8936 - model_177_loss: 0.6916 - model_177_1_loss: 0.6874\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4263 - model_176_loss: 0.4735 - model_177_loss: 0.6921 - model_177_1_loss: 0.6879\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4266 - model_176_loss: 0.4758 - model_177_loss: 0.6921 - model_177_1_loss: 0.6884\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4318 - model_176_loss: 0.4769 - model_177_loss: 0.6928 - model_177_1_loss: 0.6890\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4309 - model_176_loss: 0.4799 - model_177_loss: 0.6927 - model_177_1_loss: 0.6895\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4300 - model_176_loss: 0.4824 - model_177_loss: 0.6930 - model_177_1_loss: 0.6895\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 102us/sample - loss: 6.9125 - model_177_loss: 0.6923 - model_177_1_loss: 0.6899\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4258 - model_176_loss: 0.4850 - model_177_loss: 0.6928 - model_177_1_loss: 0.6894\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4268 - model_176_loss: 0.4854 - model_177_loss: 0.6928 - model_177_1_loss: 0.6897\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4262 - model_176_loss: 0.4882 - model_177_loss: 0.6926 - model_177_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4258 - model_176_loss: 0.4895 - model_177_loss: 0.6927 - model_177_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4278 - model_176_loss: 0.4915 - model_177_loss: 0.6931 - model_177_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 100us/sample - loss: 6.9208 - model_177_loss: 0.6932 - model_177_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4261 - model_176_loss: 0.4935 - model_177_loss: 0.6931 - model_177_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4287 - model_176_loss: 0.4925 - model_177_loss: 0.6933 - model_177_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4297 - model_176_loss: 0.4919 - model_177_loss: 0.6930 - model_177_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4309 - model_176_loss: 0.4931 - model_177_loss: 0.6934 - model_177_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4330 - model_176_loss: 0.4924 - model_177_loss: 0.6936 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 97us/sample - loss: 6.9247 - model_177_loss: 0.6928 - model_177_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4280 - model_176_loss: 0.4938 - model_177_loss: 0.6930 - model_177_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4288 - model_176_loss: 0.4929 - model_177_loss: 0.6929 - model_177_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4316 - model_176_loss: 0.4911 - model_177_loss: 0.6932 - model_177_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4327 - model_176_loss: 0.4897 - model_177_loss: 0.6929 - model_177_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4326 - model_176_loss: 0.4902 - model_177_loss: 0.6932 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 102us/sample - loss: 6.9259 - model_177_loss: 0.6926 - model_177_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4325 - model_176_loss: 0.4894 - model_177_loss: 0.6927 - model_177_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4381 - model_176_loss: 0.4860 - model_177_loss: 0.6929 - model_177_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4386 - model_176_loss: 0.4845 - model_177_loss: 0.6928 - model_177_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4392 - model_176_loss: 0.4846 - model_177_loss: 0.6929 - model_177_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4404 - model_176_loss: 0.4829 - model_177_loss: 0.6929 - model_177_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.9206 - model_177_loss: 0.6927 - model_177_1_loss: 0.6913s - loss: 6.8154 - model_17\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4386 - model_176_loss: 0.4813 - model_177_loss: 0.6925 - model_177_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4409 - model_176_loss: 0.4796 - model_177_loss: 0.6926 - model_177_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4403 - model_176_loss: 0.4797 - model_177_loss: 0.6926 - model_177_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4427 - model_176_loss: 0.4776 - model_177_loss: 0.6924 - model_177_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4403 - model_176_loss: 0.4784 - model_177_loss: 0.6924 - model_177_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 111us/sample - loss: 6.9167 - model_177_loss: 0.6931 - model_177_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4404 - model_176_loss: 0.4771 - model_177_loss: 0.6923 - model_177_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4420 - model_176_loss: 0.4755 - model_177_loss: 0.6923 - model_177_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4403 - model_176_loss: 0.4742 - model_177_loss: 0.6921 - model_177_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4409 - model_176_loss: 0.4751 - model_177_loss: 0.6923 - model_177_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4441 - model_176_loss: 0.4716 - model_177_loss: 0.6921 - model_177_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 130us/sample - loss: 6.9143 - model_177_loss: 0.6922 - model_177_1_loss: 0.6908s - loss: 6.8996 - model_177_loss: 0.6885 - model_177_1_los\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: -6.4382 - model_176_loss: 0.4723 - model_177_loss: 0.6916 - model_177_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4388 - model_176_loss: 0.4698 - model_177_loss: 0.6917 - model_177_1_loss: 0.6900\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4393 - model_176_loss: 0.4719 - model_177_loss: 0.6922 - model_177_1_loss: 0.6901\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4393 - model_176_loss: 0.4709 - model_177_loss: 0.6919 - model_177_1_loss: 0.6901\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4357 - model_176_loss: 0.4715 - model_177_loss: 0.6917 - model_177_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 117us/sample - loss: 6.9089 - model_177_loss: 0.6919 - model_177_1_loss: 0.6899\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4354 - model_176_loss: 0.4713 - model_177_loss: 0.6915 - model_177_1_loss: 0.6898\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4361 - model_176_loss: 0.4701 - model_177_loss: 0.6915 - model_177_1_loss: 0.6898\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4352 - model_176_loss: 0.4709 - model_177_loss: 0.6912 - model_177_1_loss: 0.6900\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4362 - model_176_loss: 0.4702 - model_177_loss: 0.6912 - model_177_1_loss: 0.6901\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4371 - model_176_loss: 0.4704 - model_177_loss: 0.6914 - model_177_1_loss: 0.6901\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 91us/sample - loss: 6.9107 - model_177_loss: 0.6919 - model_177_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4370 - model_176_loss: 0.4709 - model_177_loss: 0.6913 - model_177_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4374 - model_176_loss: 0.4728 - model_177_loss: 0.6914 - model_177_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4408 - model_176_loss: 0.4708 - model_177_loss: 0.6917 - model_177_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4399 - model_176_loss: 0.4715 - model_177_loss: 0.6917 - model_177_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4394 - model_176_loss: 0.4717 - model_177_loss: 0.6915 - model_177_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 94us/sample - loss: 6.9100 - model_177_loss: 0.6911 - model_177_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4405 - model_176_loss: 0.4718 - model_177_loss: 0.6917 - model_177_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4387 - model_176_loss: 0.4736 - model_177_loss: 0.6918 - model_177_1_loss: 0.6906\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4396 - model_176_loss: 0.4738 - model_177_loss: 0.6917 - model_177_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4432 - model_176_loss: 0.4712 - model_177_loss: 0.6919 - model_177_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4409 - model_176_loss: 0.4752 - model_177_loss: 0.6922 - model_177_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 100us/sample - loss: 6.9152 - model_177_loss: 0.6917 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4398 - model_176_loss: 0.4720 - model_177_loss: 0.6914 - model_177_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4382 - model_176_loss: 0.4746 - model_177_loss: 0.6916 - model_177_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4408 - model_176_loss: 0.4727 - model_177_loss: 0.6914 - model_177_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4411 - model_176_loss: 0.4742 - model_177_loss: 0.6920 - model_177_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4415 - model_176_loss: 0.4748 - model_177_loss: 0.6919 - model_177_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.9183 - model_177_loss: 0.6920 - model_177_1_loss: 0.6919s - loss: 6.7813 - model_177\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4412 - model_176_loss: 0.4742 - model_177_loss: 0.6916 - model_177_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4411 - model_176_loss: 0.4739 - model_177_loss: 0.6920 - model_177_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4454 - model_176_loss: 0.4739 - model_177_loss: 0.6921 - model_177_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4415 - model_176_loss: 0.4752 - model_177_loss: 0.6921 - model_177_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4427 - model_176_loss: 0.4753 - model_177_loss: 0.6922 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.9187 - model_177_loss: 0.6921 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4427 - model_176_loss: 0.4755 - model_177_loss: 0.6924 - model_177_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4450 - model_176_loss: 0.4760 - model_177_loss: 0.6925 - model_177_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4427 - model_176_loss: 0.4751 - model_177_loss: 0.6924 - model_177_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4433 - model_176_loss: 0.4752 - model_177_loss: 0.6922 - model_177_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4452 - model_176_loss: 0.4736 - model_177_loss: 0.6924 - model_177_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 118us/sample - loss: 6.9225 - model_177_loss: 0.6926 - model_177_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4447 - model_176_loss: 0.4754 - model_177_loss: 0.6926 - model_177_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4447 - model_176_loss: 0.4750 - model_177_loss: 0.6924 - model_177_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4448 - model_176_loss: 0.4739 - model_177_loss: 0.6922 - model_177_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4462 - model_176_loss: 0.4752 - model_177_loss: 0.6926 - model_177_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4448 - model_176_loss: 0.4761 - model_177_loss: 0.6928 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 104us/sample - loss: 6.9225 - model_177_loss: 0.6931 - model_177_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4445 - model_176_loss: 0.4752 - model_177_loss: 0.6925 - model_177_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4477 - model_176_loss: 0.4745 - model_177_loss: 0.6929 - model_177_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4497 - model_176_loss: 0.4734 - model_177_loss: 0.6931 - model_177_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4479 - model_176_loss: 0.4740 - model_177_loss: 0.6929 - model_177_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4483 - model_176_loss: 0.4722 - model_177_loss: 0.6927 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 96us/sample - loss: 6.9227 - model_177_loss: 0.6929 - model_177_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4461 - model_176_loss: 0.4739 - model_177_loss: 0.6927 - model_177_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4473 - model_176_loss: 0.4738 - model_177_loss: 0.6927 - model_177_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4446 - model_176_loss: 0.4740 - model_177_loss: 0.6923 - model_177_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4490 - model_176_loss: 0.4719 - model_177_loss: 0.6927 - model_177_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4467 - model_176_loss: 0.4719 - model_177_loss: 0.6926 - model_177_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 122us/sample - loss: 6.9201 - model_177_loss: 0.6921 - model_177_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4449 - model_176_loss: 0.4722 - model_177_loss: 0.6925 - model_177_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4468 - model_176_loss: 0.4694 - model_177_loss: 0.6923 - model_177_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4456 - model_176_loss: 0.4699 - model_177_loss: 0.6924 - model_177_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4486 - model_176_loss: 0.4685 - model_177_loss: 0.6924 - model_177_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4484 - model_176_loss: 0.4681 - model_177_loss: 0.6926 - model_177_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 122us/sample - loss: 6.9177 - model_177_loss: 0.6921 - model_177_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4459 - model_176_loss: 0.4677 - model_177_loss: 0.6921 - model_177_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4473 - model_176_loss: 0.4681 - model_177_loss: 0.6923 - model_177_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4446 - model_176_loss: 0.4685 - model_177_loss: 0.6921 - model_177_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4453 - model_176_loss: 0.4676 - model_177_loss: 0.6922 - model_177_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4489 - model_176_loss: 0.4676 - model_177_loss: 0.6926 - model_177_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 104us/sample - loss: 6.9165 - model_177_loss: 0.6923 - model_177_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4454 - model_176_loss: 0.4681 - model_177_loss: 0.6918 - model_177_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4445 - model_176_loss: 0.4699 - model_177_loss: 0.6920 - model_177_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4482 - model_176_loss: 0.4681 - model_177_loss: 0.6922 - model_177_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4457 - model_176_loss: 0.4689 - model_177_loss: 0.6921 - model_177_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4457 - model_176_loss: 0.4683 - model_177_loss: 0.6920 - model_177_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.9217 - model_177_loss: 0.6933 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4474 - model_176_loss: 0.4692 - model_177_loss: 0.6922 - model_177_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4482 - model_176_loss: 0.4691 - model_177_loss: 0.6924 - model_177_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4490 - model_176_loss: 0.4703 - model_177_loss: 0.6924 - model_177_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4506 - model_176_loss: 0.4686 - model_177_loss: 0.6924 - model_177_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4507 - model_176_loss: 0.4686 - model_177_loss: 0.6924 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 109us/sample - loss: 6.9206 - model_177_loss: 0.6933 - model_177_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4484 - model_176_loss: 0.4692 - model_177_loss: 0.6924 - model_177_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4511 - model_176_loss: 0.4690 - model_177_loss: 0.6926 - model_177_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4477 - model_176_loss: 0.4707 - model_177_loss: 0.6924 - model_177_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4518 - model_176_loss: 0.4682 - model_177_loss: 0.6926 - model_177_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4512 - model_176_loss: 0.4704 - model_177_loss: 0.6927 - model_177_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 116us/sample - loss: 6.9225 - model_177_loss: 0.6919 - model_177_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4523 - model_176_loss: 0.4702 - model_177_loss: 0.6927 - model_177_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4496 - model_176_loss: 0.4711 - model_177_loss: 0.6924 - model_177_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4537 - model_176_loss: 0.4694 - model_177_loss: 0.6928 - model_177_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4496 - model_176_loss: 0.4720 - model_177_loss: 0.6925 - model_177_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4515 - model_176_loss: 0.4717 - model_177_loss: 0.6927 - model_177_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.9215 - model_177_loss: 0.6925 - model_177_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4504 - model_176_loss: 0.4716 - model_177_loss: 0.6928 - model_177_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4522 - model_176_loss: 0.4704 - model_177_loss: 0.6927 - model_177_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4491 - model_176_loss: 0.4705 - model_177_loss: 0.6925 - model_177_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4498 - model_176_loss: 0.4714 - model_177_loss: 0.6927 - model_177_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4508 - model_176_loss: 0.4707 - model_177_loss: 0.6926 - model_177_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 90us/sample - loss: 6.9236 - model_177_loss: 0.6929 - model_177_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4504 - model_176_loss: 0.4704 - model_177_loss: 0.6926 - model_177_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4501 - model_176_loss: 0.4711 - model_177_loss: 0.6926 - model_177_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4512 - model_176_loss: 0.4701 - model_177_loss: 0.6925 - model_177_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4508 - model_176_loss: 0.4691 - model_177_loss: 0.6925 - model_177_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4517 - model_176_loss: 0.4678 - model_177_loss: 0.6925 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9210 - model_177_loss: 0.6928 - model_177_1_loss: 0.6916s - loss: 6.8944 - model_177_loss: 0.6873 - model_177_1_loss: 0. - ETA: 0s - loss: 6.8893 - model_177_loss: 0.6858 - model_177_1_l\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4550 - model_176_loss: 0.4667 - model_177_loss: 0.6925 - model_177_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4549 - model_176_loss: 0.4677 - model_177_loss: 0.6927 - model_177_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4510 - model_176_loss: 0.4704 - model_177_loss: 0.6929 - model_177_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4510 - model_176_loss: 0.4693 - model_177_loss: 0.6925 - model_177_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4516 - model_176_loss: 0.4686 - model_177_loss: 0.6926 - model_177_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 93us/sample - loss: 6.9205 - model_177_loss: 0.6929 - model_177_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4517 - model_176_loss: 0.4684 - model_177_loss: 0.6924 - model_177_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4509 - model_176_loss: 0.4689 - model_177_loss: 0.6925 - model_177_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4522 - model_176_loss: 0.4677 - model_177_loss: 0.6924 - model_177_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4519 - model_176_loss: 0.4685 - model_177_loss: 0.6924 - model_177_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4527 - model_176_loss: 0.4678 - model_177_loss: 0.6927 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 101us/sample - loss: 6.9205 - model_177_loss: 0.6924 - model_177_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4514 - model_176_loss: 0.4689 - model_177_loss: 0.6927 - model_177_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4500 - model_176_loss: 0.4668 - model_177_loss: 0.6921 - model_177_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4516 - model_176_loss: 0.4679 - model_177_loss: 0.6927 - model_177_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4506 - model_176_loss: 0.4691 - model_177_loss: 0.6926 - model_177_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4516 - model_176_loss: 0.4688 - model_177_loss: 0.6926 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.9212 - model_177_loss: 0.6923 - model_177_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4517 - model_176_loss: 0.4680 - model_177_loss: 0.6926 - model_177_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4517 - model_176_loss: 0.4665 - model_177_loss: 0.6925 - model_177_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4514 - model_176_loss: 0.4679 - model_177_loss: 0.6926 - model_177_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4536 - model_176_loss: 0.4676 - model_177_loss: 0.6925 - model_177_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4524 - model_176_loss: 0.4694 - model_177_loss: 0.6928 - model_177_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 98us/sample - loss: 6.9204 - model_177_loss: 0.6927 - model_177_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4508 - model_176_loss: 0.4679 - model_177_loss: 0.6925 - model_177_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4530 - model_176_loss: 0.4688 - model_177_loss: 0.6927 - model_177_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4534 - model_176_loss: 0.4683 - model_177_loss: 0.6927 - model_177_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4515 - model_176_loss: 0.4689 - model_177_loss: 0.6926 - model_177_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4530 - model_176_loss: 0.4665 - model_177_loss: 0.6925 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 117us/sample - loss: 6.9248 - model_177_loss: 0.6940 - model_177_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4528 - model_176_loss: 0.4689 - model_177_loss: 0.6926 - model_177_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4527 - model_176_loss: 0.4687 - model_177_loss: 0.6926 - model_177_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4497 - model_176_loss: 0.4706 - model_177_loss: 0.6925 - model_177_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4539 - model_176_loss: 0.4685 - model_177_loss: 0.6926 - model_177_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4550 - model_176_loss: 0.4677 - model_177_loss: 0.6925 - model_177_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.9246 - model_177_loss: 0.6934 - model_177_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4537 - model_176_loss: 0.4699 - model_177_loss: 0.6928 - model_177_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4519 - model_176_loss: 0.4701 - model_177_loss: 0.6925 - model_177_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4535 - model_176_loss: 0.4691 - model_177_loss: 0.6926 - model_177_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4531 - model_176_loss: 0.4715 - model_177_loss: 0.6928 - model_177_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4527 - model_176_loss: 0.4708 - model_177_loss: 0.6926 - model_177_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 121us/sample - loss: 6.9257 - model_177_loss: 0.6930 - model_177_1_loss: 0.6921s - loss: 6.9293 - model_177_loss\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4556 - model_176_loss: 0.4684 - model_177_loss: 0.6927 - model_177_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4556 - model_176_loss: 0.4682 - model_177_loss: 0.6926 - model_177_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4550 - model_176_loss: 0.4692 - model_177_loss: 0.6928 - model_177_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4576 - model_176_loss: 0.4666 - model_177_loss: 0.6928 - model_177_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4559 - model_176_loss: 0.4684 - model_177_loss: 0.6928 - model_177_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 108us/sample - loss: 6.9211 - model_177_loss: 0.6928 - model_177_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4564 - model_176_loss: 0.4661 - model_177_loss: 0.6928 - model_177_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4542 - model_176_loss: 0.4649 - model_177_loss: 0.6925 - model_177_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4545 - model_176_loss: 0.4638 - model_177_loss: 0.6925 - model_177_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4549 - model_176_loss: 0.4650 - model_177_loss: 0.6924 - model_177_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4548 - model_176_loss: 0.4634 - model_177_loss: 0.6922 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 102us/sample - loss: 6.9190 - model_177_loss: 0.6921 - model_177_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4550 - model_176_loss: 0.4621 - model_177_loss: 0.6923 - model_177_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4548 - model_176_loss: 0.4634 - model_177_loss: 0.6925 - model_177_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4550 - model_176_loss: 0.4635 - model_177_loss: 0.6926 - model_177_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4533 - model_176_loss: 0.4642 - model_177_loss: 0.6926 - model_177_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4545 - model_176_loss: 0.4635 - model_177_loss: 0.6924 - model_177_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 130us/sample - loss: 6.9154 - model_177_loss: 0.6928 - model_177_1_loss: 0.6908s - loss: 6.9138 - model_177_loss: 0.6919 - model_177_1_loss: 0.690\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4545 - model_176_loss: 0.4618 - model_177_loss: 0.6921 - model_177_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4545 - model_176_loss: 0.4627 - model_177_loss: 0.6924 - model_177_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4548 - model_176_loss: 0.4619 - model_177_loss: 0.6924 - model_177_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4541 - model_176_loss: 0.4625 - model_177_loss: 0.6923 - model_177_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4550 - model_176_loss: 0.4629 - model_177_loss: 0.6927 - model_177_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 108us/sample - loss: 6.9198 - model_177_loss: 0.6930 - model_177_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4587 - model_176_loss: 0.4609 - model_177_loss: 0.6927 - model_177_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4562 - model_176_loss: 0.4619 - model_177_loss: 0.6925 - model_177_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4556 - model_176_loss: 0.4629 - model_177_loss: 0.6925 - model_177_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4563 - model_176_loss: 0.4612 - model_177_loss: 0.6923 - model_177_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4568 - model_176_loss: 0.4611 - model_177_loss: 0.6924 - model_177_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 115us/sample - loss: 6.9198 - model_177_loss: 0.6921 - model_177_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4547 - model_176_loss: 0.4602 - model_177_loss: 0.6923 - model_177_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4542 - model_176_loss: 0.4608 - model_177_loss: 0.6921 - model_177_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4559 - model_176_loss: 0.4612 - model_177_loss: 0.6924 - model_177_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4565 - model_176_loss: 0.4608 - model_177_loss: 0.6924 - model_177_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4551 - model_176_loss: 0.4624 - model_177_loss: 0.6925 - model_177_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 99us/sample - loss: 6.9172 - model_177_loss: 0.6925 - model_177_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4560 - model_176_loss: 0.4598 - model_177_loss: 0.6924 - model_177_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4541 - model_176_loss: 0.4597 - model_177_loss: 0.6921 - model_177_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4568 - model_176_loss: 0.4603 - model_177_loss: 0.6923 - model_177_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4535 - model_176_loss: 0.4620 - model_177_loss: 0.6923 - model_177_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4548 - model_176_loss: 0.4611 - model_177_loss: 0.6923 - model_177_1_loss: 0.6909\n",
      "For Attention Module: 4.6\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.3604 - model_181_loss: 0.6612 - model_181_1_loss: 0.6109s - loss: 6.3668 - model_181_loss: 0.6616\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: -5.9769 - model_180_loss: 0.3774 - model_181_loss: 0.6604 - model_181_1_loss: 0.6104\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -5.9832 - model_180_loss: 0.3771 - model_181_loss: 0.6599 - model_181_1_loss: 0.6122\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -5.9980 - model_180_loss: 0.3787 - model_181_loss: 0.6608 - model_181_1_loss: 0.6146\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0079 - model_180_loss: 0.3784 - model_181_loss: 0.6614 - model_181_1_loss: 0.6159\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0122 - model_180_loss: 0.3788 - model_181_loss: 0.6610 - model_181_1_loss: 0.6172\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 140us/sample - loss: 6.4107 - model_181_loss: 0.6630 - model_181_1_loss: 0.6190\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.0290 - model_180_loss: 0.3786 - model_181_loss: 0.6619 - model_181_1_loss: 0.6196\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0434 - model_180_loss: 0.3802 - model_181_loss: 0.6638 - model_181_1_loss: 0.6209\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.0530 - model_180_loss: 0.3812 - model_181_loss: 0.6632 - model_181_1_loss: 0.6236\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0603 - model_180_loss: 0.3825 - model_181_loss: 0.6639 - model_181_1_loss: 0.6247\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0787 - model_180_loss: 0.3837 - model_181_loss: 0.6648 - model_181_1_loss: 0.6276\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 4s 149us/sample - loss: 6.4839 - model_181_loss: 0.6657 - model_181_1_loss: 0.6308s - loss: 6.5433 - m - ETA: 0s - loss: 6.5071 - model_181_loss: 0.6694 - model_181_1_loss:\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.0846 - model_180_loss: 0.3850 - model_181_loss: 0.6641 - model_181_1_loss: 0.6298\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0917 - model_180_loss: 0.3881 - model_181_loss: 0.6652 - model_181_1_loss: 0.6307\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1187 - model_180_loss: 0.3863 - model_181_loss: 0.6671 - model_181_1_loss: 0.6339\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1204 - model_180_loss: 0.3902 - model_181_loss: 0.6669 - model_181_1_loss: 0.6353\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.1341 - model_180_loss: 0.3895 - model_181_loss: 0.6677 - model_181_1_loss: 0.6370\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 4s 151us/sample - loss: 6.5509 - model_181_loss: 0.6689 - model_181_1_loss: 0.6411s - loss: 6.6104 - \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.1647 - model_180_loss: 0.3915 - model_181_loss: 0.6695 - model_181_1_loss: 0.6417\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1714 - model_180_loss: 0.3951 - model_181_loss: 0.6703 - model_181_1_loss: 0.6430\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.1936 - model_180_loss: 0.3977 - model_181_loss: 0.6705 - model_181_1_loss: 0.6478\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.2095 - model_180_loss: 0.4007 - model_181_loss: 0.6718 - model_181_1_loss: 0.6502\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2296 - model_180_loss: 0.4025 - model_181_loss: 0.6731 - model_181_1_loss: 0.6533\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.6337 - model_181_loss: 0.6751 - model_181_1_loss: 0.6521\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.2224 - model_180_loss: 0.4067 - model_181_loss: 0.6735 - model_181_1_loss: 0.6523\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2293 - model_180_loss: 0.4093 - model_181_loss: 0.6736 - model_181_1_loss: 0.6541\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2469 - model_180_loss: 0.4112 - model_181_loss: 0.6753 - model_181_1_loss: 0.6563\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2634 - model_180_loss: 0.4143 - model_181_loss: 0.6761 - model_181_1_loss: 0.6595\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2795 - model_180_loss: 0.4187 - model_181_loss: 0.6762 - model_181_1_loss: 0.6635\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 97us/sample - loss: 6.7140 - model_181_loss: 0.6786 - model_181_1_loss: 0.6644\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.2814 - model_180_loss: 0.4210 - model_181_loss: 0.6772 - model_181_1_loss: 0.6633\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2905 - model_180_loss: 0.4243 - model_181_loss: 0.6780 - model_181_1_loss: 0.6650\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3094 - model_180_loss: 0.4261 - model_181_loss: 0.6798 - model_181_1_loss: 0.6673\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3199 - model_180_loss: 0.4287 - model_181_loss: 0.6794 - model_181_1_loss: 0.6703\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3305 - model_180_loss: 0.4310 - model_181_loss: 0.6809 - model_181_1_loss: 0.6714\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 98us/sample - loss: 6.7689 - model_181_loss: 0.6819 - model_181_1_loss: 0.6721\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3323 - model_180_loss: 0.4345 - model_181_loss: 0.6817 - model_181_1_loss: 0.6716\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3368 - model_180_loss: 0.4373 - model_181_loss: 0.6813 - model_181_1_loss: 0.6735\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3491 - model_180_loss: 0.4399 - model_181_loss: 0.6826 - model_181_1_loss: 0.6752\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3573 - model_180_loss: 0.4414 - model_181_loss: 0.6833 - model_181_1_loss: 0.6764\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3664 - model_180_loss: 0.4453 - model_181_loss: 0.6844 - model_181_1_loss: 0.6779\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 120us/sample - loss: 6.8138 - model_181_loss: 0.6841 - model_181_1_loss: 0.6786\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.3646 - model_180_loss: 0.4475 - model_181_loss: 0.6844 - model_181_1_loss: 0.6780\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3721 - model_180_loss: 0.4498 - model_181_loss: 0.6850 - model_181_1_loss: 0.6794\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3808 - model_180_loss: 0.4521 - model_181_loss: 0.6859 - model_181_1_loss: 0.6806\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3948 - model_180_loss: 0.4541 - model_181_loss: 0.6871 - model_181_1_loss: 0.6827\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3951 - model_180_loss: 0.4572 - model_181_loss: 0.6871 - model_181_1_loss: 0.6833\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.8521 - model_181_loss: 0.6866 - model_181_1_loss: 0.6835\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3917 - model_180_loss: 0.4592 - model_181_loss: 0.6872 - model_181_1_loss: 0.6830\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3955 - model_180_loss: 0.4642 - model_181_loss: 0.6882 - model_181_1_loss: 0.6838\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4055 - model_180_loss: 0.4659 - model_181_loss: 0.6888 - model_181_1_loss: 0.6855\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4111 - model_180_loss: 0.4663 - model_181_loss: 0.6888 - model_181_1_loss: 0.6866\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4129 - model_180_loss: 0.4700 - model_181_loss: 0.6898 - model_181_1_loss: 0.6868\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.8852 - model_181_loss: 0.6902 - model_181_1_loss: 0.6867\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4077 - model_180_loss: 0.4727 - model_181_loss: 0.6895 - model_181_1_loss: 0.6866\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4120 - model_180_loss: 0.4734 - model_181_loss: 0.6902 - model_181_1_loss: 0.6869\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4110 - model_180_loss: 0.4766 - model_181_loss: 0.6902 - model_181_1_loss: 0.6873\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4203 - model_180_loss: 0.4775 - model_181_loss: 0.6913 - model_181_1_loss: 0.6883\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4144 - model_180_loss: 0.4806 - model_181_loss: 0.6911 - model_181_1_loss: 0.6879\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.9034 - model_181_loss: 0.6916 - model_181_1_loss: 0.6889\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4155 - model_180_loss: 0.4815 - model_181_loss: 0.6911 - model_181_1_loss: 0.6883\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4181 - model_180_loss: 0.4846 - model_181_loss: 0.6917 - model_181_1_loss: 0.6889\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4221 - model_180_loss: 0.4820 - model_181_loss: 0.6917 - model_181_1_loss: 0.6891\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4187 - model_180_loss: 0.4866 - model_181_loss: 0.6917 - model_181_1_loss: 0.6894\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4240 - model_180_loss: 0.4850 - model_181_loss: 0.6920 - model_181_1_loss: 0.6897\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 103us/sample - loss: 6.9182 - model_181_loss: 0.6936 - model_181_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4254 - model_180_loss: 0.4874 - model_181_loss: 0.6922 - model_181_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4261 - model_180_loss: 0.4891 - model_181_loss: 0.6926 - model_181_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4280 - model_180_loss: 0.4883 - model_181_loss: 0.6930 - model_181_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4314 - model_180_loss: 0.4885 - model_181_loss: 0.6934 - model_181_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4275 - model_180_loss: 0.4890 - model_181_loss: 0.6926 - model_181_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 109us/sample - loss: 6.9201 - model_181_loss: 0.6921 - model_181_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4276 - model_180_loss: 0.4916 - model_181_loss: 0.6928 - model_181_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4326 - model_180_loss: 0.4886 - model_181_loss: 0.6932 - model_181_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4337 - model_180_loss: 0.4887 - model_181_loss: 0.6932 - model_181_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4315 - model_180_loss: 0.4928 - model_181_loss: 0.6934 - model_181_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4369 - model_180_loss: 0.4872 - model_181_loss: 0.6934 - model_181_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 103us/sample - loss: 6.9272 - model_181_loss: 0.6940 - model_181_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4340 - model_180_loss: 0.4888 - model_181_loss: 0.6930 - model_181_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4367 - model_180_loss: 0.4883 - model_181_loss: 0.6933 - model_181_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4406 - model_180_loss: 0.4868 - model_181_loss: 0.6936 - model_181_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4372 - model_180_loss: 0.4884 - model_181_loss: 0.6934 - model_181_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4422 - model_180_loss: 0.4857 - model_181_loss: 0.6936 - model_181_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 118us/sample - loss: 6.9264 - model_181_loss: 0.6933 - model_181_1_loss: 0.6920s - loss: 6.9637 - model_181_loss: 0.7007 - mod\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4429 - model_180_loss: 0.4836 - model_181_loss: 0.6933 - model_181_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4407 - model_180_loss: 0.4848 - model_181_loss: 0.6935 - model_181_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4393 - model_180_loss: 0.4836 - model_181_loss: 0.6929 - model_181_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4419 - model_180_loss: 0.4818 - model_181_loss: 0.6930 - model_181_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4424 - model_180_loss: 0.4833 - model_181_loss: 0.6934 - model_181_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 123us/sample - loss: 6.9240 - model_181_loss: 0.6925 - model_181_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4382 - model_180_loss: 0.4836 - model_181_loss: 0.6927 - model_181_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4396 - model_180_loss: 0.4815 - model_181_loss: 0.6929 - model_181_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4358 - model_180_loss: 0.4807 - model_181_loss: 0.6919 - model_181_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4425 - model_180_loss: 0.4783 - model_181_loss: 0.6930 - model_181_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4401 - model_180_loss: 0.4783 - model_181_loss: 0.6923 - model_181_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.9207 - model_181_loss: 0.6928 - model_181_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4360 - model_180_loss: 0.4801 - model_181_loss: 0.6921 - model_181_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4402 - model_180_loss: 0.4791 - model_181_loss: 0.6924 - model_181_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4365 - model_180_loss: 0.4793 - model_181_loss: 0.6921 - model_181_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4377 - model_180_loss: 0.4781 - model_181_loss: 0.6917 - model_181_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4410 - model_180_loss: 0.4791 - model_181_loss: 0.6921 - model_181_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 98us/sample - loss: 6.9173 - model_181_loss: 0.6921 - model_181_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4451 - model_180_loss: 0.4781 - model_181_loss: 0.6924 - model_181_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4416 - model_180_loss: 0.4782 - model_181_loss: 0.6919 - model_181_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4435 - model_180_loss: 0.4792 - model_181_loss: 0.6922 - model_181_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4417 - model_180_loss: 0.4790 - model_181_loss: 0.6918 - model_181_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4439 - model_180_loss: 0.4792 - model_181_loss: 0.6923 - model_181_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 117us/sample - loss: 6.9204 - model_181_loss: 0.6921 - model_181_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4368 - model_180_loss: 0.4802 - model_181_loss: 0.6919 - model_181_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4368 - model_180_loss: 0.4792 - model_181_loss: 0.6921 - model_181_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4359 - model_180_loss: 0.4817 - model_181_loss: 0.6919 - model_181_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4432 - model_180_loss: 0.4781 - model_181_loss: 0.6924 - model_181_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4406 - model_180_loss: 0.4799 - model_181_loss: 0.6923 - model_181_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 121us/sample - loss: 6.9192 - model_181_loss: 0.6924 - model_181_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4407 - model_180_loss: 0.4794 - model_181_loss: 0.6920 - model_181_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4384 - model_180_loss: 0.4797 - model_181_loss: 0.6919 - model_181_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4375 - model_180_loss: 0.4819 - model_181_loss: 0.6922 - model_181_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4410 - model_180_loss: 0.4795 - model_181_loss: 0.6923 - model_181_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4417 - model_180_loss: 0.4793 - model_181_loss: 0.6922 - model_181_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 118us/sample - loss: 6.9188 - model_181_loss: 0.6922 - model_181_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4383 - model_180_loss: 0.4781 - model_181_loss: 0.6917 - model_181_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4394 - model_180_loss: 0.4782 - model_181_loss: 0.6921 - model_181_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4364 - model_180_loss: 0.4810 - model_181_loss: 0.6918 - model_181_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4406 - model_180_loss: 0.4763 - model_181_loss: 0.6919 - model_181_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4370 - model_180_loss: 0.4789 - model_181_loss: 0.6919 - model_181_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 118us/sample - loss: 6.9198 - model_181_loss: 0.6921 - model_181_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4382 - model_180_loss: 0.4775 - model_181_loss: 0.6917 - model_181_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4407 - model_180_loss: 0.4765 - model_181_loss: 0.6919 - model_181_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4394 - model_180_loss: 0.4777 - model_181_loss: 0.6917 - model_181_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4391 - model_180_loss: 0.4772 - model_181_loss: 0.6920 - model_181_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4417 - model_180_loss: 0.4748 - model_181_loss: 0.6919 - model_181_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.9179 - model_181_loss: 0.6920 - model_181_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4398 - model_180_loss: 0.4756 - model_181_loss: 0.6916 - model_181_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4404 - model_180_loss: 0.4756 - model_181_loss: 0.6918 - model_181_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4418 - model_180_loss: 0.4752 - model_181_loss: 0.6918 - model_181_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4433 - model_180_loss: 0.4730 - model_181_loss: 0.6920 - model_181_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4441 - model_180_loss: 0.4723 - model_181_loss: 0.6919 - model_181_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 123us/sample - loss: 6.9138 - model_181_loss: 0.6927 - model_181_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4438 - model_180_loss: 0.4717 - model_181_loss: 0.6917 - model_181_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4468 - model_180_loss: 0.4712 - model_181_loss: 0.6919 - model_181_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4472 - model_180_loss: 0.4707 - model_181_loss: 0.6922 - model_181_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4475 - model_180_loss: 0.4715 - model_181_loss: 0.6921 - model_181_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4479 - model_180_loss: 0.4691 - model_181_loss: 0.6923 - model_181_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.9182 - model_181_loss: 0.6921 - model_181_1_loss: 0.6911s - loss: 6.8622 - model_181_loss: 0.6861 - model_181_ - ETA: 1s - loss: 6.8962 - model_181_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4458 - model_180_loss: 0.4703 - model_181_loss: 0.6920 - model_181_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4466 - model_180_loss: 0.4699 - model_181_loss: 0.6921 - model_181_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4446 - model_180_loss: 0.4708 - model_181_loss: 0.6919 - model_181_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4482 - model_180_loss: 0.4699 - model_181_loss: 0.6924 - model_181_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4463 - model_180_loss: 0.4695 - model_181_loss: 0.6920 - model_181_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9183 - model_181_loss: 0.6927 - model_181_1_loss: 0.691 - 3s 115us/sample - loss: 6.9183 - model_181_loss: 0.6917 - model_181_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4437 - model_180_loss: 0.4705 - model_181_loss: 0.6920 - model_181_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4488 - model_180_loss: 0.4691 - model_181_loss: 0.6923 - model_181_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4492 - model_180_loss: 0.4711 - model_181_loss: 0.6925 - model_181_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4492 - model_180_loss: 0.4687 - model_181_loss: 0.6924 - model_181_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4454 - model_180_loss: 0.4702 - model_181_loss: 0.6922 - model_181_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 108us/sample - loss: 6.9208 - model_181_loss: 0.6922 - model_181_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4476 - model_180_loss: 0.4708 - model_181_loss: 0.6923 - model_181_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4510 - model_180_loss: 0.4708 - model_181_loss: 0.6930 - model_181_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4492 - model_180_loss: 0.4714 - model_181_loss: 0.6926 - model_181_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4498 - model_180_loss: 0.4699 - model_181_loss: 0.6925 - model_181_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4485 - model_180_loss: 0.4721 - model_181_loss: 0.6927 - model_181_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 116us/sample - loss: 6.9215 - model_181_loss: 0.6920 - model_181_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4478 - model_180_loss: 0.4722 - model_181_loss: 0.6924 - model_181_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4482 - model_180_loss: 0.4719 - model_181_loss: 0.6926 - model_181_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4484 - model_180_loss: 0.4708 - model_181_loss: 0.6922 - model_181_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4479 - model_180_loss: 0.4717 - model_181_loss: 0.6925 - model_181_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4473 - model_180_loss: 0.4718 - model_181_loss: 0.6924 - model_181_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9214 - model_181_loss: 0.6931 - model_181_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4466 - model_180_loss: 0.4735 - model_181_loss: 0.6927 - model_181_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4506 - model_180_loss: 0.4706 - model_181_loss: 0.6925 - model_181_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4486 - model_180_loss: 0.4705 - model_181_loss: 0.6924 - model_181_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4489 - model_180_loss: 0.4701 - model_181_loss: 0.6924 - model_181_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4486 - model_180_loss: 0.4697 - model_181_loss: 0.6924 - model_181_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 104us/sample - loss: 6.9168 - model_181_loss: 0.6928 - model_181_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4476 - model_180_loss: 0.4694 - model_181_loss: 0.6920 - model_181_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4510 - model_180_loss: 0.4706 - model_181_loss: 0.6927 - model_181_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4504 - model_180_loss: 0.4677 - model_181_loss: 0.6925 - model_181_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4517 - model_180_loss: 0.4696 - model_181_loss: 0.6929 - model_181_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4493 - model_180_loss: 0.4687 - model_181_loss: 0.6924 - model_181_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 103us/sample - loss: 6.9208 - model_181_loss: 0.6932 - model_181_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4499 - model_180_loss: 0.4711 - model_181_loss: 0.6926 - model_181_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4503 - model_180_loss: 0.4693 - model_181_loss: 0.6925 - model_181_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4488 - model_180_loss: 0.4707 - model_181_loss: 0.6923 - model_181_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4521 - model_180_loss: 0.4702 - model_181_loss: 0.6928 - model_181_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4507 - model_180_loss: 0.4697 - model_181_loss: 0.6924 - model_181_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 139us/sample - loss: 6.9241 - model_181_loss: 0.6932 - model_181_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: -6.4514 - model_180_loss: 0.4701 - model_181_loss: 0.6926 - model_181_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4507 - model_180_loss: 0.4701 - model_181_loss: 0.6925 - model_181_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4498 - model_180_loss: 0.4706 - model_181_loss: 0.6925 - model_181_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4509 - model_180_loss: 0.4702 - model_181_loss: 0.6928 - model_181_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4504 - model_180_loss: 0.4707 - model_181_loss: 0.6925 - model_181_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 115us/sample - loss: 6.9207 - model_181_loss: 0.6923 - model_181_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4529 - model_180_loss: 0.4695 - model_181_loss: 0.6926 - model_181_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4515 - model_180_loss: 0.4693 - model_181_loss: 0.6924 - model_181_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4508 - model_180_loss: 0.4706 - model_181_loss: 0.6927 - model_181_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4515 - model_180_loss: 0.4697 - model_181_loss: 0.6925 - model_181_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4552 - model_180_loss: 0.4696 - model_181_loss: 0.6928 - model_181_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 109us/sample - loss: 6.9231 - model_181_loss: 0.6924 - model_181_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4494 - model_180_loss: 0.4703 - model_181_loss: 0.6925 - model_181_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4502 - model_180_loss: 0.4697 - model_181_loss: 0.6927 - model_181_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4525 - model_180_loss: 0.4697 - model_181_loss: 0.6928 - model_181_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4505 - model_180_loss: 0.4698 - model_181_loss: 0.6925 - model_181_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4513 - model_180_loss: 0.4714 - model_181_loss: 0.6928 - model_181_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 119us/sample - loss: 6.9230 - model_181_loss: 0.6937 - model_181_1_loss: 0.6919s - loss: 6.9416 - model_181\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4507 - model_180_loss: 0.4704 - model_181_loss: 0.6927 - model_181_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4509 - model_180_loss: 0.4695 - model_181_loss: 0.6925 - model_181_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4513 - model_180_loss: 0.4696 - model_181_loss: 0.6927 - model_181_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4545 - model_180_loss: 0.4670 - model_181_loss: 0.6927 - model_181_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4528 - model_180_loss: 0.4662 - model_181_loss: 0.6924 - model_181_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 103us/sample - loss: 6.9225 - model_181_loss: 0.6932 - model_181_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4523 - model_180_loss: 0.4678 - model_181_loss: 0.6925 - model_181_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4524 - model_180_loss: 0.4658 - model_181_loss: 0.6924 - model_181_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4526 - model_180_loss: 0.4681 - model_181_loss: 0.6929 - model_181_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4534 - model_180_loss: 0.4648 - model_181_loss: 0.6925 - model_181_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4536 - model_180_loss: 0.4646 - model_181_loss: 0.6924 - model_181_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.9190 - model_181_loss: 0.6930 - model_181_1_loss: 0.6909s - loss: 6.9350 - model_181_loss: 0.6972 - model_181_1_l\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4520 - model_180_loss: 0.4650 - model_181_loss: 0.6922 - model_181_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4528 - model_180_loss: 0.4662 - model_181_loss: 0.6926 - model_181_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4503 - model_180_loss: 0.4661 - model_181_loss: 0.6923 - model_181_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4523 - model_180_loss: 0.4637 - model_181_loss: 0.6924 - model_181_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4502 - model_180_loss: 0.4657 - model_181_loss: 0.6923 - model_181_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 104us/sample - loss: 6.9180 - model_181_loss: 0.6931 - model_181_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4509 - model_180_loss: 0.4644 - model_181_loss: 0.6922 - model_181_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4533 - model_180_loss: 0.4645 - model_181_loss: 0.6925 - model_181_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4508 - model_180_loss: 0.4664 - model_181_loss: 0.6924 - model_181_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4524 - model_180_loss: 0.4653 - model_181_loss: 0.6924 - model_181_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4532 - model_180_loss: 0.4666 - model_181_loss: 0.6924 - model_181_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 116us/sample - loss: 6.9189 - model_181_loss: 0.6926 - model_181_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4514 - model_180_loss: 0.4662 - model_181_loss: 0.6925 - model_181_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4576 - model_180_loss: 0.4629 - model_181_loss: 0.6927 - model_181_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4553 - model_180_loss: 0.4651 - model_181_loss: 0.6926 - model_181_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4539 - model_180_loss: 0.4660 - model_181_loss: 0.6927 - model_181_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4546 - model_180_loss: 0.4663 - model_181_loss: 0.6926 - model_181_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 118us/sample - loss: 6.9224 - model_181_loss: 0.6936 - model_181_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4559 - model_180_loss: 0.4665 - model_181_loss: 0.6927 - model_181_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4549 - model_180_loss: 0.4666 - model_181_loss: 0.6926 - model_181_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4550 - model_180_loss: 0.4677 - model_181_loss: 0.6926 - model_181_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4526 - model_180_loss: 0.4683 - model_181_loss: 0.6926 - model_181_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4564 - model_180_loss: 0.4700 - model_181_loss: 0.6929 - model_181_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 98us/sample - loss: 6.9244 - model_181_loss: 0.6933 - model_181_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4523 - model_180_loss: 0.4701 - model_181_loss: 0.6926 - model_181_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4538 - model_180_loss: 0.4708 - model_181_loss: 0.6927 - model_181_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4501 - model_180_loss: 0.4732 - model_181_loss: 0.6925 - model_181_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4514 - model_180_loss: 0.4722 - model_181_loss: 0.6927 - model_181_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4541 - model_180_loss: 0.4722 - model_181_loss: 0.6930 - model_181_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 115us/sample - loss: 6.9262 - model_181_loss: 0.6922 - model_181_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4521 - model_180_loss: 0.4718 - model_181_loss: 0.6927 - model_181_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4519 - model_180_loss: 0.4722 - model_181_loss: 0.6926 - model_181_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4524 - model_180_loss: 0.4722 - model_181_loss: 0.6927 - model_181_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4544 - model_180_loss: 0.4721 - model_181_loss: 0.6930 - model_181_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4495 - model_180_loss: 0.4740 - model_181_loss: 0.6926 - model_181_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 98us/sample - loss: 6.9257 - model_181_loss: 0.6923 - model_181_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4505 - model_180_loss: 0.4719 - model_181_loss: 0.6925 - model_181_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4552 - model_180_loss: 0.4703 - model_181_loss: 0.6928 - model_181_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4524 - model_180_loss: 0.4707 - model_181_loss: 0.6926 - model_181_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4556 - model_180_loss: 0.4686 - model_181_loss: 0.6927 - model_181_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4546 - model_180_loss: 0.4691 - model_181_loss: 0.6928 - model_181_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9207 - model_181_loss: 0.6924 - model_181_1_loss: 0.691 - 3s 101us/sample - loss: 6.9215 - model_181_loss: 0.6922 - model_181_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4542 - model_180_loss: 0.4670 - model_181_loss: 0.6925 - model_181_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4550 - model_180_loss: 0.4682 - model_181_loss: 0.6929 - model_181_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4531 - model_180_loss: 0.4682 - model_181_loss: 0.6926 - model_181_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4534 - model_180_loss: 0.4669 - model_181_loss: 0.6924 - model_181_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4553 - model_180_loss: 0.4653 - model_181_loss: 0.6926 - model_181_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.9200 - model_181_loss: 0.6929 - model_181_1_loss: 0.6913s - loss: 6.9333 - model_181_l\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4543 - model_180_loss: 0.4630 - model_181_loss: 0.6924 - model_181_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4542 - model_180_loss: 0.4656 - model_181_loss: 0.6926 - model_181_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4560 - model_180_loss: 0.4641 - model_181_loss: 0.6925 - model_181_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4561 - model_180_loss: 0.4631 - model_181_loss: 0.6923 - model_181_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4538 - model_180_loss: 0.4632 - model_181_loss: 0.6923 - model_181_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 120us/sample - loss: 6.9182 - model_181_loss: 0.6922 - model_181_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4577 - model_180_loss: 0.4614 - model_181_loss: 0.6925 - model_181_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4557 - model_180_loss: 0.4616 - model_181_loss: 0.6921 - model_181_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4531 - model_180_loss: 0.4646 - model_181_loss: 0.6925 - model_181_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4570 - model_180_loss: 0.4611 - model_181_loss: 0.6921 - model_181_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4557 - model_180_loss: 0.4628 - model_181_loss: 0.6924 - model_181_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.9184 - model_181_loss: 0.6919 - model_181_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4536 - model_180_loss: 0.4633 - model_181_loss: 0.6923 - model_181_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4526 - model_180_loss: 0.4639 - model_181_loss: 0.6924 - model_181_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4530 - model_180_loss: 0.4635 - model_181_loss: 0.6924 - model_181_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4545 - model_180_loss: 0.4640 - model_181_loss: 0.6926 - model_181_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4572 - model_180_loss: 0.4618 - model_181_loss: 0.6928 - model_181_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.9198 - model_181_loss: 0.6928 - model_181_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4539 - model_180_loss: 0.4643 - model_181_loss: 0.6923 - model_181_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4591 - model_180_loss: 0.4627 - model_181_loss: 0.6929 - model_181_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4535 - model_180_loss: 0.4642 - model_181_loss: 0.6925 - model_181_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4562 - model_180_loss: 0.4646 - model_181_loss: 0.6927 - model_181_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4543 - model_180_loss: 0.4648 - model_181_loss: 0.6924 - model_181_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 99us/sample - loss: 6.9221 - model_181_loss: 0.6923 - model_181_1_loss: 0.69171s - loss: 6.9107 - model_181_l\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4561 - model_180_loss: 0.4654 - model_181_loss: 0.6927 - model_181_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4543 - model_180_loss: 0.4655 - model_181_loss: 0.6926 - model_181_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4537 - model_180_loss: 0.4638 - model_181_loss: 0.6923 - model_181_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4562 - model_180_loss: 0.4644 - model_181_loss: 0.6928 - model_181_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4555 - model_180_loss: 0.4653 - model_181_loss: 0.6925 - model_181_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 116us/sample - loss: 6.9214 - model_181_loss: 0.6928 - model_181_1_loss: 0.6916s - loss: 6.8713 - model_ - ETA: 0s - loss: 6.9234 - model_181_loss: 0.6930 - model_181_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4540 - model_180_loss: 0.4657 - model_181_loss: 0.6926 - model_181_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4545 - model_180_loss: 0.4668 - model_181_loss: 0.6926 - model_181_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4550 - model_180_loss: 0.4654 - model_181_loss: 0.6926 - model_181_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4554 - model_180_loss: 0.4656 - model_181_loss: 0.6926 - model_181_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4563 - model_180_loss: 0.4647 - model_181_loss: 0.6927 - model_181_1_loss: 0.6915\n",
      "For Attention Module: 4.7\n",
      "features X: 30940 samples, 69 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 118us/sample - loss: 6.3665 - model_185_loss: 0.6608 - model_185_1_loss: 0.6128\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 2s 61us/sample - loss: -5.9844 - model_184_loss: 0.3847 - model_185_loss: 0.6605 - model_185_1_loss: 0.6133\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0002 - model_184_loss: 0.3842 - model_185_loss: 0.6620 - model_185_1_loss: 0.6149\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.0020 - model_184_loss: 0.3843 - model_185_loss: 0.6615 - model_185_1_loss: 0.6157\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0111 - model_184_loss: 0.3864 - model_185_loss: 0.6614 - model_185_1_loss: 0.6181\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0220 - model_184_loss: 0.3861 - model_185_loss: 0.6624 - model_185_1_loss: 0.6192\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 133us/sample - loss: 6.4092 - model_185_loss: 0.6623 - model_185_1_loss: 0.6201\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: -6.0323 - model_184_loss: 0.3865 - model_185_loss: 0.6623 - model_185_1_loss: 0.6215\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.0380 - model_184_loss: 0.3882 - model_185_loss: 0.6628 - model_185_1_loss: 0.6224\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.0498 - model_184_loss: 0.3894 - model_185_loss: 0.6635 - model_185_1_loss: 0.6244\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0617 - model_184_loss: 0.3899 - model_185_loss: 0.6642 - model_185_1_loss: 0.6261\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.0762 - model_184_loss: 0.3908 - model_185_loss: 0.6642 - model_185_1_loss: 0.6292\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 140us/sample - loss: 6.4706 - model_185_loss: 0.6654 - model_185_1_loss: 0.6292\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -6.0724 - model_184_loss: 0.3917 - model_185_loss: 0.6650 - model_185_1_loss: 0.6278\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0762 - model_184_loss: 0.3953 - model_185_loss: 0.6653 - model_185_1_loss: 0.6290\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.0935 - model_184_loss: 0.3965 - model_185_loss: 0.6678 - model_185_1_loss: 0.6302\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.0948 - model_184_loss: 0.3973 - model_185_loss: 0.6663 - model_185_1_loss: 0.6321\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.1190 - model_184_loss: 0.3987 - model_185_loss: 0.6696 - model_185_1_loss: 0.6340\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 124us/sample - loss: 6.5285 - model_185_loss: 0.6686 - model_185_1_loss: 0.6368\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.1267 - model_184_loss: 0.3991 - model_185_loss: 0.6689 - model_185_1_loss: 0.6363\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.1323 - model_184_loss: 0.4020 - model_185_loss: 0.6695 - model_185_1_loss: 0.6374\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.1502 - model_184_loss: 0.4021 - model_185_loss: 0.6706 - model_185_1_loss: 0.6398\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.1621 - model_184_loss: 0.4049 - model_185_loss: 0.6718 - model_185_1_loss: 0.6416\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.1698 - model_184_loss: 0.4060 - model_185_loss: 0.6729 - model_185_1_loss: 0.6423\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 132us/sample - loss: 6.5911 - model_185_loss: 0.6728 - model_185_1_loss: 0.6454\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: -6.1725 - model_184_loss: 0.4081 - model_185_loss: 0.6720 - model_185_1_loss: 0.6442\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.1887 - model_184_loss: 0.4108 - model_185_loss: 0.6742 - model_185_1_loss: 0.6457\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.1954 - model_184_loss: 0.4142 - model_185_loss: 0.6732 - model_185_1_loss: 0.6488\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.2131 - model_184_loss: 0.4146 - model_185_loss: 0.6752 - model_185_1_loss: 0.6503\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.2231 - model_184_loss: 0.4166 - model_185_loss: 0.6755 - model_185_1_loss: 0.6524\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 130us/sample - loss: 6.6469 - model_185_loss: 0.6761 - model_185_1_loss: 0.6537\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.2290 - model_184_loss: 0.4205 - model_185_loss: 0.6761 - model_185_1_loss: 0.6538\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.2448 - model_184_loss: 0.4213 - model_185_loss: 0.6777 - model_185_1_loss: 0.6555\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2541 - model_184_loss: 0.4219 - model_185_loss: 0.6776 - model_185_1_loss: 0.6576\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2663 - model_184_loss: 0.4260 - model_185_loss: 0.6792 - model_185_1_loss: 0.6592\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2858 - model_184_loss: 0.4275 - model_185_loss: 0.6804 - model_185_1_loss: 0.6622\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 125us/sample - loss: 6.7288 - model_185_loss: 0.6814 - model_185_1_loss: 0.6648\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: -6.2853 - model_184_loss: 0.4304 - model_185_loss: 0.6797 - model_185_1_loss: 0.6634\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.2954 - model_184_loss: 0.4322 - model_185_loss: 0.6798 - model_185_1_loss: 0.6657\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3092 - model_184_loss: 0.4330 - model_185_loss: 0.6807 - model_185_1_loss: 0.6678\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3257 - model_184_loss: 0.4352 - model_185_loss: 0.6823 - model_185_1_loss: 0.6699\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3313 - model_184_loss: 0.4374 - model_185_loss: 0.6818 - model_185_1_loss: 0.6719\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 125us/sample - loss: 6.7705 - model_185_loss: 0.6832 - model_185_1_loss: 0.6710\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.3314 - model_184_loss: 0.4381 - model_185_loss: 0.6830 - model_185_1_loss: 0.6709\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3407 - model_184_loss: 0.4410 - model_185_loss: 0.6837 - model_185_1_loss: 0.6726\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3484 - model_184_loss: 0.4417 - model_185_loss: 0.6839 - model_185_1_loss: 0.6741\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3569 - model_184_loss: 0.4436 - model_185_loss: 0.6845 - model_185_1_loss: 0.6756\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3719 - model_184_loss: 0.4440 - model_185_loss: 0.6852 - model_185_1_loss: 0.6779\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 121us/sample - loss: 6.8177 - model_185_loss: 0.6871 - model_185_1_loss: 0.6769\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -6.3654 - model_184_loss: 0.4445 - model_185_loss: 0.6851 - model_185_1_loss: 0.6769\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3727 - model_184_loss: 0.4472 - model_185_loss: 0.6865 - model_185_1_loss: 0.6775\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3752 - model_184_loss: 0.4481 - model_185_loss: 0.6863 - model_185_1_loss: 0.6783\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3872 - model_184_loss: 0.4483 - model_185_loss: 0.6877 - model_185_1_loss: 0.6794\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3914 - model_184_loss: 0.4523 - model_185_loss: 0.6876 - model_185_1_loss: 0.6811\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 134us/sample - loss: 6.8490 - model_185_loss: 0.6882 - model_185_1_loss: 0.6818\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -6.3896 - model_184_loss: 0.4522 - model_185_loss: 0.6873 - model_185_1_loss: 0.6811\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3986 - model_184_loss: 0.4530 - model_185_loss: 0.6882 - model_185_1_loss: 0.6821\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3987 - model_184_loss: 0.4550 - model_185_loss: 0.6887 - model_185_1_loss: 0.6820\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4030 - model_184_loss: 0.4557 - model_185_loss: 0.6892 - model_185_1_loss: 0.6826\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3999 - model_184_loss: 0.4565 - model_185_loss: 0.6881 - model_185_1_loss: 0.6832\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.8675 - model_185_loss: 0.6892 - model_185_1_loss: 0.6839\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4059 - model_184_loss: 0.4580 - model_185_loss: 0.6894 - model_185_1_loss: 0.6834\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4087 - model_184_loss: 0.4607 - model_185_loss: 0.6891 - model_185_1_loss: 0.6848\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4141 - model_184_loss: 0.4619 - model_185_loss: 0.6904 - model_185_1_loss: 0.6848\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4164 - model_184_loss: 0.4650 - model_185_loss: 0.6905 - model_185_1_loss: 0.6858\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4108 - model_184_loss: 0.4655 - model_185_loss: 0.6896 - model_185_1_loss: 0.6857\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 121us/sample - loss: 6.8912 - model_185_loss: 0.6903 - model_185_1_loss: 0.6875\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4171 - model_184_loss: 0.4677 - model_185_loss: 0.6904 - model_185_1_loss: 0.6866\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4219 - model_184_loss: 0.4693 - model_185_loss: 0.6910 - model_185_1_loss: 0.6872\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4299 - model_184_loss: 0.4689 - model_185_loss: 0.6914 - model_185_1_loss: 0.6884\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4299 - model_184_loss: 0.4707 - model_185_loss: 0.6914 - model_185_1_loss: 0.6888\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4301 - model_184_loss: 0.4722 - model_185_loss: 0.6917 - model_185_1_loss: 0.6888\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.9039 - model_185_loss: 0.6921 - model_185_1_loss: 0.6890\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4301 - model_184_loss: 0.4727 - model_185_loss: 0.6917 - model_185_1_loss: 0.6889\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4250 - model_184_loss: 0.4764 - model_185_loss: 0.6910 - model_185_1_loss: 0.6893\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4303 - model_184_loss: 0.4778 - model_185_loss: 0.6921 - model_185_1_loss: 0.6895\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4320 - model_184_loss: 0.4790 - model_185_loss: 0.6918 - model_185_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4306 - model_184_loss: 0.4787 - model_185_loss: 0.6916 - model_185_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.9158 - model_185_loss: 0.6920 - model_185_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4295 - model_184_loss: 0.4806 - model_185_loss: 0.6915 - model_185_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4327 - model_184_loss: 0.4803 - model_185_loss: 0.6918 - model_185_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4302 - model_184_loss: 0.4806 - model_185_loss: 0.6915 - model_185_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4291 - model_184_loss: 0.4835 - model_185_loss: 0.6917 - model_185_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4314 - model_184_loss: 0.4812 - model_185_loss: 0.6916 - model_185_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 118us/sample - loss: 6.9186 - model_185_loss: 0.6923 - model_185_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4319 - model_184_loss: 0.4835 - model_185_loss: 0.6920 - model_185_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4294 - model_184_loss: 0.4824 - model_185_loss: 0.6914 - model_185_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4309 - model_184_loss: 0.4827 - model_185_loss: 0.6916 - model_185_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4306 - model_184_loss: 0.4848 - model_185_loss: 0.6917 - model_185_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4382 - model_184_loss: 0.4831 - model_185_loss: 0.6922 - model_185_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 123us/sample - loss: 6.9184 - model_185_loss: 0.6920 - model_185_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4345 - model_184_loss: 0.4820 - model_185_loss: 0.6917 - model_185_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4368 - model_184_loss: 0.4827 - model_185_loss: 0.6919 - model_185_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4364 - model_184_loss: 0.4827 - model_185_loss: 0.6919 - model_185_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4373 - model_184_loss: 0.4814 - model_185_loss: 0.6919 - model_185_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4420 - model_184_loss: 0.4797 - model_185_loss: 0.6922 - model_185_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 111us/sample - loss: 6.9202 - model_185_loss: 0.6921 - model_185_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4372 - model_184_loss: 0.4797 - model_185_loss: 0.6919 - model_185_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4352 - model_184_loss: 0.4794 - model_185_loss: 0.6918 - model_185_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4374 - model_184_loss: 0.4785 - model_185_loss: 0.6919 - model_185_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4382 - model_184_loss: 0.4791 - model_185_loss: 0.6919 - model_185_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4364 - model_184_loss: 0.4767 - model_185_loss: 0.6915 - model_185_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.9191 - model_185_loss: 0.6923 - model_185_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4427 - model_184_loss: 0.4783 - model_185_loss: 0.6920 - model_185_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4380 - model_184_loss: 0.4789 - model_185_loss: 0.6918 - model_185_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4411 - model_184_loss: 0.4771 - model_185_loss: 0.6920 - model_185_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4425 - model_184_loss: 0.4746 - model_185_loss: 0.6918 - model_185_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4440 - model_184_loss: 0.4759 - model_185_loss: 0.6922 - model_185_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.9172 - model_185_loss: 0.6917 - model_185_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4430 - model_184_loss: 0.4756 - model_185_loss: 0.6921 - model_185_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4454 - model_184_loss: 0.4732 - model_185_loss: 0.6922 - model_185_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4420 - model_184_loss: 0.4760 - model_185_loss: 0.6922 - model_185_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4465 - model_184_loss: 0.4746 - model_185_loss: 0.6925 - model_185_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4425 - model_184_loss: 0.4766 - model_185_loss: 0.6926 - model_185_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 108us/sample - loss: 6.9186 - model_185_loss: 0.6926 - model_185_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4434 - model_184_loss: 0.4750 - model_185_loss: 0.6923 - model_185_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4448 - model_184_loss: 0.4743 - model_185_loss: 0.6924 - model_185_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4454 - model_184_loss: 0.4740 - model_185_loss: 0.6924 - model_185_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4427 - model_184_loss: 0.4729 - model_185_loss: 0.6918 - model_185_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4431 - model_184_loss: 0.4737 - model_185_loss: 0.6924 - model_185_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 113us/sample - loss: 6.9219 - model_185_loss: 0.6920 - model_185_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4415 - model_184_loss: 0.4740 - model_185_loss: 0.6918 - model_185_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4478 - model_184_loss: 0.4725 - model_185_loss: 0.6928 - model_185_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4420 - model_184_loss: 0.4739 - model_185_loss: 0.6920 - model_185_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4452 - model_184_loss: 0.4728 - model_185_loss: 0.6924 - model_185_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4442 - model_184_loss: 0.4722 - model_185_loss: 0.6921 - model_185_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 108us/sample - loss: 6.9178 - model_185_loss: 0.6925 - model_185_1_loss: 0.6912s - loss: 6.8598 - model_185_loss: 0.6823\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4426 - model_184_loss: 0.4720 - model_185_loss: 0.6920 - model_185_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4472 - model_184_loss: 0.4703 - model_185_loss: 0.6928 - model_185_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4447 - model_184_loss: 0.4711 - model_185_loss: 0.6924 - model_185_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4477 - model_184_loss: 0.4713 - model_185_loss: 0.6928 - model_185_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4456 - model_184_loss: 0.4698 - model_185_loss: 0.6922 - model_185_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.9168 - model_185_loss: 0.6932 - model_185_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4434 - model_184_loss: 0.4701 - model_185_loss: 0.6922 - model_185_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4492 - model_184_loss: 0.4686 - model_185_loss: 0.6925 - model_185_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4473 - model_184_loss: 0.4703 - model_185_loss: 0.6926 - model_185_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4438 - model_184_loss: 0.4715 - model_185_loss: 0.6921 - model_185_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4484 - model_184_loss: 0.4704 - model_185_loss: 0.6923 - model_185_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 123us/sample - loss: 6.9207 - model_185_loss: 0.6924 - model_185_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4496 - model_184_loss: 0.4674 - model_185_loss: 0.6923 - model_185_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4484 - model_184_loss: 0.4688 - model_185_loss: 0.6922 - model_185_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4485 - model_184_loss: 0.4699 - model_185_loss: 0.6924 - model_185_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4481 - model_184_loss: 0.4698 - model_185_loss: 0.6923 - model_185_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4492 - model_184_loss: 0.4690 - model_185_loss: 0.6922 - model_185_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 101us/sample - loss: 6.9186 - model_185_loss: 0.6920 - model_185_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4459 - model_184_loss: 0.4711 - model_185_loss: 0.6922 - model_185_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4512 - model_184_loss: 0.4705 - model_185_loss: 0.6928 - model_185_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4465 - model_184_loss: 0.4698 - model_185_loss: 0.6919 - model_185_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4481 - model_184_loss: 0.4707 - model_185_loss: 0.6923 - model_185_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4496 - model_184_loss: 0.4701 - model_185_loss: 0.6923 - model_185_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 103us/sample - loss: 6.9212 - model_185_loss: 0.6928 - model_185_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4468 - model_184_loss: 0.4692 - model_185_loss: 0.6918 - model_185_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4443 - model_184_loss: 0.4713 - model_185_loss: 0.6917 - model_185_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4477 - model_184_loss: 0.4716 - model_185_loss: 0.6922 - model_185_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4476 - model_184_loss: 0.4710 - model_185_loss: 0.6924 - model_185_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4485 - model_184_loss: 0.4705 - model_185_loss: 0.6923 - model_185_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 125us/sample - loss: 6.9217 - model_185_loss: 0.6935 - model_185_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4491 - model_184_loss: 0.4715 - model_185_loss: 0.6925 - model_185_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4485 - model_184_loss: 0.4709 - model_185_loss: 0.6921 - model_185_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4493 - model_184_loss: 0.4712 - model_185_loss: 0.6925 - model_185_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4521 - model_184_loss: 0.4705 - model_185_loss: 0.6927 - model_185_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4477 - model_184_loss: 0.4732 - model_185_loss: 0.6925 - model_185_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 127us/sample - loss: 6.9206 - model_185_loss: 0.6917 - model_185_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4478 - model_184_loss: 0.4705 - model_185_loss: 0.6921 - model_185_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4465 - model_184_loss: 0.4713 - model_185_loss: 0.6921 - model_185_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4474 - model_184_loss: 0.4713 - model_185_loss: 0.6921 - model_185_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4482 - model_184_loss: 0.4720 - model_185_loss: 0.6924 - model_185_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4500 - model_184_loss: 0.4698 - model_185_loss: 0.6922 - model_185_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 109us/sample - loss: 6.9242 - model_185_loss: 0.6930 - model_185_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4520 - model_184_loss: 0.4703 - model_185_loss: 0.6928 - model_185_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4527 - model_184_loss: 0.4692 - model_185_loss: 0.6927 - model_185_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4505 - model_184_loss: 0.4698 - model_185_loss: 0.6923 - model_185_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4490 - model_184_loss: 0.4717 - model_185_loss: 0.6927 - model_185_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4519 - model_184_loss: 0.4704 - model_185_loss: 0.6929 - model_185_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.9233 - model_185_loss: 0.6925 - model_185_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4480 - model_184_loss: 0.4717 - model_185_loss: 0.6926 - model_185_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4537 - model_184_loss: 0.4690 - model_185_loss: 0.6930 - model_185_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4498 - model_184_loss: 0.4724 - model_185_loss: 0.6927 - model_185_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4503 - model_184_loss: 0.4698 - model_185_loss: 0.6924 - model_185_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4512 - model_184_loss: 0.4698 - model_185_loss: 0.6926 - model_185_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 111us/sample - loss: 6.9232 - model_185_loss: 0.6925 - model_185_1_loss: 0.6917s - loss: 6.9281 - model_185_loss: 0.6953 - mod\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4491 - model_184_loss: 0.4708 - model_185_loss: 0.6924 - model_185_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4505 - model_184_loss: 0.4704 - model_185_loss: 0.6925 - model_185_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4505 - model_184_loss: 0.4699 - model_185_loss: 0.6926 - model_185_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4498 - model_184_loss: 0.4702 - model_185_loss: 0.6924 - model_185_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4539 - model_184_loss: 0.4678 - model_185_loss: 0.6927 - model_185_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 126us/sample - loss: 6.9202 - model_185_loss: 0.6930 - model_185_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -6.4532 - model_184_loss: 0.4661 - model_185_loss: 0.6923 - model_185_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4516 - model_184_loss: 0.4677 - model_185_loss: 0.6925 - model_185_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4498 - model_184_loss: 0.4686 - model_185_loss: 0.6924 - model_185_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4519 - model_184_loss: 0.4666 - model_185_loss: 0.6924 - model_185_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4516 - model_184_loss: 0.4672 - model_185_loss: 0.6924 - model_185_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 102us/sample - loss: 6.9181 - model_185_loss: 0.6921 - model_185_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4506 - model_184_loss: 0.4682 - model_185_loss: 0.6924 - model_185_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4516 - model_184_loss: 0.4669 - model_185_loss: 0.6924 - model_185_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4534 - model_184_loss: 0.4650 - model_185_loss: 0.6925 - model_185_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4512 - model_184_loss: 0.4680 - model_185_loss: 0.6925 - model_185_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4505 - model_184_loss: 0.4666 - model_185_loss: 0.6925 - model_185_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 113us/sample - loss: 6.9197 - model_185_loss: 0.6932 - model_185_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4504 - model_184_loss: 0.4657 - model_185_loss: 0.6922 - model_185_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4518 - model_184_loss: 0.4659 - model_185_loss: 0.6925 - model_185_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4526 - model_184_loss: 0.4639 - model_185_loss: 0.6921 - model_185_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4509 - model_184_loss: 0.4658 - model_185_loss: 0.6922 - model_185_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4515 - model_184_loss: 0.4661 - model_185_loss: 0.6923 - model_185_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.9181 - model_185_loss: 0.6922 - model_185_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4543 - model_184_loss: 0.4651 - model_185_loss: 0.6926 - model_185_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4534 - model_184_loss: 0.4641 - model_185_loss: 0.6923 - model_185_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4522 - model_184_loss: 0.4664 - model_185_loss: 0.6925 - model_185_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4540 - model_184_loss: 0.4656 - model_185_loss: 0.6926 - model_185_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4529 - model_184_loss: 0.4656 - model_185_loss: 0.6925 - model_185_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 100us/sample - loss: 6.9197 - model_185_loss: 0.6931 - model_185_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4506 - model_184_loss: 0.4682 - model_185_loss: 0.6925 - model_185_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4511 - model_184_loss: 0.4675 - model_185_loss: 0.6924 - model_185_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4508 - model_184_loss: 0.4681 - model_185_loss: 0.6926 - model_185_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4485 - model_184_loss: 0.4691 - model_185_loss: 0.6921 - model_185_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4530 - model_184_loss: 0.4682 - model_185_loss: 0.6927 - model_185_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.9227 - model_185_loss: 0.6929 - model_185_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4520 - model_184_loss: 0.4672 - model_185_loss: 0.6923 - model_185_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4544 - model_184_loss: 0.4661 - model_185_loss: 0.6923 - model_185_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4540 - model_184_loss: 0.4674 - model_185_loss: 0.6926 - model_185_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4551 - model_184_loss: 0.4685 - model_185_loss: 0.6925 - model_185_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4507 - model_184_loss: 0.4704 - model_185_loss: 0.6923 - model_185_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 121us/sample - loss: 6.9218 - model_185_loss: 0.6922 - model_185_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4497 - model_184_loss: 0.4710 - model_185_loss: 0.6923 - model_185_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4519 - model_184_loss: 0.4701 - model_185_loss: 0.6926 - model_185_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4506 - model_184_loss: 0.4705 - model_185_loss: 0.6927 - model_185_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4536 - model_184_loss: 0.4675 - model_185_loss: 0.6924 - model_185_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4495 - model_184_loss: 0.4708 - model_185_loss: 0.6922 - model_185_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9237 - model_185_loss: 0.6930 - model_185_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4521 - model_184_loss: 0.4707 - model_185_loss: 0.6925 - model_185_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4529 - model_184_loss: 0.4697 - model_185_loss: 0.6925 - model_185_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4502 - model_184_loss: 0.4708 - model_185_loss: 0.6923 - model_185_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4501 - model_184_loss: 0.4710 - model_185_loss: 0.6924 - model_185_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4515 - model_184_loss: 0.4702 - model_185_loss: 0.6924 - model_185_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.9234 - model_185_loss: 0.6919 - model_185_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4530 - model_184_loss: 0.4692 - model_185_loss: 0.6926 - model_185_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4525 - model_184_loss: 0.4695 - model_185_loss: 0.6924 - model_185_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4527 - model_184_loss: 0.4702 - model_185_loss: 0.6926 - model_185_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4556 - model_184_loss: 0.4700 - model_185_loss: 0.6929 - model_185_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4535 - model_184_loss: 0.4705 - model_185_loss: 0.6925 - model_185_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 98us/sample - loss: 6.9247 - model_185_loss: 0.6924 - model_185_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4527 - model_184_loss: 0.4692 - model_185_loss: 0.6923 - model_185_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4540 - model_184_loss: 0.4710 - model_185_loss: 0.6928 - model_185_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4541 - model_184_loss: 0.4690 - model_185_loss: 0.6925 - model_185_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4534 - model_184_loss: 0.4697 - model_185_loss: 0.6922 - model_185_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4531 - model_184_loss: 0.4720 - model_185_loss: 0.6927 - model_185_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 121us/sample - loss: 6.9256 - model_185_loss: 0.6922 - model_185_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4553 - model_184_loss: 0.4696 - model_185_loss: 0.6926 - model_185_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4557 - model_184_loss: 0.4695 - model_185_loss: 0.6930 - model_185_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4538 - model_184_loss: 0.4705 - model_185_loss: 0.6925 - model_185_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4531 - model_184_loss: 0.4703 - model_185_loss: 0.6927 - model_185_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4552 - model_184_loss: 0.4694 - model_185_loss: 0.6928 - model_185_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.9256 - model_185_loss: 0.6928 - model_185_1_loss: 0.6922s - loss: 6.9196 - model_185_loss: 0.6917 - model - ETA: 0s - loss: 6.9264 - model_185_loss: 0.6932 - model_185_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4537 - model_184_loss: 0.4697 - model_185_loss: 0.6925 - model_185_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4543 - model_184_loss: 0.4694 - model_185_loss: 0.6927 - model_185_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4555 - model_184_loss: 0.4695 - model_185_loss: 0.6928 - model_185_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4544 - model_184_loss: 0.4686 - model_185_loss: 0.6927 - model_185_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4531 - model_184_loss: 0.4698 - model_185_loss: 0.6926 - model_185_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 120us/sample - loss: 6.9231 - model_185_loss: 0.6921 - model_185_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -6.4539 - model_184_loss: 0.4683 - model_185_loss: 0.6927 - model_185_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4540 - model_184_loss: 0.4678 - model_185_loss: 0.6925 - model_185_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4550 - model_184_loss: 0.4666 - model_185_loss: 0.6927 - model_185_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4547 - model_184_loss: 0.4681 - model_185_loss: 0.6927 - model_185_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4553 - model_184_loss: 0.4666 - model_185_loss: 0.6925 - model_185_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 128us/sample - loss: 6.9231 - model_185_loss: 0.6932 - model_185_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4547 - model_184_loss: 0.4648 - model_185_loss: 0.6923 - model_185_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4505 - model_184_loss: 0.4669 - model_185_loss: 0.6920 - model_185_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4546 - model_184_loss: 0.4654 - model_185_loss: 0.6922 - model_185_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4529 - model_184_loss: 0.4657 - model_185_loss: 0.6922 - model_185_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4526 - model_184_loss: 0.4653 - model_185_loss: 0.6919 - model_185_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 120us/sample - loss: 6.9224 - model_185_loss: 0.6921 - model_185_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4557 - model_184_loss: 0.4637 - model_185_loss: 0.6924 - model_185_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4540 - model_184_loss: 0.4653 - model_185_loss: 0.6921 - model_185_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4529 - model_184_loss: 0.4661 - model_185_loss: 0.6922 - model_185_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4567 - model_184_loss: 0.4642 - model_185_loss: 0.6926 - model_185_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4540 - model_184_loss: 0.4650 - model_185_loss: 0.6923 - model_185_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 5s 201us/sample - loss: 6.9194 - model_185_loss: 0.6918 - model_185_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4569 - model_184_loss: 0.4646 - model_185_loss: 0.6924 - model_185_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4567 - model_184_loss: 0.4640 - model_185_loss: 0.6925 - model_185_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4575 - model_184_loss: 0.4642 - model_185_loss: 0.6926 - model_185_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4556 - model_184_loss: 0.4644 - model_185_loss: 0.6924 - model_185_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4553 - model_184_loss: 0.4637 - model_185_loss: 0.6923 - model_185_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 118us/sample - loss: 6.9212 - model_185_loss: 0.6925 - model_185_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4582 - model_184_loss: 0.4622 - model_185_loss: 0.6926 - model_185_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4552 - model_184_loss: 0.4644 - model_185_loss: 0.6924 - model_185_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4502 - model_184_loss: 0.4657 - model_185_loss: 0.6920 - model_185_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4564 - model_184_loss: 0.4637 - model_185_loss: 0.6925 - model_185_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4556 - model_184_loss: 0.4641 - model_185_loss: 0.6922 - model_185_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 115us/sample - loss: 6.9212 - model_185_loss: 0.6926 - model_185_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4543 - model_184_loss: 0.4660 - model_185_loss: 0.6924 - model_185_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4559 - model_184_loss: 0.4642 - model_185_loss: 0.6924 - model_185_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4542 - model_184_loss: 0.4648 - model_185_loss: 0.6923 - model_185_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4574 - model_184_loss: 0.4635 - model_185_loss: 0.6925 - model_185_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4577 - model_184_loss: 0.4636 - model_185_loss: 0.6924 - model_185_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 129us/sample - loss: 6.9207 - model_185_loss: 0.6932 - model_185_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4549 - model_184_loss: 0.4665 - model_185_loss: 0.6927 - model_185_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4553 - model_184_loss: 0.4648 - model_185_loss: 0.6926 - model_185_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4546 - model_184_loss: 0.4650 - model_185_loss: 0.6924 - model_185_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4557 - model_184_loss: 0.4648 - model_185_loss: 0.6925 - model_185_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4564 - model_184_loss: 0.4648 - model_185_loss: 0.6929 - model_185_1_loss: 0.6913\n",
      "For Attention Module: 4.8\n",
      "features X: 30940 samples, 69 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 90us/sample - loss: 6.3705 - model_189_loss: 0.6603 - model_189_1_loss: 0.6131\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 56us/sample - loss: -5.9915 - model_188_loss: 0.3868 - model_189_loss: 0.6617 - model_189_1_loss: 0.6140\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -5.9961 - model_188_loss: 0.3831 - model_189_loss: 0.6601 - model_189_1_loss: 0.6158\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.0125 - model_188_loss: 0.3840 - model_189_loss: 0.6616 - model_189_1_loss: 0.6177\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0219 - model_188_loss: 0.3855 - model_189_loss: 0.6620 - model_189_1_loss: 0.6194\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0276 - model_188_loss: 0.3848 - model_189_loss: 0.6607 - model_189_1_loss: 0.6218\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 99us/sample - loss: 6.4185 - model_189_loss: 0.6623 - model_189_1_loss: 0.6219\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0391 - model_188_loss: 0.3848 - model_189_loss: 0.6619 - model_189_1_loss: 0.6228\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.0510 - model_188_loss: 0.3843 - model_189_loss: 0.6624 - model_189_1_loss: 0.6246\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0573 - model_188_loss: 0.3859 - model_189_loss: 0.6629 - model_189_1_loss: 0.6257\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.0752 - model_188_loss: 0.3858 - model_189_loss: 0.6629 - model_189_1_loss: 0.6293\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.0851 - model_188_loss: 0.3868 - model_189_loss: 0.6647 - model_189_1_loss: 0.6297\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 93us/sample - loss: 6.4803 - model_189_loss: 0.6648 - model_189_1_loss: 0.63171s - loss: 6.4355 - model_189_loss: 0.659\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0951 - model_188_loss: 0.3859 - model_189_loss: 0.6647 - model_189_1_loss: 0.6316\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.1077 - model_188_loss: 0.3889 - model_189_loss: 0.6654 - model_189_1_loss: 0.6339\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1196 - model_188_loss: 0.3902 - model_189_loss: 0.6654 - model_189_1_loss: 0.6366\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1260 - model_188_loss: 0.3909 - model_189_loss: 0.6656 - model_189_1_loss: 0.6378\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.1454 - model_188_loss: 0.3923 - model_189_loss: 0.6657 - model_189_1_loss: 0.6419\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.5532 - model_189_loss: 0.6684 - model_189_1_loss: 0.6428\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.1423 - model_188_loss: 0.3935 - model_189_loss: 0.6660 - model_189_1_loss: 0.6411\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.1687 - model_188_loss: 0.3951 - model_189_loss: 0.6685 - model_189_1_loss: 0.6443\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.1787 - model_188_loss: 0.3990 - model_189_loss: 0.6688 - model_189_1_loss: 0.6467\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.1982 - model_188_loss: 0.3974 - model_189_loss: 0.6700 - model_189_1_loss: 0.6491\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2163 - model_188_loss: 0.4012 - model_189_loss: 0.6709 - model_189_1_loss: 0.6526\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 120us/sample - loss: 6.6208 - model_189_loss: 0.6711 - model_189_1_loss: 0.6530s - loss: 6.6287 - model_189_loss: 0.6736 \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.2175 - model_188_loss: 0.4038 - model_189_loss: 0.6716 - model_189_1_loss: 0.6526\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2411 - model_188_loss: 0.4033 - model_189_loss: 0.6731 - model_189_1_loss: 0.6558\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2587 - model_188_loss: 0.4076 - model_189_loss: 0.6741 - model_189_1_loss: 0.6592\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.2632 - model_188_loss: 0.4081 - model_189_loss: 0.6741 - model_189_1_loss: 0.6601\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.2709 - model_188_loss: 0.4111 - model_189_loss: 0.6750 - model_189_1_loss: 0.6614\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 91us/sample - loss: 6.6830 - model_189_loss: 0.6739 - model_189_1_loss: 0.6624\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.2738 - model_188_loss: 0.4167 - model_189_loss: 0.6762 - model_189_1_loss: 0.6619\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2810 - model_188_loss: 0.4146 - model_189_loss: 0.6755 - model_189_1_loss: 0.6636\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.2844 - model_188_loss: 0.4195 - model_189_loss: 0.6767 - model_189_1_loss: 0.6641\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3025 - model_188_loss: 0.4197 - model_189_loss: 0.6776 - model_189_1_loss: 0.6668\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3025 - model_188_loss: 0.4239 - model_189_loss: 0.6772 - model_189_1_loss: 0.6681\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 94us/sample - loss: 6.7356 - model_189_loss: 0.6796 - model_189_1_loss: 0.6688\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3074 - model_188_loss: 0.4254 - model_189_loss: 0.6785 - model_189_1_loss: 0.6681\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3163 - model_188_loss: 0.4258 - model_189_loss: 0.6786 - model_189_1_loss: 0.6698\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3209 - model_188_loss: 0.4300 - model_189_loss: 0.6795 - model_189_1_loss: 0.6707\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3239 - model_188_loss: 0.4342 - model_189_loss: 0.6803 - model_189_1_loss: 0.6713\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3301 - model_188_loss: 0.4352 - model_189_loss: 0.6807 - model_189_1_loss: 0.6724\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 122us/sample - loss: 6.7763 - model_189_loss: 0.6814 - model_189_1_loss: 0.6736\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3368 - model_188_loss: 0.4370 - model_189_loss: 0.6813 - model_189_1_loss: 0.6735\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3382 - model_188_loss: 0.4377 - model_189_loss: 0.6819 - model_189_1_loss: 0.6733\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3446 - model_188_loss: 0.4416 - model_189_loss: 0.6825 - model_189_1_loss: 0.6748\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3593 - model_188_loss: 0.4412 - model_189_loss: 0.6837 - model_189_1_loss: 0.6763\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3634 - model_188_loss: 0.4453 - model_189_loss: 0.6844 - model_189_1_loss: 0.6773\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 4s 166us/sample - loss: 6.8138 - model_189_loss: 0.6841 - model_189_1_loss: 0.6785\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3638 - model_188_loss: 0.4487 - model_189_loss: 0.6845 - model_189_1_loss: 0.6780\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3646 - model_188_loss: 0.4505 - model_189_loss: 0.6843 - model_189_1_loss: 0.6787\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3753 - model_188_loss: 0.4522 - model_189_loss: 0.6858 - model_189_1_loss: 0.6797\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3853 - model_188_loss: 0.4528 - model_189_loss: 0.6869 - model_189_1_loss: 0.6807\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.3789 - model_188_loss: 0.4562 - model_189_loss: 0.6859 - model_189_1_loss: 0.6811\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.8502 - model_189_loss: 0.6880 - model_189_1_loss: 0.6824s - loss: 6.9899 - model_189_loss: 0 - ETA: 0s - loss: 6.8594 - model_189_loss: 0.6898 - model_189_1_lo\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3847 - model_188_loss: 0.4579 - model_189_loss: 0.6870 - model_189_1_loss: 0.6815\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3897 - model_188_loss: 0.4613 - model_189_loss: 0.6879 - model_189_1_loss: 0.6823\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.3978 - model_188_loss: 0.4604 - model_189_loss: 0.6881 - model_189_1_loss: 0.6835\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4047 - model_188_loss: 0.4616 - model_189_loss: 0.6885 - model_189_1_loss: 0.6847\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4051 - model_188_loss: 0.4634 - model_189_loss: 0.6882 - model_189_1_loss: 0.6855\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 99us/sample - loss: 6.8758 - model_189_loss: 0.6895 - model_189_1_loss: 0.6857\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4125 - model_188_loss: 0.4668 - model_189_loss: 0.6896 - model_189_1_loss: 0.6862\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4115 - model_188_loss: 0.4692 - model_189_loss: 0.6895 - model_189_1_loss: 0.6867\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4165 - model_188_loss: 0.4707 - model_189_loss: 0.6904 - model_189_1_loss: 0.6871\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4172 - model_188_loss: 0.4720 - model_189_loss: 0.6900 - model_189_1_loss: 0.6878\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4194 - model_188_loss: 0.4723 - model_189_loss: 0.6899 - model_189_1_loss: 0.6885\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.9001 - model_189_loss: 0.6904 - model_189_1_loss: 0.6891s - loss: 6.9713 - model_189_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4209 - model_188_loss: 0.4739 - model_189_loss: 0.6908 - model_189_1_loss: 0.6881\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4180 - model_188_loss: 0.4779 - model_189_loss: 0.6907 - model_189_1_loss: 0.6885\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4260 - model_188_loss: 0.4767 - model_189_loss: 0.6911 - model_189_1_loss: 0.6894\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4274 - model_188_loss: 0.4800 - model_189_loss: 0.6916 - model_189_1_loss: 0.6899\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4229 - model_188_loss: 0.4817 - model_189_loss: 0.6909 - model_189_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.9105 - model_189_loss: 0.6910 - model_189_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4249 - model_188_loss: 0.4839 - model_189_loss: 0.6916 - model_189_1_loss: 0.6902\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4300 - model_188_loss: 0.4823 - model_189_loss: 0.6918 - model_189_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4273 - model_188_loss: 0.4860 - model_189_loss: 0.6917 - model_189_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4246 - model_188_loss: 0.4887 - model_189_loss: 0.6917 - model_189_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4307 - model_188_loss: 0.4875 - model_189_loss: 0.6922 - model_189_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 100us/sample - loss: 6.9236 - model_189_loss: 0.6924 - model_189_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4282 - model_188_loss: 0.4891 - model_189_loss: 0.6919 - model_189_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4342 - model_188_loss: 0.4879 - model_189_loss: 0.6925 - model_189_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4322 - model_188_loss: 0.4898 - model_189_loss: 0.6925 - model_189_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4344 - model_188_loss: 0.4900 - model_189_loss: 0.6926 - model_189_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4348 - model_188_loss: 0.4916 - model_189_loss: 0.6927 - model_189_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 109us/sample - loss: 6.9306 - model_189_loss: 0.6934 - model_189_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4371 - model_188_loss: 0.4922 - model_189_loss: 0.6932 - model_189_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4373 - model_188_loss: 0.4924 - model_189_loss: 0.6932 - model_189_1_loss: 0.6928\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4342 - model_188_loss: 0.4939 - model_189_loss: 0.6930 - model_189_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4340 - model_188_loss: 0.4935 - model_189_loss: 0.6927 - model_189_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4372 - model_188_loss: 0.4929 - model_189_loss: 0.6931 - model_189_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9286 - model_189_loss: 0.6929 - model_189_1_loss: 0.692 - 2s 92us/sample - loss: 6.9285 - model_189_loss: 0.6937 - model_189_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4324 - model_188_loss: 0.4949 - model_189_loss: 0.6929 - model_189_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4329 - model_188_loss: 0.4929 - model_189_loss: 0.6925 - model_189_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4338 - model_188_loss: 0.4921 - model_189_loss: 0.6926 - model_189_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4317 - model_188_loss: 0.4945 - model_189_loss: 0.6925 - model_189_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4317 - model_188_loss: 0.4936 - model_189_loss: 0.6926 - model_189_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 95us/sample - loss: 6.9258 - model_189_loss: 0.6919 - model_189_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4327 - model_188_loss: 0.4909 - model_189_loss: 0.6921 - model_189_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4324 - model_188_loss: 0.4911 - model_189_loss: 0.6925 - model_189_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4354 - model_188_loss: 0.4898 - model_189_loss: 0.6924 - model_189_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4323 - model_188_loss: 0.4902 - model_189_loss: 0.6920 - model_189_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4381 - model_188_loss: 0.4877 - model_189_loss: 0.6927 - model_189_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 93us/sample - loss: 6.9214 - model_189_loss: 0.6914 - model_189_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4374 - model_188_loss: 0.4876 - model_189_loss: 0.6927 - model_189_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4375 - model_188_loss: 0.4880 - model_189_loss: 0.6928 - model_189_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4385 - model_188_loss: 0.4873 - model_189_loss: 0.6928 - model_189_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4396 - model_188_loss: 0.4861 - model_189_loss: 0.6927 - model_189_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4396 - model_188_loss: 0.4860 - model_189_loss: 0.6928 - model_189_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 115us/sample - loss: 6.9256 - model_189_loss: 0.6934 - model_189_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4351 - model_188_loss: 0.4848 - model_189_loss: 0.6919 - model_189_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4356 - model_188_loss: 0.4848 - model_189_loss: 0.6919 - model_189_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4355 - model_188_loss: 0.4852 - model_189_loss: 0.6920 - model_189_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4381 - model_188_loss: 0.4824 - model_189_loss: 0.6920 - model_189_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4374 - model_188_loss: 0.4828 - model_189_loss: 0.6922 - model_189_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.9206 - model_189_loss: 0.6924 - model_189_1_loss: 0.6921s - loss: 6.9621 - model_189_loss: 0.70\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4357 - model_188_loss: 0.4818 - model_189_loss: 0.6914 - model_189_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4383 - model_188_loss: 0.4802 - model_189_loss: 0.6919 - model_189_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4391 - model_188_loss: 0.4803 - model_189_loss: 0.6919 - model_189_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4431 - model_188_loss: 0.4783 - model_189_loss: 0.6924 - model_189_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4391 - model_188_loss: 0.4803 - model_189_loss: 0.6920 - model_189_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9210 - model_189_loss: 0.6924 - model_189_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4377 - model_188_loss: 0.4784 - model_189_loss: 0.6916 - model_189_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4397 - model_188_loss: 0.4795 - model_189_loss: 0.6919 - model_189_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4375 - model_188_loss: 0.4784 - model_189_loss: 0.6919 - model_189_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4427 - model_188_loss: 0.4765 - model_189_loss: 0.6925 - model_189_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4412 - model_188_loss: 0.4771 - model_189_loss: 0.6922 - model_189_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 115us/sample - loss: 6.9189 - model_189_loss: 0.6915 - model_189_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4391 - model_188_loss: 0.4765 - model_189_loss: 0.6919 - model_189_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4402 - model_188_loss: 0.4773 - model_189_loss: 0.6922 - model_189_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4375 - model_188_loss: 0.4777 - model_189_loss: 0.6920 - model_189_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4405 - model_188_loss: 0.4773 - model_189_loss: 0.6925 - model_189_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4423 - model_188_loss: 0.4754 - model_189_loss: 0.6924 - model_189_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 81us/sample - loss: 6.9160 - model_189_loss: 0.6916 - model_189_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4381 - model_188_loss: 0.4752 - model_189_loss: 0.6917 - model_189_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4390 - model_188_loss: 0.4766 - model_189_loss: 0.6921 - model_189_1_loss: 0.6910\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4406 - model_188_loss: 0.4753 - model_189_loss: 0.6922 - model_189_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4423 - model_188_loss: 0.4737 - model_189_loss: 0.6922 - model_189_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4411 - model_188_loss: 0.4752 - model_189_loss: 0.6923 - model_189_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 90us/sample - loss: 6.9161 - model_189_loss: 0.6929 - model_189_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4410 - model_188_loss: 0.4766 - model_189_loss: 0.6920 - model_189_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4434 - model_188_loss: 0.4740 - model_189_loss: 0.6921 - model_189_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4410 - model_188_loss: 0.4753 - model_189_loss: 0.6919 - model_189_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4435 - model_188_loss: 0.4752 - model_189_loss: 0.6922 - model_189_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4457 - model_188_loss: 0.4751 - model_189_loss: 0.6923 - model_189_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 93us/sample - loss: 6.9183 - model_189_loss: 0.6923 - model_189_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4401 - model_188_loss: 0.4751 - model_189_loss: 0.6922 - model_189_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4436 - model_188_loss: 0.4735 - model_189_loss: 0.6921 - model_189_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4429 - model_188_loss: 0.4747 - model_189_loss: 0.6922 - model_189_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4417 - model_188_loss: 0.4736 - model_189_loss: 0.6922 - model_189_1_loss: 0.6908\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4418 - model_188_loss: 0.4744 - model_189_loss: 0.6920 - model_189_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 94us/sample - loss: 6.9209 - model_189_loss: 0.6936 - model_189_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4426 - model_188_loss: 0.4732 - model_189_loss: 0.6919 - model_189_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4442 - model_188_loss: 0.4735 - model_189_loss: 0.6921 - model_189_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4414 - model_188_loss: 0.4741 - model_189_loss: 0.6920 - model_189_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4427 - model_188_loss: 0.4745 - model_189_loss: 0.6919 - model_189_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4441 - model_188_loss: 0.4747 - model_189_loss: 0.6923 - model_189_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 94us/sample - loss: 6.9193 - model_189_loss: 0.6913 - model_189_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4415 - model_188_loss: 0.4734 - model_189_loss: 0.6917 - model_189_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4448 - model_188_loss: 0.4733 - model_189_loss: 0.6924 - model_189_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4449 - model_188_loss: 0.4737 - model_189_loss: 0.6925 - model_189_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4470 - model_188_loss: 0.4732 - model_189_loss: 0.6927 - model_189_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4491 - model_188_loss: 0.4720 - model_189_loss: 0.6928 - model_189_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 88us/sample - loss: 6.9204 - model_189_loss: 0.6920 - model_189_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4452 - model_188_loss: 0.4724 - model_189_loss: 0.6920 - model_189_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4438 - model_188_loss: 0.4720 - model_189_loss: 0.6918 - model_189_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4448 - model_188_loss: 0.4723 - model_189_loss: 0.6921 - model_189_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4433 - model_188_loss: 0.4743 - model_189_loss: 0.6921 - model_189_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4452 - model_188_loss: 0.4724 - model_189_loss: 0.6922 - model_189_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 96us/sample - loss: 6.9201 - model_189_loss: 0.6923 - model_189_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4434 - model_188_loss: 0.4732 - model_189_loss: 0.6920 - model_189_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4467 - model_188_loss: 0.4727 - model_189_loss: 0.6922 - model_189_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4447 - model_188_loss: 0.4733 - model_189_loss: 0.6923 - model_189_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4482 - model_188_loss: 0.4711 - model_189_loss: 0.6923 - model_189_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4462 - model_188_loss: 0.4722 - model_189_loss: 0.6923 - model_189_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 85us/sample - loss: 6.9210 - model_189_loss: 0.6932 - model_189_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4484 - model_188_loss: 0.4729 - model_189_loss: 0.6924 - model_189_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4482 - model_188_loss: 0.4722 - model_189_loss: 0.6927 - model_189_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4489 - model_188_loss: 0.4712 - model_189_loss: 0.6924 - model_189_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4482 - model_188_loss: 0.4728 - model_189_loss: 0.6926 - model_189_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4491 - model_188_loss: 0.4731 - model_189_loss: 0.6923 - model_189_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 95us/sample - loss: 6.9222 - model_189_loss: 0.6921 - model_189_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4456 - model_188_loss: 0.4749 - model_189_loss: 0.6925 - model_189_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4471 - model_188_loss: 0.4737 - model_189_loss: 0.6924 - model_189_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4495 - model_188_loss: 0.4743 - model_189_loss: 0.6926 - model_189_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4493 - model_188_loss: 0.4734 - model_189_loss: 0.6926 - model_189_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4478 - model_188_loss: 0.4748 - model_189_loss: 0.6925 - model_189_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 95us/sample - loss: 6.9238 - model_189_loss: 0.6926 - model_189_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4490 - model_188_loss: 0.4739 - model_189_loss: 0.6924 - model_189_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4496 - model_188_loss: 0.4755 - model_189_loss: 0.6928 - model_189_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 26us/sample - loss: -6.4503 - model_188_loss: 0.4743 - model_189_loss: 0.6927 - model_189_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4506 - model_188_loss: 0.4732 - model_189_loss: 0.6928 - model_189_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4501 - model_188_loss: 0.4739 - model_189_loss: 0.6926 - model_189_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 91us/sample - loss: 6.9237 - model_189_loss: 0.6921 - model_189_1_loss: 0.69211s - loss: 6.8517 - model_189_loss: 0.679\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4474 - model_188_loss: 0.4753 - model_189_loss: 0.6925 - model_189_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4485 - model_188_loss: 0.4749 - model_189_loss: 0.6926 - model_189_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4493 - model_188_loss: 0.4752 - model_189_loss: 0.6927 - model_189_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4484 - model_188_loss: 0.4745 - model_189_loss: 0.6925 - model_189_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4509 - model_188_loss: 0.4723 - model_189_loss: 0.6928 - model_189_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 91us/sample - loss: 6.9227 - model_189_loss: 0.6930 - model_189_1_loss: 0.69200s - loss: 6.9308 - model_189_loss: 0.6940 - model_189\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4494 - model_188_loss: 0.4723 - model_189_loss: 0.6924 - model_189_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4486 - model_188_loss: 0.4735 - model_189_loss: 0.6925 - model_189_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4488 - model_188_loss: 0.4724 - model_189_loss: 0.6924 - model_189_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4487 - model_188_loss: 0.4720 - model_189_loss: 0.6926 - model_189_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4491 - model_188_loss: 0.4737 - model_189_loss: 0.6928 - model_189_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 93us/sample - loss: 6.9219 - model_189_loss: 0.6929 - model_189_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4510 - model_188_loss: 0.4707 - model_189_loss: 0.6923 - model_189_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4504 - model_188_loss: 0.4702 - model_189_loss: 0.6924 - model_189_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4486 - model_188_loss: 0.4715 - model_189_loss: 0.6922 - model_189_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4483 - model_188_loss: 0.4700 - model_189_loss: 0.6921 - model_189_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4490 - model_188_loss: 0.4709 - model_189_loss: 0.6925 - model_189_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 89us/sample - loss: 6.9212 - model_189_loss: 0.6926 - model_189_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4476 - model_188_loss: 0.4698 - model_189_loss: 0.6921 - model_189_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4503 - model_188_loss: 0.4684 - model_189_loss: 0.6923 - model_189_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4512 - model_188_loss: 0.4698 - model_189_loss: 0.6927 - model_189_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4494 - model_188_loss: 0.4694 - model_189_loss: 0.6922 - model_189_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4490 - model_188_loss: 0.4691 - model_189_loss: 0.6922 - model_189_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 87us/sample - loss: 6.9221 - model_189_loss: 0.6926 - model_189_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4505 - model_188_loss: 0.4681 - model_189_loss: 0.6922 - model_189_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4503 - model_188_loss: 0.4691 - model_189_loss: 0.6923 - model_189_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4507 - model_188_loss: 0.4677 - model_189_loss: 0.6923 - model_189_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4516 - model_188_loss: 0.4685 - model_189_loss: 0.6923 - model_189_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4532 - model_188_loss: 0.4674 - model_189_loss: 0.6924 - model_189_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9217 - model_189_loss: 0.6926 - model_189_1_loss: 0.691 - 2s 96us/sample - loss: 6.9202 - model_189_loss: 0.6924 - model_189_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4531 - model_188_loss: 0.4694 - model_189_loss: 0.6929 - model_189_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4525 - model_188_loss: 0.4681 - model_189_loss: 0.6925 - model_189_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4521 - model_188_loss: 0.4677 - model_189_loss: 0.6924 - model_189_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4522 - model_188_loss: 0.4692 - model_189_loss: 0.6927 - model_189_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4542 - model_188_loss: 0.4680 - model_189_loss: 0.6926 - model_189_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 91us/sample - loss: 6.9214 - model_189_loss: 0.6919 - model_189_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4509 - model_188_loss: 0.4701 - model_189_loss: 0.6927 - model_189_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4506 - model_188_loss: 0.4698 - model_189_loss: 0.6924 - model_189_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4523 - model_188_loss: 0.4680 - model_189_loss: 0.6925 - model_189_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4507 - model_188_loss: 0.4698 - model_189_loss: 0.6922 - model_189_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4494 - model_188_loss: 0.4692 - model_189_loss: 0.6922 - model_189_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 102us/sample - loss: 6.9234 - model_189_loss: 0.6932 - model_189_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4538 - model_188_loss: 0.4704 - model_189_loss: 0.6929 - model_189_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4520 - model_188_loss: 0.4697 - model_189_loss: 0.6926 - model_189_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4532 - model_188_loss: 0.4698 - model_189_loss: 0.6927 - model_189_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4532 - model_188_loss: 0.4690 - model_189_loss: 0.6927 - model_189_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4549 - model_188_loss: 0.4690 - model_189_loss: 0.6930 - model_189_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 100us/sample - loss: 6.9213 - model_189_loss: 0.6923 - model_189_1_loss: 0.6915s - loss: 6.9534 - model_189_loss: 0.7014 - model_189_1 - ETA: 1s - loss: 6.8928 - model_189_loss: 0.6876\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4503 - model_188_loss: 0.4694 - model_189_loss: 0.6923 - model_189_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4490 - model_188_loss: 0.4691 - model_189_loss: 0.6924 - model_189_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4490 - model_188_loss: 0.4688 - model_189_loss: 0.6923 - model_189_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4518 - model_188_loss: 0.4692 - model_189_loss: 0.6930 - model_189_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4520 - model_188_loss: 0.4699 - model_189_loss: 0.6927 - model_189_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 100us/sample - loss: 6.9225 - model_189_loss: 0.6931 - model_189_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4536 - model_188_loss: 0.4677 - model_189_loss: 0.6926 - model_189_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4508 - model_188_loss: 0.4679 - model_189_loss: 0.6922 - model_189_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4543 - model_188_loss: 0.4675 - model_189_loss: 0.6926 - model_189_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4551 - model_188_loss: 0.4677 - model_189_loss: 0.6926 - model_189_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4524 - model_188_loss: 0.4675 - model_189_loss: 0.6924 - model_189_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 95us/sample - loss: 6.9218 - model_189_loss: 0.6935 - model_189_1_loss: 0.69141s - loss: 6.9030 - model_189_loss: 0.6869 - mod\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4550 - model_188_loss: 0.4684 - model_189_loss: 0.6925 - model_189_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4532 - model_188_loss: 0.4691 - model_189_loss: 0.6926 - model_189_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4539 - model_188_loss: 0.4688 - model_189_loss: 0.6926 - model_189_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4512 - model_188_loss: 0.4692 - model_189_loss: 0.6923 - model_189_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4545 - model_188_loss: 0.4679 - model_189_loss: 0.6928 - model_189_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 92us/sample - loss: 6.9257 - model_189_loss: 0.6925 - model_189_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4532 - model_188_loss: 0.4673 - model_189_loss: 0.6925 - model_189_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4520 - model_188_loss: 0.4678 - model_189_loss: 0.6923 - model_189_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4523 - model_188_loss: 0.4683 - model_189_loss: 0.6925 - model_189_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4568 - model_188_loss: 0.4671 - model_189_loss: 0.6930 - model_189_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4526 - model_188_loss: 0.4682 - model_189_loss: 0.6925 - model_189_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 97us/sample - loss: 6.9239 - model_189_loss: 0.6931 - model_189_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4538 - model_188_loss: 0.4681 - model_189_loss: 0.6926 - model_189_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4520 - model_188_loss: 0.4687 - model_189_loss: 0.6926 - model_189_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4532 - model_188_loss: 0.4667 - model_189_loss: 0.6923 - model_189_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4548 - model_188_loss: 0.4663 - model_189_loss: 0.6926 - model_189_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 27us/sample - loss: -6.4531 - model_188_loss: 0.4678 - model_189_loss: 0.6925 - model_189_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.9205 - model_189_loss: 0.6926 - model_189_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4529 - model_188_loss: 0.4673 - model_189_loss: 0.6923 - model_189_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4550 - model_188_loss: 0.4658 - model_189_loss: 0.6925 - model_189_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4537 - model_188_loss: 0.4661 - model_189_loss: 0.6924 - model_189_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4529 - model_188_loss: 0.4671 - model_189_loss: 0.6924 - model_189_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 28us/sample - loss: -6.4530 - model_188_loss: 0.4657 - model_189_loss: 0.6923 - model_189_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 103us/sample - loss: 6.9193 - model_189_loss: 0.6919 - model_189_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4523 - model_188_loss: 0.4663 - model_189_loss: 0.6925 - model_189_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4514 - model_188_loss: 0.4652 - model_189_loss: 0.6921 - model_189_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4534 - model_188_loss: 0.4633 - model_189_loss: 0.6922 - model_189_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4526 - model_188_loss: 0.4633 - model_189_loss: 0.6922 - model_189_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4507 - model_188_loss: 0.4654 - model_189_loss: 0.6920 - model_189_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 115us/sample - loss: 6.9183 - model_189_loss: 0.6918 - model_189_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4489 - model_188_loss: 0.4638 - model_189_loss: 0.6915 - model_189_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4543 - model_188_loss: 0.4614 - model_189_loss: 0.6920 - model_189_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4510 - model_188_loss: 0.4644 - model_189_loss: 0.6919 - model_189_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4564 - model_188_loss: 0.4614 - model_189_loss: 0.6924 - model_189_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4541 - model_188_loss: 0.4627 - model_189_loss: 0.6922 - model_189_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 100us/sample - loss: 6.9178 - model_189_loss: 0.6925 - model_189_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4546 - model_188_loss: 0.4617 - model_189_loss: 0.6922 - model_189_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4534 - model_188_loss: 0.4626 - model_189_loss: 0.6922 - model_189_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4528 - model_188_loss: 0.4631 - model_189_loss: 0.6920 - model_189_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4538 - model_188_loss: 0.4635 - model_189_loss: 0.6922 - model_189_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4546 - model_188_loss: 0.4639 - model_189_loss: 0.6925 - model_189_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.9231 - model_189_loss: 0.6925 - model_189_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -6.4604 - model_188_loss: 0.4624 - model_189_loss: 0.6927 - model_189_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4554 - model_188_loss: 0.4653 - model_189_loss: 0.6924 - model_189_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4547 - model_188_loss: 0.4662 - model_189_loss: 0.6924 - model_189_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4573 - model_188_loss: 0.4649 - model_189_loss: 0.6925 - model_189_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4575 - model_188_loss: 0.4660 - model_189_loss: 0.6927 - model_189_1_loss: 0.6920\n",
      "For Attention Module: 4.9\n",
      "features X: 30940 samples, 63 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 98us/sample - loss: 6.8230 - model_193_loss: 0.6752 - model_193_1_loss: 0.6890\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 2s 65us/sample - loss: -6.3321 - model_192_loss: 0.4860 - model_193_loss: 0.6746 - model_193_1_loss: 0.6890\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3343 - model_192_loss: 0.4854 - model_193_loss: 0.6746 - model_193_1_loss: 0.6893\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3416 - model_192_loss: 0.4857 - model_193_loss: 0.6760 - model_193_1_loss: 0.6894\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3409 - model_192_loss: 0.4859 - model_193_loss: 0.6760 - model_193_1_loss: 0.6894\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3441 - model_192_loss: 0.4867 - model_193_loss: 0.6764 - model_193_1_loss: 0.6898\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 111us/sample - loss: 6.8319 - model_193_loss: 0.6769 - model_193_1_loss: 0.6894\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3475 - model_192_loss: 0.4858 - model_193_loss: 0.6770 - model_193_1_loss: 0.6896\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3446 - model_192_loss: 0.4857 - model_193_loss: 0.6765 - model_193_1_loss: 0.6896\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3462 - model_192_loss: 0.4863 - model_193_loss: 0.6770 - model_193_1_loss: 0.6895\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3538 - model_192_loss: 0.4869 - model_193_loss: 0.6782 - model_193_1_loss: 0.6899\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3586 - model_192_loss: 0.4865 - model_193_loss: 0.6794 - model_193_1_loss: 0.6896\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.8493 - model_193_loss: 0.6797 - model_193_1_loss: 0.6902\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3626 - model_192_loss: 0.4862 - model_193_loss: 0.6800 - model_193_1_loss: 0.6897\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3644 - model_192_loss: 0.4864 - model_193_loss: 0.6801 - model_193_1_loss: 0.6900\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3646 - model_192_loss: 0.4873 - model_193_loss: 0.6803 - model_193_1_loss: 0.6900\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3734 - model_192_loss: 0.4875 - model_193_loss: 0.6821 - model_193_1_loss: 0.6901\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3765 - model_192_loss: 0.4857 - model_193_loss: 0.6821 - model_193_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 99us/sample - loss: 6.8589 - model_193_loss: 0.6814 - model_193_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.3700 - model_192_loss: 0.4884 - model_193_loss: 0.6814 - model_193_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3830 - model_192_loss: 0.4873 - model_193_loss: 0.6837 - model_193_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3827 - model_192_loss: 0.4877 - model_193_loss: 0.6837 - model_193_1_loss: 0.6904\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3890 - model_192_loss: 0.4883 - model_193_loss: 0.6843 - model_193_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3882 - model_192_loss: 0.4877 - model_193_loss: 0.6845 - model_193_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.8792 - model_193_loss: 0.6859 - model_193_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3946 - model_192_loss: 0.4908 - model_193_loss: 0.6861 - model_193_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.3965 - model_192_loss: 0.4887 - model_193_loss: 0.6859 - model_193_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3999 - model_192_loss: 0.4905 - model_193_loss: 0.6869 - model_193_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4054 - model_192_loss: 0.4913 - model_193_loss: 0.6880 - model_193_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4043 - model_192_loss: 0.4918 - model_193_loss: 0.6879 - model_193_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 99us/sample - loss: 6.9013 - model_193_loss: 0.6893 - model_193_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4086 - model_192_loss: 0.4911 - model_193_loss: 0.6886 - model_193_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4123 - model_192_loss: 0.4923 - model_193_loss: 0.6897 - model_193_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4097 - model_192_loss: 0.4944 - model_193_loss: 0.6892 - model_193_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4171 - model_192_loss: 0.4950 - model_193_loss: 0.6908 - model_193_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4198 - model_192_loss: 0.4944 - model_193_loss: 0.6910 - model_193_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.9118 - model_193_loss: 0.6902 - model_193_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4169 - model_192_loss: 0.4959 - model_193_loss: 0.6908 - model_193_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4174 - model_192_loss: 0.4974 - model_193_loss: 0.6910 - model_193_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4215 - model_192_loss: 0.4977 - model_193_loss: 0.6919 - model_193_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4241 - model_192_loss: 0.4975 - model_193_loss: 0.6923 - model_193_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4269 - model_192_loss: 0.4981 - model_193_loss: 0.6927 - model_193_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.9265 - model_193_loss: 0.6935 - model_193_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4206 - model_192_loss: 0.5003 - model_193_loss: 0.6921 - model_193_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4219 - model_192_loss: 0.5005 - model_193_loss: 0.6921 - model_193_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4219 - model_192_loss: 0.5017 - model_193_loss: 0.6924 - model_193_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4252 - model_192_loss: 0.5002 - model_193_loss: 0.6927 - model_193_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4269 - model_192_loss: 0.5021 - model_193_loss: 0.6934 - model_193_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 102us/sample - loss: 6.9292 - model_193_loss: 0.6933 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4215 - model_192_loss: 0.5032 - model_193_loss: 0.6927 - model_193_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4235 - model_192_loss: 0.5034 - model_193_loss: 0.6930 - model_193_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4225 - model_192_loss: 0.5025 - model_193_loss: 0.6927 - model_193_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4227 - model_192_loss: 0.5047 - model_193_loss: 0.6929 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4243 - model_192_loss: 0.5040 - model_193_loss: 0.6932 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 96us/sample - loss: 6.9272 - model_193_loss: 0.6929 - model_193_1_loss: 0.69270s - loss: 6.9521 - model_193_loss: 0.6977 - model_193_1_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4199 - model_192_loss: 0.5036 - model_193_loss: 0.6923 - model_193_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4199 - model_192_loss: 0.5033 - model_193_loss: 0.6922 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4198 - model_192_loss: 0.5052 - model_193_loss: 0.6924 - model_193_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4183 - model_192_loss: 0.5051 - model_193_loss: 0.6921 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4205 - model_192_loss: 0.5043 - model_193_loss: 0.6923 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.9234 - model_193_loss: 0.6920 - model_193_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4141 - model_192_loss: 0.5052 - model_193_loss: 0.6914 - model_193_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4144 - model_192_loss: 0.5062 - model_193_loss: 0.6917 - model_193_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4158 - model_192_loss: 0.5062 - model_193_loss: 0.6919 - model_193_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4173 - model_192_loss: 0.5047 - model_193_loss: 0.6916 - model_193_1_loss: 0.6928\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4171 - model_192_loss: 0.5053 - model_193_loss: 0.6919 - model_193_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 98us/sample - loss: 6.9217 - model_193_loss: 0.6921 - model_193_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4156 - model_192_loss: 0.5031 - model_193_loss: 0.6911 - model_193_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4167 - model_192_loss: 0.5033 - model_193_loss: 0.6915 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4146 - model_192_loss: 0.5037 - model_193_loss: 0.6912 - model_193_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4163 - model_192_loss: 0.5035 - model_193_loss: 0.6914 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4178 - model_192_loss: 0.5029 - model_193_loss: 0.6916 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 104us/sample - loss: 6.9202 - model_193_loss: 0.6916 - model_193_1_loss: 0.6928s - loss: 7.0098 - model_19\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4178 - model_192_loss: 0.5031 - model_193_loss: 0.6917 - model_193_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4201 - model_192_loss: 0.5019 - model_193_loss: 0.6919 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4175 - model_192_loss: 0.5026 - model_193_loss: 0.6917 - model_193_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4165 - model_192_loss: 0.5004 - model_193_loss: 0.6911 - model_193_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4201 - model_192_loss: 0.5016 - model_193_loss: 0.6918 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 116us/sample - loss: 6.9221 - model_193_loss: 0.6925 - model_193_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4228 - model_192_loss: 0.5018 - model_193_loss: 0.6926 - model_193_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4243 - model_192_loss: 0.5009 - model_193_loss: 0.6926 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4276 - model_192_loss: 0.4989 - model_193_loss: 0.6930 - model_193_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4262 - model_192_loss: 0.5003 - model_193_loss: 0.6930 - model_193_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4304 - model_192_loss: 0.4992 - model_193_loss: 0.6935 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 95us/sample - loss: 6.9277 - model_193_loss: 0.6932 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4222 - model_192_loss: 0.5005 - model_193_loss: 0.6923 - model_193_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4241 - model_192_loss: 0.4990 - model_193_loss: 0.6923 - model_193_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4239 - model_192_loss: 0.5005 - model_193_loss: 0.6927 - model_193_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4240 - model_192_loss: 0.4984 - model_193_loss: 0.6922 - model_193_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4234 - model_192_loss: 0.4998 - model_193_loss: 0.6924 - model_193_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 104us/sample - loss: 6.9244 - model_193_loss: 0.6921 - model_193_1_loss: 0.6925s - loss: 6.9367 - model_193_loss: 0.6934 - m\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4248 - model_192_loss: 0.4997 - model_193_loss: 0.6926 - model_193_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4251 - model_192_loss: 0.4989 - model_193_loss: 0.6925 - model_193_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4243 - model_192_loss: 0.4990 - model_193_loss: 0.6924 - model_193_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4247 - model_192_loss: 0.4983 - model_193_loss: 0.6923 - model_193_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4240 - model_192_loss: 0.4990 - model_193_loss: 0.6923 - model_193_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 95us/sample - loss: 6.9230 - model_193_loss: 0.6918 - model_193_1_loss: 0.69221s - loss: 7.0013 - model_193_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4264 - model_192_loss: 0.4986 - model_193_loss: 0.6926 - model_193_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4236 - model_192_loss: 0.4987 - model_193_loss: 0.6923 - model_193_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4256 - model_192_loss: 0.4980 - model_193_loss: 0.6924 - model_193_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4209 - model_192_loss: 0.5003 - model_193_loss: 0.6921 - model_193_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4257 - model_192_loss: 0.4978 - model_193_loss: 0.6925 - model_193_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 94us/sample - loss: 6.9262 - model_193_loss: 0.6928 - model_193_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4239 - model_192_loss: 0.4988 - model_193_loss: 0.6924 - model_193_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4256 - model_192_loss: 0.4986 - model_193_loss: 0.6924 - model_193_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4256 - model_192_loss: 0.4979 - model_193_loss: 0.6924 - model_193_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4245 - model_192_loss: 0.4994 - model_193_loss: 0.6925 - model_193_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4264 - model_192_loss: 0.4986 - model_193_loss: 0.6926 - model_193_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 108us/sample - loss: 6.9238 - model_193_loss: 0.6927 - model_193_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4260 - model_192_loss: 0.4994 - model_193_loss: 0.6927 - model_193_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4256 - model_192_loss: 0.4973 - model_193_loss: 0.6922 - model_193_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4249 - model_192_loss: 0.4977 - model_193_loss: 0.6924 - model_193_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4237 - model_192_loss: 0.4986 - model_193_loss: 0.6922 - model_193_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4257 - model_192_loss: 0.4982 - model_193_loss: 0.6925 - model_193_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9223 - model_193_loss: 0.6918 - model_193_1_loss: 0.692 - 2s 100us/sample - loss: 6.9252 - model_193_loss: 0.6926 - model_193_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4260 - model_192_loss: 0.4983 - model_193_loss: 0.6925 - model_193_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4258 - model_192_loss: 0.4995 - model_193_loss: 0.6927 - model_193_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4255 - model_192_loss: 0.4991 - model_193_loss: 0.6927 - model_193_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4272 - model_192_loss: 0.4988 - model_193_loss: 0.6928 - model_193_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4290 - model_192_loss: 0.4976 - model_193_loss: 0.6928 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 103us/sample - loss: 6.9244 - model_193_loss: 0.6925 - model_193_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4250 - model_192_loss: 0.4985 - model_193_loss: 0.6924 - model_193_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4265 - model_192_loss: 0.4976 - model_193_loss: 0.6924 - model_193_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4256 - model_192_loss: 0.4980 - model_193_loss: 0.6925 - model_193_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4259 - model_192_loss: 0.4978 - model_193_loss: 0.6923 - model_193_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4262 - model_192_loss: 0.4977 - model_193_loss: 0.6925 - model_193_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 101us/sample - loss: 6.9268 - model_193_loss: 0.6926 - model_193_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4255 - model_192_loss: 0.4983 - model_193_loss: 0.6923 - model_193_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4268 - model_192_loss: 0.4989 - model_193_loss: 0.6927 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4285 - model_192_loss: 0.4973 - model_193_loss: 0.6928 - model_193_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4261 - model_192_loss: 0.4982 - model_193_loss: 0.6924 - model_193_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4243 - model_192_loss: 0.5002 - model_193_loss: 0.6926 - model_193_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 101us/sample - loss: 6.9254 - model_193_loss: 0.6932 - model_193_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4278 - model_192_loss: 0.4973 - model_193_loss: 0.6923 - model_193_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4271 - model_192_loss: 0.4985 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4291 - model_192_loss: 0.4973 - model_193_loss: 0.6926 - model_193_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4288 - model_192_loss: 0.4979 - model_193_loss: 0.6927 - model_193_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4282 - model_192_loss: 0.4975 - model_193_loss: 0.6924 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 111us/sample - loss: 6.9271 - model_193_loss: 0.6943 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4273 - model_192_loss: 0.4982 - model_193_loss: 0.6927 - model_193_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4262 - model_192_loss: 0.4976 - model_193_loss: 0.6923 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4260 - model_192_loss: 0.4986 - model_193_loss: 0.6925 - model_193_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4262 - model_192_loss: 0.4984 - model_193_loss: 0.6925 - model_193_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4264 - model_192_loss: 0.4988 - model_193_loss: 0.6925 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 104us/sample - loss: 6.9257 - model_193_loss: 0.6929 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4264 - model_192_loss: 0.4976 - model_193_loss: 0.6924 - model_193_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4266 - model_192_loss: 0.4981 - model_193_loss: 0.6925 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4263 - model_192_loss: 0.4987 - model_193_loss: 0.6926 - model_193_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4280 - model_192_loss: 0.4975 - model_193_loss: 0.6927 - model_193_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4275 - model_192_loss: 0.4969 - model_193_loss: 0.6924 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.9269 - model_193_loss: 0.6924 - model_193_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4285 - model_192_loss: 0.4969 - model_193_loss: 0.6926 - model_193_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4270 - model_192_loss: 0.4969 - model_193_loss: 0.6923 - model_193_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4279 - model_192_loss: 0.4974 - model_193_loss: 0.6925 - model_193_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4261 - model_192_loss: 0.4983 - model_193_loss: 0.6925 - model_193_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4265 - model_192_loss: 0.4975 - model_193_loss: 0.6925 - model_193_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.9255 - model_193_loss: 0.6924 - model_193_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4283 - model_192_loss: 0.4975 - model_193_loss: 0.6928 - model_193_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4286 - model_192_loss: 0.4969 - model_193_loss: 0.6927 - model_193_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4262 - model_192_loss: 0.4979 - model_193_loss: 0.6925 - model_193_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4291 - model_192_loss: 0.4962 - model_193_loss: 0.6926 - model_193_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4275 - model_192_loss: 0.4986 - model_193_loss: 0.6928 - model_193_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 99us/sample - loss: 6.9260 - model_193_loss: 0.6922 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4273 - model_192_loss: 0.4966 - model_193_loss: 0.6923 - model_193_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4262 - model_192_loss: 0.4959 - model_193_loss: 0.6921 - model_193_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4270 - model_192_loss: 0.4970 - model_193_loss: 0.6923 - model_193_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4293 - model_192_loss: 0.4963 - model_193_loss: 0.6926 - model_193_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4289 - model_192_loss: 0.4969 - model_193_loss: 0.6927 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 98us/sample - loss: 6.9258 - model_193_loss: 0.6932 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4271 - model_192_loss: 0.4964 - model_193_loss: 0.6923 - model_193_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4280 - model_192_loss: 0.4957 - model_193_loss: 0.6923 - model_193_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4286 - model_192_loss: 0.4956 - model_193_loss: 0.6925 - model_193_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4277 - model_192_loss: 0.4956 - model_193_loss: 0.6922 - model_193_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4281 - model_192_loss: 0.4969 - model_193_loss: 0.6926 - model_193_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 113us/sample - loss: 6.9264 - model_193_loss: 0.6930 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4281 - model_192_loss: 0.4967 - model_193_loss: 0.6926 - model_193_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4300 - model_192_loss: 0.4960 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4279 - model_192_loss: 0.4962 - model_193_loss: 0.6925 - model_193_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4284 - model_192_loss: 0.4966 - model_193_loss: 0.6925 - model_193_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4309 - model_192_loss: 0.4957 - model_193_loss: 0.6928 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 104us/sample - loss: 6.9272 - model_193_loss: 0.6928 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4281 - model_192_loss: 0.4967 - model_193_loss: 0.6926 - model_193_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4258 - model_192_loss: 0.4993 - model_193_loss: 0.6925 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4293 - model_192_loss: 0.4983 - model_193_loss: 0.6930 - model_193_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4299 - model_192_loss: 0.4968 - model_193_loss: 0.6928 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4292 - model_192_loss: 0.4966 - model_193_loss: 0.6927 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 96us/sample - loss: 6.9264 - model_193_loss: 0.6924 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4295 - model_192_loss: 0.4958 - model_193_loss: 0.6925 - model_193_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4282 - model_192_loss: 0.4978 - model_193_loss: 0.6927 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4300 - model_192_loss: 0.4965 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4301 - model_192_loss: 0.4958 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4300 - model_192_loss: 0.4977 - model_193_loss: 0.6928 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 101us/sample - loss: 6.9271 - model_193_loss: 0.6929 - model_193_1_loss: 0.6928s - loss: 6.9252 - model_193_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4301 - model_192_loss: 0.4960 - model_193_loss: 0.6925 - model_193_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4287 - model_192_loss: 0.4978 - model_193_loss: 0.6926 - model_193_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4292 - model_192_loss: 0.4968 - model_193_loss: 0.6927 - model_193_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4290 - model_192_loss: 0.4966 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4292 - model_192_loss: 0.4970 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 98us/sample - loss: 6.9268 - model_193_loss: 0.6924 - model_193_1_loss: 0.69281s - loss: 7.0102 - model_193_loss: 0.7\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4281 - model_192_loss: 0.4969 - model_193_loss: 0.6925 - model_193_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4269 - model_192_loss: 0.4986 - model_193_loss: 0.6924 - model_193_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4304 - model_192_loss: 0.4956 - model_193_loss: 0.6925 - model_193_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4294 - model_192_loss: 0.4961 - model_193_loss: 0.6925 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4287 - model_192_loss: 0.4969 - model_193_loss: 0.6924 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 94us/sample - loss: 6.9270 - model_193_loss: 0.6924 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4274 - model_192_loss: 0.4973 - model_193_loss: 0.6925 - model_193_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4274 - model_192_loss: 0.4975 - model_193_loss: 0.6924 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4284 - model_192_loss: 0.4968 - model_193_loss: 0.6924 - model_193_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4288 - model_192_loss: 0.4962 - model_193_loss: 0.6923 - model_193_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4310 - model_192_loss: 0.4957 - model_193_loss: 0.6928 - model_193_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 94us/sample - loss: 6.9254 - model_193_loss: 0.6922 - model_193_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4280 - model_192_loss: 0.4962 - model_193_loss: 0.6922 - model_193_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 29us/sample - loss: -6.4282 - model_192_loss: 0.4970 - model_193_loss: 0.6923 - model_193_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4304 - model_192_loss: 0.4963 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4277 - model_192_loss: 0.4968 - model_193_loss: 0.6924 - model_193_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4286 - model_192_loss: 0.4969 - model_193_loss: 0.6925 - model_193_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.9273 - model_193_loss: 0.6932 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4293 - model_192_loss: 0.4972 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4295 - model_192_loss: 0.4963 - model_193_loss: 0.6925 - model_193_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4299 - model_192_loss: 0.4970 - model_193_loss: 0.6927 - model_193_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4313 - model_192_loss: 0.4952 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4312 - model_192_loss: 0.4959 - model_193_loss: 0.6928 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 101us/sample - loss: 6.9273 - model_193_loss: 0.6932 - model_193_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4302 - model_192_loss: 0.4965 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4302 - model_192_loss: 0.4965 - model_193_loss: 0.6926 - model_193_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4292 - model_192_loss: 0.4964 - model_193_loss: 0.6925 - model_193_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4298 - model_192_loss: 0.4960 - model_193_loss: 0.6925 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4295 - model_192_loss: 0.4973 - model_193_loss: 0.6927 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 102us/sample - loss: 6.9272 - model_193_loss: 0.6925 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4298 - model_192_loss: 0.4959 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4296 - model_192_loss: 0.4972 - model_193_loss: 0.6928 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4314 - model_192_loss: 0.4956 - model_193_loss: 0.6928 - model_193_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4281 - model_192_loss: 0.4978 - model_193_loss: 0.6927 - model_193_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4300 - model_192_loss: 0.4971 - model_193_loss: 0.6927 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.9267 - model_193_loss: 0.6923 - model_193_1_loss: 0.6925s - loss: 6.9511 - model_193_loss: 0.6958\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4293 - model_192_loss: 0.4956 - model_193_loss: 0.6924 - model_193_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4299 - model_192_loss: 0.4966 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4298 - model_192_loss: 0.4956 - model_193_loss: 0.6925 - model_193_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4297 - model_192_loss: 0.4957 - model_193_loss: 0.6925 - model_193_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4291 - model_192_loss: 0.4964 - model_193_loss: 0.6925 - model_193_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 100us/sample - loss: 6.9258 - model_193_loss: 0.6933 - model_193_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4273 - model_192_loss: 0.4976 - model_193_loss: 0.6923 - model_193_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4276 - model_192_loss: 0.4974 - model_193_loss: 0.6925 - model_193_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4294 - model_192_loss: 0.4965 - model_193_loss: 0.6925 - model_193_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4310 - model_192_loss: 0.4955 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4310 - model_192_loss: 0.4952 - model_193_loss: 0.6927 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 99us/sample - loss: 6.9268 - model_193_loss: 0.6922 - model_193_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4293 - model_192_loss: 0.4970 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4300 - model_192_loss: 0.4966 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4302 - model_192_loss: 0.4956 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4312 - model_192_loss: 0.4960 - model_193_loss: 0.6929 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4302 - model_192_loss: 0.4969 - model_193_loss: 0.6927 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 100us/sample - loss: 6.9277 - model_193_loss: 0.6926 - model_193_1_loss: 0.6929\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4288 - model_192_loss: 0.4963 - model_193_loss: 0.6925 - model_193_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4302 - model_192_loss: 0.4957 - model_193_loss: 0.6925 - model_193_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4308 - model_192_loss: 0.4951 - model_193_loss: 0.6926 - model_193_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4289 - model_192_loss: 0.4970 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4298 - model_192_loss: 0.4956 - model_193_loss: 0.6924 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 95us/sample - loss: 6.9270 - model_193_loss: 0.6919 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4302 - model_192_loss: 0.4975 - model_193_loss: 0.6929 - model_193_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4302 - model_192_loss: 0.4955 - model_193_loss: 0.6926 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4299 - model_192_loss: 0.4969 - model_193_loss: 0.6928 - model_193_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4326 - model_192_loss: 0.4944 - model_193_loss: 0.6928 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4317 - model_192_loss: 0.4944 - model_193_loss: 0.6928 - model_193_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 111us/sample - loss: 6.9274 - model_193_loss: 0.6926 - model_193_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4306 - model_192_loss: 0.4964 - model_193_loss: 0.6928 - model_193_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4286 - model_192_loss: 0.4976 - model_193_loss: 0.6927 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4296 - model_192_loss: 0.4964 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4302 - model_192_loss: 0.4968 - model_193_loss: 0.6928 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4322 - model_192_loss: 0.4956 - model_193_loss: 0.6930 - model_193_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 103us/sample - loss: 6.9279 - model_193_loss: 0.6936 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4279 - model_192_loss: 0.4979 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4307 - model_192_loss: 0.4967 - model_193_loss: 0.6928 - model_193_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4302 - model_192_loss: 0.4952 - model_193_loss: 0.6925 - model_193_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4311 - model_192_loss: 0.4947 - model_193_loss: 0.6926 - model_193_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4308 - model_192_loss: 0.4955 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 108us/sample - loss: 6.9269 - model_193_loss: 0.6922 - model_193_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4305 - model_192_loss: 0.4957 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4293 - model_192_loss: 0.4969 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4316 - model_192_loss: 0.4952 - model_193_loss: 0.6928 - model_193_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4305 - model_192_loss: 0.4963 - model_193_loss: 0.6929 - model_193_1_loss: 0.6925\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4300 - model_192_loss: 0.4961 - model_193_loss: 0.6925 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 98us/sample - loss: 6.9266 - model_193_loss: 0.6921 - model_193_1_loss: 0.69240s - loss: 6.9442 - model_193_loss: 0.6967 - model_193_1_loss: \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4289 - model_192_loss: 0.4966 - model_193_loss: 0.6926 - model_193_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4297 - model_192_loss: 0.4959 - model_193_loss: 0.6925 - model_193_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4292 - model_192_loss: 0.4965 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4312 - model_192_loss: 0.4955 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4290 - model_192_loss: 0.4972 - model_193_loss: 0.6926 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 101us/sample - loss: 6.9282 - model_193_loss: 0.6928 - model_193_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4298 - model_192_loss: 0.4965 - model_193_loss: 0.6927 - model_193_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4307 - model_192_loss: 0.4951 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4302 - model_192_loss: 0.4951 - model_193_loss: 0.6925 - model_193_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4304 - model_192_loss: 0.4955 - model_193_loss: 0.6925 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4307 - model_192_loss: 0.4949 - model_193_loss: 0.6925 - model_193_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 102us/sample - loss: 6.9272 - model_193_loss: 0.6931 - model_193_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4302 - model_192_loss: 0.4960 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4295 - model_192_loss: 0.4960 - model_193_loss: 0.6926 - model_193_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4310 - model_192_loss: 0.4949 - model_193_loss: 0.6926 - model_193_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4297 - model_192_loss: 0.4963 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4312 - model_192_loss: 0.4950 - model_193_loss: 0.6926 - model_193_1_loss: 0.6926\n",
      "For Attention Module: 5.0\n",
      "features X: 30940 samples, 70 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 122us/sample - loss: 6.3205 - model_197_loss: 0.6580 - model_197_1_loss: 0.6060s - loss: 6.3253 - model_197_loss: 0.6570 - model_197_1_loss: \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 2s 67us/sample - loss: -5.9515 - model_196_loss: 0.3748 - model_197_loss: 0.6589 - model_197_1_loss: 0.6064\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -5.9621 - model_196_loss: 0.3764 - model_197_loss: 0.6591 - model_197_1_loss: 0.6086\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -5.9700 - model_196_loss: 0.3766 - model_197_loss: 0.6595 - model_197_1_loss: 0.6098\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -5.9913 - model_196_loss: 0.3764 - model_197_loss: 0.6602 - model_197_1_loss: 0.6133\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -5.9972 - model_196_loss: 0.3760 - model_197_loss: 0.6601 - model_197_1_loss: 0.6145\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 126us/sample - loss: 6.3838 - model_197_loss: 0.6610 - model_197_1_loss: 0.6155\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.0095 - model_196_loss: 0.3755 - model_197_loss: 0.6623 - model_197_1_loss: 0.6147\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.0168 - model_196_loss: 0.3783 - model_197_loss: 0.6620 - model_197_1_loss: 0.6170\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.0246 - model_196_loss: 0.3788 - model_197_loss: 0.6621 - model_197_1_loss: 0.6186\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0386 - model_196_loss: 0.3801 - model_197_loss: 0.6626 - model_197_1_loss: 0.6211\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0446 - model_196_loss: 0.3793 - model_197_loss: 0.6621 - model_197_1_loss: 0.6226\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 109us/sample - loss: 6.4346 - model_197_loss: 0.6630 - model_197_1_loss: 0.6243\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.0620 - model_196_loss: 0.3803 - model_197_loss: 0.6633 - model_197_1_loss: 0.6252\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0833 - model_196_loss: 0.3815 - model_197_loss: 0.6656 - model_197_1_loss: 0.6274\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0895 - model_196_loss: 0.3819 - model_197_loss: 0.6644 - model_197_1_loss: 0.6298\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.1159 - model_196_loss: 0.3850 - model_197_loss: 0.6664 - model_197_1_loss: 0.6337\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.1178 - model_196_loss: 0.3880 - model_197_loss: 0.6660 - model_197_1_loss: 0.6351\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.5228 - model_197_loss: 0.6680 - model_197_1_loss: 0.6368\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.1118 - model_196_loss: 0.3892 - model_197_loss: 0.6658 - model_197_1_loss: 0.6344\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.1364 - model_196_loss: 0.3911 - model_197_loss: 0.6675 - model_197_1_loss: 0.6380\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.1550 - model_196_loss: 0.3924 - model_197_loss: 0.6681 - model_197_1_loss: 0.6414\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.1670 - model_196_loss: 0.3953 - model_197_loss: 0.6690 - model_197_1_loss: 0.6435\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.1700 - model_196_loss: 0.3972 - model_197_loss: 0.6688 - model_197_1_loss: 0.6446\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 118us/sample - loss: 6.5881 - model_197_loss: 0.6700 - model_197_1_loss: 0.6473\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.1883 - model_196_loss: 0.3991 - model_197_loss: 0.6698 - model_197_1_loss: 0.6476\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2055 - model_196_loss: 0.4007 - model_197_loss: 0.6714 - model_197_1_loss: 0.6498\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2205 - model_196_loss: 0.4035 - model_197_loss: 0.6722 - model_197_1_loss: 0.6526\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2233 - model_196_loss: 0.4079 - model_197_loss: 0.6715 - model_197_1_loss: 0.6548\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.2525 - model_196_loss: 0.4092 - model_197_loss: 0.6737 - model_197_1_loss: 0.6587\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.6683 - model_197_loss: 0.6743 - model_197_1_loss: 0.6596\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.2439 - model_196_loss: 0.4122 - model_197_loss: 0.6733 - model_197_1_loss: 0.6580\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2626 - model_196_loss: 0.4145 - model_197_loss: 0.6747 - model_197_1_loss: 0.6608\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.2785 - model_196_loss: 0.4144 - model_197_loss: 0.6757 - model_197_1_loss: 0.6629\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2855 - model_196_loss: 0.4208 - model_197_loss: 0.6766 - model_197_1_loss: 0.6647\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.2940 - model_196_loss: 0.4234 - model_197_loss: 0.6770 - model_197_1_loss: 0.6665\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.7283 - model_197_loss: 0.6776 - model_197_1_loss: 0.6677s - loss: 6.7432 - model_197_loss: 0.6802 - \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.3099 - model_196_loss: 0.4226 - model_197_loss: 0.6778 - model_197_1_loss: 0.6687\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3179 - model_196_loss: 0.4287 - model_197_loss: 0.6786 - model_197_1_loss: 0.6707\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3258 - model_196_loss: 0.4317 - model_197_loss: 0.6785 - model_197_1_loss: 0.6730\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3399 - model_196_loss: 0.4338 - model_197_loss: 0.6806 - model_197_1_loss: 0.6741\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3504 - model_196_loss: 0.4337 - model_197_loss: 0.6810 - model_197_1_loss: 0.6758\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 102us/sample - loss: 6.7872 - model_197_loss: 0.6807 - model_197_1_loss: 0.6762s - loss: 6.7969 - model_197_loss: 0.6819 - model_197_ - ETA: 0s - loss: 6.7932 - model_197_loss: 0.6822 - model_197_1_loss: 0.\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.3469 - model_196_loss: 0.4370 - model_197_loss: 0.6813 - model_197_1_loss: 0.6755\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3541 - model_196_loss: 0.4404 - model_197_loss: 0.6823 - model_197_1_loss: 0.6765\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3628 - model_196_loss: 0.4438 - model_197_loss: 0.6825 - model_197_1_loss: 0.6788\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3767 - model_196_loss: 0.4472 - model_197_loss: 0.6847 - model_197_1_loss: 0.6801\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3763 - model_196_loss: 0.4514 - model_197_loss: 0.6841 - model_197_1_loss: 0.6814\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 122us/sample - loss: 6.8417 - model_197_loss: 0.6856 - model_197_1_loss: 0.6823\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.3827 - model_196_loss: 0.4523 - model_197_loss: 0.6853 - model_197_1_loss: 0.6817\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3850 - model_196_loss: 0.4569 - model_197_loss: 0.6853 - model_197_1_loss: 0.6830\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.3913 - model_196_loss: 0.4582 - model_197_loss: 0.6861 - model_197_1_loss: 0.6838\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3933 - model_196_loss: 0.4620 - model_197_loss: 0.6860 - model_197_1_loss: 0.6851\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.3980 - model_196_loss: 0.4643 - model_197_loss: 0.6869 - model_197_1_loss: 0.6855\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 123us/sample - loss: 6.8719 - model_197_loss: 0.6881 - model_197_1_loss: 0.6861\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.3968 - model_196_loss: 0.4661 - model_197_loss: 0.6869 - model_197_1_loss: 0.6857\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4038 - model_196_loss: 0.4697 - model_197_loss: 0.6883 - model_197_1_loss: 0.6864\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4053 - model_196_loss: 0.4747 - model_197_loss: 0.6885 - model_197_1_loss: 0.6875\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4041 - model_196_loss: 0.4768 - model_197_loss: 0.6885 - model_197_1_loss: 0.6877\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4075 - model_196_loss: 0.4768 - model_197_loss: 0.6884 - model_197_1_loss: 0.6885\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 104us/sample - loss: 6.8910 - model_197_loss: 0.6901 - model_197_1_loss: 0.6880\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4036 - model_196_loss: 0.4799 - model_197_loss: 0.6892 - model_197_1_loss: 0.6875\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4086 - model_196_loss: 0.4819 - model_197_loss: 0.6903 - model_197_1_loss: 0.6878\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4095 - model_196_loss: 0.4837 - model_197_loss: 0.6905 - model_197_1_loss: 0.6882\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4123 - model_196_loss: 0.4851 - model_197_loss: 0.6907 - model_197_1_loss: 0.6888\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4150 - model_196_loss: 0.4878 - model_197_loss: 0.6913 - model_197_1_loss: 0.6893\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 124us/sample - loss: 6.9078 - model_197_loss: 0.6915 - model_197_1_loss: 0.6896s - loss: 6.9193 - model_\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4118 - model_196_loss: 0.4898 - model_197_loss: 0.6911 - model_197_1_loss: 0.6892\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4139 - model_196_loss: 0.4916 - model_197_loss: 0.6916 - model_197_1_loss: 0.6895\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4175 - model_196_loss: 0.4902 - model_197_loss: 0.6918 - model_197_1_loss: 0.6897\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4195 - model_196_loss: 0.4899 - model_197_loss: 0.6919 - model_197_1_loss: 0.6899\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4200 - model_196_loss: 0.4935 - model_197_loss: 0.6922 - model_197_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 120us/sample - loss: 6.9173 - model_197_loss: 0.6925 - model_197_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4176 - model_196_loss: 0.4936 - model_197_loss: 0.6919 - model_197_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4217 - model_196_loss: 0.4919 - model_197_loss: 0.6918 - model_197_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4273 - model_196_loss: 0.4897 - model_197_loss: 0.6924 - model_197_1_loss: 0.6909\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4287 - model_196_loss: 0.4920 - model_197_loss: 0.6927 - model_197_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4329 - model_196_loss: 0.4899 - model_197_loss: 0.6929 - model_197_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9242 - model_197_loss: 0.6931 - model_197_1_loss: 0.6918s - loss: 6.9921 - model_197_los\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4306 - model_196_loss: 0.4899 - model_197_loss: 0.6923 - model_197_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4318 - model_196_loss: 0.4901 - model_197_loss: 0.6923 - model_197_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4348 - model_196_loss: 0.4896 - model_197_loss: 0.6929 - model_197_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4350 - model_196_loss: 0.4880 - model_197_loss: 0.6924 - model_197_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4333 - model_196_loss: 0.4915 - model_197_loss: 0.6927 - model_197_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 113us/sample - loss: 6.9259 - model_197_loss: 0.6925 - model_197_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4351 - model_196_loss: 0.4892 - model_197_loss: 0.6925 - model_197_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4362 - model_196_loss: 0.4893 - model_197_loss: 0.6927 - model_197_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4384 - model_196_loss: 0.4887 - model_197_loss: 0.6930 - model_197_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4384 - model_196_loss: 0.4884 - model_197_loss: 0.6931 - model_197_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4392 - model_196_loss: 0.4867 - model_197_loss: 0.6925 - model_197_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 117us/sample - loss: 6.9242 - model_197_loss: 0.6917 - model_197_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4323 - model_196_loss: 0.4864 - model_197_loss: 0.6920 - model_197_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4323 - model_196_loss: 0.4855 - model_197_loss: 0.6916 - model_197_1_loss: 0.6920\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4302 - model_196_loss: 0.4879 - model_197_loss: 0.6915 - model_197_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4292 - model_196_loss: 0.4868 - model_197_loss: 0.6916 - model_197_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4305 - model_196_loss: 0.4854 - model_197_loss: 0.6916 - model_197_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.9182 - model_197_loss: 0.6924 - model_197_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4299 - model_196_loss: 0.4850 - model_197_loss: 0.6914 - model_197_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4289 - model_196_loss: 0.4835 - model_197_loss: 0.6912 - model_197_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4269 - model_196_loss: 0.4851 - model_197_loss: 0.6911 - model_197_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4307 - model_196_loss: 0.4812 - model_197_loss: 0.6910 - model_197_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4244 - model_196_loss: 0.4826 - model_197_loss: 0.6907 - model_197_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9120 - model_197_loss: 0.6910 - model_197_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4296 - model_196_loss: 0.4798 - model_197_loss: 0.6909 - model_197_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4252 - model_196_loss: 0.4809 - model_197_loss: 0.6903 - model_197_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4257 - model_196_loss: 0.4813 - model_197_loss: 0.6907 - model_197_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4355 - model_196_loss: 0.4767 - model_197_loss: 0.6914 - model_197_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4311 - model_196_loss: 0.4780 - model_197_loss: 0.6910 - model_197_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.9079 - model_197_loss: 0.6912 - model_197_1_loss: 0.6907\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4324 - model_196_loss: 0.4766 - model_197_loss: 0.6907 - model_197_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4317 - model_196_loss: 0.4770 - model_197_loss: 0.6903 - model_197_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4364 - model_196_loss: 0.4752 - model_197_loss: 0.6909 - model_197_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 30us/sample - loss: -6.4346 - model_196_loss: 0.4762 - model_197_loss: 0.6911 - model_197_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4379 - model_196_loss: 0.4744 - model_197_loss: 0.6913 - model_197_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 101us/sample - loss: 6.9131 - model_197_loss: 0.6909 - model_197_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4307 - model_196_loss: 0.4768 - model_197_loss: 0.6907 - model_197_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4340 - model_196_loss: 0.4755 - model_197_loss: 0.6907 - model_197_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4330 - model_196_loss: 0.4749 - model_197_loss: 0.6903 - model_197_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4391 - model_196_loss: 0.4755 - model_197_loss: 0.6914 - model_197_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4363 - model_196_loss: 0.4773 - model_197_loss: 0.6914 - model_197_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 111us/sample - loss: 6.9161 - model_197_loss: 0.6915 - model_197_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4386 - model_196_loss: 0.4756 - model_197_loss: 0.6914 - model_197_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4396 - model_196_loss: 0.4778 - model_197_loss: 0.6917 - model_197_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4389 - model_196_loss: 0.4769 - model_197_loss: 0.6916 - model_197_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4414 - model_196_loss: 0.4779 - model_197_loss: 0.6917 - model_197_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4413 - model_196_loss: 0.4746 - model_197_loss: 0.6915 - model_197_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 106us/sample - loss: 6.9179 - model_197_loss: 0.6918 - model_197_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4356 - model_196_loss: 0.4792 - model_197_loss: 0.6913 - model_197_1_loss: 0.6917\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4371 - model_196_loss: 0.4774 - model_197_loss: 0.6913 - model_197_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4380 - model_196_loss: 0.4768 - model_197_loss: 0.6914 - model_197_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4353 - model_196_loss: 0.4787 - model_197_loss: 0.6912 - model_197_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4419 - model_196_loss: 0.4762 - model_197_loss: 0.6920 - model_197_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.9181 - model_197_loss: 0.6920 - model_197_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4372 - model_196_loss: 0.4782 - model_197_loss: 0.6919 - model_197_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4383 - model_196_loss: 0.4758 - model_197_loss: 0.6917 - model_197_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4376 - model_196_loss: 0.4759 - model_197_loss: 0.6916 - model_197_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4382 - model_196_loss: 0.4763 - model_197_loss: 0.6919 - model_197_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4399 - model_196_loss: 0.4753 - model_197_loss: 0.6919 - model_197_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 119us/sample - loss: 6.9165 - model_197_loss: 0.6926 - model_197_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4370 - model_196_loss: 0.4753 - model_197_loss: 0.6919 - model_197_1_loss: 0.6906\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4355 - model_196_loss: 0.4752 - model_197_loss: 0.6915 - model_197_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4384 - model_196_loss: 0.4739 - model_197_loss: 0.6919 - model_197_1_loss: 0.6906\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4369 - model_196_loss: 0.4741 - model_197_loss: 0.6918 - model_197_1_loss: 0.6904\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4386 - model_196_loss: 0.4731 - model_197_loss: 0.6918 - model_197_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 122us/sample - loss: 6.9146 - model_197_loss: 0.6918 - model_197_1_loss: 0.6907s - loss: 6.9128 - model_197_loss: 0.6919 - model_197_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4380 - model_196_loss: 0.4715 - model_197_loss: 0.6916 - model_197_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4413 - model_196_loss: 0.4696 - model_197_loss: 0.6917 - model_197_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4428 - model_196_loss: 0.4706 - model_197_loss: 0.6922 - model_197_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4382 - model_196_loss: 0.4723 - model_197_loss: 0.6918 - model_197_1_loss: 0.6903\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4412 - model_196_loss: 0.4693 - model_197_loss: 0.6916 - model_197_1_loss: 0.6905\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9116 - model_197_loss: 0.6918 - model_197_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4421 - model_196_loss: 0.4677 - model_197_loss: 0.6915 - model_197_1_loss: 0.6904\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4410 - model_196_loss: 0.4681 - model_197_loss: 0.6916 - model_197_1_loss: 0.6902\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4435 - model_196_loss: 0.4691 - model_197_loss: 0.6920 - model_197_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4466 - model_196_loss: 0.4660 - model_197_loss: 0.6920 - model_197_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4442 - model_196_loss: 0.4662 - model_197_loss: 0.6915 - model_197_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.9139 - model_197_loss: 0.6921 - model_197_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4454 - model_196_loss: 0.4680 - model_197_loss: 0.6922 - model_197_1_loss: 0.6905\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4432 - model_196_loss: 0.4668 - model_197_loss: 0.6913 - model_197_1_loss: 0.6907\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4451 - model_196_loss: 0.4669 - model_197_loss: 0.6916 - model_197_1_loss: 0.6908\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4464 - model_196_loss: 0.4674 - model_197_loss: 0.6919 - model_197_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4470 - model_196_loss: 0.4683 - model_197_loss: 0.6920 - model_197_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.9145 - model_197_loss: 0.6928 - model_197_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4486 - model_196_loss: 0.4648 - model_197_loss: 0.6919 - model_197_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4454 - model_196_loss: 0.4669 - model_197_loss: 0.6917 - model_197_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4476 - model_196_loss: 0.4681 - model_197_loss: 0.6920 - model_197_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4463 - model_196_loss: 0.4646 - model_197_loss: 0.6916 - model_197_1_loss: 0.6906\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4463 - model_196_loss: 0.4666 - model_197_loss: 0.6918 - model_197_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.9152 - model_197_loss: 0.6923 - model_197_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4484 - model_196_loss: 0.4651 - model_197_loss: 0.6920 - model_197_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4483 - model_196_loss: 0.4663 - model_197_loss: 0.6921 - model_197_1_loss: 0.6908\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4498 - model_196_loss: 0.4651 - model_197_loss: 0.6920 - model_197_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4497 - model_196_loss: 0.4671 - model_197_loss: 0.6923 - model_197_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4484 - model_196_loss: 0.4682 - model_197_loss: 0.6921 - model_197_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 124us/sample - loss: 6.9158 - model_197_loss: 0.6927 - model_197_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4505 - model_196_loss: 0.4659 - model_197_loss: 0.6924 - model_197_1_loss: 0.6909\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4469 - model_196_loss: 0.4694 - model_197_loss: 0.6924 - model_197_1_loss: 0.6909\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4462 - model_196_loss: 0.4715 - model_197_loss: 0.6925 - model_197_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4459 - model_196_loss: 0.4699 - model_197_loss: 0.6921 - model_197_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4472 - model_196_loss: 0.4724 - model_197_loss: 0.6926 - model_197_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 109us/sample - loss: 6.9176 - model_197_loss: 0.6922 - model_197_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4475 - model_196_loss: 0.4722 - model_197_loss: 0.6926 - model_197_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4465 - model_196_loss: 0.4718 - model_197_loss: 0.6923 - model_197_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4447 - model_196_loss: 0.4736 - model_197_loss: 0.6920 - model_197_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4486 - model_196_loss: 0.4710 - model_197_loss: 0.6926 - model_197_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4465 - model_196_loss: 0.4727 - model_197_loss: 0.6926 - model_197_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.9209 - model_197_loss: 0.6928 - model_197_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4443 - model_196_loss: 0.4738 - model_197_loss: 0.6921 - model_197_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4480 - model_196_loss: 0.4724 - model_197_loss: 0.6924 - model_197_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4456 - model_196_loss: 0.4731 - model_197_loss: 0.6923 - model_197_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4463 - model_196_loss: 0.4741 - model_197_loss: 0.6923 - model_197_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4483 - model_196_loss: 0.4734 - model_197_loss: 0.6928 - model_197_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.9224 - model_197_loss: 0.6922 - model_197_1_loss: 0.6919s - loss: 6.9047 - model_1\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: -6.4493 - model_196_loss: 0.4731 - model_197_loss: 0.6926 - model_197_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4485 - model_196_loss: 0.4742 - model_197_loss: 0.6927 - model_197_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4482 - model_196_loss: 0.4738 - model_197_loss: 0.6925 - model_197_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4488 - model_196_loss: 0.4735 - model_197_loss: 0.6924 - model_197_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4506 - model_196_loss: 0.4717 - model_197_loss: 0.6923 - model_197_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.9235 - model_197_loss: 0.6921 - model_197_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4488 - model_196_loss: 0.4725 - model_197_loss: 0.6923 - model_197_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4481 - model_196_loss: 0.4727 - model_197_loss: 0.6925 - model_197_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4499 - model_196_loss: 0.4727 - model_197_loss: 0.6926 - model_197_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4523 - model_196_loss: 0.4705 - model_197_loss: 0.6927 - model_197_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4514 - model_196_loss: 0.4714 - model_197_loss: 0.6926 - model_197_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 115us/sample - loss: 6.9212 - model_197_loss: 0.6925 - model_197_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4496 - model_196_loss: 0.4710 - model_197_loss: 0.6923 - model_197_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4536 - model_196_loss: 0.4694 - model_197_loss: 0.6924 - model_197_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4524 - model_196_loss: 0.4683 - model_197_loss: 0.6925 - model_197_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4523 - model_196_loss: 0.4699 - model_197_loss: 0.6925 - model_197_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4548 - model_196_loss: 0.4675 - model_197_loss: 0.6924 - model_197_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 125us/sample - loss: 6.9212 - model_197_loss: 0.6925 - model_197_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4544 - model_196_loss: 0.4666 - model_197_loss: 0.6926 - model_197_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4532 - model_196_loss: 0.4665 - model_197_loss: 0.6922 - model_197_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4524 - model_196_loss: 0.4656 - model_197_loss: 0.6922 - model_197_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4513 - model_196_loss: 0.4681 - model_197_loss: 0.6924 - model_197_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4546 - model_196_loss: 0.4635 - model_197_loss: 0.6922 - model_197_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 119us/sample - loss: 6.9158 - model_197_loss: 0.6918 - model_197_1_loss: 0.6908s - loss: 6.9645 - model_197_loss: 0.6984 -\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4505 - model_196_loss: 0.4666 - model_197_loss: 0.6925 - model_197_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4513 - model_196_loss: 0.4659 - model_197_loss: 0.6922 - model_197_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4514 - model_196_loss: 0.4649 - model_197_loss: 0.6922 - model_197_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4523 - model_196_loss: 0.4657 - model_197_loss: 0.6921 - model_197_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4536 - model_196_loss: 0.4652 - model_197_loss: 0.6919 - model_197_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 121us/sample - loss: 6.9177 - model_197_loss: 0.6927 - model_197_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: -6.4496 - model_196_loss: 0.4666 - model_197_loss: 0.6922 - model_197_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4512 - model_196_loss: 0.4663 - model_197_loss: 0.6922 - model_197_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4524 - model_196_loss: 0.4683 - model_197_loss: 0.6926 - model_197_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4536 - model_196_loss: 0.4677 - model_197_loss: 0.6926 - model_197_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4538 - model_196_loss: 0.4667 - model_197_loss: 0.6923 - model_197_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.9220 - model_197_loss: 0.6921 - model_197_1_loss: 0.6916s - loss: 6.9220 - model_197_loss: 0.6927 - model_197_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4525 - model_196_loss: 0.4674 - model_197_loss: 0.6923 - model_197_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4552 - model_196_loss: 0.4694 - model_197_loss: 0.6928 - model_197_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4519 - model_196_loss: 0.4701 - model_197_loss: 0.6925 - model_197_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4509 - model_196_loss: 0.4701 - model_197_loss: 0.6925 - model_197_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4540 - model_196_loss: 0.4720 - model_197_loss: 0.6927 - model_197_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 113us/sample - loss: 6.9275 - model_197_loss: 0.6929 - model_197_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4539 - model_196_loss: 0.4716 - model_197_loss: 0.6927 - model_197_1_loss: 0.6924\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4538 - model_196_loss: 0.4715 - model_197_loss: 0.6926 - model_197_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4548 - model_196_loss: 0.4718 - model_197_loss: 0.6928 - model_197_1_loss: 0.6925\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4543 - model_196_loss: 0.4723 - model_197_loss: 0.6927 - model_197_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4540 - model_196_loss: 0.4729 - model_197_loss: 0.6929 - model_197_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 116us/sample - loss: 6.9271 - model_197_loss: 0.6925 - model_197_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4513 - model_196_loss: 0.4731 - model_197_loss: 0.6927 - model_197_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4556 - model_196_loss: 0.4700 - model_197_loss: 0.6928 - model_197_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4522 - model_196_loss: 0.4710 - model_197_loss: 0.6925 - model_197_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4545 - model_196_loss: 0.4683 - model_197_loss: 0.6926 - model_197_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4525 - model_196_loss: 0.4687 - model_197_loss: 0.6923 - model_197_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.9235 - model_197_loss: 0.6929 - model_197_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4499 - model_196_loss: 0.4679 - model_197_loss: 0.6923 - model_197_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4504 - model_196_loss: 0.4681 - model_197_loss: 0.6923 - model_197_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4536 - model_196_loss: 0.4645 - model_197_loss: 0.6925 - model_197_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4529 - model_196_loss: 0.4647 - model_197_loss: 0.6923 - model_197_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4555 - model_196_loss: 0.4634 - model_197_loss: 0.6927 - model_197_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 130us/sample - loss: 6.9165 - model_197_loss: 0.6933 - model_197_1_loss: 0.6905s - loss: 6.9201 - model_197_loss: 0.6923 - mod\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4545 - model_196_loss: 0.4611 - model_197_loss: 0.6925 - model_197_1_loss: 0.6907\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4520 - model_196_loss: 0.4604 - model_197_loss: 0.6921 - model_197_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4522 - model_196_loss: 0.4605 - model_197_loss: 0.6922 - model_197_1_loss: 0.6903\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4540 - model_196_loss: 0.4592 - model_197_loss: 0.6921 - model_197_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4553 - model_196_loss: 0.4576 - model_197_loss: 0.6921 - model_197_1_loss: 0.6904\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 109us/sample - loss: 6.9126 - model_197_loss: 0.6917 - model_197_1_loss: 0.6903\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4561 - model_196_loss: 0.4583 - model_197_loss: 0.6921 - model_197_1_loss: 0.6908\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4542 - model_196_loss: 0.4586 - model_197_loss: 0.6922 - model_197_1_loss: 0.6904\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4565 - model_196_loss: 0.4582 - model_197_loss: 0.6922 - model_197_1_loss: 0.6907\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4575 - model_196_loss: 0.4570 - model_197_loss: 0.6918 - model_197_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4540 - model_196_loss: 0.4590 - model_197_loss: 0.6917 - model_197_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 102us/sample - loss: 6.9150 - model_197_loss: 0.6925 - model_197_1_loss: 0.6908\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4519 - model_196_loss: 0.4587 - model_197_loss: 0.6919 - model_197_1_loss: 0.6903\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4504 - model_196_loss: 0.4593 - model_197_loss: 0.6916 - model_197_1_loss: 0.6903\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4540 - model_196_loss: 0.4587 - model_197_loss: 0.6920 - model_197_1_loss: 0.6905\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4561 - model_196_loss: 0.4582 - model_197_loss: 0.6920 - model_197_1_loss: 0.6909\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4563 - model_196_loss: 0.4601 - model_197_loss: 0.6923 - model_197_1_loss: 0.6909\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9166 - model_197_loss: 0.6924 - model_197_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4534 - model_196_loss: 0.4619 - model_197_loss: 0.6920 - model_197_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4563 - model_196_loss: 0.4613 - model_197_loss: 0.6923 - model_197_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4539 - model_196_loss: 0.4636 - model_197_loss: 0.6922 - model_197_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4543 - model_196_loss: 0.4645 - model_197_loss: 0.6925 - model_197_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4565 - model_196_loss: 0.4648 - model_197_loss: 0.6926 - model_197_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 131us/sample - loss: 6.9224 - model_197_loss: 0.6932 - model_197_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4535 - model_196_loss: 0.4650 - model_197_loss: 0.6921 - model_197_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4545 - model_196_loss: 0.4662 - model_197_loss: 0.6925 - model_197_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4539 - model_196_loss: 0.4690 - model_197_loss: 0.6925 - model_197_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4564 - model_196_loss: 0.4676 - model_197_loss: 0.6929 - model_197_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4520 - model_196_loss: 0.4703 - model_197_loss: 0.6925 - model_197_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9265 - model_197_loss: 0.6933 - model_197_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4543 - model_196_loss: 0.4710 - model_197_loss: 0.6928 - model_197_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4557 - model_196_loss: 0.4702 - model_197_loss: 0.6928 - model_197_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4544 - model_196_loss: 0.4713 - model_197_loss: 0.6927 - model_197_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4537 - model_196_loss: 0.4700 - model_197_loss: 0.6927 - model_197_1_loss: 0.6921\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4572 - model_196_loss: 0.4683 - model_197_loss: 0.6928 - model_197_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 100us/sample - loss: 6.9251 - model_197_loss: 0.6927 - model_197_1_loss: 0.6921s - loss: 6.9241 - model_197_loss: 0.6934 - model_197_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4541 - model_196_loss: 0.4700 - model_197_loss: 0.6928 - model_197_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4543 - model_196_loss: 0.4699 - model_197_loss: 0.6927 - model_197_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4534 - model_196_loss: 0.4689 - model_197_loss: 0.6927 - model_197_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4545 - model_196_loss: 0.4685 - model_197_loss: 0.6927 - model_197_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4563 - model_196_loss: 0.4673 - model_197_loss: 0.6929 - model_197_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9221 - model_197_loss: 0.6922 - model_197_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4543 - model_196_loss: 0.4672 - model_197_loss: 0.6927 - model_197_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4550 - model_196_loss: 0.4651 - model_197_loss: 0.6925 - model_197_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4546 - model_196_loss: 0.4650 - model_197_loss: 0.6926 - model_197_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4571 - model_196_loss: 0.4627 - model_197_loss: 0.6928 - model_197_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4571 - model_196_loss: 0.4613 - model_197_loss: 0.6925 - model_197_1_loss: 0.6912\n",
      "For Attention Module: 5.1\n",
      "features X: 30940 samples, 69 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 104us/sample - loss: 6.3636 - model_201_loss: 0.6606 - model_201_1_loss: 0.6118\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: -5.9861 - model_200_loss: 0.3831 - model_201_loss: 0.6622 - model_201_1_loss: 0.6116\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -5.9823 - model_200_loss: 0.3842 - model_201_loss: 0.6610 - model_201_1_loss: 0.6123\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -5.9956 - model_200_loss: 0.3867 - model_201_loss: 0.6612 - model_201_1_loss: 0.6153\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.0108 - model_200_loss: 0.3844 - model_201_loss: 0.6632 - model_201_1_loss: 0.6158\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.0225 - model_200_loss: 0.3839 - model_201_loss: 0.6630 - model_201_1_loss: 0.6183\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 108us/sample - loss: 6.4193 - model_201_loss: 0.6647 - model_201_1_loss: 0.6204\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.0258 - model_200_loss: 0.3876 - model_201_loss: 0.6629 - model_201_1_loss: 0.6198\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0366 - model_200_loss: 0.3879 - model_201_loss: 0.6630 - model_201_1_loss: 0.6219\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.0459 - model_200_loss: 0.3869 - model_201_loss: 0.6633 - model_201_1_loss: 0.6232\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.0613 - model_200_loss: 0.3858 - model_201_loss: 0.6637 - model_201_1_loss: 0.6258\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.0788 - model_200_loss: 0.3885 - model_201_loss: 0.6652 - model_201_1_loss: 0.6282\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 102us/sample - loss: 6.4573 - model_201_loss: 0.6635 - model_201_1_loss: 0.6281\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.0810 - model_200_loss: 0.3880 - model_201_loss: 0.6649 - model_201_1_loss: 0.6289\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0855 - model_200_loss: 0.3902 - model_201_loss: 0.6656 - model_201_1_loss: 0.6295\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.0961 - model_200_loss: 0.3910 - model_201_loss: 0.6657 - model_201_1_loss: 0.6317\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.1027 - model_200_loss: 0.3910 - model_201_loss: 0.6664 - model_201_1_loss: 0.6323\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.1207 - model_200_loss: 0.3935 - model_201_loss: 0.6681 - model_201_1_loss: 0.6347\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.5287 - model_201_loss: 0.6686 - model_201_1_loss: 0.6374\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.1305 - model_200_loss: 0.3929 - model_201_loss: 0.6679 - model_201_1_loss: 0.6368\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.1479 - model_200_loss: 0.3942 - model_201_loss: 0.6683 - model_201_1_loss: 0.6401\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.1580 - model_200_loss: 0.3980 - model_201_loss: 0.6695 - model_201_1_loss: 0.6417\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.1691 - model_200_loss: 0.3960 - model_201_loss: 0.6693 - model_201_1_loss: 0.6437\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.1843 - model_200_loss: 0.3966 - model_201_loss: 0.6708 - model_201_1_loss: 0.6453\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 109us/sample - loss: 6.5860 - model_201_loss: 0.6696 - model_201_1_loss: 0.6471\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.1913 - model_200_loss: 0.4005 - model_201_loss: 0.6722 - model_201_1_loss: 0.6462\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2043 - model_200_loss: 0.4024 - model_201_loss: 0.6728 - model_201_1_loss: 0.6486\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2059 - model_200_loss: 0.4054 - model_201_loss: 0.6729 - model_201_1_loss: 0.6494\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2342 - model_200_loss: 0.4047 - model_201_loss: 0.6741 - model_201_1_loss: 0.6537\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2403 - model_200_loss: 0.4064 - model_201_loss: 0.6752 - model_201_1_loss: 0.6542\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 116us/sample - loss: 6.6528 - model_201_loss: 0.6738 - model_201_1_loss: 0.6560s - loss: 6.6422 - model_201_loss: 0.671\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.2352 - model_200_loss: 0.4078 - model_201_loss: 0.6737 - model_201_1_loss: 0.6549\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2516 - model_200_loss: 0.4091 - model_201_loss: 0.6747 - model_201_1_loss: 0.6575\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2627 - model_200_loss: 0.4103 - model_201_loss: 0.6761 - model_201_1_loss: 0.6585\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.2715 - model_200_loss: 0.4125 - model_201_loss: 0.6764 - model_201_1_loss: 0.6604\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.2826 - model_200_loss: 0.4154 - model_201_loss: 0.6763 - model_201_1_loss: 0.6633\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 123us/sample - loss: 6.7057 - model_201_loss: 0.6763 - model_201_1_loss: 0.6640s - loss: 6.8309 - mode\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.2909 - model_200_loss: 0.4186 - model_201_loss: 0.6782 - model_201_1_loss: 0.6637\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2961 - model_200_loss: 0.4177 - model_201_loss: 0.6780 - model_201_1_loss: 0.6648\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.2991 - model_200_loss: 0.4206 - model_201_loss: 0.6790 - model_201_1_loss: 0.6649\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3072 - model_200_loss: 0.4219 - model_201_loss: 0.6794 - model_201_1_loss: 0.6665\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3149 - model_200_loss: 0.4212 - model_201_loss: 0.6793 - model_201_1_loss: 0.6679\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 108us/sample - loss: 6.7508 - model_201_loss: 0.6806 - model_201_1_loss: 0.6695\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.3266 - model_200_loss: 0.4263 - model_201_loss: 0.6814 - model_201_1_loss: 0.6691\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3225 - model_200_loss: 0.4260 - model_201_loss: 0.6801 - model_201_1_loss: 0.6696\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3293 - model_200_loss: 0.4300 - model_201_loss: 0.6810 - model_201_1_loss: 0.6709\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3323 - model_200_loss: 0.4314 - model_201_loss: 0.6805 - model_201_1_loss: 0.6722\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3531 - model_200_loss: 0.4311 - model_201_loss: 0.6830 - model_201_1_loss: 0.6738\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.7869 - model_201_loss: 0.6841 - model_201_1_loss: 0.6735\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.3479 - model_200_loss: 0.4340 - model_201_loss: 0.6831 - model_201_1_loss: 0.6733\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3550 - model_200_loss: 0.4361 - model_201_loss: 0.6837 - model_201_1_loss: 0.6745\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.3558 - model_200_loss: 0.4383 - model_201_loss: 0.6834 - model_201_1_loss: 0.6754\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3582 - model_200_loss: 0.4377 - model_201_loss: 0.6833 - model_201_1_loss: 0.6759\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.3621 - model_200_loss: 0.4408 - model_201_loss: 0.6839 - model_201_1_loss: 0.6767\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 119us/sample - loss: 6.8109 - model_201_loss: 0.6845 - model_201_1_loss: 0.6777\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.3693 - model_200_loss: 0.4417 - model_201_loss: 0.6838 - model_201_1_loss: 0.6784\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.3683 - model_200_loss: 0.4434 - model_201_loss: 0.6849 - model_201_1_loss: 0.6775\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -6.3774 - model_200_loss: 0.4450 - model_201_loss: 0.6852 - model_201_1_loss: 0.6793\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.3816 - model_200_loss: 0.4459 - model_201_loss: 0.6858 - model_201_1_loss: 0.6797\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -6.3875 - model_200_loss: 0.4462 - model_201_loss: 0.6860 - model_201_1_loss: 0.6807\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.8402 - model_201_loss: 0.6869 - model_201_1_loss: 0.6813\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: -6.3896 - model_200_loss: 0.4488 - model_201_loss: 0.6858 - model_201_1_loss: 0.6819\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3959 - model_200_loss: 0.4513 - model_201_loss: 0.6870 - model_201_1_loss: 0.6825\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3962 - model_200_loss: 0.4528 - model_201_loss: 0.6871 - model_201_1_loss: 0.6827\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.3968 - model_200_loss: 0.4540 - model_201_loss: 0.6873 - model_201_1_loss: 0.6828\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4039 - model_200_loss: 0.4557 - model_201_loss: 0.6882 - model_201_1_loss: 0.6837\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.8651 - model_201_loss: 0.6887 - model_201_1_loss: 0.6843\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4038 - model_200_loss: 0.4578 - model_201_loss: 0.6880 - model_201_1_loss: 0.6843\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4088 - model_200_loss: 0.4578 - model_201_loss: 0.6882 - model_201_1_loss: 0.6851\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4121 - model_200_loss: 0.4595 - model_201_loss: 0.6890 - model_201_1_loss: 0.6854\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4112 - model_200_loss: 0.4629 - model_201_loss: 0.6889 - model_201_1_loss: 0.6859\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4153 - model_200_loss: 0.4631 - model_201_loss: 0.6893 - model_201_1_loss: 0.6864\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 119us/sample - loss: 6.8767 - model_201_loss: 0.6890 - model_201_1_loss: 0.6864s - loss: 6.8675 - model_201_loss: 0.685\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4152 - model_200_loss: 0.4658 - model_201_loss: 0.6894 - model_201_1_loss: 0.6868\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4213 - model_200_loss: 0.4658 - model_201_loss: 0.6901 - model_201_1_loss: 0.6873\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4215 - model_200_loss: 0.4676 - model_201_loss: 0.6901 - model_201_1_loss: 0.6877\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4260 - model_200_loss: 0.4695 - model_201_loss: 0.6901 - model_201_1_loss: 0.6891\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4258 - model_200_loss: 0.4705 - model_201_loss: 0.6907 - model_201_1_loss: 0.6886\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 111us/sample - loss: 6.9073 - model_201_loss: 0.6909 - model_201_1_loss: 0.6900\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4252 - model_200_loss: 0.4739 - model_201_loss: 0.6910 - model_201_1_loss: 0.6889\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4314 - model_200_loss: 0.4735 - model_201_loss: 0.6913 - model_201_1_loss: 0.6897\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4307 - model_200_loss: 0.4737 - model_201_loss: 0.6912 - model_201_1_loss: 0.6896\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4319 - model_200_loss: 0.4770 - model_201_loss: 0.6912 - model_201_1_loss: 0.6905\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4311 - model_200_loss: 0.4802 - model_201_loss: 0.6917 - model_201_1_loss: 0.6906\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 120us/sample - loss: 6.9123 - model_201_loss: 0.6916 - model_201_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4331 - model_200_loss: 0.4783 - model_201_loss: 0.6913 - model_201_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4357 - model_200_loss: 0.4810 - model_201_loss: 0.6922 - model_201_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4340 - model_200_loss: 0.4819 - model_201_loss: 0.6919 - model_201_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4329 - model_200_loss: 0.4845 - model_201_loss: 0.6920 - model_201_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4385 - model_200_loss: 0.4829 - model_201_loss: 0.6925 - model_201_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 103us/sample - loss: 6.9219 - model_201_loss: 0.6919 - model_201_1_loss: 0.6917s - loss: 6.8840 - model_201_loss: 0.68\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4373 - model_200_loss: 0.4835 - model_201_loss: 0.6921 - model_201_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4344 - model_200_loss: 0.4856 - model_201_loss: 0.6921 - model_201_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4382 - model_200_loss: 0.4857 - model_201_loss: 0.6925 - model_201_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4379 - model_200_loss: 0.4851 - model_201_loss: 0.6920 - model_201_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4400 - model_200_loss: 0.4877 - model_201_loss: 0.6927 - model_201_1_loss: 0.6928\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 121us/sample - loss: 6.9253 - model_201_loss: 0.6926 - model_201_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4352 - model_200_loss: 0.4861 - model_201_loss: 0.6920 - model_201_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4395 - model_200_loss: 0.4858 - model_201_loss: 0.6928 - model_201_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4337 - model_200_loss: 0.4894 - model_201_loss: 0.6922 - model_201_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4375 - model_200_loss: 0.4880 - model_201_loss: 0.6927 - model_201_1_loss: 0.6924\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4397 - model_200_loss: 0.4880 - model_201_loss: 0.6928 - model_201_1_loss: 0.6927\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.9248 - model_201_loss: 0.6922 - model_201_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4357 - model_200_loss: 0.4880 - model_201_loss: 0.6922 - model_201_1_loss: 0.6925\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4366 - model_200_loss: 0.4882 - model_201_loss: 0.6923 - model_201_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4352 - model_200_loss: 0.4877 - model_201_loss: 0.6922 - model_201_1_loss: 0.6924\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4389 - model_200_loss: 0.4871 - model_201_loss: 0.6925 - model_201_1_loss: 0.6927\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4378 - model_200_loss: 0.4871 - model_201_loss: 0.6924 - model_201_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 120us/sample - loss: 6.9260 - model_201_loss: 0.6918 - model_201_1_loss: 0.6927s - loss: 6.9218 - model_201_loss: 0.6898\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4371 - model_200_loss: 0.4866 - model_201_loss: 0.6921 - model_201_1_loss: 0.6926\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4368 - model_200_loss: 0.4866 - model_201_loss: 0.6920 - model_201_1_loss: 0.6927\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4362 - model_200_loss: 0.4874 - model_201_loss: 0.6920 - model_201_1_loss: 0.6927\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4384 - model_200_loss: 0.4875 - model_201_loss: 0.6926 - model_201_1_loss: 0.6926\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4406 - model_200_loss: 0.4849 - model_201_loss: 0.6925 - model_201_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.9246 - model_201_loss: 0.6926 - model_201_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4378 - model_200_loss: 0.4860 - model_201_loss: 0.6921 - model_201_1_loss: 0.6927\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4373 - model_200_loss: 0.4855 - model_201_loss: 0.6921 - model_201_1_loss: 0.6925\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4402 - model_200_loss: 0.4847 - model_201_loss: 0.6923 - model_201_1_loss: 0.6926\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4376 - model_200_loss: 0.4832 - model_201_loss: 0.6919 - model_201_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4423 - model_200_loss: 0.4824 - model_201_loss: 0.6924 - model_201_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 103us/sample - loss: 6.9240 - model_201_loss: 0.6920 - model_201_1_loss: 0.6926\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4383 - model_200_loss: 0.4827 - model_201_loss: 0.6919 - model_201_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4398 - model_200_loss: 0.4808 - model_201_loss: 0.6918 - model_201_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4371 - model_200_loss: 0.4817 - model_201_loss: 0.6915 - model_201_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4408 - model_200_loss: 0.4810 - model_201_loss: 0.6921 - model_201_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4425 - model_200_loss: 0.4803 - model_201_loss: 0.6924 - model_201_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 113us/sample - loss: 6.9226 - model_201_loss: 0.6921 - model_201_1_loss: 0.6925\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4413 - model_200_loss: 0.4807 - model_201_loss: 0.6923 - model_201_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4397 - model_200_loss: 0.4799 - model_201_loss: 0.6917 - model_201_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4384 - model_200_loss: 0.4805 - model_201_loss: 0.6918 - model_201_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4407 - model_200_loss: 0.4779 - model_201_loss: 0.6917 - model_201_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4425 - model_200_loss: 0.4781 - model_201_loss: 0.6920 - model_201_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 118us/sample - loss: 6.9179 - model_201_loss: 0.6912 - model_201_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: -6.4420 - model_200_loss: 0.4773 - model_201_loss: 0.6920 - model_201_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4421 - model_200_loss: 0.4776 - model_201_loss: 0.6921 - model_201_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4400 - model_200_loss: 0.4775 - model_201_loss: 0.6919 - model_201_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4427 - model_200_loss: 0.4749 - model_201_loss: 0.6920 - model_201_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4430 - model_200_loss: 0.4771 - model_201_loss: 0.6920 - model_201_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9216 - model_201_loss: 0.6930 - model_201_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4423 - model_200_loss: 0.4778 - model_201_loss: 0.6921 - model_201_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4419 - model_200_loss: 0.4767 - model_201_loss: 0.6918 - model_201_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4412 - model_200_loss: 0.4778 - model_201_loss: 0.6918 - model_201_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4468 - model_200_loss: 0.4754 - model_201_loss: 0.6926 - model_201_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4406 - model_200_loss: 0.4772 - model_201_loss: 0.6918 - model_201_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 116us/sample - loss: 6.9217 - model_201_loss: 0.6926 - model_201_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4450 - model_200_loss: 0.4768 - model_201_loss: 0.6926 - model_201_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4464 - model_200_loss: 0.4772 - model_201_loss: 0.6926 - model_201_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4455 - model_200_loss: 0.4751 - model_201_loss: 0.6924 - model_201_1_loss: 0.6917\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4473 - model_200_loss: 0.4755 - model_201_loss: 0.6927 - model_201_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4435 - model_200_loss: 0.4762 - model_201_loss: 0.6924 - model_201_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9222 - model_201_loss: 0.6925 - model_201_1_loss: 0.6916\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4436 - model_200_loss: 0.4763 - model_201_loss: 0.6922 - model_201_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4464 - model_200_loss: 0.4745 - model_201_loss: 0.6923 - model_201_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4468 - model_200_loss: 0.4746 - model_201_loss: 0.6924 - model_201_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4449 - model_200_loss: 0.4738 - model_201_loss: 0.6921 - model_201_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4465 - model_200_loss: 0.4733 - model_201_loss: 0.6922 - model_201_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 122us/sample - loss: 6.9210 - model_201_loss: 0.6918 - model_201_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4460 - model_200_loss: 0.4726 - model_201_loss: 0.6921 - model_201_1_loss: 0.6916\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4456 - model_200_loss: 0.4724 - model_201_loss: 0.6922 - model_201_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4458 - model_200_loss: 0.4727 - model_201_loss: 0.6923 - model_201_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4461 - model_200_loss: 0.4719 - model_201_loss: 0.6923 - model_201_1_loss: 0.6913\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4462 - model_200_loss: 0.4701 - model_201_loss: 0.6919 - model_201_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 107us/sample - loss: 6.9211 - model_201_loss: 0.6926 - model_201_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4457 - model_200_loss: 0.4715 - model_201_loss: 0.6921 - model_201_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4449 - model_200_loss: 0.4711 - model_201_loss: 0.6921 - model_201_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4449 - model_200_loss: 0.4720 - model_201_loss: 0.6919 - model_201_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4444 - model_200_loss: 0.4703 - model_201_loss: 0.6918 - model_201_1_loss: 0.6911\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4459 - model_200_loss: 0.4695 - model_201_loss: 0.6920 - model_201_1_loss: 0.6910\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 114us/sample - loss: 6.9207 - model_201_loss: 0.6921 - model_201_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4475 - model_200_loss: 0.4687 - model_201_loss: 0.6919 - model_201_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4487 - model_200_loss: 0.4690 - model_201_loss: 0.6921 - model_201_1_loss: 0.6914\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4452 - model_200_loss: 0.4710 - model_201_loss: 0.6919 - model_201_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4483 - model_200_loss: 0.4688 - model_201_loss: 0.6920 - model_201_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4460 - model_200_loss: 0.4690 - model_201_loss: 0.6919 - model_201_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 100us/sample - loss: 6.9169 - model_201_loss: 0.6920 - model_201_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4466 - model_200_loss: 0.4680 - model_201_loss: 0.6918 - model_201_1_loss: 0.6911\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4477 - model_200_loss: 0.4683 - model_201_loss: 0.6919 - model_201_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4472 - model_200_loss: 0.4684 - model_201_loss: 0.6916 - model_201_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4504 - model_200_loss: 0.4687 - model_201_loss: 0.6919 - model_201_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4473 - model_200_loss: 0.4680 - model_201_loss: 0.6919 - model_201_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 119us/sample - loss: 6.9192 - model_201_loss: 0.6928 - model_201_1_loss: 0.6917s - loss: 6.9022 - model\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: -6.4474 - model_200_loss: 0.4690 - model_201_loss: 0.6920 - model_201_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4480 - model_200_loss: 0.4708 - model_201_loss: 0.6919 - model_201_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4472 - model_200_loss: 0.4703 - model_201_loss: 0.6919 - model_201_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4467 - model_200_loss: 0.4715 - model_201_loss: 0.6920 - model_201_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4509 - model_200_loss: 0.4698 - model_201_loss: 0.6925 - model_201_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 109us/sample - loss: 6.9257 - model_201_loss: 0.6930 - model_201_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4479 - model_200_loss: 0.4730 - model_201_loss: 0.6922 - model_201_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4498 - model_200_loss: 0.4715 - model_201_loss: 0.6924 - model_201_1_loss: 0.6918\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4490 - model_200_loss: 0.4723 - model_201_loss: 0.6924 - model_201_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4472 - model_200_loss: 0.4739 - model_201_loss: 0.6923 - model_201_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4464 - model_200_loss: 0.4745 - model_201_loss: 0.6923 - model_201_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - ETA: 0s - loss: 6.9217 - model_201_loss: 0.6926 - model_201_1_loss: 0.6918- ETA: 2s - loss: 6.8761 - mo - 3s 121us/sample - loss: 6.9226 - model_201_loss: 0.6925 - model_201_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4492 - model_200_loss: 0.4734 - model_201_loss: 0.6926 - model_201_1_loss: 0.6919\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4498 - model_200_loss: 0.4722 - model_201_loss: 0.6921 - model_201_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4466 - model_200_loss: 0.4740 - model_201_loss: 0.6922 - model_201_1_loss: 0.6919\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4491 - model_200_loss: 0.4733 - model_201_loss: 0.6926 - model_201_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4491 - model_200_loss: 0.4753 - model_201_loss: 0.6925 - model_201_1_loss: 0.6924\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 2s 99us/sample - loss: 6.9250 - model_201_loss: 0.6918 - model_201_1_loss: 0.69250s - loss: 6.9239 - model_201_loss: 0.6926 - model_201_1_loss: 0.6\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4481 - model_200_loss: 0.4749 - model_201_loss: 0.6924 - model_201_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4478 - model_200_loss: 0.4757 - model_201_loss: 0.6925 - model_201_1_loss: 0.6922\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4486 - model_200_loss: 0.4751 - model_201_loss: 0.6927 - model_201_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4506 - model_200_loss: 0.4735 - model_201_loss: 0.6926 - model_201_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4463 - model_200_loss: 0.4755 - model_201_loss: 0.6922 - model_201_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 123us/sample - loss: 6.9246 - model_201_loss: 0.6928 - model_201_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4472 - model_200_loss: 0.4750 - model_201_loss: 0.6924 - model_201_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4487 - model_200_loss: 0.4732 - model_201_loss: 0.6923 - model_201_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4472 - model_200_loss: 0.4749 - model_201_loss: 0.6923 - model_201_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4512 - model_200_loss: 0.4718 - model_201_loss: 0.6924 - model_201_1_loss: 0.6922\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4505 - model_200_loss: 0.4717 - model_201_loss: 0.6924 - model_201_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 110us/sample - loss: 6.9236 - model_201_loss: 0.6937 - model_201_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: -6.4516 - model_200_loss: 0.4728 - model_201_loss: 0.6928 - model_201_1_loss: 0.6921\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4484 - model_200_loss: 0.4731 - model_201_loss: 0.6924 - model_201_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4530 - model_200_loss: 0.4704 - model_201_loss: 0.6925 - model_201_1_loss: 0.6922\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4528 - model_200_loss: 0.4709 - model_201_loss: 0.6927 - model_201_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4536 - model_200_loss: 0.4687 - model_201_loss: 0.6925 - model_201_1_loss: 0.6919\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 105us/sample - loss: 6.9200 - model_201_loss: 0.6928 - model_201_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4479 - model_200_loss: 0.4697 - model_201_loss: 0.6920 - model_201_1_loss: 0.6915\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4491 - model_200_loss: 0.4678 - model_201_loss: 0.6920 - model_201_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4499 - model_200_loss: 0.4672 - model_201_loss: 0.6921 - model_201_1_loss: 0.6913\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4513 - model_200_loss: 0.4670 - model_201_loss: 0.6922 - model_201_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 31us/sample - loss: -6.4506 - model_200_loss: 0.4673 - model_201_loss: 0.6922 - model_201_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 122us/sample - loss: 6.9182 - model_201_loss: 0.6918 - model_201_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4521 - model_200_loss: 0.4663 - model_201_loss: 0.6923 - model_201_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4478 - model_200_loss: 0.4663 - model_201_loss: 0.6918 - model_201_1_loss: 0.6911\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4482 - model_200_loss: 0.4674 - model_201_loss: 0.6922 - model_201_1_loss: 0.6910\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4502 - model_200_loss: 0.4665 - model_201_loss: 0.6921 - model_201_1_loss: 0.6912\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4512 - model_200_loss: 0.4660 - model_201_loss: 0.6921 - model_201_1_loss: 0.6913\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 120us/sample - loss: 6.9188 - model_201_loss: 0.6918 - model_201_1_loss: 0.6912s - loss: 6.9179 - model_201_loss: 0.6910 - m\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 42us/sample - loss: -6.4512 - model_200_loss: 0.4669 - model_201_loss: 0.6923 - model_201_1_loss: 0.6913\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4509 - model_200_loss: 0.4675 - model_201_loss: 0.6924 - model_201_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4505 - model_200_loss: 0.4669 - model_201_loss: 0.6922 - model_201_1_loss: 0.6912\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4512 - model_200_loss: 0.4671 - model_201_loss: 0.6922 - model_201_1_loss: 0.6915\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4510 - model_200_loss: 0.4672 - model_201_loss: 0.6921 - model_201_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 120us/sample - loss: 6.9201 - model_201_loss: 0.6922 - model_201_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: -6.4502 - model_200_loss: 0.4669 - model_201_loss: 0.6922 - model_201_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4529 - model_200_loss: 0.4677 - model_201_loss: 0.6925 - model_201_1_loss: 0.6917\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4516 - model_200_loss: 0.4689 - model_201_loss: 0.6923 - model_201_1_loss: 0.6918\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4526 - model_200_loss: 0.4671 - model_201_loss: 0.6924 - model_201_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4523 - model_200_loss: 0.4680 - model_201_loss: 0.6922 - model_201_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 117us/sample - loss: 6.9200 - model_201_loss: 0.6916 - model_201_1_loss: 0.6918s - loss: 6.9497 - model_201_loss: 0.6983 \n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4534 - model_200_loss: 0.4681 - model_201_loss: 0.6923 - model_201_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4537 - model_200_loss: 0.4680 - model_201_loss: 0.6927 - model_201_1_loss: 0.6916\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4522 - model_200_loss: 0.4681 - model_201_loss: 0.6925 - model_201_1_loss: 0.6915\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4524 - model_200_loss: 0.4694 - model_201_loss: 0.6926 - model_201_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4510 - model_200_loss: 0.4702 - model_201_loss: 0.6926 - model_201_1_loss: 0.6917\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 120us/sample - loss: 6.9232 - model_201_loss: 0.6922 - model_201_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 39us/sample - loss: -6.4523 - model_200_loss: 0.4700 - model_201_loss: 0.6925 - model_201_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4509 - model_200_loss: 0.4694 - model_201_loss: 0.6921 - model_201_1_loss: 0.6919\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4519 - model_200_loss: 0.4719 - model_201_loss: 0.6927 - model_201_1_loss: 0.6921\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4499 - model_200_loss: 0.4726 - model_201_loss: 0.6926 - model_201_1_loss: 0.6918\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4511 - model_200_loss: 0.4723 - model_201_loss: 0.6924 - model_201_1_loss: 0.6922\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 118us/sample - loss: 6.9260 - model_201_loss: 0.6927 - model_201_1_loss: 0.6921\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4549 - model_200_loss: 0.4705 - model_201_loss: 0.6929 - model_201_1_loss: 0.6922\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4536 - model_200_loss: 0.4707 - model_201_loss: 0.6925 - model_201_1_loss: 0.6924\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4534 - model_200_loss: 0.4715 - model_201_loss: 0.6927 - model_201_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4531 - model_200_loss: 0.4723 - model_201_loss: 0.6927 - model_201_1_loss: 0.6923\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4503 - model_200_loss: 0.4728 - model_201_loss: 0.6926 - model_201_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 120us/sample - loss: 6.9241 - model_201_loss: 0.6924 - model_201_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4532 - model_200_loss: 0.4714 - model_201_loss: 0.6926 - model_201_1_loss: 0.6923\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4517 - model_200_loss: 0.4730 - model_201_loss: 0.6926 - model_201_1_loss: 0.6923\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4522 - model_200_loss: 0.4722 - model_201_loss: 0.6926 - model_201_1_loss: 0.6923\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4500 - model_200_loss: 0.4720 - model_201_loss: 0.6924 - model_201_1_loss: 0.6920\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4523 - model_200_loss: 0.4701 - model_201_loss: 0.6925 - model_201_1_loss: 0.6920\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 112us/sample - loss: 6.9258 - model_201_loss: 0.6925 - model_201_1_loss: 0.6923\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4524 - model_200_loss: 0.4704 - model_201_loss: 0.6925 - model_201_1_loss: 0.6920\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4517 - model_200_loss: 0.4709 - model_201_loss: 0.6924 - model_201_1_loss: 0.6921\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4527 - model_200_loss: 0.4686 - model_201_loss: 0.6923 - model_201_1_loss: 0.6920\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4545 - model_200_loss: 0.4686 - model_201_loss: 0.6927 - model_201_1_loss: 0.6919\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.4534 - model_200_loss: 0.4674 - model_201_loss: 0.6924 - model_201_1_loss: 0.6918\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 130us/sample - loss: 6.9222 - model_201_loss: 0.6926 - model_201_1_loss: 0.6914s - loss: 6.9250 - model_201_loss: 0.6928 - model_201_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4522 - model_200_loss: 0.4678 - model_201_loss: 0.6922 - model_201_1_loss: 0.6918\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 32us/sample - loss: -6.4519 - model_200_loss: 0.4668 - model_201_loss: 0.6923 - model_201_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 329s 13ms/sample - loss: -6.4531 - model_200_loss: 0.4670 - model_201_loss: 0.6925 - model_201_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: -6.4530 - model_200_loss: 0.4660 - model_201_loss: 0.6923 - model_201_1_loss: 0.6916\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: -6.4520 - model_200_loss: 0.4660 - model_201_loss: 0.6922 - model_201_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 139us/sample - loss: 6.9199 - model_201_loss: 0.6932 - model_201_1_loss: 0.6917s - loss: 6.9023 - model_201_loss: 0.6895 - model_201_1_loss: 0.69\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 2s 74us/sample - loss: -6.4539 - model_200_loss: 0.4645 - model_201_loss: 0.6925 - model_201_1_loss: 0.6912\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 53us/sample - loss: -6.4512 - model_200_loss: 0.4665 - model_201_loss: 0.6923 - model_201_1_loss: 0.6913\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: -6.4533 - model_200_loss: 0.4641 - model_201_loss: 0.6921 - model_201_1_loss: 0.6914\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4537 - model_200_loss: 0.4631 - model_201_loss: 0.6920 - model_201_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4526 - model_200_loss: 0.4630 - model_201_loss: 0.6920 - model_201_1_loss: 0.6912\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 5s 184us/sample - loss: 6.9185 - model_201_loss: 0.6924 - model_201_1_loss: 0.6915s - loss: 6.9242 - m - ETA: 0s - loss: 6.9179 - model_201_loss: 0.6921 - model_201_1_loss: 0\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 3s 128us/sample - loss: -6.4549 - model_200_loss: 0.4623 - model_201_loss: 0.6925 - model_201_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 2s 78us/sample - loss: -6.4510 - model_200_loss: 0.4635 - model_201_loss: 0.6917 - model_201_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 60us/sample - loss: -6.4541 - model_200_loss: 0.4616 - model_201_loss: 0.6920 - model_201_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 43us/sample - loss: -6.4518 - model_200_loss: 0.4627 - model_201_loss: 0.6919 - model_201_1_loss: 0.6910\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.4541 - model_200_loss: 0.4634 - model_201_loss: 0.6921 - model_201_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 141us/sample - loss: 6.9178 - model_201_loss: 0.6920 - model_201_1_loss: 0.6914\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 57us/sample - loss: -6.4535 - model_200_loss: 0.4611 - model_201_loss: 0.6919 - model_201_1_loss: 0.6910\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.4527 - model_200_loss: 0.4632 - model_201_loss: 0.6919 - model_201_1_loss: 0.6912\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4495 - model_200_loss: 0.4638 - model_201_loss: 0.6915 - model_201_1_loss: 0.6911\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4565 - model_200_loss: 0.4636 - model_201_loss: 0.6923 - model_201_1_loss: 0.6917\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.4509 - model_200_loss: 0.4636 - model_201_loss: 0.6918 - model_201_1_loss: 0.6911\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 140us/sample - loss: 6.9184 - model_201_loss: 0.6923 - model_201_1_loss: 0.6915\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 45us/sample - loss: -6.4505 - model_200_loss: 0.4657 - model_201_loss: 0.6919 - model_201_1_loss: 0.6914\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4517 - model_200_loss: 0.4664 - model_201_loss: 0.6921 - model_201_1_loss: 0.6915\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.4529 - model_200_loss: 0.4649 - model_201_loss: 0.6920 - model_201_1_loss: 0.6916\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4516 - model_200_loss: 0.4673 - model_201_loss: 0.6924 - model_201_1_loss: 0.6914\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.4542 - model_200_loss: 0.4659 - model_201_loss: 0.6922 - model_201_1_loss: 0.6918\n",
      "For Attention Module: 5.2\n",
      "features X: 30940 samples, 69 attributes\n",
      "targets y: 30940 samples\n",
      "sensitives Z: 30940 samples, 2 attributes\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 122us/sample - loss: 6.3716 - model_205_loss: 0.6598 - model_205_1_loss: 0.6149\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 2s 78us/sample - loss: -5.9890 - model_204_loss: 0.3864 - model_205_loss: 0.6600 - model_205_1_loss: 0.6151\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -5.9975 - model_204_loss: 0.3864 - model_205_loss: 0.6595 - model_205_1_loss: 0.6172\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.0069 - model_204_loss: 0.3866 - model_205_loss: 0.6598 - model_205_1_loss: 0.6189\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.0154 - model_204_loss: 0.3879 - model_205_loss: 0.6601 - model_205_1_loss: 0.6206\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.0234 - model_204_loss: 0.3885 - model_205_loss: 0.6610 - model_205_1_loss: 0.6214\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 132us/sample - loss: 6.4264 - model_205_loss: 0.6603 - model_205_1_loss: 0.6244\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 41us/sample - loss: -6.0257 - model_204_loss: 0.3873 - model_205_loss: 0.6605 - model_205_1_loss: 0.6221\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.0474 - model_204_loss: 0.3892 - model_205_loss: 0.6615 - model_205_1_loss: 0.6258\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.0639 - model_204_loss: 0.3875 - model_205_loss: 0.6628 - model_205_1_loss: 0.6275\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 33us/sample - loss: -6.0640 - model_204_loss: 0.3900 - model_205_loss: 0.6624 - model_205_1_loss: 0.6284\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.0674 - model_204_loss: 0.3885 - model_205_loss: 0.6621 - model_205_1_loss: 0.6291\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 140us/sample - loss: 6.4821 - model_205_loss: 0.6635 - model_205_1_loss: 0.6332\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 48us/sample - loss: -6.0857 - model_204_loss: 0.3902 - model_205_loss: 0.6638 - model_205_1_loss: 0.6314\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.1049 - model_204_loss: 0.3910 - model_205_loss: 0.6642 - model_205_1_loss: 0.6350\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 34us/sample - loss: -6.1096 - model_204_loss: 0.3919 - model_205_loss: 0.6642 - model_205_1_loss: 0.6361\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.1269 - model_204_loss: 0.3929 - model_205_loss: 0.6651 - model_205_1_loss: 0.6388\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 35us/sample - loss: -6.1427 - model_204_loss: 0.3931 - model_205_loss: 0.6658 - model_205_1_loss: 0.6414\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 3s 135us/sample - loss: 6.5327 - model_205_loss: 0.6652 - model_205_1_loss: 0.6413\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 50us/sample - loss: -6.1505 - model_204_loss: 0.3959 - model_205_loss: 0.6663 - model_205_1_loss: 0.6430\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.1640 - model_204_loss: 0.3964 - model_205_loss: 0.6676 - model_205_1_loss: 0.6444\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 47us/sample - loss: -6.1728 - model_204_loss: 0.3984 - model_205_loss: 0.6669 - model_205_1_loss: 0.6473\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: -6.1871 - model_204_loss: 0.3985 - model_205_loss: 0.6679 - model_205_1_loss: 0.6493\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.2078 - model_204_loss: 0.4015 - model_205_loss: 0.6702 - model_205_1_loss: 0.6517\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 4s 168us/sample - loss: 6.6027 - model_205_loss: 0.6685 - model_205_1_loss: 0.6520\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 2s 91us/sample - loss: -6.2085 - model_204_loss: 0.4027 - model_205_loss: 0.6711 - model_205_1_loss: 0.6511\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 58us/sample - loss: -6.2152 - model_204_loss: 0.4039 - model_205_loss: 0.6702 - model_205_1_loss: 0.6536\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.2254 - model_204_loss: 0.4068 - model_205_loss: 0.6709 - model_205_1_loss: 0.6555\n",
      "Epoch 4/5\n",
      "24752/24752 [==============================] - 1s 36us/sample - loss: -6.2433 - model_204_loss: 0.4075 - model_205_loss: 0.6731 - model_205_1_loss: 0.6570\n",
      "Epoch 5/5\n",
      "24752/24752 [==============================] - 1s 37us/sample - loss: -6.2500 - model_204_loss: 0.4088 - model_205_loss: 0.6731 - model_205_1_loss: 0.6586\n",
      "Train on 24752 samples\n",
      "24752/24752 [==============================] - 4s 156us/sample - loss: 6.6772 - model_205_loss: 0.6738 - model_205_1_loss: 0.6618s - loss: 6.6634 - model_205_loss:\n",
      "Train on 24752 samples\n",
      "Epoch 1/5\n",
      "24752/24752 [==============================] - 1s 54us/sample - loss: -6.2565 - model_204_loss: 0.4115 - model_205_loss: 0.6732 - model_205_1_loss: 0.6604\n",
      "Epoch 2/5\n",
      "24752/24752 [==============================] - 1s 40us/sample - loss: -6.2622 - model_204_loss: 0.4164 - model_205_loss: 0.6731 - model_205_1_loss: 0.6627\n",
      "Epoch 3/5\n",
      "24752/24752 [==============================] - 1s 38us/sample - loss: -6.2859 - model_204_loss: 0.4159 - model_205_loss: 0.6758 - model_205_1_loss: 0.6645\n",
      "Epoch 4/5\n"
     ]
    }
   ],
   "source": [
    "# threshold = [0.1, 0.5, 1.5, 3.1, 4.5, 5.5, 10.5]\n",
    "threshold = np.arange(0.1, 10, 0.1)\n",
    "models = []\n",
    "for i in threshold:\n",
    "        print(\"For Attention Module:\", i)\n",
    "        X, y, Z = load_ICU_data('E:/canada syntex/Github/fair_classifier_ml/data/adult.data', i)\n",
    "\n",
    "        # split into train/test set\n",
    "        X_train, X_test, y_train, y_test, Z_train, Z_test = train_test_split(X, y, Z, test_size=0.2,\n",
    "                                                                                stratify=y, random_state=7)\n",
    "\n",
    "        # standardize the data\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        scale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), columns=df.columns, index=df.index)\n",
    "        X_train = X_train.pipe(scale_df, scaler) \n",
    "        X_test = X_test.pipe(scale_df, scaler) \n",
    "\n",
    "        pathlib.Path(f'E:/canada syntex/Github/fair_classifier_ml/output').mkdir(parents=True, exist_ok=True)\n",
    "        # initialise FairClassifier\n",
    "        clf = FairClassifier(n_features=X_train.shape[1], n_sensitive=Z_train.shape[1],\n",
    "                                lambdas=[5., 5.])\n",
    "\n",
    "        # pre-train both adverserial and classifier networks\n",
    "        clf.pretrain(X_train, y_train, Z_train, verbose=0, epochs=5)\n",
    "        clf.fit(X_train, y_train, Z_train, \n",
    "                validation_data=(X_test, y_test, Z_test),\n",
    "                T_iter=50, save_figs=True, verbose=1)\n",
    "        \n",
    "        models.append(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_race = []\n",
    "p_sex = []\n",
    "for i, j in zip(models, threshold):\n",
    "    # print(\"Threshould: \",j)\n",
    "    # print(i._fairness_metrics.loc[40])\n",
    "    p_race.append(i._fairness_metrics.loc[40, 'race'])\n",
    "    p_sex.append(i._fairness_metrics.loc[40, 'sex'])\n",
    "\n",
    "print(p_race)\n",
    "print(p_sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(threshold, p_race, p_sex, fname):\n",
    "\n",
    "    x1 = np.array(threshold)\n",
    "    x2 = np.array(threshold)\n",
    "    y1 = np.array(p_race)\n",
    "    y2 = np.array(p_sex)\n",
    "\n",
    "    # smoothing graph\n",
    "    X_Y_1 = make_interp_spline(x1, y1)\n",
    "    X_Y_2 = make_interp_spline(x2, y2)\n",
    "\n",
    "    # Returns evenly spaced numbers\n",
    "    # over a specified interval.\n",
    "    X_1 = np.linspace(x1.min(), x1.max(), 500)\n",
    "    Y_1 = X_Y_1(X_1)\n",
    "\n",
    "    X_2 = np.linspace(x2.min(), x2.max(), 500)\n",
    "    Y_2 = X_Y_2(X_2)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.title(\"Training for different Attention weight\")\n",
    "\n",
    "    plt.plot(X_1, Y_1, '-', label='Race')\n",
    "    plt.plot(X_2, Y_2, '--', label='Sex')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.xlabel(\"Attention weight\")\n",
    "    plt.ylabel(\"p% rule\")\n",
    "    # plt.xticks(ticks=threshold,labels=threshold)\n",
    "    plt.savefig(fname, bbox_inches='tight',  dpi = 800)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fname = 'E:/canada syntex/Github/fair_classifier_ml/output/P_ruleVsAttention.jpg'\n",
    "plot_curve(threshold, p_race, p_sex, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# Dataset\n",
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "y = np.array([20, 30, 5, 12, 39, 48, 50, 3])\n",
    " \n",
    "X_Y_Spline = make_interp_spline(x, y)\n",
    " \n",
    "# Returns evenly spaced numbers\n",
    "# over a specified interval.\n",
    "X_ = np.linspace(x.min(), x.max(), 500)\n",
    "Y_ = X_Y_Spline(X_)\n",
    " \n",
    "# Plotting the Graph\n",
    "plt.plot(X_, Y_)\n",
    "plt.title(\"Plot Smooth Curve Using the scipy.interpolate.make_interp_spline() Class\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "images = list()\n",
    "for y in range(50):\n",
    "    images.append(imageio.imread(f'E:/canada syntex/Github/fair_classifier_ml/output/{y+1:08d}.png'))\n",
    "imageio.mimsave(\"E:/canada syntex/Github/fair_classifier_ml/output/adv.gif\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in X.iterrows():\n",
    "#     print(i, row)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
